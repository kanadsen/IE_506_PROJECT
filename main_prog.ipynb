{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "#from torcheval.metrics import R2Score # To be implemented\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "\n",
    "from aux_func import*\n",
    "from datasets.datset_process import *\n",
    "\n",
    "# Import the pretrained default model resnet18/resnet50/resnet101\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arg parser\n",
    "seed=200\n",
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "np.random.seed(200)\n",
    "torch.manual_seed(seed)\n",
    "device=input(\"Enter cuda or cpu for device type\")\n",
    "device = torch.device(device)\n",
    "#'cuda' if torch.cuda.is_available() else\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take user inputs \n",
    "args.dataset='miniimagenet'\n",
    "args.data_path=''\n",
    "args.num_classes=64 # By default for miniimagenet\n",
    "args.image_size=84\n",
    "\n",
    "# FSL definitions\n",
    "args.num_ways=5 # Number of classes per batch\n",
    "args.k_shot=5 # number of Images per class\n",
    "args.query=15 # Query set of the FSL\n",
    "args.unlabel=50 # Number of unlabel samples per class \n",
    "args.step=5 # Select how many unlabeled data for each class in one iteration.\n",
    "args.threshold=0.2  # Since we have 5 classes in each support set. So if all the classes are equally probable then mininmum p=0.2\n",
    "\n",
    "# set in semi-supervised few-shot learning\n",
    "num_support = args.k_shot * args.num_ways\n",
    "num_query = args.query * args.num_ways\n",
    "num_unlabeled = args.unlabel * args.num_ways\n",
    "\n",
    "# Training or testing definitions \n",
    "args.episodes=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sets of unlabeled data\n",
    "num_select = int(args.unlabel / args.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the resnet model and define the model to be used \n",
    "model=resnet50(args.num_classes,pretrained=False)\n",
    "\n",
    "# define last layer\n",
    "\n",
    "'''\n",
    "The input shape of last layer is found from the output shape of the resnet layer defined as \n",
    "'''\n",
    "last_layer=nn.Dense(in_channels=2048*3*3,out_channels=args.num_ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the features from the resnet model\n",
    "\n",
    "def get_features(model,input):\n",
    "    '''\n",
    "    The function first checks if the input batch size exceeds a desired batch size. If it does, the input batch is split into smaller batches of size 64, and the \n",
    "    ResNet model is called on each smaller batch using the model function with the return_feat=True argument to return the output features in addition to the classification results. \n",
    "    The output features are then detached from the computation graph, transferred to the CPU, and appended to a list embed. \n",
    "    Once all batches have been processed, the list of output features is concatenated using torch.cat to form a single tensor embed.\n",
    "    If the input batch size is less than or equal to the desired batch size, the ResNet model is called once with the input batch using the model function with the return_feat=True argument to return the output features.\n",
    "    Finally, the function checks if the shape of the output features embed matches the shape of the input batch, and returns the output features as a NumPy array using the numpy() method.\n",
    "    '''\n",
    "    batch_size = 64  # Use the desired batch size\n",
    "    # Check to prevent the input shape from exceeding the desired batch size\n",
    "    if input.shape[0] > batch_size:\n",
    "        embed = []\n",
    "        i = 0\n",
    "        while i <= input.shape[0]-1:\n",
    "            embed.append(model(input[i:i+batch_size].cuda(), return_feat=True).detach().cpu())\n",
    "            i += batch_size\n",
    "        embed = torch.cat(embed)\n",
    "    else:\n",
    "        embed = model(input.cuda(), return_feat=True).detach().cpu()\n",
    "    assert embed.shape[0] == input.shape[0] # Check if input shape = embed shape  as we will be working on input shape.\n",
    "    return embed.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataset, loss_fn, optimizer, inputs, targets):\n",
    "    # Define forward function\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label) + entropy_loss(logits) # Combination of two loss functions used here. The entropy_loss act as regularizer\n",
    "        # loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    \n",
    "    def train_step(data, label):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = forward_fn(data, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item(), logits\n",
    "    \n",
    "    model.train()\n",
    "    inputs = torch.tensor(inputs)\n",
    "    targets = torch.tensor(targets)\n",
    "    loss, logits = train_step(inputs, targets)\n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(out):\n",
    "    preds = torch.argmin(out, dim=0).item()\n",
    "    return preds, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preds_position_(unlabel_out, position, _postion, thres=0.001):\n",
    "    length = len(position)\n",
    "    r = []\n",
    "    un_idx = []\n",
    "    for idx in range(length):\n",
    "        pos = position[idx]\n",
    "        _pos = _postion[idx]\n",
    "        _out = unlabel_out[idx][pos]\n",
    "        out = F.softmax(_out,dim=0)  # Check if dim=0 or 1\n",
    "        if len(pos)==1:\n",
    "            un_idx.append(idx)\n",
    "            continue\n",
    "        conf =  torch.argmin(out).item()\n",
    "        if conf>thres:\n",
    "            un_idx.append(idx)\n",
    "            if len(_pos)==0:\n",
    "                r.append(torch.argmin(out, dim=0).item().asnumpy())  # check if asnumpy works here or not\n",
    "            else:\n",
    "                r.append(_pos[-1])\n",
    "            continue\n",
    "        t, _ = get_preds(out)\n",
    "        a = pos[t]\n",
    "        _postion[idx].append(a)\n",
    "        position[idx].remove(a)\n",
    "        r.append(a)\n",
    "    return np.asarray(r), un_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the respective loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process \n",
    "\n",
    "# in the below code I am not using query set which should be concated with unlabeled data.\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "\n",
    "        # create different sets of data from the train loader\n",
    "        data = data.to(device)\n",
    "        targets = torch.arange(args.way).repeat(args.shot+args.query+args.unlabel).long()\n",
    "    \n",
    "        support_data = data[:num_support]\n",
    "        query_data = data[num_support:num_support+num_query]\n",
    "        unlabel_data = data[num_support+num_query:]\n",
    "\n",
    "        support_inputs = normalize(get_features(model, support_data))  # get feature embeddings for \n",
    "        support_targets = targets[:num_support].cpu().numpy()\n",
    "\n",
    "        query_inputs = normalize(get_features(model, query_data))\n",
    "        query_targets = targets[num_support:num_support+num_query].cpu().numpy()\n",
    "\n",
    "        unlabel_inputs = normalize(get_features(model, unlabel_data))\n",
    "        unlabel_targets = targets[num_support+num_query:].cpu().numpy()\n",
    "\n",
    "        # The classifier has already been decided as linear classifier with a single dense layer and the output dimension=5\n",
    "\n",
    "        ori_index = [x for x in range(250)]  # Store the index position of 250 images\n",
    "        _POSITION = [[] for _ in range(250)] # Create a 2D list to store the list of 5 classes in passed along with the image batch.\n",
    "        POSITION = [[0, 1, 2, 3, 4] for _ in range(250)] # [0,1,2,3,4] was chosen for encoding the 5 classes\n",
    "        \n",
    "        # Define the loss criterion and the SGD optimizer used here for initial training of model.\n",
    "        criterion = nn.CrossEntropyLoss(sparse=True, reduction='mean')\n",
    "        optimizer = torch.optim.SGD(last_layer.trainable_params(), learning_rate = 1e-3, momentum=0.9, weight_decay=5e-4)  # weight decay is for L2 regularization.\n",
    "\n",
    "        # Begin initial training\n",
    "\n",
    "        print('\\n********************************************  Initial training the model')\n",
    "        for epoch in range(100):\n",
    "              loss=train_loop(last_layer, None, criterion, optimizer, support_inputs, support_targets)\n",
    "              print(f\"Train_Epoch: {epoch}  Train_Loss: {loss}\")\n",
    "        \n",
    "        # Start of code using complimentary labels\n",
    "        print('\\n********************************************  Training with complimentatry labels')\n",
    "        while(True):\n",
    "            select_idx=[]\n",
    "            unlabel_out = last_layer(unlabel_inputs)\n",
    "            nl_pred, unselect_idx = get_preds_position_(unlabel_out, POSITION, _POSITION, args.threshold)\n",
    "            select_idx = [x for x in ori_index if x not in unselect_idx]\n",
    "            _unlabel_embeddings = unlabel_inputs[select_idx]\n",
    "            _unlabel_t = unlabel_targets[select_idx]\n",
    "            nl_pred = nl_pred[select_idx]\n",
    "            optimizer_NL = torch.optim.SGD(last_layer.trainable_params(), learning_rate = 1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "            for epoch in range(10):\n",
    "                loss = train_loop_NL(clf, None, NL_loss, optimizer_NL, _unlabel_embeddings, nl_pred)\n",
    "                print(f\"Epoch: {epoch}  Loss: {loss}\")\n",
    "            \n",
    "            # Break condition no negative label found below threshold condition\n",
    "            print(nl_pred)\n",
    "            if(len(nl_pred)==0):\n",
    "                  break\n",
    "            class_num = [0 for _ in range(5)]\n",
    "            pseudo_label = []\n",
    "            index_pl = []\n",
    "            for idx in range(len(POSITION)):\n",
    "                item = POSITION[idx]\n",
    "                if len(item) == 1:\n",
    "                    lab = item[0]\n",
    "                    pseudo_label.append(item[-1])\n",
    "                    class_num[lab] += 1\n",
    "                    index_pl.append(idx)\n",
    "            class_num = [item + 8 for item in class_num]\n",
    "            max_ = max(class_num) * 1.0\n",
    "            pseudo_label = np.asarray(pseudo_label)\n",
    "            t1_ = unlabel_inputs[index_pl]\n",
    "            t2_ = torch.tensor(pseudo_label, dtype=torch.int64)\n",
    "            for epoch in range(100):\n",
    "                loss=train_loop(last_layer, None, criterion, optimizer, t1_, t2_)\n",
    "                print(f\"Epoch: {epoch}  Loss: {loss}\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "793_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
