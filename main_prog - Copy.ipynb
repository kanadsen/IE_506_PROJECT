{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "#from torcheval.metrics import R2Score # To be implemented\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "\n",
    "#from aux_func import*\n",
    "from datset_process import *\n",
    "from torch.utils.data import DataLoader\n",
    "from statistics import mode\n",
    "# Import the pretrained default model resnet18/resnet50/resnet101\n",
    "from torchvision.models import resnet50,resnet152,resnet101,efficientnet_v2_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entropy_loss function measures the entropy of the predicted probability distribution. \n",
    "# It aims to maximize the entropy of the output probabilities, which can encourage the network to output less confident predictions. \n",
    "# The entropy loss can be used as a regularization technique to prevent overfitting.\n",
    "\n",
    "def entropy_loss(p):\n",
    "    p = F.softmax(p, dim=1)\n",
    "    epsilon = 1e-5\n",
    "    return (-1 * torch.sum(p * torch.log(p + epsilon))) / p.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune for class exclusion with values of Q\n",
    "def NL_loss_2(f,labels, global_negative_pred):\n",
    "    Q_1 = 1 - F.softmax(f, dim=1) # softmax of f\n",
    "    return F.cross_entropy(Q_1, labels)  # ignore_index=global_negative_pred  Ignores all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tune for class exclusion with values of Q\n",
    "def NL_loss(f,labels, global_negative_pred):\n",
    "    '''\n",
    "    def th_delete(tensor, indices):\n",
    "       mask = torch.ones(tensor.numel(), dtype=torch.bool)\n",
    "       mask[indices] = False\n",
    "       print(mask)\n",
    "       return tensor[:, mask]\n",
    "   \n",
    "    f=th_delete(f,global_negative_pred)\n",
    "\n",
    "    This method of masking softmax does not work\n",
    "    '''\n",
    "    \n",
    "    Q_1 = 1 - F.softmax(f, dim=1) # softmax of f\n",
    "   \n",
    "    '''\n",
    "    for i in global_negative_pred:              # Giving NAN values\n",
    "        Q_1[0][i]=-1000\n",
    "    #print(Q_1)\n",
    "    '''\n",
    "    Q = F.softmax(Q_1, dim=1) # for calculating weights\n",
    "    weight = 1 - Q        \n",
    "    # Set the weights of indices in global_negative_pred to zero\n",
    "    #weight[:, global_negative_pred] = 0\n",
    "\n",
    "    out = weight *torch.log(Q) # weight *  Changed here to see difference\n",
    "    '''\n",
    "    for i in global_negative_pred:\n",
    "        out[0][i]=0.001                     # This increases accuracy of pseudo labels\n",
    "    #print(weight,weight.shape)\n",
    "    '''\n",
    "    #out.require_grad = False    # Changed in this code\n",
    "    weight.require_grad = False\n",
    "    return F.nll_loss(out, labels)  # ignore_index=global_negative_pred  Ignores all classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define arg parser\n",
    "seed=200\n",
    "paser = argparse.ArgumentParser() \n",
    "args = paser.parse_args(\"\")\n",
    "np.random.seed(200)\n",
    "torch.manual_seed(seed)\n",
    "device=input(\"Enter cuda or cpu for device type\")\n",
    "device = torch.device(device)\n",
    "#'cuda' if torch.cuda.is_available() else\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take user inputs \n",
    "args.dataset='miniimagenet'\n",
    "args.data_path='datasets/data/mini-imagenet/'\n",
    "args.num_classes=5 # Output dimension\n",
    "args.image_size=84\n",
    "\n",
    "# FSL definitions\n",
    "args.num_ways=5 # Number of classes per batch\n",
    "args.k_shot=5 # number of Images per class\n",
    "args.query=30 # Query set of the FSL\n",
    "args.unlabel=50 # Number of unlabel samples per class \n",
    "args.steps=5 # Select how many unlabeled data for each class in one iteration.\n",
    "args.threshold=0.2  # Since we have 5 classes in each support set. So if all the classes are equally probable then mininmum p=0.2\n",
    "\n",
    "# set in semi-supervised few-shot learning\n",
    "num_support = args.k_shot * args.num_ways\n",
    "num_query = args.query * args.num_ways\n",
    "num_unlabeled = args.unlabel * args.num_ways\n",
    "\n",
    "# Training or testing definitions \n",
    "args.episodes=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sets of unlabeled data\n",
    "num_select = int(args.unlabel / args.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_V2_L_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_L_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "), 2: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "), 3: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (23): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (24): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (25): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (26): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (27): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (28): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (29): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (30): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (31): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (32): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (33): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (34): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (35): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# Import the resnet model and define the model to be used \n",
    "model1=resnet50(num_classes=1000,pretrained=True)\n",
    "model2=efficientnet_v2_l(num_classes=1000,pretrained=True)\n",
    "model3=resnet152(num_classes=1000,pretrained=True)\n",
    "model4=resnet101(num_classes=1000,pretrained=True)\n",
    "model={}\n",
    "model[1]=model1\n",
    "model[2]=model4\n",
    "model[3]=model3\n",
    "#model[4]=model4\n",
    "#model=torch.load('Mymodel.pt')\n",
    "#model=resnet12(args.num_classes)\n",
    "for i in range(2):\n",
    "   model[i+1]=model[i+1].to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      "), 2: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      "), 3: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (23): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (24): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (25): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (26): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (27): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (28): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (29): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (30): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (31): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (32): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (33): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (34): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (35): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# Freeze the CNN layers of Resnet\n",
    "'''\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "'''\n",
    "for i in range(3):\n",
    "   model[i+1].fc=nn.Flatten()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ANN classifier\n",
    "Classifier1=nn.Sequential(nn.Linear(2048,512,bias=True),nn.ReLU(),nn.Linear(512,128,bias=True),nn.ReLU(),nn.Linear(128,5,bias=True))\n",
    "Classifier2=nn.Sequential(nn.Linear(2048,512,bias=True),nn.ReLU(),nn.Linear(512,128,bias=True),nn.ReLU(),nn.Linear(128,5,bias=True))\n",
    "Classifier3=nn.Sequential(nn.Linear(2048,512,bias=True),nn.ReLU(),nn.Linear(512,128,bias=True),nn.ReLU(),nn.Linear(128,5,bias=True))\n",
    "Cls={}\n",
    "Cls[0]=Classifier1\n",
    "Cls[1]=Classifier2\n",
    "Cls[2]=Classifier3\n",
    "for i in range(3):\n",
    "   Cls[i]=Cls[i].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the features from the resnet model\n",
    "\n",
    "def get_features(model,input):\n",
    "    '''\n",
    "    The function first checks if the input batch size exceeds a desired batch size. If it does, the input batch is split into smaller batches of size 64, and the \n",
    "    ResNet model is called on each smaller batch using the model function with the return_feat=True argument to return the output features in addition to the classification results. \n",
    "    The output features are then detached from the computation graph, transferred to the CPU, and appended to a list embed. \n",
    "    Once all batches have been processed, the list of output features is concatenated using torch.cat to form a single tensor embed.\n",
    "    If the input batch size is less than or equal to the desired batch size, the ResNet model is called once with the input batch using the model function with the return_feat=True argument to return the output features.\n",
    "    Finally, the function checks if the shape of the output features embed matches the shape of the input batch, and returns the output features as a NumPy array using the numpy() method.\n",
    "    '''\n",
    "    batch_size = 64  # Use the desired batch size\n",
    "    input = torch.tensor(input).to(device)\n",
    "    \n",
    "    # Check to prevent the input shape from exceeding the desired batch size\n",
    "    if input.shape[0] > batch_size:\n",
    "        embed = []\n",
    "        i = 0\n",
    "        while i <= input.shape[0]-1:\n",
    "            embed.append(model(input[i:i+batch_size])) # Changed. Check\n",
    "            i += batch_size\n",
    "        embed = torch.cat(embed)\n",
    "    else:\n",
    "        embed = model(input) # Changed. Check. Removed return feat\n",
    "    assert embed.shape[0] == input.shape[0] # Check if input shape = embed shape  as we will be working on input shape.\n",
    "    return embed.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model1, dataset, loss_fn, optimizer, inputs, targets,negative_pred):\n",
    "    acc =0\n",
    "    def forward_fn(data, label,neg_pr):\n",
    "        logits = model1(data)\n",
    "        logits.require_grad = False  # Changed\n",
    "        #logits=F.softmax(logits) # Changed here       \n",
    "        loss = loss_fn(logits, label,neg_pr) #+ entropy_loss(logits) # Combination of two loss functions used here. The entropy_loss act as regularizer\n",
    "        # loss = loss_fn(logits, label)\n",
    "        #print(loss)\n",
    "        return loss, logits\n",
    "    \n",
    "    def train_step(data, label,neg_pre):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = forward_fn(data, label,neg_pre)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item(), logits\n",
    "    \n",
    "    model1.train()\n",
    "    final_loss=0\n",
    "     #print(i)\n",
    "    img = torch.tensor(inputs)\n",
    "    y = torch.tensor(targets).long() # Changed\n",
    "    img=img.to(device)\n",
    "    y=y.to(device)\n",
    "    print(img.shape,y.shape)\n",
    "    loss, logits = train_step(img, y,negative_pred)\n",
    "    final_loss+=loss\n",
    "    final_loss/=len(inputs)\n",
    "    # Check accuracy\n",
    "    for i in range(len(inputs)):\n",
    "        model1.eval()\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        preds = model1(img)\n",
    "        preds = torch.argmax(preds, 1).reshape(-1)\n",
    "        y = y.reshape(-1)\n",
    "        if (preds==y):\n",
    "          acc +=1\n",
    "    acc = acc/len(inputs)*100\n",
    "    return final_loss, logits,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop2(model1, dataset, loss_fn, optimizer, inputs, targets):\n",
    "    acc =0\n",
    "    def forward_fn(data, label):\n",
    "        logits = model1(data)\n",
    "        logits.require_grad = False  # Changed\n",
    "        #logits=F.softmax(logits) # Changed here       \n",
    "        loss = loss_fn(logits, label) #+ entropy_loss(logits) # Combination of two loss functions used here. The entropy_loss act as regularizer\n",
    "        # loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    \n",
    "    def train_step(data, label):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = forward_fn(data, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item(), logits\n",
    "    \n",
    "    model1.train()\n",
    "    final_loss=0\n",
    "    for i in range(len(inputs)):\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        loss, logits = train_step(img, y)\n",
    "        final_loss+=loss\n",
    "    final_loss/=len(inputs)\n",
    "    # Check accuracy\n",
    "    \n",
    "    model1.eval()\n",
    "    # Check accuracy\n",
    "    for i in range(len(inputs)):\n",
    "        model1.eval()\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        preds = model1(img)\n",
    "        preds = torch.argmax(preds, 1).reshape(-1)\n",
    "        y = y.reshape(-1)\n",
    "        if (preds==y):\n",
    "          acc +=1\n",
    "    acc = acc/len(inputs)*100\n",
    "    return final_loss, logits,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model1,inputs, targets):\n",
    "    acc =0\n",
    "    # Check accuracy\n",
    "    for i in range(len(inputs)):\n",
    "        model1.eval()\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        preds = model1(img)\n",
    "        preds = torch.argmax(preds, 1).reshape(-1)\n",
    "        y = y.reshape(-1)\n",
    "        if (preds==y):\n",
    "          acc +=1\n",
    "    acc = acc/len(inputs)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(out):\n",
    "    preds = torch.argmin(out, dim=0).item()\n",
    "    return preds, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_preds(out,nl_pred):\n",
    "    def th_delete(tensor, indices):\n",
    "       mask = torch.ones(tensor.numel(), dtype=torch.bool)\n",
    "       mask[indices] = False\n",
    "       #print(mask)\n",
    "       return tensor[mask]\n",
    "    #print(out,out.shape)\n",
    "    new_out=th_delete(out,nl_pred)\n",
    "    index =torch.argmin(new_out).item()\n",
    "    val=new_out.view(-1)[index].item()\n",
    "    original_index=0\n",
    "    for i in range(out.shape[0]):\n",
    "        if out[i]==val:\n",
    "            original_index=i\n",
    "    print(original_index,val,out)\n",
    "    return original_index,val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_preds_position_(unlabel_out, position, _postion, thres=0.001):\\n        length = len(position)\\n        r = []\\n        un_idx = []\\n        for idx in range(length):\\n           pos = position[idx]\\n           _pos = _postion[idx]\\n           _out = unlabel_out[idx][pos]\\n           out = F.softmax(_out,dim=0)  # Check if dim=0 or 1\\n           #print(out,idx)\\n           if len(pos)==1:\\n               un_idx.append(idx)\\n               continue\\n           conf =torch.argmin(out).item()\\n           conf=out.view(-1)[conf].item()\\n           #print(\"conf\",conf)\\n           if conf>thres:\\n               un_idx.append(idx)\\n               if len(_pos)==0:\\n                   r.append(np.array(torch.argmin(out, dim=0).item()))  # check if asnumpy works here or not\\n               else:\\n                   r.append(_pos[-1])\\n               continue\\n           t, _ = get_preds(out)\\n           #print(\"Negative label :\",t)\\n           a = pos[t]\\n           _postion[idx].append(a)\\n           position[idx].remove(a)\\n           r.append(a)\\n        #print(\"New _POSITION :\", _postion)\\n        return np.asarray(r), un_idx,_postion,position\\n        '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_preds_position_(unlabel_out, position, _postion, thres=0.001):\n",
    "        length = len(position)\n",
    "        r = []\n",
    "        un_idx = []\n",
    "        for idx in range(length):\n",
    "           pos = position[idx]\n",
    "           _pos = _postion[idx]\n",
    "           _out = unlabel_out[idx][pos]\n",
    "           out = F.softmax(_out,dim=0)  # Check if dim=0 or 1\n",
    "           #print(out,idx)\n",
    "           if len(pos)==1:\n",
    "               un_idx.append(idx)\n",
    "               continue\n",
    "           conf =torch.argmin(out).item()\n",
    "           conf=out.view(-1)[conf].item()\n",
    "           #print(\"conf\",conf)\n",
    "           if conf>thres:\n",
    "               un_idx.append(idx)\n",
    "               if len(_pos)==0:\n",
    "                   r.append(np.array(torch.argmin(out, dim=0).item()))  # check if asnumpy works here or not\n",
    "               else:\n",
    "                   r.append(_pos[-1])\n",
    "               continue\n",
    "           t, _ = get_preds(out)\n",
    "           #print(\"Negative label :\",t)\n",
    "           a = pos[t]\n",
    "           _postion[idx].append(a)\n",
    "           position[idx].remove(a)\n",
    "           r.append(a)\n",
    "        #print(\"New _POSITION :\", _postion)\n",
    "        return np.asarray(r), un_idx,_postion,position\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preds_position_(unlabel_out, position, _postion, thres=0.001):\n",
    "    length = len(position)\n",
    "    r = []\n",
    "    un_idx = []\n",
    "    for idx in range(length):\n",
    "        pos = position[idx]\n",
    "        _pos = _postion[idx]\n",
    "        _out = unlabel_out[idx]  # [pos] removed\n",
    "        out = F.softmax(_out,dim=0)  # Check if dim=0 or 1 # Correct\n",
    "        out.require_grad = False  # Changed\n",
    "        #print(out)\n",
    "        nl_pred=global_nl_pred[idx] # defined later in the main loop. Taking the list of negative labels already found\n",
    "        \n",
    "        \n",
    "        # The logic is changed here for proper implementation\n",
    "        \n",
    "        if len(pos)==1:\n",
    "           un_idx.append(idx)\n",
    "           continue\n",
    "        t,conf=get_neg_preds(out,nl_pred)\n",
    "        #print(conf)\n",
    "        \n",
    "        if conf>thres:\n",
    "            un_idx.append(idx)\n",
    "            '''\n",
    "            if len(_pos)==0:\n",
    "                r.append(torch.argmin(out,dim=1).item().asnumpy())  # check if asnumpy works here or not\n",
    "            else:\n",
    "                r.append(_pos[-1])\n",
    "            '''\n",
    "            continue\n",
    "        #t, _ = get_preds(out)\n",
    "        #a = pos[t]\n",
    "        _postion[idx].append(t)\n",
    "        position[idx].remove(t)\n",
    "        #print(position)\n",
    "        global_nl_pred[idx].append(t)   # Append the nl_value index\n",
    "        r.append(t)\n",
    "    return np.asarray(r), un_idx,_postion,position  # Changed here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the respective loaders\n",
    "args.train_episodes=150\n",
    "train_dataset = DataSet( args.image_size, 'test',args.data_path)\n",
    "train_sampler = EpisodeSampler(train_dataset.label, args.train_episodes,args.num_ways, args.k_shot, args.query, args.unlabel)\n",
    "trainloader = DataLoader(train_dataset, batch_sampler=train_sampler,shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(li):\n",
    "    print(li)\n",
    "    if(li[0]!=li[1] and li[1]!=li[2] and li[0]!=li[2]):\n",
    "        return li[0]\n",
    "    return max(set(li), key = li.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 1.6117324352264404  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 1.611010694503784  Accuracy on Support set:20.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6104543542861938  Accuracy on Support set:20.0\n",
      "Train_Epoch: 3  Train_Loss: 1.6099377536773682  Accuracy on Support set:20.0\n",
      "Train_Epoch: 4  Train_Loss: 1.6094453191757203  Accuracy on Support set:20.0\n",
      "Train_Epoch: 5  Train_Loss: 1.6089776182174682  Accuracy on Support set:20.0\n",
      "Train_Epoch: 6  Train_Loss: 1.608534026145935  Accuracy on Support set:20.0\n",
      "Train_Epoch: 7  Train_Loss: 1.6081163024902343  Accuracy on Support set:20.0\n",
      "Train_Epoch: 8  Train_Loss: 1.6077124404907226  Accuracy on Support set:20.0\n",
      "Train_Epoch: 9  Train_Loss: 1.6073239040374756  Accuracy on Support set:20.0\n",
      "Train_Epoch: 10  Train_Loss: 1.6069390535354615  Accuracy on Support set:20.0\n",
      "Train_Epoch: 11  Train_Loss: 1.6065584087371827  Accuracy on Support set:20.0\n",
      "Train_Epoch: 12  Train_Loss: 1.6061833477020264  Accuracy on Support set:20.0\n",
      "Train_Epoch: 13  Train_Loss: 1.6058178663253784  Accuracy on Support set:20.0\n",
      "Train_Epoch: 14  Train_Loss: 1.605458722114563  Accuracy on Support set:20.0\n",
      "Train_Epoch: 15  Train_Loss: 1.605092878341675  Accuracy on Support set:20.0\n",
      "Train_Epoch: 16  Train_Loss: 1.6047308826446534  Accuracy on Support set:20.0\n",
      "Train_Epoch: 17  Train_Loss: 1.6043841075897216  Accuracy on Support set:20.0\n",
      "Train_Epoch: 18  Train_Loss: 1.6040211915969849  Accuracy on Support set:20.0\n",
      "Train_Epoch: 19  Train_Loss: 1.6036730337142944  Accuracy on Support set:20.0\n",
      "Train_Epoch: 20  Train_Loss: 1.60331214427948  Accuracy on Support set:20.0\n",
      "Train_Epoch: 21  Train_Loss: 1.6029492378234864  Accuracy on Support set:20.0\n",
      "Train_Epoch: 22  Train_Loss: 1.6025699043273927  Accuracy on Support set:20.0\n",
      "Train_Epoch: 23  Train_Loss: 1.6021987676620484  Accuracy on Support set:20.0\n",
      "Train_Epoch: 24  Train_Loss: 1.6018173313140869  Accuracy on Support set:20.0\n",
      "Train_Epoch: 25  Train_Loss: 1.6014375972747803  Accuracy on Support set:20.0\n",
      "Train_Epoch: 26  Train_Loss: 1.6010478258132934  Accuracy on Support set:20.0\n",
      "Train_Epoch: 27  Train_Loss: 1.6006480264663696  Accuracy on Support set:24.0\n",
      "Train_Epoch: 28  Train_Loss: 1.6002537393569947  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 29  Train_Loss: 1.5998410081863403  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 30  Train_Loss: 1.599432954788208  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 31  Train_Loss: 1.5990066814422608  Accuracy on Support set:36.0\n",
      "Train_Epoch: 32  Train_Loss: 1.5985802698135376  Accuracy on Support set:52.0\n",
      "Train_Epoch: 33  Train_Loss: 1.598130588531494  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 34  Train_Loss: 1.5976807403564453  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 35  Train_Loss: 1.5972107219696046  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 36  Train_Loss: 1.5967511653900146  Accuracy on Support set:64.0\n",
      "Train_Epoch: 37  Train_Loss: 1.5962637710571288  Accuracy on Support set:64.0\n",
      "Train_Epoch: 38  Train_Loss: 1.5957657480239869  Accuracy on Support set:64.0\n",
      "Train_Epoch: 39  Train_Loss: 1.5952590799331665  Accuracy on Support set:72.0\n",
      "Train_Epoch: 40  Train_Loss: 1.594741644859314  Accuracy on Support set:76.0\n",
      "Train_Epoch: 41  Train_Loss: 1.5941961431503295  Accuracy on Support set:80.0\n",
      "Train_Epoch: 42  Train_Loss: 1.593643102645874  Accuracy on Support set:84.0\n",
      "Train_Epoch: 43  Train_Loss: 1.593079400062561  Accuracy on Support set:92.0\n",
      "Train_Epoch: 44  Train_Loss: 1.5924939584732056  Accuracy on Support set:96.0\n",
      "Train_Epoch: 45  Train_Loss: 1.591884789466858  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 1.591272358894348  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 1.5906317710876465  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 1.5899747467041017  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 1.5892901706695557  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  50.66666666666667\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.19071196019649506 tensor([0.2034, 0.1993, 0.2107, 0.1959, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1908894181251526 tensor([0.2035, 0.1995, 0.2108, 0.1953, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19082839787006378 tensor([0.2036, 0.1992, 0.2106, 0.1958, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19083313643932343 tensor([0.2032, 0.1999, 0.2105, 0.1956, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902066469192505 tensor([0.2040, 0.1994, 0.2106, 0.1959, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19088628888130188 tensor([0.2039, 0.1991, 0.2104, 0.1957, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1909058839082718 tensor([0.2036, 0.1994, 0.2108, 0.1953, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905260682106018 tensor([0.2037, 0.1993, 0.2104, 0.1961, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.190595343708992 tensor([0.2038, 0.1992, 0.2105, 0.1960, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19094222784042358 tensor([0.2035, 0.1991, 0.2102, 0.1963, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19084960222244263 tensor([0.2032, 0.1992, 0.2104, 0.1964, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19010740518569946 tensor([0.2037, 0.1992, 0.2106, 0.1964, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19070389866828918 tensor([0.2033, 0.1992, 0.2106, 0.1962, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19076910614967346 tensor([0.2039, 0.1993, 0.2101, 0.1960, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19031654298305511 tensor([0.2035, 0.1992, 0.2103, 0.1966, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1910882145166397 tensor([0.2037, 0.1989, 0.2104, 0.1960, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19046737253665924 tensor([0.2035, 0.1990, 0.2112, 0.1959, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1908024400472641 tensor([0.2041, 0.1992, 0.2102, 0.1956, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19105157256126404 tensor([0.2031, 0.2000, 0.2103, 0.1955, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19055546820163727 tensor([0.2035, 0.1992, 0.2104, 0.1963, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19093413650989532 tensor([0.2036, 0.1994, 0.2105, 0.1956, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902395635843277 tensor([0.2040, 0.1995, 0.2106, 0.1958, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19109275937080383 tensor([0.2030, 0.1993, 0.2104, 0.1962, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19051826000213623 tensor([0.2025, 0.1996, 0.2108, 0.1965, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19049805402755737 tensor([0.2035, 0.1998, 0.2102, 0.1959, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19062131643295288 tensor([0.2036, 0.1990, 0.2102, 0.1966, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907803863286972 tensor([0.2036, 0.1993, 0.2104, 0.1959, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19071510434150696 tensor([0.2036, 0.1990, 0.2102, 0.1964, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19075505435466766 tensor([0.2029, 0.2002, 0.2104, 0.1958, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19032157957553864 tensor([0.2034, 0.1996, 0.2103, 0.1963, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19055916368961334 tensor([0.2036, 0.1995, 0.2105, 0.1959, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904422491788864 tensor([0.2039, 0.1995, 0.2103, 0.1958, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080482423305511 tensor([0.2043, 0.1990, 0.2103, 0.1957, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19044208526611328 tensor([0.2035, 0.1995, 0.2103, 0.1963, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1908089965581894 tensor([0.2030, 0.1992, 0.2105, 0.1965, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1909288465976715 tensor([0.2036, 0.1993, 0.2101, 0.1961, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19070912897586823 tensor([0.2036, 0.1998, 0.2103, 0.1956, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19059734046459198 tensor([0.2035, 0.1995, 0.2100, 0.1963, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19087623059749603 tensor([0.2035, 0.1991, 0.2104, 0.1962, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19074854254722595 tensor([0.2034, 0.1990, 0.2108, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19109168648719788 tensor([0.2038, 0.1988, 0.2104, 0.1960, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19021742045879364 tensor([0.2038, 0.1994, 0.2107, 0.1959, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19114939868450165 tensor([0.2032, 0.1993, 0.2105, 0.1959, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19071707129478455 tensor([0.2033, 0.1999, 0.2102, 0.1958, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19043873250484467 tensor([0.2042, 0.1989, 0.2105, 0.1959, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1909683793783188 tensor([0.2036, 0.1987, 0.2109, 0.1959, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905032992362976 tensor([0.2039, 0.1994, 0.2104, 0.1958, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1906537264585495 tensor([0.2036, 0.1995, 0.2104, 0.1959, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902000904083252 tensor([0.2038, 0.1991, 0.2107, 0.1963, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904359757900238 tensor([0.2035, 0.1994, 0.2104, 0.1963, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19100593030452728 tensor([0.2034, 0.1992, 0.2105, 0.1960, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905587911605835 tensor([0.2040, 0.1990, 0.2110, 0.1955, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19075192511081696 tensor([0.2036, 0.1992, 0.2106, 0.1959, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19113099575042725 tensor([0.2034, 0.1994, 0.2109, 0.1952, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903405636548996 tensor([0.2032, 0.2000, 0.2103, 0.1961, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19104528427124023 tensor([0.2034, 0.1993, 0.2102, 0.1960, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19084514677524567 tensor([0.2041, 0.1993, 0.2103, 0.1955, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1908879280090332 tensor([0.2034, 0.1993, 0.2108, 0.1956, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080470502376556 tensor([0.2038, 0.1992, 0.2105, 0.1957, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19026240706443787 tensor([0.2038, 0.1995, 0.2100, 0.1964, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19101892411708832 tensor([0.2035, 0.1990, 0.2105, 0.1960, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1908533126115799 tensor([0.2035, 0.1994, 0.2105, 0.1957, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19089840352535248 tensor([0.2036, 0.1990, 0.2108, 0.1957, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19053588807582855 tensor([0.2036, 0.1997, 0.2100, 0.1961, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19033198058605194 tensor([0.2035, 0.1996, 0.2103, 0.1962, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19066858291625977 tensor([0.2031, 0.1999, 0.2102, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19025473296642303 tensor([0.2033, 0.1996, 0.2107, 0.1961, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19065025448799133 tensor([0.2039, 0.1989, 0.2108, 0.1957, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19060634076595306 tensor([0.2034, 0.1997, 0.2104, 0.1959, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19054099917411804 tensor([0.2039, 0.1992, 0.2102, 0.1961, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19118505716323853 tensor([0.2035, 0.1994, 0.2103, 0.1957, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19033898413181305 tensor([0.2035, 0.1999, 0.2105, 0.1958, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19119040668010712 tensor([0.2034, 0.1987, 0.2110, 0.1957, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19075243175029755 tensor([0.2032, 0.1992, 0.2108, 0.1960, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19087643921375275 tensor([0.2039, 0.1993, 0.2104, 0.1956, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19064386188983917 tensor([0.2035, 0.1992, 0.2103, 0.1963, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19032002985477448 tensor([0.2037, 0.1994, 0.2107, 0.1959, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19069291651248932 tensor([0.2039, 0.1989, 0.2104, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907672882080078 tensor([0.2036, 0.1996, 0.2104, 0.1957, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19081242382526398 tensor([0.2035, 0.1993, 0.2105, 0.1958, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19105851650238037 tensor([0.2031, 0.1996, 0.2105, 0.1958, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19034424424171448 tensor([0.2038, 0.1995, 0.2103, 0.1961, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19087736308574677 tensor([0.2033, 0.1989, 0.2103, 0.1965, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903345137834549 tensor([0.2034, 0.1999, 0.2105, 0.1958, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905970275402069 tensor([0.2038, 0.1992, 0.2103, 0.1960, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19073931872844696 tensor([0.2034, 0.1994, 0.2104, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1900174766778946 tensor([0.2038, 0.1997, 0.2106, 0.1959, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1910952776670456 tensor([0.2032, 0.1991, 0.2102, 0.1964, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902787834405899 tensor([0.2031, 0.2003, 0.2102, 0.1961, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19067205488681793 tensor([0.2036, 0.1992, 0.2103, 0.1962, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19101768732070923 tensor([0.2033, 0.1992, 0.2107, 0.1957, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19035740196704865 tensor([0.2038, 0.1993, 0.2106, 0.1960, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19088980555534363 tensor([0.2036, 0.1994, 0.2098, 0.1963, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19069315493106842 tensor([0.2035, 0.1994, 0.2106, 0.1958, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1911810338497162 tensor([0.2034, 0.1990, 0.2102, 0.1962, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1911820024251938 tensor([0.2036, 0.1991, 0.2105, 0.1956, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1912396103143692 tensor([0.2038, 0.1994, 0.2105, 0.1950, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19054186344146729 tensor([0.2036, 0.1995, 0.2101, 0.1961, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19086776673793793 tensor([0.2034, 0.1996, 0.2106, 0.1955, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19075191020965576 tensor([0.2039, 0.1993, 0.2109, 0.1952, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1911262571811676 tensor([0.2035, 0.1996, 0.2100, 0.1958, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19050286710262299 tensor([0.2037, 0.1995, 0.2106, 0.1956, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080688059329987 tensor([0.2033, 0.1989, 0.2100, 0.1970, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19052529335021973 tensor([0.2031, 0.1998, 0.2108, 0.1958, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19090792536735535 tensor([0.2036, 0.1994, 0.2099, 0.1961, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19082482159137726 tensor([0.2038, 0.1992, 0.2107, 0.1955, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19059622287750244 tensor([0.2044, 0.1990, 0.2101, 0.1960, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907072514295578 tensor([0.2033, 0.1989, 0.2111, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080978631973267 tensor([0.2036, 0.1995, 0.2108, 0.1953, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19033634662628174 tensor([0.2036, 0.1997, 0.2104, 0.1959, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19097213447093964 tensor([0.2034, 0.1992, 0.2104, 0.1960, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080519676208496 tensor([0.2037, 0.1994, 0.2106, 0.1954, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19110965728759766 tensor([0.2038, 0.1991, 0.2102, 0.1958, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19074705243110657 tensor([0.2033, 0.1989, 0.2110, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1906183511018753 tensor([0.2035, 0.1997, 0.2102, 0.1960, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19077546894550323 tensor([0.2039, 0.1989, 0.2106, 0.1957, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19011449813842773 tensor([0.2038, 0.1994, 0.2106, 0.1961, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905098706483841 tensor([0.2033, 0.1994, 0.2108, 0.1960, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19081734120845795 tensor([0.2036, 0.1995, 0.2102, 0.1958, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19003279507160187 tensor([0.2036, 0.1995, 0.2106, 0.1962, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1909646838903427 tensor([0.2034, 0.1990, 0.2108, 0.1958, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19054467976093292 tensor([0.2039, 0.1994, 0.2107, 0.1954, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905837208032608 tensor([0.2036, 0.1994, 0.2101, 0.1962, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19063211977481842 tensor([0.2037, 0.1993, 0.2105, 0.1959, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1906987726688385 tensor([0.2035, 0.1995, 0.2100, 0.1964, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19072429835796356 tensor([0.2038, 0.1989, 0.2104, 0.1962, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19086867570877075 tensor([0.2037, 0.1992, 0.2106, 0.1956, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19054223597049713 tensor([0.2033, 0.1995, 0.2107, 0.1960, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907263547182083 tensor([0.2034, 0.1993, 0.2105, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905708611011505 tensor([0.2042, 0.1994, 0.2102, 0.1957, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1909521073102951 tensor([0.2032, 0.1994, 0.2102, 0.1962, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19057178497314453 tensor([0.2038, 0.1995, 0.2109, 0.1953, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19117267429828644 tensor([0.2036, 0.1989, 0.2109, 0.1954, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907622218132019 tensor([0.2033, 0.1993, 0.2107, 0.1959, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19073550403118134 tensor([0.2037, 0.1993, 0.2106, 0.1957, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907847821712494 tensor([0.2037, 0.1991, 0.2104, 0.1960, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19039630889892578 tensor([0.2035, 0.1994, 0.2107, 0.1960, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19056808948516846 tensor([0.2033, 0.1990, 0.2108, 0.1963, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19072449207305908 tensor([0.2032, 0.1999, 0.2102, 0.1959, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19062761962413788 tensor([0.2036, 0.1992, 0.2103, 0.1963, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19137945771217346 tensor([0.2033, 0.1993, 0.2104, 0.1956, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904178410768509 tensor([0.2041, 0.1986, 0.2107, 0.1961, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19073544442653656 tensor([0.2038, 0.1987, 0.2106, 0.1962, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903451830148697 tensor([0.2029, 0.2002, 0.2109, 0.1956, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19045375287532806 tensor([0.2032, 0.1996, 0.2105, 0.1963, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19082270562648773 tensor([0.2039, 0.1991, 0.2099, 0.1962, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19032402336597443 tensor([0.2040, 0.1992, 0.2106, 0.1959, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19076621532440186 tensor([0.2036, 0.1990, 0.2104, 0.1963, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19002151489257812 tensor([0.2032, 0.1999, 0.2109, 0.1960, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1913098245859146 tensor([0.2036, 0.1991, 0.2104, 0.1955, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19025814533233643 tensor([0.2043, 0.1986, 0.2104, 0.1964, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.190501868724823 tensor([0.2036, 0.1992, 0.2109, 0.1959, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19051530957221985 tensor([0.2033, 0.1996, 0.2102, 0.1964, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19079343974590302 tensor([0.2035, 0.1996, 0.2099, 0.1961, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19081228971481323 tensor([0.2037, 0.1992, 0.2104, 0.1958, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.191283717751503 tensor([0.2039, 0.1983, 0.2104, 0.1961, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1912437528371811 tensor([0.2041, 0.1993, 0.2102, 0.1952, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19088618457317352 tensor([0.2037, 0.1989, 0.2108, 0.1958, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19056935608386993 tensor([0.2033, 0.1996, 0.2102, 0.1963, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902206540107727 tensor([0.2035, 0.1995, 0.2110, 0.1957, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19108425080776215 tensor([0.2038, 0.1991, 0.2106, 0.1954, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903112232685089 tensor([0.2032, 0.1999, 0.2105, 0.1962, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19050036370754242 tensor([0.2032, 0.1992, 0.2107, 0.1964, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19078785181045532 tensor([0.2038, 0.1995, 0.2102, 0.1956, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19110505282878876 tensor([0.2037, 0.1992, 0.2100, 0.1960, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.191178098320961 tensor([0.2031, 0.1993, 0.2101, 0.1963, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19072698056697845 tensor([0.2037, 0.1990, 0.2109, 0.1956, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1906072050333023 tensor([0.2037, 0.1990, 0.2106, 0.1961, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19128911197185516 tensor([0.2034, 0.1991, 0.2105, 0.1957, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1906876116991043 tensor([0.2039, 0.1994, 0.2099, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19075509905815125 tensor([0.2031, 0.1997, 0.2100, 0.1965, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1901884526014328 tensor([0.2038, 0.2000, 0.2102, 0.1958, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907343715429306 tensor([0.2037, 0.1992, 0.2103, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19114626944065094 tensor([0.2032, 0.1996, 0.2105, 0.1956, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19033421576023102 tensor([0.2036, 0.1996, 0.2100, 0.1965, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19125477969646454 tensor([0.2030, 0.1996, 0.2100, 0.1962, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905575543642044 tensor([0.2040, 0.1993, 0.2105, 0.1957, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19072218239307404 tensor([0.2031, 0.2002, 0.2100, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904614269733429 tensor([0.2039, 0.1993, 0.2107, 0.1956, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080868363380432 tensor([0.2034, 0.1993, 0.2103, 0.1962, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1911633163690567 tensor([0.2037, 0.1986, 0.2107, 0.1958, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1901630461215973 tensor([0.2041, 0.1993, 0.2107, 0.1957, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19089771807193756 tensor([0.2035, 0.1994, 0.2101, 0.1961, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19033508002758026 tensor([0.2034, 0.1996, 0.2104, 0.1963, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19072920083999634 tensor([0.2038, 0.1990, 0.2105, 0.1959, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19133105874061584 tensor([0.2038, 0.1990, 0.2105, 0.1953, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19049808382987976 tensor([0.2042, 0.1988, 0.2112, 0.1953, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907523274421692 tensor([0.2039, 0.1994, 0.2104, 0.1955, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19077710807323456 tensor([0.2034, 0.1987, 0.2105, 0.1965, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19063439965248108 tensor([0.2032, 0.1997, 0.2105, 0.1960, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19074967503547668 tensor([0.2039, 0.1990, 0.2103, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19074133038520813 tensor([0.2044, 0.1990, 0.2102, 0.1956, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905011534690857 tensor([0.2033, 0.1995, 0.2106, 0.1961, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1905241161584854 tensor([0.2034, 0.1991, 0.2106, 0.1963, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904706507921219 tensor([0.2032, 0.1993, 0.2107, 0.1964, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19094723463058472 tensor([0.2035, 0.1991, 0.2106, 0.1959, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19039084017276764 tensor([0.2036, 0.1998, 0.2107, 0.1956, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902928352355957 tensor([0.2036, 0.1991, 0.2107, 0.1963, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18985815346240997 tensor([0.2035, 0.2002, 0.2106, 0.1959, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1911526769399643 tensor([0.2034, 0.1994, 0.2098, 0.1963, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19081942737102509 tensor([0.2033, 0.1992, 0.2101, 0.1966, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903575211763382 tensor([0.2039, 0.1994, 0.2107, 0.1957, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1906908005475998 tensor([0.2035, 0.1994, 0.2104, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19046607613563538 tensor([0.2029, 0.2001, 0.2105, 0.1960, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19030620157718658 tensor([0.2035, 0.1994, 0.2101, 0.1967, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19126829504966736 tensor([0.2037, 0.1991, 0.2104, 0.1956, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19042113423347473 tensor([0.2041, 0.1992, 0.2107, 0.1956, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19058062136173248 tensor([0.2035, 0.1995, 0.2103, 0.1961, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902666985988617 tensor([0.2032, 0.1998, 0.2104, 0.1963, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19082513451576233 tensor([0.2032, 0.1995, 0.2103, 0.1962, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19108062982559204 tensor([0.2033, 0.1997, 0.2102, 0.1957, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19068986177444458 tensor([0.2040, 0.1991, 0.2109, 0.1954, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19103547930717468 tensor([0.2037, 0.1987, 0.2106, 0.1960, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904793530702591 tensor([0.2036, 0.1994, 0.2106, 0.1959, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19041533768177032 tensor([0.2037, 0.1995, 0.2102, 0.1962, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19154776632785797 tensor([0.2034, 0.1989, 0.2104, 0.1958, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19020898640155792 tensor([0.2038, 0.1994, 0.2107, 0.1959, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19061177968978882 tensor([0.2041, 0.1984, 0.2107, 0.1962, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19088295102119446 tensor([0.2035, 0.1989, 0.2106, 0.1961, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19070929288864136 tensor([0.2033, 0.1995, 0.2102, 0.1963, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19058075547218323 tensor([0.2040, 0.1991, 0.2106, 0.1957, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19052225351333618 tensor([0.2040, 0.1993, 0.2105, 0.1957, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19050279259681702 tensor([0.2038, 0.1995, 0.2103, 0.1960, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19070926308631897 tensor([0.2035, 0.1993, 0.2104, 0.1960, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19105476140975952 tensor([0.2041, 0.1991, 0.2104, 0.1953, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1908571422100067 tensor([0.2033, 0.1990, 0.2103, 0.1966, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19119682908058167 tensor([0.2035, 0.1991, 0.2105, 0.1958, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080300629138947 tensor([0.2039, 0.1992, 0.2105, 0.1956, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907324641942978 tensor([0.2033, 0.1995, 0.2106, 0.1959, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19122664630413055 tensor([0.2033, 0.1990, 0.2102, 0.1963, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1911543905735016 tensor([0.2036, 0.1987, 0.2105, 0.1960, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19046059250831604 tensor([0.2035, 0.1994, 0.2107, 0.1959, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19080844521522522 tensor([0.2037, 0.1991, 0.2105, 0.1958, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19085316359996796 tensor([0.2032, 0.1997, 0.2102, 0.1961, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903872936964035 tensor([0.2035, 0.1995, 0.2101, 0.1964, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19131919741630554 tensor([0.2035, 0.1991, 0.2105, 0.1956, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19078822433948517 tensor([0.2037, 0.1993, 0.2106, 0.1957, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903245449066162 tensor([0.2036, 0.1993, 0.2105, 0.1962, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19073349237442017 tensor([0.2032, 0.1998, 0.2105, 0.1958, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19124741852283478 tensor([0.2034, 0.1992, 0.2106, 0.1956, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1913887858390808 tensor([0.2033, 0.1990, 0.2104, 0.1959, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19130682945251465 tensor([0.2038, 0.1993, 0.2110, 0.1946, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19086435437202454 tensor([0.2034, 0.1991, 0.2105, 0.1962, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1912044882774353 tensor([0.2035, 0.1995, 0.2100, 0.1958, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1903677135705948 tensor([0.2030, 0.1996, 0.2105, 0.1964, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19042037427425385 tensor([0.2033, 0.1992, 0.2109, 0.1962, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19031061232089996 tensor([0.2035, 0.1997, 0.2105, 0.1959, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19059345126152039 tensor([0.2036, 0.1996, 0.2104, 0.1959, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907244473695755 tensor([0.2033, 0.1997, 0.2102, 0.1961, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1910552978515625 tensor([0.2035, 0.2000, 0.2102, 0.1953, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "[[4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4]]\n",
      "[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n",
      "NL_pred of 0th iteration [[4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0051086339950561525  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005108447074890137  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0051080923080444336  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005107587814331055  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005106947422027588  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005106185436248779  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0051053147315979  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005104346752166748  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0051032910346984865  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0051021575927734375  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005100954532623291  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005099689483642578  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00509837007522583  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005097002029418945  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00509558916091919  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005094138622283936  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005092654228210449  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.0050911402702331545  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005089598655700684  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0050880351066589355  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.1972656548023224 tensor([0.2045, 0.2006, 0.2119, 0.1973, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19665835797786713 tensor([0.2046, 0.2009, 0.2119, 0.1967, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19710268080234528 tensor([0.2047, 0.2006, 0.2118, 0.1971, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19692440330982208 tensor([0.2043, 0.2012, 0.2117, 0.1969, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19715909659862518 tensor([0.2051, 0.2008, 0.2117, 0.1972, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19697347283363342 tensor([0.2051, 0.2005, 0.2116, 0.1970, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1966429501771927 tensor([0.2047, 0.2008, 0.2119, 0.1966, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19744166731834412 tensor([0.2048, 0.2006, 0.2116, 0.1974, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973276138305664 tensor([0.2049, 0.2005, 0.2117, 0.1973, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975865364074707 tensor([0.2046, 0.2004, 0.2114, 0.1976, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19773855805397034 tensor([0.2043, 0.2005, 0.2116, 0.1977, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19767822325229645 tensor([0.2048, 0.2006, 0.2118, 0.1977, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19752585887908936 tensor([0.2044, 0.2005, 0.2118, 0.1975, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19729161262512207 tensor([0.2050, 0.2006, 0.2113, 0.1973, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19795382022857666 tensor([0.2046, 0.2006, 0.2115, 0.1980, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19727584719657898 tensor([0.2048, 0.2002, 0.2115, 0.1973, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19720780849456787 tensor([0.2046, 0.2003, 0.2123, 0.1972, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19691996276378632 tensor([0.2052, 0.2006, 0.2114, 0.1969, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19683320820331573 tensor([0.2042, 0.2014, 0.2115, 0.1968, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19763025641441345 tensor([0.2047, 0.2006, 0.2115, 0.1976, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19693608582019806 tensor([0.2047, 0.2008, 0.2116, 0.1969, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19707484543323517 tensor([0.2051, 0.2008, 0.2118, 0.1971, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19747227430343628 tensor([0.2041, 0.2007, 0.2116, 0.1975, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19785906374454498 tensor([0.2037, 0.2010, 0.2119, 0.1979, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19726388156414032 tensor([0.2046, 0.2012, 0.2114, 0.1973, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19790080189704895 tensor([0.2047, 0.2004, 0.2114, 0.1979, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972118467092514 tensor([0.2047, 0.2007, 0.2116, 0.1972, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19773708283901215 tensor([0.2047, 0.2004, 0.2114, 0.1977, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19712619483470917 tensor([0.2041, 0.2015, 0.2115, 0.1971, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19762887060642242 tensor([0.2045, 0.2010, 0.2115, 0.1976, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19718974828720093 tensor([0.2047, 0.2008, 0.2117, 0.1972, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19713370501995087 tensor([0.2050, 0.2009, 0.2115, 0.1971, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.196975439786911 tensor([0.2054, 0.2003, 0.2114, 0.1970, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19766788184642792 tensor([0.2046, 0.2008, 0.2115, 0.1977, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19785186648368835 tensor([0.2041, 0.2006, 0.2117, 0.1979, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19742056727409363 tensor([0.2047, 0.2006, 0.2113, 0.1974, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19695088267326355 tensor([0.2047, 0.2011, 0.2114, 0.1970, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.197614848613739 tensor([0.2046, 0.2009, 0.2112, 0.1976, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974766105413437 tensor([0.2046, 0.2004, 0.2116, 0.1975, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19738908112049103 tensor([0.2046, 0.2003, 0.2120, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19725841283798218 tensor([0.2049, 0.2002, 0.2116, 0.1973, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19718918204307556 tensor([0.2049, 0.2008, 0.2119, 0.1972, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19718295335769653 tensor([0.2043, 0.2006, 0.2117, 0.1972, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19715887308120728 tensor([0.2044, 0.2013, 0.2114, 0.1972, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19723980128765106 tensor([0.2054, 0.2002, 0.2117, 0.1972, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971798986196518 tensor([0.2048, 0.2000, 0.2121, 0.1972, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970943957567215 tensor([0.2051, 0.2007, 0.2116, 0.1971, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19723494350910187 tensor([0.2047, 0.2009, 0.2115, 0.1972, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976046860218048 tensor([0.2049, 0.2004, 0.2118, 0.1976, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19758178293704987 tensor([0.2046, 0.2008, 0.2116, 0.1976, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19730311632156372 tensor([0.2045, 0.2005, 0.2116, 0.1973, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19677619636058807 tensor([0.2051, 0.2004, 0.2121, 0.1968, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971803605556488 tensor([0.2047, 0.2006, 0.2118, 0.1972, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1965625286102295 tensor([0.2045, 0.2007, 0.2121, 0.1966, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19737271964550018 tensor([0.2043, 0.2014, 0.2115, 0.1974, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19732733070850372 tensor([0.2045, 0.2007, 0.2114, 0.1973, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19682322442531586 tensor([0.2052, 0.2007, 0.2115, 0.1968, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19687405228614807 tensor([0.2045, 0.2007, 0.2120, 0.1969, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19706453382968903 tensor([0.2049, 0.2005, 0.2116, 0.1971, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19775067269802094 tensor([0.2049, 0.2008, 0.2112, 0.1978, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19726839661598206 tensor([0.2046, 0.2004, 0.2117, 0.1973, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970013976097107 tensor([0.2046, 0.2008, 0.2117, 0.1970, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19703221321105957 tensor([0.2047, 0.2003, 0.2120, 0.1970, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19743266701698303 tensor([0.2047, 0.2011, 0.2112, 0.1974, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975400149822235 tensor([0.2046, 0.2010, 0.2114, 0.1975, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19739790260791779 tensor([0.2042, 0.2012, 0.2114, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19739475846290588 tensor([0.2044, 0.2010, 0.2119, 0.1974, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19704623520374298 tensor([0.2050, 0.2003, 0.2120, 0.1970, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971876621246338 tensor([0.2044, 0.2011, 0.2116, 0.1972, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19746719300746918 tensor([0.2050, 0.2005, 0.2114, 0.1975, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19696643948554993 tensor([0.2046, 0.2007, 0.2115, 0.1970, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971094310283661 tensor([0.2046, 0.2012, 0.2117, 0.1971, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19702212512493134 tensor([0.2045, 0.2001, 0.2122, 0.1970, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19733339548110962 tensor([0.2043, 0.2005, 0.2120, 0.1973, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19690150022506714 tensor([0.2050, 0.2006, 0.2116, 0.1969, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976359337568283 tensor([0.2046, 0.2005, 0.2115, 0.1976, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719424843788147 tensor([0.2048, 0.2008, 0.2118, 0.1972, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19743812084197998 tensor([0.2050, 0.2003, 0.2115, 0.1974, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19699978828430176 tensor([0.2047, 0.2009, 0.2115, 0.1970, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971323937177658 tensor([0.2046, 0.2007, 0.2117, 0.1971, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19713552296161652 tensor([0.2042, 0.2009, 0.2116, 0.1971, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974489837884903 tensor([0.2049, 0.2008, 0.2115, 0.1974, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19786466658115387 tensor([0.2044, 0.2003, 0.2115, 0.1979, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719110429286957 tensor([0.2045, 0.2013, 0.2117, 0.1972, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19733262062072754 tensor([0.2049, 0.2006, 0.2115, 0.1973, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19733589887619019 tensor([0.2046, 0.2008, 0.2116, 0.1973, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719627499580383 tensor([0.2049, 0.2011, 0.2118, 0.1972, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19766932725906372 tensor([0.2044, 0.2005, 0.2114, 0.1977, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19743742048740387 tensor([0.2042, 0.2016, 0.2114, 0.1974, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19751107692718506 tensor([0.2047, 0.2006, 0.2115, 0.1975, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970406472682953 tensor([0.2044, 0.2006, 0.2119, 0.1970, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19734670221805573 tensor([0.2049, 0.2006, 0.2118, 0.1973, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19764624536037445 tensor([0.2046, 0.2008, 0.2110, 0.1976, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19710204005241394 tensor([0.2046, 0.2008, 0.2118, 0.1971, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975613385438919 tensor([0.2045, 0.2004, 0.2114, 0.1976, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19692613184452057 tensor([0.2047, 0.2005, 0.2117, 0.1969, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1962781846523285 tensor([0.2049, 0.2008, 0.2117, 0.1963, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19746850430965424 tensor([0.2047, 0.2009, 0.2113, 0.1975, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19685062766075134 tensor([0.2045, 0.2010, 0.2118, 0.1969, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19652913510799408 tensor([0.2050, 0.2006, 0.2121, 0.1965, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19711175560951233 tensor([0.2046, 0.2009, 0.2112, 0.1971, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19696775078773499 tensor([0.2049, 0.2008, 0.2118, 0.1970, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19830332696437836 tensor([0.2044, 0.2003, 0.2112, 0.1983, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19706541299819946 tensor([0.2042, 0.2012, 0.2120, 0.1971, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19745807349681854 tensor([0.2047, 0.2008, 0.2111, 0.1975, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1968294233083725 tensor([0.2049, 0.2006, 0.2119, 0.1968, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972702145576477 tensor([0.2055, 0.2003, 0.2112, 0.1973, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19734978675842285 tensor([0.2044, 0.2003, 0.2122, 0.1973, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19663992524147034 tensor([0.2047, 0.2008, 0.2120, 0.1966, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972620040178299 tensor([0.2047, 0.2011, 0.2116, 0.1973, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19727128744125366 tensor([0.2045, 0.2006, 0.2116, 0.1973, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19674557447433472 tensor([0.2048, 0.2008, 0.2118, 0.1967, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970878392457962 tensor([0.2049, 0.2004, 0.2114, 0.1971, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973249763250351 tensor([0.2045, 0.2003, 0.2121, 0.1973, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19731350243091583 tensor([0.2046, 0.2011, 0.2114, 0.1973, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970631629228592 tensor([0.2051, 0.2003, 0.2118, 0.1971, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19739092886447906 tensor([0.2049, 0.2008, 0.2117, 0.1974, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19732257723808289 tensor([0.2044, 0.2008, 0.2119, 0.1973, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19709429144859314 tensor([0.2048, 0.2009, 0.2114, 0.1971, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19754882156848907 tensor([0.2047, 0.2009, 0.2117, 0.1975, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.197107195854187 tensor([0.2045, 0.2004, 0.2120, 0.1971, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1967213898897171 tensor([0.2050, 0.2008, 0.2119, 0.1967, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975671499967575 tensor([0.2047, 0.2008, 0.2113, 0.1976, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724370539188385 tensor([0.2048, 0.2007, 0.2116, 0.1972, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19769717752933502 tensor([0.2046, 0.2009, 0.2112, 0.1977, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19753797352313995 tensor([0.2048, 0.2002, 0.2116, 0.1975, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19696694612503052 tensor([0.2048, 0.2006, 0.2117, 0.1970, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19726164638996124 tensor([0.2044, 0.2009, 0.2119, 0.1973, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974426656961441 tensor([0.2045, 0.2007, 0.2117, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19698308408260345 tensor([0.2053, 0.2007, 0.2114, 0.1970, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19756881892681122 tensor([0.2043, 0.2007, 0.2114, 0.1976, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19661666452884674 tensor([0.2049, 0.2008, 0.2121, 0.1966, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19674132764339447 tensor([0.2047, 0.2002, 0.2121, 0.1967, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719277322292328 tensor([0.2044, 0.2006, 0.2119, 0.1972, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970289647579193 tensor([0.2048, 0.2007, 0.2118, 0.1970, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19727511703968048 tensor([0.2048, 0.2005, 0.2116, 0.1973, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19731153547763824 tensor([0.2046, 0.2008, 0.2119, 0.1973, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19760066270828247 tensor([0.2044, 0.2004, 0.2120, 0.1976, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724810123443604 tensor([0.2043, 0.2013, 0.2114, 0.1972, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19755344092845917 tensor([0.2047, 0.2006, 0.2114, 0.1976, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969403475522995 tensor([0.2044, 0.2007, 0.2116, 0.1969, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19740568101406097 tensor([0.2053, 0.2000, 0.2119, 0.1974, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975782811641693 tensor([0.2049, 0.2000, 0.2117, 0.1976, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969549059867859 tensor([0.2041, 0.2016, 0.2120, 0.1970, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976076364517212 tensor([0.2043, 0.2009, 0.2117, 0.1976, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19752874970436096 tensor([0.2050, 0.2005, 0.2111, 0.1975, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971757411956787 tensor([0.2051, 0.2006, 0.2118, 0.1972, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19760631024837494 tensor([0.2047, 0.2003, 0.2115, 0.1976, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972857266664505 tensor([0.2043, 0.2013, 0.2121, 0.1973, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19684334099292755 tensor([0.2047, 0.2005, 0.2116, 0.1968, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977289915084839 tensor([0.2054, 0.2000, 0.2116, 0.1977, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19717256724834442 tensor([0.2046, 0.2005, 0.2121, 0.1972, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19770225882530212 tensor([0.2044, 0.2010, 0.2113, 0.1977, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19746586680412292 tensor([0.2046, 0.2010, 0.2111, 0.1975, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971510946750641 tensor([0.2048, 0.2006, 0.2116, 0.1972, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974358707666397 tensor([0.2050, 0.1997, 0.2116, 0.1974, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1965053528547287 tensor([0.2052, 0.2007, 0.2113, 0.1965, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19708473980426788 tensor([0.2048, 0.2002, 0.2119, 0.1971, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19766730070114136 tensor([0.2044, 0.2010, 0.2114, 0.1977, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19706189632415771 tensor([0.2046, 0.2009, 0.2122, 0.1971, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1967221349477768 tensor([0.2049, 0.2005, 0.2118, 0.1967, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19749172031879425 tensor([0.2043, 0.2012, 0.2117, 0.1975, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19777223467826843 tensor([0.2043, 0.2006, 0.2119, 0.1978, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969127655029297 tensor([0.2050, 0.2009, 0.2114, 0.1969, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19729027152061462 tensor([0.2048, 0.2006, 0.2112, 0.1973, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975991278886795 tensor([0.2043, 0.2007, 0.2112, 0.1976, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19695521891117096 tensor([0.2049, 0.2004, 0.2120, 0.1970, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19745007157325745 tensor([0.2048, 0.2003, 0.2118, 0.1975, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19703169167041779 tensor([0.2045, 0.2004, 0.2117, 0.1970, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973409205675125 tensor([0.2051, 0.2008, 0.2111, 0.1973, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19780947268009186 tensor([0.2042, 0.2010, 0.2112, 0.1978, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19713756442070007 tensor([0.2049, 0.2014, 0.2114, 0.1971, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19742493331432343 tensor([0.2048, 0.2006, 0.2115, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19687700271606445 tensor([0.2043, 0.2009, 0.2117, 0.1969, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19777537882328033 tensor([0.2047, 0.2010, 0.2112, 0.1978, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19746899604797363 tensor([0.2041, 0.2010, 0.2112, 0.1975, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19701167941093445 tensor([0.2051, 0.2007, 0.2116, 0.1970, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19733822345733643 tensor([0.2042, 0.2015, 0.2112, 0.1973, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19693365693092346 tensor([0.2050, 0.2007, 0.2119, 0.1969, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19752681255340576 tensor([0.2045, 0.2007, 0.2115, 0.1975, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971314549446106 tensor([0.2048, 0.2000, 0.2118, 0.1971, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19697615504264832 tensor([0.2052, 0.2007, 0.2119, 0.1970, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19739438593387604 tensor([0.2047, 0.2007, 0.2113, 0.1974, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19767986238002777 tensor([0.2045, 0.2009, 0.2115, 0.1977, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971924602985382 tensor([0.2049, 0.2004, 0.2117, 0.1972, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1966419816017151 tensor([0.2050, 0.2004, 0.2117, 0.1966, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19663678109645844 tensor([0.2053, 0.2002, 0.2124, 0.1966, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1968681663274765 tensor([0.2050, 0.2008, 0.2116, 0.1969, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1978456676006317 tensor([0.2045, 0.2001, 0.2117, 0.1978, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19729892909526825 tensor([0.2043, 0.2010, 0.2117, 0.1973, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19740363955497742 tensor([0.2050, 0.2004, 0.2115, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19688661396503448 tensor([0.2055, 0.2004, 0.2114, 0.1969, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974361687898636 tensor([0.2044, 0.2008, 0.2118, 0.1974, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19764818251132965 tensor([0.2045, 0.2005, 0.2118, 0.1976, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19772900640964508 tensor([0.2043, 0.2006, 0.2118, 0.1977, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719761610031128 tensor([0.2046, 0.2004, 0.2118, 0.1972, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19687554240226746 tensor([0.2047, 0.2012, 0.2119, 0.1969, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19759970903396606 tensor([0.2047, 0.2004, 0.2119, 0.1976, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972014605998993 tensor([0.2046, 0.2015, 0.2118, 0.1972, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19762028753757477 tensor([0.2045, 0.2008, 0.2109, 0.1976, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19785557687282562 tensor([0.2045, 0.2006, 0.2112, 0.1979, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969911903142929 tensor([0.2050, 0.2007, 0.2119, 0.1970, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973533034324646 tensor([0.2046, 0.2007, 0.2116, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19736361503601074 tensor([0.2040, 0.2015, 0.2116, 0.1974, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19796711206436157 tensor([0.2046, 0.2008, 0.2113, 0.1980, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19691051542758942 tensor([0.2048, 0.2004, 0.2116, 0.1969, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1968662291765213 tensor([0.2052, 0.2006, 0.2119, 0.1969, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19745366275310516 tensor([0.2046, 0.2009, 0.2115, 0.1975, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19761142134666443 tensor([0.2043, 0.2012, 0.2116, 0.1976, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19752921164035797 tensor([0.2043, 0.2009, 0.2114, 0.1975, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19704748690128326 tensor([0.2044, 0.2010, 0.2114, 0.1970, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1966862976551056 tensor([0.2051, 0.2005, 0.2120, 0.1967, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972680240869522 tensor([0.2048, 0.2001, 0.2118, 0.1973, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19723941385746002 tensor([0.2047, 0.2008, 0.2118, 0.1972, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19748353958129883 tensor([0.2048, 0.2009, 0.2114, 0.1975, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19708625972270966 tensor([0.2045, 0.2003, 0.2116, 0.1971, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971583515405655 tensor([0.2049, 0.2008, 0.2119, 0.1972, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19747094810009003 tensor([0.2052, 0.1998, 0.2119, 0.1975, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19738823175430298 tensor([0.2046, 0.2003, 0.2118, 0.1974, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19757425785064697 tensor([0.2045, 0.2008, 0.2114, 0.1976, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970086544752121 tensor([0.2051, 0.2005, 0.2118, 0.1970, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19702889025211334 tensor([0.2051, 0.2007, 0.2116, 0.1970, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724200665950775 tensor([0.2049, 0.2009, 0.2115, 0.1972, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19735969603061676 tensor([0.2046, 0.2007, 0.2116, 0.1974, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1966143101453781 tensor([0.2052, 0.2005, 0.2116, 0.1966, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19790047407150269 tensor([0.2044, 0.2004, 0.2114, 0.1979, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19708719849586487 tensor([0.2046, 0.2004, 0.2117, 0.1971, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19693896174430847 tensor([0.2050, 0.2006, 0.2117, 0.1969, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19721700251102448 tensor([0.2044, 0.2009, 0.2118, 0.1972, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19763882458209991 tensor([0.2044, 0.2004, 0.2114, 0.1976, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973736435174942 tensor([0.2047, 0.2001, 0.2117, 0.1974, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19716989994049072 tensor([0.2047, 0.2008, 0.2119, 0.1972, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971353143453598 tensor([0.2049, 0.2005, 0.2117, 0.1971, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19740130007266998 tensor([0.2043, 0.2010, 0.2114, 0.1974, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19773390889167786 tensor([0.2046, 0.2009, 0.2113, 0.1977, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19697076082229614 tensor([0.2046, 0.2004, 0.2116, 0.1970, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19700896739959717 tensor([0.2048, 0.2006, 0.2117, 0.1970, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19757068157196045 tensor([0.2047, 0.2007, 0.2116, 0.1976, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19714532792568207 tensor([0.2043, 0.2012, 0.2116, 0.1971, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969117671251297 tensor([0.2045, 0.2006, 0.2118, 0.1969, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19717498123645782 tensor([0.2044, 0.2004, 0.2116, 0.1972, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1959053874015808 tensor([0.2049, 0.2007, 0.2121, 0.1959, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974835842847824 tensor([0.2045, 0.2004, 0.2117, 0.1975, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.197099506855011 tensor([0.2046, 0.2009, 0.2112, 0.1971, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977517455816269 tensor([0.2041, 0.2010, 0.2117, 0.1978, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975177675485611 tensor([0.2044, 0.2005, 0.2121, 0.1975, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19725558161735535 tensor([0.2046, 0.2011, 0.2117, 0.1973, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971675157546997 tensor([0.2047, 0.2010, 0.2115, 0.1972, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974591165781021 tensor([0.2044, 0.2011, 0.2113, 0.1975, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19655351340770721 tensor([0.2046, 0.2013, 0.2114, 0.1966, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 1th iteration [[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0051380615234375  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005137857437133789  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005137467861175537  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005136914730072021  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0051362113952636715  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.0051353759765625  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005134420871734619  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0051333589553833004  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005132201194763183  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005130958557128906  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005129640102386475  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005128254890441894  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005126807689666748  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005125308513641357  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005123761653900147  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005122172832489014  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005120548248291015  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.0051188902854919435  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0051172046661376955  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00511549425125122  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20185568928718567 tensor([0.2059, 0.2019, 0.2134, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020895928144455 tensor([0.2060, 0.2021, 0.2134, 0.1912, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20178130269050598 tensor([0.2061, 0.2018, 0.2133, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20248295366764069 tensor([0.2057, 0.2025, 0.2132, 0.1915, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20196786522865295 tensor([0.2065, 0.2020, 0.2132, 0.1918, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20169490575790405 tensor([0.2065, 0.2017, 0.2131, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019672393798828 tensor([0.2061, 0.2020, 0.2134, 0.1912, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20179909467697144 tensor([0.2062, 0.2018, 0.2131, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20175249874591827 tensor([0.2063, 0.2018, 0.2131, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20166334509849548 tensor([0.2061, 0.2017, 0.2128, 0.1921, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20172694325447083 tensor([0.2057, 0.2017, 0.2130, 0.1923, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017885148525238 tensor([0.2063, 0.2018, 0.2132, 0.1923, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017638385295868 tensor([0.2059, 0.2018, 0.2132, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018425017595291 tensor([0.2064, 0.2018, 0.2128, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018110156059265 tensor([0.2060, 0.2018, 0.2130, 0.1925, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20144401490688324 tensor([0.2063, 0.2014, 0.2130, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015414535999298 tensor([0.2061, 0.2015, 0.2138, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018028050661087 tensor([0.2067, 0.2018, 0.2129, 0.1915, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20261713862419128 tensor([0.2057, 0.2026, 0.2129, 0.1914, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20180878043174744 tensor([0.2061, 0.2018, 0.2130, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20197103917598724 tensor([0.2061, 0.2020, 0.2131, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020256221294403 tensor([0.2065, 0.2020, 0.2132, 0.1917, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20193061232566833 tensor([0.2056, 0.2019, 0.2131, 0.1920, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20220060646533966 tensor([0.2051, 0.2022, 0.2134, 0.1924, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20246043801307678 tensor([0.2061, 0.2025, 0.2128, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20160982012748718 tensor([0.2062, 0.2016, 0.2129, 0.1924, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190154016017914 tensor([0.2061, 0.2019, 0.2131, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20160189270973206 tensor([0.2062, 0.2016, 0.2129, 0.1923, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20274056494235992 tensor([0.2055, 0.2027, 0.2130, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20220528542995453 tensor([0.2060, 0.2022, 0.2130, 0.1922, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20202672481536865 tensor([0.2061, 0.2020, 0.2132, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20209458470344543 tensor([0.2065, 0.2021, 0.2130, 0.1917, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20156411826610565 tensor([0.2068, 0.2016, 0.2129, 0.1916, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20203739404678345 tensor([0.2060, 0.2020, 0.2130, 0.1922, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017907351255417 tensor([0.2055, 0.2018, 0.2132, 0.1924, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20186148583889008 tensor([0.2062, 0.2019, 0.2128, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20234796404838562 tensor([0.2062, 0.2023, 0.2129, 0.1915, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20214135944843292 tensor([0.2061, 0.2021, 0.2127, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20167241990566254 tensor([0.2061, 0.2017, 0.2131, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.201537624001503 tensor([0.2060, 0.2015, 0.2134, 0.1920, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20141522586345673 tensor([0.2063, 0.2014, 0.2131, 0.1918, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019970864057541 tensor([0.2063, 0.2020, 0.2134, 0.1918, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20185497403144836 tensor([0.2058, 0.2019, 0.2132, 0.1917, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2025272101163864 tensor([0.2058, 0.2025, 0.2129, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20147362351417542 tensor([0.2068, 0.2015, 0.2132, 0.1918, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20119614899158478 tensor([0.2062, 0.2012, 0.2135, 0.1917, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20194120705127716 tensor([0.2065, 0.2019, 0.2130, 0.1917, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20209676027297974 tensor([0.2061, 0.2021, 0.2130, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20165643095970154 tensor([0.2063, 0.2017, 0.2133, 0.1921, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202023446559906 tensor([0.2060, 0.2020, 0.2130, 0.1921, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20174573361873627 tensor([0.2059, 0.2017, 0.2131, 0.1919, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20162515342235565 tensor([0.2065, 0.2016, 0.2136, 0.1914, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017868608236313 tensor([0.2061, 0.2018, 0.2133, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20196235179901123 tensor([0.2059, 0.2020, 0.2135, 0.1912, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20265300571918488 tensor([0.2058, 0.2027, 0.2130, 0.1919, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018824964761734 tensor([0.2060, 0.2019, 0.2129, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20188650488853455 tensor([0.2066, 0.2019, 0.2129, 0.1914, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20189087092876434 tensor([0.2059, 0.2019, 0.2135, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20175164937973022 tensor([0.2064, 0.2018, 0.2131, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20205068588256836 tensor([0.2064, 0.2021, 0.2127, 0.1923, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20158016681671143 tensor([0.2060, 0.2016, 0.2132, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200398564338684 tensor([0.2061, 0.2020, 0.2132, 0.1916, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015152871608734 tensor([0.2061, 0.2015, 0.2135, 0.1916, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2023376226425171 tensor([0.2061, 0.2023, 0.2127, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20223505795001984 tensor([0.2061, 0.2022, 0.2129, 0.1922, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20243996381759644 tensor([0.2057, 0.2024, 0.2129, 0.1920, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20222871005535126 tensor([0.2059, 0.2022, 0.2134, 0.1920, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20151206851005554 tensor([0.2064, 0.2015, 0.2135, 0.1916, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20232924818992615 tensor([0.2059, 0.2023, 0.2131, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20172981917858124 tensor([0.2065, 0.2017, 0.2129, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20195016264915466 tensor([0.2061, 0.2020, 0.2129, 0.1916, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2023928016424179 tensor([0.2061, 0.2024, 0.2131, 0.1917, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2012845277786255 tensor([0.2060, 0.2013, 0.2137, 0.1916, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20175863802433014 tensor([0.2058, 0.2018, 0.2135, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20181803405284882 tensor([0.2064, 0.2018, 0.2131, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017681896686554 tensor([0.2061, 0.2018, 0.2130, 0.1922, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20197932422161102 tensor([0.2063, 0.2020, 0.2133, 0.1918, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20148588716983795 tensor([0.2064, 0.2015, 0.2130, 0.1920, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20215332508087158 tensor([0.2062, 0.2022, 0.2130, 0.1916, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.201911062002182 tensor([0.2061, 0.2019, 0.2132, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20213550329208374 tensor([0.2056, 0.2021, 0.2131, 0.1917, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20205311477184296 tensor([0.2063, 0.2021, 0.2130, 0.1920, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20154203474521637 tensor([0.2059, 0.2015, 0.2130, 0.1924, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20248691737651825 tensor([0.2059, 0.2025, 0.2131, 0.1918, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017885148525238 tensor([0.2064, 0.2018, 0.2130, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20199480652809143 tensor([0.2060, 0.2020, 0.2131, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20229001343250275 tensor([0.2064, 0.2023, 0.2133, 0.1917, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20171846449375153 tensor([0.2058, 0.2017, 0.2128, 0.1923, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20285369455814362 tensor([0.2057, 0.2029, 0.2128, 0.1920, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018287479877472 tensor([0.2061, 0.2018, 0.2130, 0.1921, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20181584358215332 tensor([0.2058, 0.2018, 0.2134, 0.1916, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018335461616516 tensor([0.2063, 0.2018, 0.2132, 0.1919, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200030505657196 tensor([0.2061, 0.2020, 0.2125, 0.1922, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019759863615036 tensor([0.2061, 0.2020, 0.2133, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20159529149532318 tensor([0.2059, 0.2016, 0.2129, 0.1921, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017202526330948 tensor([0.2061, 0.2017, 0.2132, 0.1915, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200784504413605 tensor([0.2064, 0.2020, 0.2132, 0.1909, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20214714109897614 tensor([0.2062, 0.2021, 0.2128, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20221418142318726 tensor([0.2059, 0.2022, 0.2132, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20179224014282227 tensor([0.2064, 0.2018, 0.2135, 0.1911, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2021576464176178 tensor([0.2060, 0.2022, 0.2127, 0.1917, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20206575095653534 tensor([0.2063, 0.2021, 0.2132, 0.1915, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20151062309741974 tensor([0.2058, 0.2015, 0.2127, 0.1928, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202384352684021 tensor([0.2057, 0.2024, 0.2135, 0.1916, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019713819026947 tensor([0.2062, 0.2020, 0.2126, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20180055499076843 tensor([0.2063, 0.2018, 0.2134, 0.1914, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015713006258011 tensor([0.2069, 0.2016, 0.2127, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015165537595749 tensor([0.2058, 0.2015, 0.2137, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204561948776245 tensor([0.2061, 0.2020, 0.2135, 0.1912, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022777497768402 tensor([0.2061, 0.2023, 0.2131, 0.1919, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20181219279766083 tensor([0.2059, 0.2018, 0.2131, 0.1919, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200258493423462 tensor([0.2063, 0.2020, 0.2132, 0.1913, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20162051916122437 tensor([0.2064, 0.2016, 0.2129, 0.1917, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20151330530643463 tensor([0.2059, 0.2015, 0.2136, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20232738554477692 tensor([0.2060, 0.2023, 0.2128, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20151862502098083 tensor([0.2065, 0.2015, 0.2132, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204642415046692 tensor([0.2063, 0.2020, 0.2132, 0.1920, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20203152298927307 tensor([0.2058, 0.2020, 0.2134, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020905464887619 tensor([0.2062, 0.2021, 0.2129, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20210134983062744 tensor([0.2061, 0.2021, 0.2132, 0.1922, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20157547295093536 tensor([0.2060, 0.2016, 0.2135, 0.1917, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020033299922943 tensor([0.2064, 0.2020, 0.2134, 0.1913, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20198874175548553 tensor([0.2062, 0.2020, 0.2127, 0.1921, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20191815495491028 tensor([0.2062, 0.2019, 0.2131, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20208220183849335 tensor([0.2060, 0.2021, 0.2127, 0.1922, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20143969357013702 tensor([0.2063, 0.2014, 0.2131, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20177745819091797 tensor([0.2063, 0.2018, 0.2132, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2021048218011856 tensor([0.2058, 0.2021, 0.2134, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190569758415222 tensor([0.2059, 0.2019, 0.2131, 0.1920, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019309103488922 tensor([0.2067, 0.2019, 0.2129, 0.1916, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20197080075740814 tensor([0.2057, 0.2020, 0.2128, 0.1921, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204539597034454 tensor([0.2063, 0.2020, 0.2135, 0.1912, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2014438807964325 tensor([0.2062, 0.2014, 0.2136, 0.1913, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018706351518631 tensor([0.2059, 0.2019, 0.2134, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018837332725525 tensor([0.2062, 0.2019, 0.2132, 0.1916, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016901671886444 tensor([0.2063, 0.2017, 0.2131, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20199954509735107 tensor([0.2060, 0.2020, 0.2134, 0.1919, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20159749686717987 tensor([0.2058, 0.2016, 0.2135, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20251603424549103 tensor([0.2058, 0.2025, 0.2129, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20183205604553223 tensor([0.2061, 0.2018, 0.2129, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20186196267604828 tensor([0.2059, 0.2019, 0.2131, 0.1915, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2011975198984146 tensor([0.2067, 0.2012, 0.2134, 0.1920, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2012445330619812 tensor([0.2063, 0.2012, 0.2132, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20278774201869965 tensor([0.2055, 0.2028, 0.2135, 0.1915, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2021460086107254 tensor([0.2057, 0.2021, 0.2132, 0.1922, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017175555229187 tensor([0.2065, 0.2017, 0.2126, 0.1921, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20185203850269318 tensor([0.2065, 0.2019, 0.2133, 0.1917, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015620768070221 tensor([0.2062, 0.2016, 0.2130, 0.1922, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20247340202331543 tensor([0.2058, 0.2025, 0.2135, 0.1918, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20170240104198456 tensor([0.2062, 0.2017, 0.2130, 0.1914, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20119813084602356 tensor([0.2069, 0.2012, 0.2131, 0.1923, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20176507532596588 tensor([0.2061, 0.2018, 0.2136, 0.1917, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20217615365982056 tensor([0.2059, 0.2022, 0.2128, 0.1923, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20218390226364136 tensor([0.2061, 0.2022, 0.2126, 0.1920, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20179852843284607 tensor([0.2062, 0.2018, 0.2131, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20093102753162384 tensor([0.2064, 0.2009, 0.2131, 0.1920, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019098997116089 tensor([0.2066, 0.2019, 0.2128, 0.1911, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20139114558696747 tensor([0.2063, 0.2014, 0.2134, 0.1917, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20222724974155426 tensor([0.2058, 0.2022, 0.2129, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204131305217743 tensor([0.2061, 0.2020, 0.2136, 0.1917, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017059475183487 tensor([0.2064, 0.2017, 0.2133, 0.1913, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2024337500333786 tensor([0.2057, 0.2024, 0.2131, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017897516489029 tensor([0.2057, 0.2018, 0.2134, 0.1923, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2021072804927826 tensor([0.2064, 0.2021, 0.2129, 0.1915, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20179593563079834 tensor([0.2062, 0.2018, 0.2127, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018948644399643 tensor([0.2057, 0.2019, 0.2127, 0.1922, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20159600675106049 tensor([0.2063, 0.2016, 0.2135, 0.1915, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015259861946106 tensor([0.2062, 0.2015, 0.2133, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20163628458976746 tensor([0.2060, 0.2016, 0.2132, 0.1916, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020118236541748 tensor([0.2065, 0.2020, 0.2126, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20228873193264008 tensor([0.2056, 0.2023, 0.2127, 0.1923, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2025984823703766 tensor([0.2063, 0.2026, 0.2129, 0.1917, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20180994272232056 tensor([0.2062, 0.2018, 0.2130, 0.1920, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20212365686893463 tensor([0.2057, 0.2021, 0.2132, 0.1915, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20217888057231903 tensor([0.2061, 0.2022, 0.2127, 0.1924, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20217658579349518 tensor([0.2055, 0.2022, 0.2127, 0.1920, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190848410129547 tensor([0.2065, 0.2019, 0.2131, 0.1916, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2027389109134674 tensor([0.2056, 0.2027, 0.2127, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190797746181488 tensor([0.2065, 0.2019, 0.2133, 0.1915, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019147425889969 tensor([0.2059, 0.2019, 0.2129, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2011813074350357 tensor([0.2063, 0.2012, 0.2133, 0.1917, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20187532901763916 tensor([0.2067, 0.2019, 0.2134, 0.1916, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20192906260490417 tensor([0.2061, 0.2019, 0.2128, 0.1919, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20216864347457886 tensor([0.2059, 0.2022, 0.2130, 0.1922, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20160506665706635 tensor([0.2063, 0.2016, 0.2132, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016139179468155 tensor([0.2064, 0.2016, 0.2131, 0.1913, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20137324929237366 tensor([0.2067, 0.2014, 0.2139, 0.1912, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200073719024658 tensor([0.2064, 0.2020, 0.2131, 0.1914, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20131540298461914 tensor([0.2060, 0.2013, 0.2132, 0.1924, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022697925567627 tensor([0.2058, 0.2023, 0.2131, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2015884667634964 tensor([0.2064, 0.2016, 0.2129, 0.1920, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20164287090301514 tensor([0.2070, 0.2016, 0.2129, 0.1914, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20207086205482483 tensor([0.2058, 0.2021, 0.2132, 0.1920, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20168431103229523 tensor([0.2060, 0.2017, 0.2133, 0.1922, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20180562138557434 tensor([0.2058, 0.2018, 0.2133, 0.1923, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016448825597763 tensor([0.2060, 0.2016, 0.2133, 0.1918, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2024104744195938 tensor([0.2061, 0.2024, 0.2133, 0.1914, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20166417956352234 tensor([0.2062, 0.2017, 0.2134, 0.1922, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20274893939495087 tensor([0.2061, 0.2027, 0.2132, 0.1918, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20198653638362885 tensor([0.2059, 0.2020, 0.2124, 0.1922, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018371820449829 tensor([0.2059, 0.2018, 0.2127, 0.1924, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20192250609397888 tensor([0.2064, 0.2019, 0.2134, 0.1916, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20194102823734283 tensor([0.2060, 0.2019, 0.2131, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20269916951656342 tensor([0.2055, 0.2027, 0.2131, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020215392112732 tensor([0.2061, 0.2020, 0.2128, 0.1925, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20165129005908966 tensor([0.2062, 0.2017, 0.2130, 0.1915, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20183274149894714 tensor([0.2067, 0.2018, 0.2133, 0.1915, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204931497573853 tensor([0.2060, 0.2020, 0.2129, 0.1921, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20243170857429504 tensor([0.2057, 0.2024, 0.2131, 0.1922, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20211778581142426 tensor([0.2057, 0.2021, 0.2129, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022683471441269 tensor([0.2059, 0.2023, 0.2128, 0.1916, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.201676145195961 tensor([0.2065, 0.2017, 0.2135, 0.1913, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20129218697547913 tensor([0.2063, 0.2013, 0.2133, 0.1918, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20198844373226166 tensor([0.2061, 0.2020, 0.2133, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20213182270526886 tensor([0.2062, 0.2021, 0.2128, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20146945118904114 tensor([0.2059, 0.2015, 0.2130, 0.1917, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204253494739532 tensor([0.2063, 0.2020, 0.2134, 0.1917, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2009955793619156 tensor([0.2066, 0.2010, 0.2134, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20153625309467316 tensor([0.2060, 0.2015, 0.2133, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20207490026950836 tensor([0.2059, 0.2021, 0.2128, 0.1921, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20169679820537567 tensor([0.2065, 0.2017, 0.2133, 0.1916, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190010964870453 tensor([0.2065, 0.2019, 0.2131, 0.1916, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20209664106369019 tensor([0.2063, 0.2021, 0.2130, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190809667110443 tensor([0.2061, 0.2019, 0.2130, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016865760087967 tensor([0.2067, 0.2017, 0.2130, 0.1912, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20158934593200684 tensor([0.2058, 0.2016, 0.2129, 0.1925, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016480565071106 tensor([0.2060, 0.2016, 0.2132, 0.1916, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20176509022712708 tensor([0.2064, 0.2018, 0.2132, 0.1915, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20208030939102173 tensor([0.2058, 0.2021, 0.2132, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20159867405891418 tensor([0.2058, 0.2016, 0.2129, 0.1922, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20130249857902527 tensor([0.2061, 0.2013, 0.2132, 0.1919, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019774466753006 tensor([0.2061, 0.2020, 0.2134, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20169666409492493 tensor([0.2063, 0.2017, 0.2132, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20226500928401947 tensor([0.2058, 0.2023, 0.2129, 0.1919, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20213259756565094 tensor([0.2061, 0.2021, 0.2128, 0.1923, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016519457101822 tensor([0.2061, 0.2017, 0.2131, 0.1915, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20183232426643372 tensor([0.2063, 0.2018, 0.2132, 0.1916, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20188066363334656 tensor([0.2062, 0.2019, 0.2131, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20238251984119415 tensor([0.2057, 0.2024, 0.2131, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20176948606967926 tensor([0.2059, 0.2018, 0.2133, 0.1915, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20160099864006042 tensor([0.2058, 0.2016, 0.2131, 0.1918, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20189633965492249 tensor([0.2064, 0.2019, 0.2136, 0.1905, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20165690779685974 tensor([0.2059, 0.2017, 0.2131, 0.1921, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20213234424591064 tensor([0.2060, 0.2021, 0.2127, 0.1917, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20224645733833313 tensor([0.2056, 0.2022, 0.2132, 0.1923, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20175929367542267 tensor([0.2059, 0.2018, 0.2135, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022612988948822 tensor([0.2061, 0.2023, 0.2132, 0.1919, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2021792083978653 tensor([0.2061, 0.2022, 0.2130, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20228873193264008 tensor([0.2058, 0.2023, 0.2128, 0.1920, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20256556570529938 tensor([0.2060, 0.2026, 0.2129, 0.1911, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 2th iteration []\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.1979992687702179 tensor([0.2014, 0.2010, 0.1991, 0.1980, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19784535467624664 tensor([0.2009, 0.2028, 0.1995, 0.1989, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19878169894218445 tensor([0.1998, 0.2017, 0.1999, 0.1999, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987667828798294 tensor([0.1994, 0.2016, 0.1995, 0.2008, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1968952864408493 tensor([0.1998, 0.2023, 0.1983, 0.1969, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19650790095329285 tensor([0.2028, 0.2021, 0.2002, 0.1965, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972474604845047 tensor([0.2003, 0.2036, 0.2001, 0.1988, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19819869101047516 tensor([0.2017, 0.2010, 0.2007, 0.1985, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979946494102478 tensor([0.1997, 0.2015, 0.2000, 0.2008, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976122409105301 tensor([0.2013, 0.2015, 0.2002, 0.1976, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19816440343856812 tensor([0.2027, 0.1995, 0.1998, 0.1982, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1983802169561386 tensor([0.1995, 0.2039, 0.1995, 0.1984, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19732140004634857 tensor([0.1997, 0.2020, 0.2012, 0.1997, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19791501760482788 tensor([0.1997, 0.2020, 0.2004, 0.1999, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19775404036045074 tensor([0.2007, 0.2013, 0.2000, 0.1978, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971805840730667 tensor([0.2038, 0.2005, 0.1999, 0.1972, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19895483553409576 tensor([0.2005, 0.2005, 0.1993, 0.1990, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19718831777572632 tensor([0.2013, 0.2011, 0.2002, 0.1972, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19808591902256012 tensor([0.2000, 0.2021, 0.1998, 0.2000, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973952054977417 tensor([0.1999, 0.2008, 0.1978, 0.1974, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19763271510601044 tensor([0.2023, 0.2012, 0.2002, 0.1976, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19739800691604614 tensor([0.2003, 0.2032, 0.1993, 0.1974, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19671596586704254 tensor([0.2009, 0.2014, 0.2028, 0.1982, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19795627892017365 tensor([0.2011, 0.2016, 0.1999, 0.1994, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19906798005104065 tensor([0.2000, 0.2002, 0.1996, 0.1991, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19777797162532806 tensor([0.2024, 0.2016, 0.2004, 0.1978, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19854886829853058 tensor([0.2000, 0.2024, 0.2000, 0.1985, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19744879007339478 tensor([0.2008, 0.2018, 0.2016, 0.1983, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19726704061031342 tensor([0.1991, 0.2019, 0.2004, 0.2013, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19794458150863647 tensor([0.2009, 0.2003, 0.2000, 0.1979, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971452832221985 tensor([0.2028, 0.2025, 0.2003, 0.1972, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1958724856376648 tensor([0.2000, 0.2041, 0.2008, 0.1992, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19721364974975586 tensor([0.2007, 0.2002, 0.1994, 0.1972, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19737203419208527 tensor([0.1995, 0.2019, 0.2001, 0.2012, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975339949131012 tensor([0.2006, 0.2011, 0.1997, 0.1975, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977216899394989 tensor([0.2018, 0.2014, 0.2007, 0.1977, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976325809955597 tensor([0.2011, 0.2023, 0.1997, 0.1976, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19834645092487335 tensor([0.2004, 0.2013, 0.2009, 0.1991, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19838666915893555 tensor([0.2000, 0.2019, 0.1996, 0.2002, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19865034520626068 tensor([0.1992, 0.1997, 0.1987, 0.1992, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19806332886219025 tensor([0.2019, 0.2012, 0.2000, 0.1981, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19866317510604858 tensor([0.1994, 0.2034, 0.1987, 0.1987, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19721709191799164 tensor([0.2000, 0.2011, 0.2024, 0.1993, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19809074699878693 tensor([0.1996, 0.2013, 0.1998, 0.2012, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19903571903705597 tensor([0.2007, 0.2017, 0.1995, 0.1991, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19825732707977295 tensor([0.2019, 0.2005, 0.2008, 0.1986, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19879139959812164 tensor([0.2002, 0.2015, 0.1998, 0.1988, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19814543426036835 tensor([0.2003, 0.2017, 0.2014, 0.1985, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19736669957637787 tensor([0.2004, 0.2022, 0.2005, 0.1994, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19847753643989563 tensor([0.2000, 0.2011, 0.1987, 0.1985, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978563815355301 tensor([0.2018, 0.2010, 0.2009, 0.1984, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19804982841014862 tensor([0.2000, 0.2035, 0.1996, 0.1989, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19802413880825043 tensor([0.2010, 0.2006, 0.2000, 0.1980, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975347399711609 tensor([0.1992, 0.2019, 0.2006, 0.2007, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719438254833221 tensor([0.2002, 0.2020, 0.1981, 0.1972, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19885097444057465 tensor([0.2017, 0.2002, 0.2002, 0.1990, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19837577641010284 tensor([0.2002, 0.2017, 0.1996, 0.2002, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19841548800468445 tensor([0.2013, 0.2007, 0.2004, 0.1984, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19792167842388153 tensor([0.2001, 0.2016, 0.2010, 0.1994, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1978255659341812 tensor([0.2019, 0.2008, 0.2001, 0.1978, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19823040068149567 tensor([0.2012, 0.2017, 0.2006, 0.1982, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1973821222782135 tensor([0.2002, 0.2034, 0.2002, 0.1988, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19777114689350128 tensor([0.2021, 0.2011, 0.2012, 0.1978, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1970992535352707 tensor([0.2004, 0.2019, 0.2008, 0.1998, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1978963166475296 tensor([0.1996, 0.2019, 0.1992, 0.1979, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19841168820858002 tensor([0.2017, 0.2006, 0.2005, 0.1988, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19881810247898102 tensor([0.1990, 0.2033, 0.2000, 0.1988, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19842077791690826 tensor([0.2008, 0.2016, 0.2003, 0.1984, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19741295278072357 tensor([0.2012, 0.2012, 0.2005, 0.1997, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19835951924324036 tensor([0.2000, 0.2001, 0.1999, 0.1984, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977350264787674 tensor([0.2017, 0.2022, 0.1995, 0.1977, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19865795969963074 tensor([0.1995, 0.2033, 0.1987, 0.1991, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19706201553344727 tensor([0.2014, 0.2008, 0.2018, 0.1989, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19642877578735352 tensor([0.1998, 0.2015, 0.2013, 0.2009, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19787399470806122 tensor([0.2000, 0.2024, 0.1993, 0.1979, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981905847787857 tensor([0.2027, 0.2004, 0.2004, 0.1983, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19741857051849365 tensor([0.1992, 0.2045, 0.1998, 0.1990, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979864537715912 tensor([0.1997, 0.2012, 0.2022, 0.1990, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19732621312141418 tensor([0.2000, 0.2007, 0.2004, 0.2016, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19824983179569244 tensor([0.2011, 0.2015, 0.1998, 0.1982, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19679021835327148 tensor([0.2028, 0.2019, 0.1995, 0.1968, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19824039936065674 tensor([0.2000, 0.2033, 0.1986, 0.1982, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19806663691997528 tensor([0.2007, 0.2012, 0.2012, 0.1988, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19766810536384583 tensor([0.2009, 0.2027, 0.1996, 0.1992, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19816438853740692 tensor([0.1991, 0.2012, 0.1991, 0.1982, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1979447901248932 tensor([0.2020, 0.2008, 0.2011, 0.1979, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19685931503772736 tensor([0.1990, 0.2042, 0.2002, 0.1998, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19858455657958984 tensor([0.2006, 0.2006, 0.2000, 0.1986, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977033168077469 tensor([0.1996, 0.2010, 0.2004, 0.2013, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19806534051895142 tensor([0.2018, 0.2002, 0.2013, 0.1981, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19806446135044098 tensor([0.2014, 0.1999, 0.2008, 0.1981, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19765423238277435 tensor([0.1987, 0.2045, 0.1995, 0.1997, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19748623669147491 tensor([0.2020, 0.2009, 0.1999, 0.1975, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19744811952114105 tensor([0.2000, 0.2021, 0.1997, 0.2007, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19896267354488373 tensor([0.1999, 0.2010, 0.2005, 0.1996, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19819118082523346 tensor([0.2016, 0.2001, 0.1999, 0.1982, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19832423329353333 tensor([0.2005, 0.2019, 0.2007, 0.1985, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19788868725299835 tensor([0.2006, 0.2011, 0.2019, 0.1985, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19745536148548126 tensor([0.2004, 0.2007, 0.2009, 0.2006, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976655125617981 tensor([0.2003, 0.2001, 0.1987, 0.1977, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19752292335033417 tensor([0.2026, 0.2003, 0.2007, 0.1975, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19756415486335754 tensor([0.2007, 0.2042, 0.1993, 0.1983, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19737565517425537 tensor([0.2008, 0.2013, 0.2017, 0.1989, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19859719276428223 tensor([0.2000, 0.2018, 0.2002, 0.1994, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19934427738189697 tensor([0.2008, 0.1993, 0.2001, 0.2001, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19780410826206207 tensor([0.2025, 0.2009, 0.2006, 0.1982, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19727784395217896 tensor([0.1997, 0.2024, 0.2011, 0.1996, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19765473902225494 tensor([0.2008, 0.2015, 0.2016, 0.1985, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975461095571518 tensor([0.2004, 0.2025, 0.2004, 0.1991, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19824279844760895 tensor([0.1998, 0.2011, 0.1997, 0.1982, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19694866240024567 tensor([0.2029, 0.2007, 0.2002, 0.1969, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19848760962486267 tensor([0.2007, 0.2002, 0.2005, 0.1985, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19746343791484833 tensor([0.2008, 0.2008, 0.2016, 0.1993, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19885075092315674 tensor([0.2007, 0.2016, 0.1997, 0.1991, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970045119524002 tensor([0.1996, 0.2020, 0.1989, 0.1970, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19749513268470764 tensor([0.2026, 0.2018, 0.2000, 0.1975, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19783754646778107 tensor([0.1986, 0.2050, 0.1988, 0.1998, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978301703929901 tensor([0.2011, 0.2017, 0.2005, 0.1989, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19782833755016327 tensor([0.2005, 0.2021, 0.2003, 0.1993, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19754786789417267 tensor([0.2007, 0.2007, 0.1985, 0.1975, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19777216017246246 tensor([0.2025, 0.2017, 0.2000, 0.1981, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977691352367401 tensor([0.2004, 0.2014, 0.1988, 0.1978, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19805480539798737 tensor([0.1997, 0.2009, 0.2011, 0.2003, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.198885977268219 tensor([0.2005, 0.2012, 0.2000, 0.1989, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19871889054775238 tensor([0.1999, 0.2011, 0.1995, 0.1987, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19712859392166138 tensor([0.2029, 0.2008, 0.2002, 0.1971, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19782814383506775 tensor([0.1998, 0.2050, 0.1990, 0.1983, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971602588891983 tensor([0.2019, 0.2007, 0.2006, 0.1972, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1973515748977661 tensor([0.2005, 0.2011, 0.2000, 0.2011, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19781959056854248 tensor([0.1997, 0.2010, 0.1981, 0.1978, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1984887421131134 tensor([0.2016, 0.2005, 0.2007, 0.1987, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977323740720749 tensor([0.1996, 0.2042, 0.1994, 0.1990, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976122409105301 tensor([0.2022, 0.2006, 0.2015, 0.1976, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19666261970996857 tensor([0.2005, 0.2014, 0.2011, 0.2003, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19935815036296844 tensor([0.1997, 0.2012, 0.1994, 0.1994, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.197942316532135 tensor([0.2020, 0.2012, 0.2009, 0.1980, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971781849861145 tensor([0.1995, 0.2025, 0.2009, 0.1999, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977204531431198 tensor([0.2020, 0.2013, 0.2007, 0.1977, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19872447848320007 tensor([0.1998, 0.2029, 0.1992, 0.1994, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19738131761550903 tensor([0.1994, 0.2018, 0.1991, 0.1974, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973128765821457 tensor([0.2026, 0.2016, 0.1999, 0.1973, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19823984801769257 tensor([0.2005, 0.2013, 0.2005, 0.1995, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19831377267837524 tensor([0.2015, 0.2002, 0.2015, 0.1985, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19763679802417755 tensor([0.2004, 0.2025, 0.2001, 0.1994, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1983383595943451 tensor([0.2003, 0.2006, 0.1995, 0.1983, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1962510049343109 tensor([0.2036, 0.2012, 0.1997, 0.1963, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975301057100296 tensor([0.1989, 0.2053, 0.1997, 0.1985, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19819071888923645 tensor([0.2011, 0.2005, 0.2005, 0.1982, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19755062460899353 tensor([0.2005, 0.2011, 0.2010, 0.1998, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19867832958698273 tensor([0.2003, 0.2018, 0.2001, 0.1987, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973240077495575 tensor([0.2024, 0.2011, 0.2004, 0.1973, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19680745899677277 tensor([0.1995, 0.2046, 0.1994, 0.1997, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19742470979690552 tensor([0.2001, 0.2016, 0.2018, 0.1992, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19804951548576355 tensor([0.1997, 0.2027, 0.1997, 0.1999, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19926567375659943 tensor([0.2006, 0.1998, 0.2002, 0.1993, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19606119394302368 tensor([0.2039, 0.2011, 0.1997, 0.1961, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19885514676570892 tensor([0.1997, 0.2027, 0.1991, 0.1989, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19767381250858307 tensor([0.2008, 0.2018, 0.2003, 0.1995, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19709892570972443 tensor([0.1995, 0.2029, 0.2003, 0.2002, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19812630116939545 tensor([0.1994, 0.2034, 0.2001, 0.1990, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19740897417068481 tensor([0.2026, 0.2011, 0.2014, 0.1975, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19882875680923462 tensor([0.2000, 0.2013, 0.2001, 0.1988, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19843655824661255 tensor([0.2009, 0.2008, 0.2006, 0.1984, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19796167314052582 tensor([0.1993, 0.2008, 0.2006, 0.2013, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19951549172401428 tensor([0.1995, 0.2011, 0.1996, 0.1996, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971483677625656 tensor([0.2031, 0.2010, 0.2001, 0.1971, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19801780581474304 tensor([0.1993, 0.2033, 0.2000, 0.1993, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19738399982452393 tensor([0.2017, 0.2016, 0.2009, 0.1974, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19810311496257782 tensor([0.1999, 0.2004, 0.1999, 0.2018, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19785968959331512 tensor([0.2008, 0.2004, 0.1986, 0.1979, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19771137833595276 tensor([0.2013, 0.2012, 0.2000, 0.1977, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983715295791626 tensor([0.1996, 0.2031, 0.1996, 0.1994, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19769228994846344 tensor([0.1997, 0.2015, 0.2015, 0.1996, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972447782754898 tensor([0.2003, 0.2011, 0.2004, 0.2010, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724424183368683 tensor([0.1996, 0.2012, 0.1987, 0.1972, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1984013170003891 tensor([0.2016, 0.2006, 0.2009, 0.1984, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19864757359027863 tensor([0.1993, 0.2029, 0.1989, 0.1986, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19743497669696808 tensor([0.2006, 0.2016, 0.2015, 0.1989, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983022540807724 tensor([0.2009, 0.2011, 0.2003, 0.1994, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1990724354982376 tensor([0.2000, 0.2015, 0.2002, 0.1991, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973658949136734 tensor([0.2020, 0.2000, 0.2004, 0.1974, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19825570285320282 tensor([0.2001, 0.2019, 0.1993, 0.1983, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19849854707717896 tensor([0.2013, 0.2005, 0.2006, 0.1991, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976287066936493 tensor([0.2009, 0.2009, 0.1988, 0.1976, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974550485610962 tensor([0.2008, 0.2001, 0.1985, 0.1975, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19693642854690552 tensor([0.2039, 0.2002, 0.1996, 0.1969, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.197606161236763 tensor([0.1997, 0.2047, 0.1993, 0.1988, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19789670407772064 tensor([0.2012, 0.2022, 0.2006, 0.1979, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19767794013023376 tensor([0.2025, 0.2009, 0.2006, 0.1977, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19903601706027985 tensor([0.1990, 0.2014, 0.2009, 0.1996, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1978733092546463 tensor([0.2026, 0.2007, 0.2008, 0.1979, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19870372116565704 tensor([0.1996, 0.2024, 0.1997, 0.1987, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19662314653396606 tensor([0.1999, 0.2024, 0.2015, 0.1996, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972334235906601 tensor([0.2007, 0.2019, 0.2010, 0.1991, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19827456772327423 tensor([0.1995, 0.2015, 0.1997, 0.1983, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19707965850830078 tensor([0.2029, 0.2015, 0.2003, 0.1971, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19708260893821716 tensor([0.1998, 0.2054, 0.1996, 0.1982, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19748543202877045 tensor([0.2008, 0.2017, 0.2014, 0.1985, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19778087735176086 tensor([0.2001, 0.2030, 0.2000, 0.1992, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19650274515151978 tensor([0.2008, 0.2007, 0.1976, 0.1965, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19783499836921692 tensor([0.2020, 0.2017, 0.2003, 0.1978, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977878212928772 tensor([0.1992, 0.2056, 0.1989, 0.1985, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19743037223815918 tensor([0.2001, 0.2016, 0.2017, 0.1991, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19754989445209503 tensor([0.1996, 0.2017, 0.2004, 0.2008, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19826006889343262 tensor([0.2008, 0.1998, 0.1991, 0.1983, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19633649289608002 tensor([0.2028, 0.2020, 0.1992, 0.1963, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974383145570755 tensor([0.2006, 0.2026, 0.2002, 0.1974, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19792276620864868 tensor([0.2011, 0.2009, 0.2018, 0.1982, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19776558876037598 tensor([0.2002, 0.2006, 0.2006, 0.2009, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19770893454551697 tensor([0.2008, 0.2006, 0.2001, 0.1977, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971231997013092 tensor([0.2022, 0.2001, 0.1986, 0.1971, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19749121367931366 tensor([0.1996, 0.2058, 0.1991, 0.1981, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1984524130821228 tensor([0.2002, 0.2019, 0.1994, 0.1985, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979086995124817 tensor([0.1993, 0.2013, 0.2004, 0.2011, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969841718673706 tensor([0.1992, 0.2015, 0.1987, 0.1970, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19689229130744934 tensor([0.2037, 0.2004, 0.2000, 0.1969, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19761422276496887 tensor([0.1999, 0.2032, 0.2005, 0.1987, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19881980121135712 tensor([0.2010, 0.2007, 0.2003, 0.1992, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19795654714107513 tensor([0.2000, 0.2017, 0.2007, 0.1996, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19806978106498718 tensor([0.2004, 0.2005, 0.1994, 0.1981, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19851459562778473 tensor([0.2019, 0.2006, 0.2003, 0.1986, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982765644788742 tensor([0.1993, 0.2023, 0.2002, 0.1999, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982262134552002 tensor([0.2007, 0.2010, 0.2008, 0.1992, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19735808670520782 tensor([0.2007, 0.2007, 0.2011, 0.2002, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19870081543922424 tensor([0.2005, 0.2012, 0.1994, 0.1987, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19727806746959686 tensor([0.2019, 0.2016, 0.2009, 0.1983, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19800131022930145 tensor([0.2011, 0.2013, 0.2007, 0.1989, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19838573038578033 tensor([0.2004, 0.2012, 0.2004, 0.1984, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988835483789444 tensor([0.1992, 0.2009, 0.1995, 0.2016, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1987621933221817 tensor([0.2014, 0.2008, 0.2002, 0.1988, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1966557651758194 tensor([0.2031, 0.2010, 0.2002, 0.1967, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1991984248161316 tensor([0.1996, 0.2016, 0.1992, 0.1999, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19756627082824707 tensor([0.2007, 0.2018, 0.2012, 0.1987, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19784043729305267 tensor([0.2002, 0.2007, 0.2002, 0.2010, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19900032877922058 tensor([0.1999, 0.2011, 0.2000, 0.1990, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19755810499191284 tensor([0.2026, 0.2011, 0.2002, 0.1976, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19664253294467926 tensor([0.1995, 0.2055, 0.1994, 0.1990, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983027160167694 tensor([0.2003, 0.2009, 0.2007, 0.1999, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19904763996601105 tensor([0.1998, 0.2016, 0.1992, 0.1990, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19693419337272644 tensor([0.2020, 0.2014, 0.1992, 0.1969, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19781677424907684 tensor([0.2026, 0.2005, 0.2004, 0.1978, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19781708717346191 tensor([0.2008, 0.2031, 0.1999, 0.1978, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977602243423462 tensor([0.2016, 0.2003, 0.2017, 0.1978, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1986691802740097 tensor([0.1994, 0.2022, 0.1998, 0.2000, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19831740856170654 tensor([0.1993, 0.2007, 0.1991, 0.1983, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19875642657279968 tensor([0.2010, 0.2005, 0.2005, 0.1988, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19868949055671692 tensor([0.2002, 0.2012, 0.1993, 0.1987, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19794930517673492 tensor([0.2002, 0.2021, 0.2010, 0.1988, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19814833998680115 tensor([0.2000, 0.2020, 0.1998, 0.2000, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19854532182216644 tensor([0.2003, 0.2013, 0.1997, 0.1985, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3], [4], [3], [4], [4], [3], [4], [4], [3], [4], [3], [3], [3], [4], [4], [2], [3], [2], [4], [4], [4], [4], [3], [4], [4], [3], [4], [4], [3], [4], [3], [4], [4], [3], [4], [3], [3], [4], [3], [4], [3], [4], [3], [3], [4], [3], [3], [2], [4], [4], [3], [4], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [3], [4], [4], [3], [4], [4], [4], [3], [3], [4], [4], [4], [1], [4], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [4], [3], [4], [3], [3], [3], [4], [3], [4], [3], [4], [4], [3], [4], [3], [4], [4], [3], [4], [3], [3], [4], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [4], [4], [3], [3], [4], [0], [3], [4], [3], [4], [3], [3], [4], [4], [4], [3], [4], [3], [4], [4], [3], [3], [3], [4], [3], [3], [3], [4], [3], [3], [0], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [4], [4], [3], [4], [4], [4], [4], [3], [4], [4], [3], [4], [3], [3], [2], [4], [4], [3], [3], [4], [4], [3], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3]]\n",
      "[[0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4]]\n",
      "NL_pred of 0th iteration [[3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3], [4], [3], [4], [4], [3], [4], [4], [3], [4], [3], [3], [3], [4], [4], [2], [3], [2], [4], [4], [4], [4], [3], [4], [4], [3], [4], [4], [3], [4], [3], [4], [4], [3], [4], [3], [3], [4], [3], [4], [3], [4], [3], [3], [4], [3], [3], [2], [4], [4], [3], [4], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [3], [4], [4], [3], [4], [4], [4], [3], [3], [4], [4], [4], [1], [4], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [4], [3], [4], [3], [3], [3], [4], [3], [4], [3], [4], [4], [3], [4], [3], [4], [4], [3], [4], [3], [3], [4], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [4], [4], [3], [3], [4], [0], [3], [4], [3], [4], [3], [3], [4], [4], [4], [3], [4], [3], [4], [4], [3], [3], [3], [4], [3], [3], [3], [4], [3], [3], [0], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [4], [4], [3], [4], [4], [4], [4], [3], [4], [4], [3], [4], [3], [3], [2], [4], [4], [3], [3], [4], [4], [3], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005140603542327881  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00514052677154541  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0051403846740722655  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005140180110931396  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005139920711517334  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005139612197875977  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005139259338378907  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005138866424560547  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0051384387016296384  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005137977123260498  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005137487411499024  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005136970520019531  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00513643217086792  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005135871410369873  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0051352930068969726  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005134696960449219  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005134086132049561  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.0051334619522094725  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005132825374603272  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0051321773529052735  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.1982196420431137 tensor([0.2029, 0.2025, 0.2004, 0.1960, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.196913942694664 tensor([0.2023, 0.2043, 0.2008, 0.1969, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19782964885234833 tensor([0.2013, 0.2032, 0.2012, 0.1978, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1988053172826767 tensor([0.2008, 0.2031, 0.2008, 0.1988, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19961866736412048 tensor([0.2013, 0.2038, 0.1996, 0.1949, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19619186222553253 tensor([0.2042, 0.2036, 0.2015, 0.1945, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19677914679050446 tensor([0.2017, 0.2051, 0.2013, 0.1968, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19645504653453827 tensor([0.2032, 0.2025, 0.2020, 0.1965, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19876709580421448 tensor([0.2012, 0.2030, 0.2013, 0.1988, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971328854560852 tensor([0.2028, 0.2030, 0.2015, 0.1956, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975470632314682 tensor([0.2042, 0.2010, 0.2011, 0.1962, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964711993932724 tensor([0.2009, 0.2054, 0.2008, 0.1964, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19772624969482422 tensor([0.2012, 0.2035, 0.2025, 0.1977, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19792485237121582 tensor([0.2012, 0.2035, 0.2017, 0.1979, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19795924425125122 tensor([0.2022, 0.2028, 0.2013, 0.1957, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1963949054479599 tensor([0.2053, 0.2019, 0.2012, 0.1952, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19844374060630798 tensor([0.2020, 0.2020, 0.2006, 0.1969, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19797097146511078 tensor([0.2027, 0.2026, 0.2015, 0.1952, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19795463979244232 tensor([0.2015, 0.2036, 0.2011, 0.1980, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19905756413936615 tensor([0.2014, 0.2023, 0.1991, 0.1954, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1965312510728836 tensor([0.2038, 0.2026, 0.2014, 0.1956, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976022571325302 tensor([0.2018, 0.2047, 0.2006, 0.1954, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19617845118045807 tensor([0.2024, 0.2029, 0.2041, 0.1962, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19740039110183716 tensor([0.2026, 0.2031, 0.2012, 0.1974, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19882723689079285 tensor([0.2015, 0.2017, 0.2009, 0.1971, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1957671046257019 tensor([0.2039, 0.2031, 0.2017, 0.1958, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967625766992569 tensor([0.2015, 0.2039, 0.2013, 0.1965, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19624604284763336 tensor([0.2023, 0.2033, 0.2029, 0.1962, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1993015706539154 tensor([0.2005, 0.2034, 0.2017, 0.1993, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1984807699918747 tensor([0.2024, 0.2018, 0.2013, 0.1959, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19523577392101288 tensor([0.2043, 0.2040, 0.2016, 0.1952, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19722899794578552 tensor([0.2014, 0.2056, 0.2020, 0.1972, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20021864771842957 tensor([0.2022, 0.2017, 0.2007, 0.1952, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19913898408412933 tensor([0.2009, 0.2034, 0.2014, 0.1991, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988210529088974 tensor([0.2020, 0.2026, 0.2010, 0.1955, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19607454538345337 tensor([0.2033, 0.2029, 0.2020, 0.1957, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19713516533374786 tensor([0.2025, 0.2038, 0.2009, 0.1956, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19703073799610138 tensor([0.2019, 0.2028, 0.2022, 0.1970, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19815649092197418 tensor([0.2014, 0.2034, 0.2009, 0.1982, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19717156887054443 tensor([0.2007, 0.2011, 0.1999, 0.1972, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19661542773246765 tensor([0.2034, 0.2027, 0.2013, 0.1960, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19668331742286682 tensor([0.2009, 0.2049, 0.1999, 0.1967, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19726675748825073 tensor([0.2015, 0.2026, 0.2037, 0.1973, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19914956390857697 tensor([0.2011, 0.2028, 0.2011, 0.1991, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19706660509109497 tensor([0.2022, 0.2031, 0.2008, 0.1971, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19654491543769836 tensor([0.2034, 0.2020, 0.2021, 0.1965, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974991410970688 tensor([0.2016, 0.2030, 0.2011, 0.1968, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19646130502223969 tensor([0.2018, 0.2031, 0.2027, 0.1965, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19738063216209412 tensor([0.2019, 0.2037, 0.2018, 0.1974, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19940431416034698 tensor([0.2015, 0.2026, 0.2000, 0.1965, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19640681147575378 tensor([0.2033, 0.2025, 0.2022, 0.1964, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19684599339962006 tensor([0.2015, 0.2050, 0.2009, 0.1968, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981024295091629 tensor([0.2025, 0.2021, 0.2013, 0.1960, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19864556193351746 tensor([0.2007, 0.2034, 0.2019, 0.1986, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993420273065567 tensor([0.2017, 0.2035, 0.1993, 0.1952, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970292031764984 tensor([0.2032, 0.2017, 0.2015, 0.1970, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19822785258293152 tensor([0.2016, 0.2031, 0.2009, 0.1982, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19688481092453003 tensor([0.2028, 0.2022, 0.2017, 0.1964, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19738593697547913 tensor([0.2016, 0.2030, 0.2023, 0.1974, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19717684388160706 tensor([0.2033, 0.2023, 0.2014, 0.1958, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19600100815296173 tensor([0.2027, 0.2032, 0.2019, 0.1962, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19676785171031952 tensor([0.2017, 0.2049, 0.2015, 0.1968, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1956106573343277 tensor([0.2036, 0.2026, 0.2025, 0.1958, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977381557226181 tensor([0.2018, 0.2034, 0.2021, 0.1977, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19915206730365753 tensor([0.2011, 0.2034, 0.2005, 0.1959, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1967327743768692 tensor([0.2032, 0.2021, 0.2018, 0.1967, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1965860277414322 tensor([0.2005, 0.2049, 0.2013, 0.1968, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668790698051453 tensor([0.2023, 0.2031, 0.2016, 0.1964, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19772855937480927 tensor([0.2026, 0.2027, 0.2018, 0.1977, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19943779706954956 tensor([0.2015, 0.2016, 0.2012, 0.1964, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19664117693901062 tensor([0.2032, 0.2037, 0.2008, 0.1957, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19706720113754272 tensor([0.2009, 0.2048, 0.1999, 0.1971, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19691801071166992 tensor([0.2029, 0.2023, 0.2031, 0.1969, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19888536632061005 tensor([0.2013, 0.2030, 0.2026, 0.1989, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981409192085266 tensor([0.2015, 0.2039, 0.2006, 0.1959, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19627885520458221 tensor([0.2042, 0.2019, 0.2017, 0.1963, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19694861769676208 tensor([0.2007, 0.2061, 0.2011, 0.1969, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19697457551956177 tensor([0.2011, 0.2027, 0.2035, 0.1970, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19953586161136627 tensor([0.2015, 0.2022, 0.2017, 0.1995, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19720366597175598 tensor([0.2025, 0.2030, 0.2011, 0.1962, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19677698612213135 tensor([0.2043, 0.2034, 0.2008, 0.1948, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19763359427452087 tensor([0.2014, 0.2048, 0.1999, 0.1962, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19682134687900543 tensor([0.2022, 0.2026, 0.2025, 0.1968, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19714102149009705 tensor([0.2024, 0.2042, 0.2009, 0.1971, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2001582831144333 tensor([0.2006, 0.2027, 0.2004, 0.1962, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19593973457813263 tensor([0.2035, 0.2023, 0.2024, 0.1959, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977740377187729 tensor([0.2005, 0.2057, 0.2015, 0.1978, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19788949191570282 tensor([0.2021, 0.2021, 0.2013, 0.1966, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19926731288433075 tensor([0.2011, 0.2025, 0.2017, 0.1993, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19636981189250946 tensor([0.2033, 0.2017, 0.2026, 0.1961, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19767539203166962 tensor([0.2029, 0.2013, 0.2020, 0.1961, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19768324494361877 tensor([0.2002, 0.2060, 0.2008, 0.1977, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974392533302307 tensor([0.2035, 0.2024, 0.2012, 0.1955, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19867543876171112 tensor([0.2015, 0.2036, 0.2010, 0.1987, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19758525490760803 tensor([0.2014, 0.2025, 0.2018, 0.1976, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19796548783779144 tensor([0.2031, 0.2016, 0.2012, 0.1962, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1965138167142868 tensor([0.2020, 0.2034, 0.2020, 0.1965, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19648055732250214 tensor([0.2021, 0.2026, 0.2032, 0.1965, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19855253398418427 tensor([0.2019, 0.2022, 0.2022, 0.1986, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1999567747116089 tensor([0.2018, 0.2016, 0.2000, 0.1957, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668219983577728 tensor([0.2041, 0.2018, 0.2020, 0.1955, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19625219702720642 tensor([0.2021, 0.2057, 0.2006, 0.1963, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19685065746307373 tensor([0.2022, 0.2028, 0.2030, 0.1969, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19741885364055634 tensor([0.2015, 0.2033, 0.2015, 0.1974, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19744233787059784 tensor([0.2022, 0.2008, 0.2014, 0.1981, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19617444276809692 tensor([0.2040, 0.2024, 0.2019, 0.1962, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19755765795707703 tensor([0.2011, 0.2039, 0.2023, 0.1976, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19643384218215942 tensor([0.2023, 0.2030, 0.2029, 0.1964, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19708587229251862 tensor([0.2019, 0.2040, 0.2017, 0.1971, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19894914329051971 tensor([0.2013, 0.2026, 0.2010, 0.1962, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19695180654525757 tensor([0.2044, 0.2022, 0.2015, 0.1949, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19788415729999542 tensor([0.2021, 0.2016, 0.2018, 0.1965, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19726625084877014 tensor([0.2023, 0.2022, 0.2030, 0.1973, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970711350440979 tensor([0.2021, 0.2031, 0.2010, 0.1971, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001357525587082 tensor([0.2011, 0.2036, 0.2001, 0.1950, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19590407609939575 tensor([0.2040, 0.2033, 0.2012, 0.1955, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.197759747505188 tensor([0.2001, 0.2065, 0.2001, 0.1978, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19687290489673615 tensor([0.2025, 0.2032, 0.2018, 0.1969, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724296033382416 tensor([0.2020, 0.2036, 0.2016, 0.1972, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19974926114082336 tensor([0.2022, 0.2022, 0.1997, 0.1955, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.196113720536232 tensor([0.2040, 0.2031, 0.2012, 0.1961, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19928330183029175 tensor([0.2019, 0.2029, 0.2001, 0.1958, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19825324416160583 tensor([0.2012, 0.2023, 0.2024, 0.1983, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19710348546504974 tensor([0.2020, 0.2027, 0.2013, 0.1969, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985005885362625 tensor([0.2014, 0.2027, 0.2008, 0.1967, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19673845171928406 tensor([0.2044, 0.2023, 0.2014, 0.1951, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19629377126693726 tensor([0.2013, 0.2065, 0.2003, 0.1963, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19730539619922638 tensor([0.2034, 0.2022, 0.2019, 0.1952, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19904522597789764 tensor([0.2020, 0.2026, 0.2013, 0.1990, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19934388995170593 tensor([0.2012, 0.2025, 0.1993, 0.1958, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19671465456485748 tensor([0.2030, 0.2020, 0.2020, 0.1967, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970020830631256 tensor([0.2010, 0.2057, 0.2007, 0.1970, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19589951634407043 tensor([0.2036, 0.2021, 0.2028, 0.1956, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19834695756435394 tensor([0.2020, 0.2029, 0.2024, 0.1983, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19804389774799347 tensor([0.2012, 0.2027, 0.2007, 0.1973, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19596679508686066 tensor([0.2035, 0.2026, 0.2022, 0.1960, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19786226749420166 tensor([0.2010, 0.2040, 0.2022, 0.1979, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19600674510002136 tensor([0.2035, 0.2028, 0.2020, 0.1957, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19742503762245178 tensor([0.2013, 0.2044, 0.2005, 0.1974, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199975848197937 tensor([0.2009, 0.2033, 0.2004, 0.1954, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19627711176872253 tensor([0.2041, 0.2031, 0.2012, 0.1953, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974572390317917 tensor([0.2020, 0.2028, 0.2018, 0.1975, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19647793471813202 tensor([0.2030, 0.2017, 0.2028, 0.1965, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973503977060318 tensor([0.2019, 0.2040, 0.2014, 0.1974, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19906312227249146 tensor([0.2018, 0.2021, 0.2008, 0.1963, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19702507555484772 tensor([0.2051, 0.2026, 0.2010, 0.1942, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19654102623462677 tensor([0.2004, 0.2068, 0.2010, 0.1965, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975516378879547 tensor([0.2026, 0.2019, 0.2018, 0.1962, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19774338603019714 tensor([0.2020, 0.2026, 0.2023, 0.1977, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19684308767318726 tensor([0.2018, 0.2033, 0.2014, 0.1967, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19658710062503815 tensor([0.2039, 0.2025, 0.2016, 0.1953, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19763755798339844 tensor([0.2010, 0.2061, 0.2007, 0.1976, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971476674079895 tensor([0.2016, 0.2031, 0.2031, 0.1971, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19789335131645203 tensor([0.2012, 0.2042, 0.2009, 0.1979, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19787493348121643 tensor([0.2021, 0.2013, 0.2015, 0.1973, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971014440059662 tensor([0.2053, 0.2025, 0.2010, 0.1941, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974625289440155 tensor([0.2012, 0.2042, 0.2004, 0.1968, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19744203984737396 tensor([0.2022, 0.2033, 0.2016, 0.1974, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1981538087129593 tensor([0.2010, 0.2044, 0.2016, 0.1982, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969454139471054 tensor([0.2009, 0.2049, 0.2014, 0.1969, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19547782838344574 tensor([0.2041, 0.2026, 0.2027, 0.1955, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975749135017395 tensor([0.2015, 0.2028, 0.2014, 0.1968, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.196988046169281 tensor([0.2024, 0.2023, 0.2019, 0.1964, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1992674469947815 tensor([0.2008, 0.2023, 0.2019, 0.1993, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19754602015018463 tensor([0.2010, 0.2026, 0.2009, 0.1975, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964360922574997 tensor([0.2046, 0.2025, 0.2014, 0.1951, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973208636045456 tensor([0.2008, 0.2048, 0.2013, 0.1973, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19623902440071106 tensor([0.2032, 0.2031, 0.2022, 0.1954, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19974638521671295 tensor([0.2014, 0.2019, 0.2012, 0.1997, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19989793002605438 tensor([0.2023, 0.2019, 0.1999, 0.1959, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19753298163414001 tensor([0.2028, 0.2027, 0.2013, 0.1957, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19733190536499023 tensor([0.2011, 0.2046, 0.2009, 0.1973, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19759510457515717 tensor([0.2011, 0.2030, 0.2028, 0.1976, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19893425703048706 tensor([0.2018, 0.2026, 0.2017, 0.1989, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19995951652526855 tensor([0.2011, 0.2027, 0.2000, 0.1952, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19643935561180115 tensor([0.2031, 0.2021, 0.2022, 0.1964, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19796380400657654 tensor([0.2008, 0.2045, 0.2002, 0.1966, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1968381106853485 tensor([0.2021, 0.2031, 0.2028, 0.1968, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19736994802951813 tensor([0.2024, 0.2026, 0.2016, 0.1974, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1970822662115097 tensor([0.2014, 0.2030, 0.2015, 0.1970, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19799114763736725 tensor([0.2035, 0.2014, 0.2017, 0.1954, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19815245270729065 tensor([0.2016, 0.2034, 0.2006, 0.1962, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19703739881515503 tensor([0.2028, 0.2020, 0.2019, 0.1970, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199555903673172 tensor([0.2023, 0.2024, 0.2000, 0.1956, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19978837668895721 tensor([0.2023, 0.2015, 0.1998, 0.1955, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19708259403705597 tensor([0.2054, 0.2017, 0.2009, 0.1949, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19675147533416748 tensor([0.2012, 0.2062, 0.2005, 0.1968, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19592195749282837 tensor([0.2027, 0.2036, 0.2018, 0.1959, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19607016444206238 tensor([0.2040, 0.2024, 0.2018, 0.1957, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1968468427658081 tensor([0.2005, 0.2029, 0.2022, 0.1976, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19577056169509888 tensor([0.2040, 0.2022, 0.2021, 0.1959, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19738568365573883 tensor([0.2011, 0.2038, 0.2010, 0.1967, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19760429859161377 tensor([0.2014, 0.2039, 0.2028, 0.1976, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970849186182022 tensor([0.2022, 0.2034, 0.2023, 0.1971, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19870267808437347 tensor([0.2010, 0.2030, 0.2010, 0.1963, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19597187638282776 tensor([0.2044, 0.2030, 0.2016, 0.1951, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19613002240657806 tensor([0.2013, 0.2069, 0.2008, 0.1961, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19653142988681793 tensor([0.2023, 0.2032, 0.2027, 0.1965, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971946805715561 tensor([0.2015, 0.2045, 0.2012, 0.1972, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19887493550777435 tensor([0.2023, 0.2022, 0.1989, 0.1945, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19583247601985931 tensor([0.2035, 0.2032, 0.2016, 0.1958, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19645078480243683 tensor([0.2007, 0.2071, 0.2002, 0.1965, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19708745181560516 tensor([0.2016, 0.2031, 0.2030, 0.1971, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1987544447183609 tensor([0.2010, 0.2032, 0.2017, 0.1988, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1997952163219452 tensor([0.2023, 0.2013, 0.2004, 0.1963, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19735045731067657 tensor([0.2043, 0.2035, 0.2005, 0.1943, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19688884913921356 tensor([0.2021, 0.2042, 0.2015, 0.1954, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19615603983402252 tensor([0.2026, 0.2024, 0.2031, 0.1962, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1988644003868103 tensor([0.2016, 0.2021, 0.2019, 0.1989, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19859199225902557 tensor([0.2022, 0.2021, 0.2014, 0.1957, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19972297549247742 tensor([0.2037, 0.2016, 0.1999, 0.1951, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1961141973733902 tensor([0.2010, 0.2073, 0.2003, 0.1961, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19785000383853912 tensor([0.2016, 0.2033, 0.2007, 0.1964, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19905002415180206 tensor([0.2008, 0.2028, 0.2017, 0.1991, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19999544322490692 tensor([0.2006, 0.2030, 0.2000, 0.1950, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19686415791511536 tensor([0.2052, 0.2018, 0.2013, 0.1949, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19669149816036224 tensor([0.2014, 0.2047, 0.2018, 0.1967, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971554160118103 tensor([0.2025, 0.2022, 0.2016, 0.1972, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19761812686920166 tensor([0.2014, 0.2032, 0.2020, 0.1976, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1993773877620697 tensor([0.2019, 0.2020, 0.2007, 0.1961, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19663356244564056 tensor([0.2034, 0.2021, 0.2016, 0.1966, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19789698719978333 tensor([0.2008, 0.2038, 0.2015, 0.1979, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19721239805221558 tensor([0.2021, 0.2025, 0.2022, 0.1972, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19817931950092316 tensor([0.2022, 0.2022, 0.2024, 0.1982, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19794714450836182 tensor([0.2020, 0.2026, 0.2007, 0.1967, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19632473587989807 tensor([0.2034, 0.2030, 0.2022, 0.1963, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19687554240226746 tensor([0.2025, 0.2028, 0.2020, 0.1969, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1973327100276947 tensor([0.2019, 0.2027, 0.2017, 0.1964, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19954174757003784 tensor([0.2006, 0.2023, 0.2008, 0.1995, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668589532375336 tensor([0.2028, 0.2022, 0.2015, 0.1967, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967911720275879 tensor([0.2046, 0.2025, 0.2015, 0.1946, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.197489932179451 tensor([0.2011, 0.2031, 0.2005, 0.1978, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19668355584144592 tensor([0.2021, 0.2033, 0.2025, 0.1967, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1989833116531372 tensor([0.2017, 0.2022, 0.2015, 0.1990, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19772598147392273 tensor([0.2014, 0.2026, 0.2013, 0.1970, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19631020724773407 tensor([0.2041, 0.2026, 0.2015, 0.1955, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19699914753437042 tensor([0.2009, 0.2070, 0.2007, 0.1970, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19784921407699585 tensor([0.2017, 0.2023, 0.2020, 0.1978, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981571465730667 tensor([0.2012, 0.2031, 0.2005, 0.1970, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982368379831314 tensor([0.2035, 0.2029, 0.2005, 0.1950, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19642439484596252 tensor([0.2041, 0.2019, 0.2017, 0.1958, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19609279930591583 tensor([0.2023, 0.2046, 0.2012, 0.1958, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964975744485855 tensor([0.2030, 0.2017, 0.2030, 0.1957, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1979338377714157 tensor([0.2008, 0.2037, 0.2010, 0.1979, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20039616525173187 tensor([0.2007, 0.2022, 0.2004, 0.1963, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19699662923812866 tensor([0.2025, 0.2020, 0.2018, 0.1967, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983957141637802 tensor([0.2016, 0.2027, 0.2006, 0.1967, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19680678844451904 tensor([0.2016, 0.2036, 0.2023, 0.1968, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19797809422016144 tensor([0.2015, 0.2035, 0.2011, 0.1980, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978999227285385 tensor([0.2018, 0.2028, 0.2010, 0.1965, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 2], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [2, 3], [3, 4], [2, 3], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 2], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [2, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [1, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [0, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 2], [3, 4], [4, 3], [3, 4], [3, 4], [0, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [2, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 2, 3], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2, 3], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 3], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 1th iteration [[3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 2], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [2, 3], [3, 4], [2, 3], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 2], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [2, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [1, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [0, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 2], [3, 4], [4, 3], [3, 4], [3, 4], [0, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [2, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005222491617125224  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005222416990171603  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005222275489714087  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005222071477068149  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.00522181609781777  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005221509836553558  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005221162385087672  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0052207732588295044  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005220349726638173  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005219893242285504  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005219408167086966  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0052188974085862075  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005218364358917485  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005217809987262013  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00521723720116344  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00521664696980298  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005216042685314891  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005215425801470997  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.00521479534909008  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005214154235715788  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.20151318609714508 tensor([0.2044, 0.2040, 0.2015, 0.1936, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20186808705329895 tensor([0.2039, 0.2059, 0.2019, 0.1945, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20225033164024353 tensor([0.2028, 0.2047, 0.2023, 0.1954, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20182804763317108 tensor([0.2024, 0.2046, 0.2018, 0.1964, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19862070679664612 tensor([0.2028, 0.2054, 0.2007, 0.1925, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2025066614151001 tensor([0.2058, 0.2051, 0.2025, 0.1921, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20237281918525696 tensor([0.2033, 0.2067, 0.2024, 0.1944, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2030496746301651 tensor([0.2047, 0.2040, 0.2030, 0.1940, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20234514772891998 tensor([0.2028, 0.2045, 0.2023, 0.1963, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20257437229156494 tensor([0.2044, 0.2045, 0.2026, 0.1932, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20213466882705688 tensor([0.2058, 0.2026, 0.2021, 0.1937, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20183049142360687 tensor([0.2025, 0.2070, 0.2018, 0.1939, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20273248851299286 tensor([0.2027, 0.2050, 0.2036, 0.1953, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20272821187973022 tensor([0.2027, 0.2050, 0.2028, 0.1955, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2023334503173828 tensor([0.2038, 0.2043, 0.2023, 0.1933, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20226341485977173 tensor([0.2069, 0.2034, 0.2023, 0.1928, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20169107615947723 tensor([0.2036, 0.2035, 0.2017, 0.1945, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2025647759437561 tensor([0.2043, 0.2041, 0.2026, 0.1928, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20219293236732483 tensor([0.2030, 0.2051, 0.2022, 0.1955, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2000945508480072 tensor([0.2029, 0.2038, 0.2001, 0.1930, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2024870365858078 tensor([0.2053, 0.2041, 0.2025, 0.1932, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2016080766916275 tensor([0.2033, 0.2062, 0.2016, 0.1930, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2038961499929428 tensor([0.2039, 0.2044, 0.2051, 0.1938, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20221295952796936 tensor([0.2042, 0.2047, 0.2022, 0.1950, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20201314985752106 tensor([0.2030, 0.2032, 0.2020, 0.1947, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20274487137794495 tensor([0.2055, 0.2046, 0.2027, 0.1933, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20235617458820343 tensor([0.2031, 0.2054, 0.2024, 0.1941, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20384809374809265 tensor([0.2038, 0.2049, 0.2040, 0.1938, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20210444927215576 tensor([0.2021, 0.2049, 0.2028, 0.1969, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2023884505033493 tensor([0.2040, 0.2033, 0.2024, 0.1936, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20265057682991028 tensor([0.2058, 0.2055, 0.2027, 0.1928, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2029615044593811 tensor([0.2030, 0.2071, 0.2030, 0.1948, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19845275580883026 tensor([0.2037, 0.2032, 0.2018, 0.1928, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2024856060743332 tensor([0.2025, 0.2049, 0.2025, 0.1967, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20206031203269958 tensor([0.2036, 0.2041, 0.2021, 0.1932, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20308078825473785 tensor([0.2049, 0.2044, 0.2031, 0.1933, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2019956111907959 tensor([0.2041, 0.2053, 0.2020, 0.1932, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2032676488161087 tensor([0.2034, 0.2043, 0.2033, 0.1946, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20193931460380554 tensor([0.2030, 0.2049, 0.2019, 0.1957, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19930224120616913 tensor([0.2023, 0.2027, 0.2010, 0.1948, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20233459770679474 tensor([0.2049, 0.2042, 0.2023, 0.1936, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1958821564912796 tensor([0.2024, 0.2064, 0.2010, 0.1943, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20304197072982788 tensor([0.2030, 0.2041, 0.2047, 0.1948, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.202183336019516 tensor([0.2026, 0.2043, 0.2022, 0.1967, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2018895447254181 tensor([0.2037, 0.2047, 0.2019, 0.1947, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20313864946365356 tensor([0.2050, 0.2035, 0.2031, 0.1941, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20218265056610107 tensor([0.2032, 0.2045, 0.2022, 0.1944, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20334523916244507 tensor([0.2033, 0.2047, 0.2038, 0.1940, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2028965950012207 tensor([0.2035, 0.2053, 0.2029, 0.1950, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2011062055826187 tensor([0.2031, 0.2041, 0.2011, 0.1941, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2032177597284317 tensor([0.2049, 0.2040, 0.2032, 0.1940, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20197612047195435 tensor([0.2030, 0.2065, 0.2020, 0.1944, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20233498513698578 tensor([0.2041, 0.2036, 0.2023, 0.1936, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2022225558757782 tensor([0.2022, 0.2050, 0.2030, 0.1962, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19852310419082642 tensor([0.2033, 0.2050, 0.2004, 0.1928, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20255185663700104 tensor([0.2048, 0.2032, 0.2026, 0.1946, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20192669332027435 tensor([0.2032, 0.2047, 0.2019, 0.1958, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20281124114990234 tensor([0.2044, 0.2037, 0.2028, 0.1940, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20312045514583588 tensor([0.2031, 0.2046, 0.2034, 0.1950, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20243030786514282 tensor([0.2049, 0.2038, 0.2024, 0.1934, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20298035442829132 tensor([0.2042, 0.2047, 0.2030, 0.1938, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20254100859165192 tensor([0.2032, 0.2064, 0.2025, 0.1943, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20352628827095032 tensor([0.2051, 0.2041, 0.2035, 0.1933, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20319895446300507 tensor([0.2034, 0.2049, 0.2032, 0.1953, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20151455700397491 tensor([0.2027, 0.2049, 0.2015, 0.1935, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20282243192195892 tensor([0.2048, 0.2036, 0.2028, 0.1943, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20200879871845245 tensor([0.2020, 0.2064, 0.2023, 0.1944, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20261657238006592 tensor([0.2039, 0.2046, 0.2026, 0.1940, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2028338611125946 tensor([0.2042, 0.2042, 0.2028, 0.1953, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20223486423492432 tensor([0.2030, 0.2031, 0.2022, 0.1940, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20183463394641876 tensor([0.2047, 0.2052, 0.2018, 0.1933, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1955036073923111 tensor([0.2025, 0.2064, 0.2010, 0.1946, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20378656685352325 tensor([0.2045, 0.2038, 0.2041, 0.1945, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20280654728412628 tensor([0.2028, 0.2045, 0.2037, 0.1964, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20164144039154053 tensor([0.2030, 0.2055, 0.2016, 0.1935, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20278434455394745 tensor([0.2058, 0.2034, 0.2028, 0.1939, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2021881639957428 tensor([0.2023, 0.2076, 0.2022, 0.1945, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20268942415714264 tensor([0.2027, 0.2042, 0.2045, 0.1945, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20273272693157196 tensor([0.2031, 0.2038, 0.2027, 0.1971, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20211338996887207 tensor([0.2041, 0.2045, 0.2021, 0.1938, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20181851089000702 tensor([0.2058, 0.2049, 0.2018, 0.1924, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20092418789863586 tensor([0.2030, 0.2064, 0.2009, 0.1938, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20356632769107819 tensor([0.2038, 0.2041, 0.2036, 0.1944, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20196828246116638 tensor([0.2039, 0.2057, 0.2020, 0.1947, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1984025537967682 tensor([0.2021, 0.2042, 0.2014, 0.1938, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20343422889709473 tensor([0.2051, 0.2038, 0.2034, 0.1935, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20200489461421967 tensor([0.2020, 0.2072, 0.2025, 0.1953, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20237867534160614 tensor([0.2037, 0.2036, 0.2024, 0.1941, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20264381170272827 tensor([0.2026, 0.2040, 0.2028, 0.1968, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20319239795207977 tensor([0.2048, 0.2032, 0.2036, 0.1937, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202836275100708 tensor([0.2045, 0.2028, 0.2031, 0.1937, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20172762870788574 tensor([0.2017, 0.2075, 0.2018, 0.1953, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2022460252046585 tensor([0.2051, 0.2039, 0.2022, 0.1931, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20208001136779785 tensor([0.2030, 0.2051, 0.2021, 0.1962, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20286889374256134 tensor([0.2030, 0.2040, 0.2029, 0.1951, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20222176611423492 tensor([0.2047, 0.2031, 0.2022, 0.1938, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2030320167541504 tensor([0.2035, 0.2050, 0.2030, 0.1941, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20364856719970703 tensor([0.2036, 0.2041, 0.2042, 0.1941, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2032340168952942 tensor([0.2035, 0.2037, 0.2032, 0.1961, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19925735890865326 tensor([0.2033, 0.2031, 0.2010, 0.1933, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2030179798603058 tensor([0.2056, 0.2033, 0.2030, 0.1931, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2016645073890686 tensor([0.2037, 0.2072, 0.2017, 0.1938, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20378723740577698 tensor([0.2038, 0.2043, 0.2041, 0.1944, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20253175497055054 tensor([0.2030, 0.2048, 0.2025, 0.1950, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19569677114486694 tensor([0.2038, 0.2023, 0.2024, 0.1957, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029791921377182 tensor([0.2055, 0.2039, 0.2030, 0.1937, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20267117023468018 tensor([0.2027, 0.2055, 0.2034, 0.1951, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.203829824924469 tensor([0.2038, 0.2046, 0.2039, 0.1940, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20275996625423431 tensor([0.2034, 0.2055, 0.2028, 0.1947, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2020411491394043 tensor([0.2028, 0.2041, 0.2020, 0.1938, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20253989100456238 tensor([0.2060, 0.2037, 0.2025, 0.1925, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20290297269821167 tensor([0.2037, 0.2031, 0.2029, 0.1941, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.203769251704216 tensor([0.2039, 0.2038, 0.2040, 0.1948, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2020939439535141 tensor([0.2037, 0.2047, 0.2021, 0.1946, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19848813116550446 tensor([0.2026, 0.2051, 0.2012, 0.1926, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20230524241924286 tensor([0.2056, 0.2048, 0.2023, 0.1931, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20111724734306335 tensor([0.2016, 0.2080, 0.2011, 0.1953, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20287178456783295 tensor([0.2041, 0.2047, 0.2029, 0.1944, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20262132585048676 tensor([0.2035, 0.2052, 0.2026, 0.1948, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19859594106674194 tensor([0.2037, 0.2037, 0.2008, 0.1932, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2022964060306549 tensor([0.2055, 0.2047, 0.2023, 0.1937, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2011888474225998 tensor([0.2035, 0.2044, 0.2012, 0.1934, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2027696669101715 tensor([0.2028, 0.2038, 0.2035, 0.1958, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20237167179584503 tensor([0.2036, 0.2042, 0.2024, 0.1944, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2018398493528366 tensor([0.2029, 0.2042, 0.2018, 0.1943, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20249329507350922 tensor([0.2059, 0.2039, 0.2025, 0.1927, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20138917863368988 tensor([0.2028, 0.2081, 0.2014, 0.1939, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20298375189304352 tensor([0.2049, 0.2037, 0.2030, 0.1928, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20232921838760376 tensor([0.2035, 0.2041, 0.2023, 0.1966, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1994558572769165 tensor([0.2027, 0.2040, 0.2004, 0.1934, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20309191942214966 tensor([0.2046, 0.2035, 0.2031, 0.1943, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20179323852062225 tensor([0.2026, 0.2073, 0.2018, 0.1946, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20358796417713165 tensor([0.2052, 0.2036, 0.2038, 0.1932, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20345836877822876 tensor([0.2035, 0.2044, 0.2035, 0.1959, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20179550349712372 tensor([0.2027, 0.2043, 0.2018, 0.1949, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20329028367996216 tensor([0.2050, 0.2041, 0.2033, 0.1936, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20250669121742249 tensor([0.2025, 0.2055, 0.2033, 0.1954, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20305491983890533 tensor([0.2050, 0.2043, 0.2031, 0.1933, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20153369009494781 tensor([0.2028, 0.2059, 0.2015, 0.1950, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2014993280172348 tensor([0.2024, 0.2049, 0.2015, 0.1930, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20223040878772736 tensor([0.2057, 0.2047, 0.2022, 0.1929, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20282834768295288 tensor([0.2035, 0.2043, 0.2028, 0.1950, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20322147011756897 tensor([0.2045, 0.2032, 0.2038, 0.1940, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20241494476795197 tensor([0.2035, 0.2055, 0.2024, 0.1949, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20183974504470825 tensor([0.2033, 0.2036, 0.2018, 0.1939, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2020684778690338 tensor([0.2066, 0.2042, 0.2021, 0.1919, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2018953412771225 tensor([0.2019, 0.2083, 0.2020, 0.1941, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2028399109840393 tensor([0.2041, 0.2034, 0.2028, 0.1938, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20336006581783295 tensor([0.2036, 0.2042, 0.2034, 0.1953, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2024594098329544 tensor([0.2033, 0.2049, 0.2025, 0.1942, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20270907878875732 tensor([0.2055, 0.2041, 0.2027, 0.1929, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20178036391735077 tensor([0.2025, 0.2076, 0.2018, 0.1952, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2031281590461731 tensor([0.2031, 0.2046, 0.2041, 0.1947, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20200501382350922 tensor([0.2027, 0.2057, 0.2020, 0.1954, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20254363119602203 tensor([0.2036, 0.2028, 0.2025, 0.1949, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20199479162693024 tensor([0.2068, 0.2041, 0.2020, 0.1918, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2014317363500595 tensor([0.2027, 0.2057, 0.2014, 0.1944, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20266041159629822 tensor([0.2038, 0.2048, 0.2027, 0.1950, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20257826149463654 tensor([0.2026, 0.2059, 0.2026, 0.1957, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20239940285682678 tensor([0.2024, 0.2064, 0.2025, 0.1945, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20376230776309967 tensor([0.2056, 0.2041, 0.2038, 0.1931, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20242196321487427 tensor([0.2030, 0.2043, 0.2024, 0.1944, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20297212898731232 tensor([0.2040, 0.2038, 0.2030, 0.1940, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20235368609428406 tensor([0.2024, 0.2038, 0.2030, 0.1968, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962268352508545 tensor([0.2025, 0.2041, 0.2020, 0.1951, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20242679119110107 tensor([0.2062, 0.2040, 0.2024, 0.1927, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20234315097332 tensor([0.2023, 0.2064, 0.2023, 0.1949, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20320792496204376 tensor([0.2047, 0.2046, 0.2032, 0.1930, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20222288370132446 tensor([0.2029, 0.2034, 0.2022, 0.1973, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19829656183719635 tensor([0.2038, 0.2034, 0.2010, 0.1935, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20232123136520386 tensor([0.2044, 0.2042, 0.2023, 0.1933, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20194660127162933 tensor([0.2026, 0.2061, 0.2019, 0.1949, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20269568264484406 tensor([0.2027, 0.2046, 0.2039, 0.1951, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20275206863880157 tensor([0.2033, 0.2041, 0.2028, 0.1965, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19924503564834595 tensor([0.2027, 0.2042, 0.2010, 0.1929, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20326721668243408 tensor([0.2047, 0.2036, 0.2033, 0.1940, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20125289261341095 tensor([0.2023, 0.2060, 0.2013, 0.1942, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20363639295101166 tensor([0.2036, 0.2046, 0.2038, 0.1944, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20266206562519073 tensor([0.2040, 0.2041, 0.2027, 0.1949, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20251889526844025 tensor([0.2030, 0.2045, 0.2025, 0.1946, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20277391374111176 tensor([0.2051, 0.2029, 0.2028, 0.1930, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20165316760540009 tensor([0.2031, 0.2050, 0.2017, 0.1938, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029401957988739 tensor([0.2044, 0.2035, 0.2029, 0.1946, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2011074423789978 tensor([0.2039, 0.2040, 0.2011, 0.1932, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19918082654476166 tensor([0.2038, 0.2031, 0.2008, 0.1931, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20193883776664734 tensor([0.2070, 0.2032, 0.2019, 0.1926, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20159126818180084 tensor([0.2027, 0.2077, 0.2016, 0.1943, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029072493314743 tensor([0.2043, 0.2052, 0.2029, 0.1935, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20288531482219696 tensor([0.2056, 0.2039, 0.2029, 0.1933, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19518770277500153 tensor([0.2020, 0.2044, 0.2033, 0.1952, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20316511392593384 tensor([0.2056, 0.2037, 0.2032, 0.1935, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20208196341991425 tensor([0.2026, 0.2054, 0.2021, 0.1943, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20291443169116974 tensor([0.2029, 0.2054, 0.2038, 0.1952, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20337380468845367 tensor([0.2037, 0.2049, 0.2034, 0.1947, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2020973414182663 tensor([0.2026, 0.2045, 0.2021, 0.1939, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20263248682022095 tensor([0.2060, 0.2045, 0.2026, 0.1927, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2018902748823166 tensor([0.2028, 0.2085, 0.2019, 0.1937, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20379012823104858 tensor([0.2038, 0.2047, 0.2038, 0.1941, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20229849219322205 tensor([0.2031, 0.2060, 0.2023, 0.1948, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20030628144741058 tensor([0.2039, 0.2037, 0.1999, 0.1922, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20265483856201172 tensor([0.2051, 0.2048, 0.2027, 0.1934, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20124566555023193 tensor([0.2022, 0.2086, 0.2012, 0.1940, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20310761034488678 tensor([0.2031, 0.2047, 0.2041, 0.1947, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20261262357234955 tensor([0.2026, 0.2047, 0.2027, 0.1963, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2014833688735962 tensor([0.2038, 0.2028, 0.2015, 0.1939, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20153966546058655 tensor([0.2059, 0.2050, 0.2015, 0.1919, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2025185376405716 tensor([0.2036, 0.2057, 0.2025, 0.1930, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20390237867832184 tensor([0.2042, 0.2039, 0.2042, 0.1937, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20295163989067078 tensor([0.2032, 0.2036, 0.2030, 0.1964, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2024950087070465 tensor([0.2038, 0.2036, 0.2025, 0.1933, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20092053711414337 tensor([0.2053, 0.2031, 0.2009, 0.1928, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20138271152973175 tensor([0.2026, 0.2088, 0.2014, 0.1937, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20178262889385223 tensor([0.2032, 0.2049, 0.2018, 0.1940, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20235653221607208 tensor([0.2024, 0.2043, 0.2027, 0.1966, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19962738454341888 tensor([0.2022, 0.2045, 0.2011, 0.1926, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20233075320720673 tensor([0.2067, 0.2033, 0.2023, 0.1925, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20286235213279724 tensor([0.2029, 0.2062, 0.2029, 0.1943, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2026393711566925 tensor([0.2041, 0.2037, 0.2026, 0.1947, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20299576222896576 tensor([0.2030, 0.2047, 0.2031, 0.1952, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20175737142562866 tensor([0.2034, 0.2035, 0.2018, 0.1937, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20264363288879395 tensor([0.2050, 0.2036, 0.2026, 0.1942, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20229864120483398 tensor([0.2023, 0.2053, 0.2026, 0.1955, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20321999490261078 tensor([0.2037, 0.2041, 0.2032, 0.1948, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20342803001403809 tensor([0.2037, 0.2037, 0.2034, 0.1958, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20177552103996277 tensor([0.2036, 0.2042, 0.2018, 0.1943, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20324097573757172 tensor([0.2050, 0.2045, 0.2032, 0.1939, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20305566489696503 tensor([0.2041, 0.2043, 0.2031, 0.1945, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20273767411708832 tensor([0.2035, 0.2042, 0.2027, 0.1939, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20189683139324188 tensor([0.2022, 0.2039, 0.2019, 0.1971, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2025713473558426 tensor([0.2044, 0.2038, 0.2026, 0.1943, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20253251492977142 tensor([0.2062, 0.2040, 0.2025, 0.1922, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19542016088962555 tensor([0.2027, 0.2046, 0.2015, 0.1954, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20358885824680328 tensor([0.2037, 0.2049, 0.2036, 0.1942, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.202539324760437 tensor([0.2033, 0.2038, 0.2025, 0.1965, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2023666948080063 tensor([0.2029, 0.2041, 0.2024, 0.1946, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20252400636672974 tensor([0.2056, 0.2041, 0.2025, 0.1932, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2017219513654709 tensor([0.2024, 0.2086, 0.2017, 0.1946, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20309454202651978 tensor([0.2033, 0.2038, 0.2031, 0.1954, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20157086849212646 tensor([0.2028, 0.2046, 0.2016, 0.1946, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20154735445976257 tensor([0.2050, 0.2044, 0.2015, 0.1926, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20275145769119263 tensor([0.2057, 0.2034, 0.2028, 0.1934, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20228584110736847 tensor([0.2039, 0.2061, 0.2023, 0.1934, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20325879752635956 tensor([0.2046, 0.2033, 0.2040, 0.1933, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20210987329483032 tensor([0.2024, 0.2053, 0.2021, 0.1955, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19864872097969055 tensor([0.2023, 0.2037, 0.2015, 0.1939, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2028270661830902 tensor([0.2041, 0.2036, 0.2028, 0.1943, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20166030526161194 tensor([0.2032, 0.2042, 0.2017, 0.1943, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20318521559238434 tensor([0.2032, 0.2051, 0.2034, 0.1944, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20218265056610107 tensor([0.2030, 0.2050, 0.2022, 0.1955, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20207008719444275 tensor([0.2033, 0.2043, 0.2021, 0.1942, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 2], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [2, 3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [1, 4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [0, 3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 2, 4], [3, 4], [4, 3], [3, 4], [3, 4], [0, 4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [2, 4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 2th iteration [[3, 2, 4], [3, 4], [2, 3, 4], [2, 3, 4], [3, 2, 4], [2, 3, 4], [3, 4], [3, 2, 4], [1, 4, 3], [3, 4], [3, 2, 4], [3, 2, 4], [0, 3, 4], [3, 2, 4], [3, 2, 4], [3, 2, 4], [0, 4, 3], [3, 2, 4], [2, 4, 3], [3, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.06426020860671997  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.06425823569297791  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.06425453424453735  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.06424922943115234  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.06424251198768616  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.06423452496528625  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.06422538757324218  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.06421524286270142  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.06420416831970215  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.06419228911399841  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.06417967081069946  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.06416641473770142  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.06415256261825561  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.0641382098197937  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.06412338614463806  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.06410816907882691  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.06409258246421815  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.06407667398452759  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0640604853630066  Accuracy on Support set:0.0\n",
      "torch.Size([20, 2048]) torch.Size([20])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0640440583229065  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.20305372774600983 tensor([0.2059, 0.2055, 0.2031, 0.1940, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20338696241378784 tensor([0.2053, 0.2074, 0.2034, 0.1948, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20374560356140137 tensor([0.2042, 0.2062, 0.2037, 0.1958, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2033756524324417 tensor([0.2038, 0.2061, 0.2034, 0.1967, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20423828065395355 tensor([0.2042, 0.2069, 0.2022, 0.1929, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040203958749771 tensor([0.2072, 0.2066, 0.2040, 0.1925, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038823664188385 tensor([0.2047, 0.2082, 0.2039, 0.1947, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2045879364013672 tensor([0.2061, 0.2055, 0.2046, 0.1944, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20387114584445953 tensor([0.2042, 0.2061, 0.2039, 0.1967, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.204132080078125 tensor([0.2058, 0.2060, 0.2041, 0.1936, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2036631852388382 tensor([0.2072, 0.2041, 0.2037, 0.1941, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20335012674331665 tensor([0.2039, 0.2085, 0.2034, 0.1943, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2041330635547638 tensor([0.2041, 0.2065, 0.2051, 0.1956, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20411117374897003 tensor([0.2041, 0.2065, 0.2043, 0.1959, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20389899611473083 tensor([0.2052, 0.2059, 0.2039, 0.1937, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20377658307552338 tensor([0.2083, 0.2050, 0.2038, 0.1931, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20322994887828827 tensor([0.2050, 0.2050, 0.2032, 0.1949, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20412953197956085 tensor([0.2057, 0.2056, 0.2041, 0.1932, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2037273794412613 tensor([0.2044, 0.2066, 0.2037, 0.1959, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1951073408126831 tensor([0.2044, 0.2054, 0.2017, 0.1935, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20402370393276215 tensor([0.2067, 0.2056, 0.2040, 0.1936, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20315413177013397 tensor([0.2047, 0.2078, 0.2032, 0.1934, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2052101045846939 tensor([0.2052, 0.2059, 0.2066, 0.1941, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20369893312454224 tensor([0.2056, 0.2062, 0.2037, 0.1953, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20356430113315582 tensor([0.2045, 0.2047, 0.2036, 0.1951, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2042650580406189 tensor([0.2069, 0.2061, 0.2043, 0.1937, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038835883140564 tensor([0.2045, 0.2070, 0.2039, 0.1945, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20523589849472046 tensor([0.2052, 0.2064, 0.2055, 0.1942, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20351243019104004 tensor([0.2035, 0.2064, 0.2042, 0.1972, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20392830669879913 tensor([0.2054, 0.2049, 0.2039, 0.1939, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20415234565734863 tensor([0.2071, 0.2070, 0.2042, 0.1932, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20431184768676758 tensor([0.2043, 0.2086, 0.2045, 0.1951, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2033434510231018 tensor([0.2052, 0.2048, 0.2033, 0.1933, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20387490093708038 tensor([0.2039, 0.2064, 0.2040, 0.1971, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20360741019248962 tensor([0.2050, 0.2056, 0.2036, 0.1936, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046126276254654 tensor([0.2063, 0.2060, 0.2046, 0.1936, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20351634919643402 tensor([0.2055, 0.2068, 0.2035, 0.1936, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20481087267398834 tensor([0.2048, 0.2058, 0.2048, 0.1950, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20346307754516602 tensor([0.2044, 0.2064, 0.2035, 0.1961, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2036905437707901 tensor([0.2037, 0.2042, 0.2026, 0.1952, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038666307926178 tensor([0.2064, 0.2057, 0.2039, 0.1940, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20382149517536163 tensor([0.2038, 0.2080, 0.2026, 0.1947, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20442330837249756 tensor([0.2044, 0.2056, 0.2063, 0.1952, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20370769500732422 tensor([0.2040, 0.2059, 0.2037, 0.1971, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20341302454471588 tensor([0.2051, 0.2062, 0.2034, 0.1950, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046552151441574 tensor([0.2064, 0.2050, 0.2047, 0.1945, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20373207330703735 tensor([0.2046, 0.2060, 0.2037, 0.1947, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20472893118858337 tensor([0.2047, 0.2062, 0.2053, 0.1944, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2043902426958084 tensor([0.2049, 0.2068, 0.2044, 0.1953, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2026745080947876 tensor([0.2045, 0.2057, 0.2027, 0.1945, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20472902059555054 tensor([0.2063, 0.2055, 0.2047, 0.1943, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20351409912109375 tensor([0.2044, 0.2080, 0.2035, 0.1948, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20389144122600555 tensor([0.2055, 0.2051, 0.2039, 0.1940, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20361925661563873 tensor([0.2036, 0.2065, 0.2045, 0.1965, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20470160245895386 tensor([0.2047, 0.2066, 0.2020, 0.1932, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20408523082733154 tensor([0.2062, 0.2047, 0.2041, 0.1950, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20346467196941376 tensor([0.2045, 0.2062, 0.2035, 0.1961, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20432807505130768 tensor([0.2058, 0.2052, 0.2043, 0.1944, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20450729131698608 tensor([0.2045, 0.2061, 0.2049, 0.1953, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20395708084106445 tensor([0.2063, 0.2054, 0.2040, 0.1938, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2045121192932129 tensor([0.2056, 0.2062, 0.2045, 0.1942, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040611356496811 tensor([0.2046, 0.2080, 0.2041, 0.1947, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20506083965301514 tensor([0.2065, 0.2057, 0.2051, 0.1937, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20473845303058624 tensor([0.2048, 0.2065, 0.2047, 0.1956, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20308898389339447 tensor([0.2041, 0.2065, 0.2031, 0.1939, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20432893931865692 tensor([0.2061, 0.2051, 0.2043, 0.1947, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20338284969329834 tensor([0.2034, 0.2079, 0.2039, 0.1948, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20414003729820251 tensor([0.2052, 0.2061, 0.2041, 0.1943, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20434996485710144 tensor([0.2056, 0.2057, 0.2043, 0.1956, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20380300283432007 tensor([0.2045, 0.2046, 0.2038, 0.1944, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20336322486400604 tensor([0.2061, 0.2067, 0.2034, 0.1937, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20387901365756989 tensor([0.2039, 0.2079, 0.2025, 0.1950, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20529158413410187 tensor([0.2058, 0.2053, 0.2056, 0.1949, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20417580008506775 tensor([0.2042, 0.2060, 0.2052, 0.1967, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2032010555267334 tensor([0.2044, 0.2070, 0.2032, 0.1939, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2043069303035736 tensor([0.2072, 0.2049, 0.2043, 0.1942, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20363813638687134 tensor([0.2036, 0.2091, 0.2037, 0.1948, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.204075425863266 tensor([0.2041, 0.2057, 0.2061, 0.1949, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20423907041549683 tensor([0.2045, 0.2052, 0.2042, 0.1974, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2036578357219696 tensor([0.2055, 0.2060, 0.2037, 0.1942, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2033134400844574 tensor([0.2072, 0.2064, 0.2033, 0.1928, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2024725377559662 tensor([0.2044, 0.2079, 0.2025, 0.1942, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20506276190280914 tensor([0.2052, 0.2056, 0.2051, 0.1948, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20349599421024323 tensor([0.2053, 0.2072, 0.2035, 0.1950, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20303432643413544 tensor([0.2036, 0.2058, 0.2030, 0.1942, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20493890345096588 tensor([0.2064, 0.2053, 0.2049, 0.1939, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20336681604385376 tensor([0.2034, 0.2087, 0.2040, 0.1957, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20391735434532166 tensor([0.2051, 0.2052, 0.2039, 0.1945, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.204046830534935 tensor([0.2040, 0.2055, 0.2043, 0.1971, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20468708872795105 tensor([0.2062, 0.2047, 0.2051, 0.1941, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20435263216495514 tensor([0.2059, 0.2044, 0.2046, 0.1940, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20312032103538513 tensor([0.2031, 0.2090, 0.2033, 0.1956, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2037932425737381 tensor([0.2065, 0.2054, 0.2038, 0.1935, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20361565053462982 tensor([0.2044, 0.2067, 0.2036, 0.1966, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20436488091945648 tensor([0.2044, 0.2055, 0.2044, 0.1955, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20377056300640106 tensor([0.2061, 0.2046, 0.2038, 0.1941, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2045641392469406 tensor([0.2050, 0.2065, 0.2046, 0.1944, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2050497978925705 tensor([0.2050, 0.2056, 0.2058, 0.1944, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20471391081809998 tensor([0.2049, 0.2052, 0.2047, 0.1965, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463718473911285 tensor([0.2048, 0.2046, 0.2026, 0.1937, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20453667640686035 tensor([0.2070, 0.2048, 0.2045, 0.1935, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2031727284193039 tensor([0.2051, 0.2088, 0.2032, 0.1941, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20517641305923462 tensor([0.2052, 0.2058, 0.2056, 0.1948, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20407147705554962 tensor([0.2045, 0.2063, 0.2041, 0.1954, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2039673775434494 tensor([0.2052, 0.2039, 0.2040, 0.1961, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20449312031269073 tensor([0.2069, 0.2054, 0.2045, 0.1941, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20405533909797668 tensor([0.2041, 0.2070, 0.2049, 0.1955, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2052256464958191 tensor([0.2052, 0.2061, 0.2055, 0.1944, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20427770912647247 tensor([0.2048, 0.2070, 0.2043, 0.1950, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.203607976436615 tensor([0.2042, 0.2056, 0.2036, 0.1942, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20405840873718262 tensor([0.2075, 0.2052, 0.2041, 0.1929, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2044527232646942 tensor([0.2051, 0.2046, 0.2045, 0.1945, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20524756610393524 tensor([0.2053, 0.2052, 0.2055, 0.1952, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20363667607307434 tensor([0.2051, 0.2062, 0.2036, 0.1950, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20277178287506104 tensor([0.2041, 0.2066, 0.2028, 0.1931, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20382560789585114 tensor([0.2070, 0.2064, 0.2038, 0.1934, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.202627032995224 tensor([0.2030, 0.2095, 0.2026, 0.1957, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20439830422401428 tensor([0.2054, 0.2063, 0.2044, 0.1948, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20413146913051605 tensor([0.2049, 0.2067, 0.2041, 0.1951, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20518413186073303 tensor([0.2052, 0.2052, 0.2024, 0.1936, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20380647480487823 tensor([0.2069, 0.2062, 0.2038, 0.1940, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2027583122253418 tensor([0.2049, 0.2060, 0.2028, 0.1937, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2041943222284317 tensor([0.2042, 0.2053, 0.2050, 0.1962, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2039223611354828 tensor([0.2050, 0.2058, 0.2039, 0.1948, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20340359210968018 tensor([0.2043, 0.2057, 0.2034, 0.1947, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20401735603809357 tensor([0.2072, 0.2054, 0.2040, 0.1931, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029123157262802 tensor([0.2042, 0.2096, 0.2029, 0.1942, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20452770590782166 tensor([0.2064, 0.2052, 0.2045, 0.1932, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20384745299816132 tensor([0.2049, 0.2056, 0.2038, 0.1970, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20414415001869202 tensor([0.2041, 0.2055, 0.2020, 0.1939, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20462143421173096 tensor([0.2060, 0.2050, 0.2046, 0.1946, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20331639051437378 tensor([0.2039, 0.2088, 0.2033, 0.1949, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2051038295030594 tensor([0.2065, 0.2051, 0.2053, 0.1936, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20490442216396332 tensor([0.2049, 0.2059, 0.2050, 0.1962, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2033490240573883 tensor([0.2041, 0.2058, 0.2033, 0.1953, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2047864943742752 tensor([0.2064, 0.2057, 0.2048, 0.1939, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20385876297950745 tensor([0.2039, 0.2070, 0.2048, 0.1958, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20457646250724792 tensor([0.2065, 0.2058, 0.2046, 0.1936, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20307734608650208 tensor([0.2042, 0.2074, 0.2031, 0.1954, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20307758450508118 tensor([0.2038, 0.2064, 0.2031, 0.1934, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20375321805477142 tensor([0.2071, 0.2062, 0.2038, 0.1932, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20436705648899078 tensor([0.2049, 0.2058, 0.2044, 0.1954, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047518938779831 tensor([0.2058, 0.2048, 0.2054, 0.1944, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20393988490104675 tensor([0.2048, 0.2071, 0.2039, 0.1952, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20340096950531006 tensor([0.2048, 0.2051, 0.2034, 0.1943, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20360809564590454 tensor([0.2081, 0.2057, 0.2036, 0.1922, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2032725065946579 tensor([0.2033, 0.2099, 0.2035, 0.1945, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20439180731773376 tensor([0.2055, 0.2049, 0.2044, 0.1942, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20487266778945923 tensor([0.2050, 0.2057, 0.2049, 0.1956, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2039967030286789 tensor([0.2047, 0.2064, 0.2040, 0.1946, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20422472059726715 tensor([0.2068, 0.2056, 0.2042, 0.1932, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20325854420661926 tensor([0.2039, 0.2091, 0.2033, 0.1955, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2045482099056244 tensor([0.2045, 0.2061, 0.2057, 0.1951, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20354706048965454 tensor([0.2041, 0.2072, 0.2035, 0.1958, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20410029590129852 tensor([0.2050, 0.2043, 0.2041, 0.1952, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20349262654781342 tensor([0.2081, 0.2056, 0.2035, 0.1921, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029573917388916 tensor([0.2041, 0.2072, 0.2030, 0.1948, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20412921905517578 tensor([0.2051, 0.2063, 0.2041, 0.1953, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20396839082241058 tensor([0.2040, 0.2074, 0.2041, 0.1960, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2037716656923294 tensor([0.2038, 0.2080, 0.2040, 0.1949, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20522421598434448 tensor([0.2069, 0.2057, 0.2052, 0.1934, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20395442843437195 tensor([0.2044, 0.2058, 0.2040, 0.1948, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20451217889785767 tensor([0.2054, 0.2053, 0.2045, 0.1944, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20372681319713593 tensor([0.2037, 0.2053, 0.2044, 0.1972, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20356465876102448 tensor([0.2039, 0.2057, 0.2036, 0.1955, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2039463371038437 tensor([0.2076, 0.2055, 0.2039, 0.1931, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20374077558517456 tensor([0.2037, 0.2079, 0.2039, 0.1953, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20473670959472656 tensor([0.2061, 0.2061, 0.2047, 0.1933, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20374493300914764 tensor([0.2043, 0.2049, 0.2037, 0.1977, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2049866020679474 tensor([0.2053, 0.2050, 0.2025, 0.1939, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038646936416626 tensor([0.2058, 0.2058, 0.2039, 0.1936, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20346388220787048 tensor([0.2040, 0.2077, 0.2035, 0.1953, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20410437881946564 tensor([0.2041, 0.2061, 0.2054, 0.1955, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2042737752199173 tensor([0.2047, 0.2056, 0.2043, 0.1968, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20413219928741455 tensor([0.2041, 0.2057, 0.2026, 0.1933, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20478221774101257 tensor([0.2060, 0.2051, 0.2048, 0.1944, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2028091847896576 tensor([0.2037, 0.2075, 0.2028, 0.1946, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20501144230365753 tensor([0.2050, 0.2061, 0.2054, 0.1948, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20419615507125854 tensor([0.2054, 0.2056, 0.2042, 0.1953, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20405156910419464 tensor([0.2044, 0.2060, 0.2041, 0.1950, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20434202253818512 tensor([0.2065, 0.2044, 0.2043, 0.1934, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20319148898124695 tensor([0.2045, 0.2065, 0.2032, 0.1942, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20446914434432983 tensor([0.2058, 0.2050, 0.2045, 0.1950, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.202675461769104 tensor([0.2054, 0.2055, 0.2027, 0.1936, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045930027961731 tensor([0.2053, 0.2046, 0.2024, 0.1935, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2034602165222168 tensor([0.2083, 0.2047, 0.2035, 0.1929, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2031291127204895 tensor([0.2041, 0.2093, 0.2031, 0.1947, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20442458987236023 tensor([0.2056, 0.2067, 0.2044, 0.1938, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2043970823287964 tensor([0.2070, 0.2054, 0.2044, 0.1936, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20483680069446564 tensor([0.2034, 0.2059, 0.2048, 0.1956, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20468413829803467 tensor([0.2070, 0.2052, 0.2047, 0.1938, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20362071692943573 tensor([0.2040, 0.2069, 0.2036, 0.1946, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20430050790309906 tensor([0.2043, 0.2069, 0.2054, 0.1955, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2048807293176651 tensor([0.2051, 0.2064, 0.2049, 0.1950, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2036661058664322 tensor([0.2040, 0.2061, 0.2037, 0.1942, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20414003729820251 tensor([0.2074, 0.2060, 0.2041, 0.1930, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20340405404567719 tensor([0.2042, 0.2100, 0.2034, 0.1940, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20517730712890625 tensor([0.2052, 0.2062, 0.2053, 0.1945, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038150578737259 tensor([0.2045, 0.2075, 0.2038, 0.1951, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19531653821468353 tensor([0.2053, 0.2053, 0.2015, 0.1926, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20417845249176025 tensor([0.2065, 0.2063, 0.2042, 0.1937, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2027774453163147 tensor([0.2036, 0.2101, 0.2028, 0.1944, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20451664924621582 tensor([0.2045, 0.2061, 0.2056, 0.1950, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20400755107402802 tensor([0.2040, 0.2062, 0.2042, 0.1966, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20303687453269958 tensor([0.2053, 0.2043, 0.2030, 0.1943, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20307965576648712 tensor([0.2073, 0.2065, 0.2031, 0.1923, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20406655967235565 tensor([0.2050, 0.2072, 0.2041, 0.1934, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2053840607404709 tensor([0.2056, 0.2054, 0.2057, 0.1941, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20447133481502533 tensor([0.2046, 0.2051, 0.2045, 0.1967, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20406502485275269 tensor([0.2052, 0.2051, 0.2041, 0.1937, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20247681438922882 tensor([0.2067, 0.2046, 0.2025, 0.1932, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20290009677410126 tensor([0.2040, 0.2104, 0.2029, 0.1940, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20331592857837677 tensor([0.2046, 0.2064, 0.2033, 0.1944, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20374202728271484 tensor([0.2037, 0.2058, 0.2043, 0.1970, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2036372572183609 tensor([0.2036, 0.2060, 0.2026, 0.1931, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20383822917938232 tensor([0.2080, 0.2048, 0.2038, 0.1929, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20430271327495575 tensor([0.2043, 0.2078, 0.2044, 0.1946, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20418620109558105 tensor([0.2054, 0.2052, 0.2042, 0.1951, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2043733149766922 tensor([0.2044, 0.2063, 0.2046, 0.1955, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2033354490995407 tensor([0.2049, 0.2050, 0.2033, 0.1941, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20418082177639008 tensor([0.2064, 0.2052, 0.2042, 0.1946, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20368093252182007 tensor([0.2037, 0.2069, 0.2041, 0.1958, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20474721491336823 tensor([0.2050, 0.2056, 0.2047, 0.1951, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20492884516716003 tensor([0.2051, 0.2052, 0.2049, 0.1961, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2033328413963318 tensor([0.2050, 0.2057, 0.2033, 0.1947, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2047276645898819 tensor([0.2064, 0.2061, 0.2047, 0.1942, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20455524325370789 tensor([0.2054, 0.2059, 0.2046, 0.1948, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2042800933122635 tensor([0.2049, 0.2058, 0.2043, 0.1943, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20340800285339355 tensor([0.2036, 0.2054, 0.2034, 0.1975, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20412389934062958 tensor([0.2058, 0.2053, 0.2041, 0.1947, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20402738451957703 tensor([0.2076, 0.2055, 0.2040, 0.1926, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20404666662216187 tensor([0.2040, 0.2061, 0.2031, 0.1958, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2051171511411667 tensor([0.2051, 0.2064, 0.2051, 0.1946, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040669173002243 tensor([0.2047, 0.2053, 0.2041, 0.1969, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20393221080303192 tensor([0.2043, 0.2057, 0.2039, 0.1950, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20403823256492615 tensor([0.2070, 0.2056, 0.2040, 0.1935, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20323503017425537 tensor([0.2038, 0.2101, 0.2032, 0.1949, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046467810869217 tensor([0.2047, 0.2054, 0.2046, 0.1958, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2031301110982895 tensor([0.2042, 0.2061, 0.2031, 0.1950, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20306900143623352 tensor([0.2064, 0.2059, 0.2031, 0.1930, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20426693558692932 tensor([0.2071, 0.2050, 0.2043, 0.1938, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038106918334961 tensor([0.2053, 0.2076, 0.2038, 0.1938, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204752579331398 tensor([0.2060, 0.2048, 0.2056, 0.1937, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20363344252109528 tensor([0.2038, 0.2068, 0.2036, 0.1959, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20303690433502197 tensor([0.2037, 0.2053, 0.2030, 0.1943, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20436067879199982 tensor([0.2055, 0.2051, 0.2044, 0.1947, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20319215953350067 tensor([0.2046, 0.2057, 0.2032, 0.1946, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2045883685350418 tensor([0.2046, 0.2066, 0.2049, 0.1947, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20371368527412415 tensor([0.2044, 0.2066, 0.2037, 0.1959, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20360754430294037 tensor([0.2047, 0.2058, 0.2036, 0.1945, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 2, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [2, 3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [1, 4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [0, 3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 2, 4], [3, 4], [4, 3], [3, 4], [3, 4], [0, 4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [2, 4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 3th iteration [[3, 2, 4], [3, 2, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.6410969495773315  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.6410683393478394  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.6410139799118042  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.6409366130828857  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.6408388614654541  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.6407225728034973  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.640589714050293  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.6404421329498291  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.6402812004089355  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.640108585357666  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.6399254202842712  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.6397328972816467  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.6395320892333984  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.6393239498138428  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.6391091346740723  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.6388885974884033  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.6386630535125732  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.638433039188385  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.6381993293762207  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.6379621028900146  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.20461392402648926 tensor([0.2072, 0.2070, 0.2046, 0.1954, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20490723848342896 tensor([0.2066, 0.2089, 0.2049, 0.1962, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20521321892738342 tensor([0.2054, 0.2077, 0.2052, 0.1972, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20488646626472473 tensor([0.2051, 0.2076, 0.2049, 0.1981, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20554426312446594 tensor([0.2055, 0.2084, 0.2039, 0.1944, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20554675161838531 tensor([0.2085, 0.2081, 0.2055, 0.1938, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20535001158714294 tensor([0.2059, 0.2096, 0.2054, 0.1961, 0.1830], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/150 [00:51<2:07:11, 51.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.2061046063899994 tensor([0.2073, 0.2070, 0.2061, 0.1958, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20535677671432495 tensor([0.2055, 0.2075, 0.2054, 0.1980, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20569707453250885 tensor([0.2070, 0.2075, 0.2057, 0.1950, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2051873505115509 tensor([0.2085, 0.2056, 0.2052, 0.1954, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20483125746250153 tensor([0.2051, 0.2100, 0.2048, 0.1956, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20544199645519257 tensor([0.2054, 0.2079, 0.2066, 0.1970, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20538663864135742 tensor([0.2054, 0.2080, 0.2058, 0.1972, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20543694496154785 tensor([0.2064, 0.2074, 0.2054, 0.1952, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20528611540794373 tensor([0.2096, 0.2065, 0.2053, 0.1944, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20478343963623047 tensor([0.2063, 0.2065, 0.2048, 0.1963, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20570537447929382 tensor([0.2071, 0.2071, 0.2057, 0.1946, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20515112578868866 tensor([0.2057, 0.2080, 0.2052, 0.1973, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2057337611913681 tensor([0.2057, 0.2069, 0.2033, 0.1949, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20556269586086273 tensor([0.2080, 0.2071, 0.2056, 0.1950, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20471453666687012 tensor([0.2060, 0.2093, 0.2047, 0.1948, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.206363245844841 tensor([0.2064, 0.2074, 0.2081, 0.1955, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20513929426670074 tensor([0.2069, 0.2076, 0.2051, 0.1966, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20513102412223816 tensor([0.2058, 0.2061, 0.2051, 0.1965, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20572268962860107 tensor([0.2081, 0.2076, 0.2057, 0.1950, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20541074872016907 tensor([0.2058, 0.2084, 0.2054, 0.1958, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2065258026123047 tensor([0.2065, 0.2078, 0.2070, 0.1955, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20476223528385162 tensor([0.2048, 0.2078, 0.2057, 0.1986, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2054869681596756 tensor([0.2067, 0.2063, 0.2055, 0.1953, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055526226758957 tensor([0.2083, 0.2085, 0.2056, 0.1945, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2055426388978958 tensor([0.2055, 0.2101, 0.2059, 0.1965, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20495551824569702 tensor([0.2065, 0.2062, 0.2050, 0.1947, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20518900454044342 tensor([0.2052, 0.2078, 0.2055, 0.1984, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20514443516731262 tensor([0.2063, 0.2071, 0.2051, 0.1949, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20610862970352173 tensor([0.2075, 0.2075, 0.2061, 0.1950, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2050390988588333 tensor([0.2068, 0.2083, 0.2050, 0.1949, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20613834261894226 tensor([0.2061, 0.2072, 0.2063, 0.1964, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2049325406551361 tensor([0.2057, 0.2079, 0.2049, 0.1974, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20503215491771698 tensor([0.2050, 0.2057, 0.2041, 0.1966, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20540252327919006 tensor([0.2077, 0.2072, 0.2054, 0.1953, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20509980618953705 tensor([0.2051, 0.2095, 0.2041, 0.1961, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20567572116851807 tensor([0.2057, 0.2071, 0.2078, 0.1965, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20521090924739838 tensor([0.2053, 0.2073, 0.2052, 0.1984, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2049400508403778 tensor([0.2064, 0.2077, 0.2049, 0.1964, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20608000457286835 tensor([0.2077, 0.2064, 0.2061, 0.1958, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2052767276763916 tensor([0.2058, 0.2075, 0.2053, 0.1961, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20600129663944244 tensor([0.2060, 0.2076, 0.2068, 0.1958, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20585553348064423 tensor([0.2062, 0.2082, 0.2059, 0.1967, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20426219701766968 tensor([0.2058, 0.2072, 0.2043, 0.1959, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20614629983901978 tensor([0.2075, 0.2070, 0.2061, 0.1956, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20496928691864014 tensor([0.2057, 0.2095, 0.2050, 0.1961, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20543962717056274 tensor([0.2068, 0.2066, 0.2054, 0.1954, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20488838851451874 tensor([0.2049, 0.2079, 0.2059, 0.1979, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20601657032966614 tensor([0.2060, 0.2081, 0.2036, 0.1947, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055967003107071 tensor([0.2074, 0.2062, 0.2056, 0.1963, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20498675107955933 tensor([0.2058, 0.2077, 0.2050, 0.1975, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2058364748954773 tensor([0.2071, 0.2067, 0.2058, 0.1958, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.205728217959404 tensor([0.2057, 0.2075, 0.2064, 0.1967, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2054791897535324 tensor([0.2076, 0.2068, 0.2055, 0.1951, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2060040831565857 tensor([0.2069, 0.2077, 0.2060, 0.1956, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20556151866912842 tensor([0.2059, 0.2094, 0.2056, 0.1960, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20654304325580597 tensor([0.2077, 0.2072, 0.2065, 0.1950, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20599929988384247 tensor([0.2060, 0.2079, 0.2062, 0.1970, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046755999326706 tensor([0.2054, 0.2080, 0.2047, 0.1953, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2058032751083374 tensor([0.2073, 0.2066, 0.2058, 0.1960, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2046695202589035 tensor([0.2047, 0.2094, 0.2054, 0.1961, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2056521475315094 tensor([0.2065, 0.2076, 0.2057, 0.1957, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20583897829055786 tensor([0.2068, 0.2072, 0.2058, 0.1970, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2053871899843216 tensor([0.2058, 0.2061, 0.2054, 0.1958, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.204880490899086 tensor([0.2074, 0.2082, 0.2049, 0.1951, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2051781266927719 tensor([0.2052, 0.2094, 0.2041, 0.1964, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20677641034126282 tensor([0.2070, 0.2068, 0.2071, 0.1962, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20540791749954224 tensor([0.2054, 0.2075, 0.2066, 0.1981, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2047674059867859 tensor([0.2057, 0.2085, 0.2048, 0.1953, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20578862726688385 tensor([0.2085, 0.2063, 0.2058, 0.1956, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2049250453710556 tensor([0.2049, 0.2106, 0.2052, 0.1962, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20537114143371582 tensor([0.2054, 0.2071, 0.2076, 0.1963, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2057068794965744 tensor([0.2058, 0.2067, 0.2057, 0.1988, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20521299540996552 tensor([0.2068, 0.2075, 0.2052, 0.1956, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20481015741825104 tensor([0.2085, 0.2079, 0.2048, 0.1941, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040289342403412 tensor([0.2056, 0.2094, 0.2040, 0.1956, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20648427307605743 tensor([0.2065, 0.2071, 0.2065, 0.1961, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20496322214603424 tensor([0.2066, 0.2087, 0.2050, 0.1964, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046443372964859 tensor([0.2049, 0.2073, 0.2046, 0.1956, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2064046859741211 tensor([0.2076, 0.2068, 0.2064, 0.1953, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.204591765999794 tensor([0.2046, 0.2102, 0.2055, 0.1971, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20544728636741638 tensor([0.2064, 0.2066, 0.2054, 0.1959, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20532645285129547 tensor([0.2053, 0.2070, 0.2057, 0.1985, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20617333054542542 tensor([0.2074, 0.2062, 0.2066, 0.1955, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20583873987197876 tensor([0.2071, 0.2058, 0.2061, 0.1954, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20442810654640198 tensor([0.2044, 0.2104, 0.2048, 0.1970, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20536087453365326 tensor([0.2078, 0.2069, 0.2054, 0.1948, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20511089265346527 tensor([0.2057, 0.2081, 0.2051, 0.1980, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20563910901546478 tensor([0.2056, 0.2070, 0.2060, 0.1969, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2053331732749939 tensor([0.2075, 0.2061, 0.2053, 0.1955, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20609577000141144 tensor([0.2063, 0.2079, 0.2061, 0.1958, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.206362783908844 tensor([0.2064, 0.2070, 0.2073, 0.1958, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20617300271987915 tensor([0.2062, 0.2066, 0.2062, 0.1978, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2061259150505066 tensor([0.2061, 0.2061, 0.2042, 0.1952, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2060021013021469 tensor([0.2081, 0.2063, 0.2060, 0.1949, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20466157793998718 tensor([0.2063, 0.2103, 0.2047, 0.1955, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.206373929977417 tensor([0.2064, 0.2073, 0.2070, 0.1961, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20555542409420013 tensor([0.2058, 0.2078, 0.2056, 0.1968, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20550420880317688 tensor([0.2065, 0.2053, 0.2055, 0.1975, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.205957293510437 tensor([0.2081, 0.2070, 0.2060, 0.1954, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2053273469209671 tensor([0.2053, 0.2084, 0.2064, 0.1968, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2064475566148758 tensor([0.2064, 0.2075, 0.2070, 0.1957, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20577022433280945 tensor([0.2061, 0.2085, 0.2058, 0.1964, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2051701545715332 tensor([0.2055, 0.2071, 0.2052, 0.1956, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055470496416092 tensor([0.2087, 0.2068, 0.2055, 0.1942, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20599958300590515 tensor([0.2065, 0.2061, 0.2060, 0.1958, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20647267997264862 tensor([0.2065, 0.2067, 0.2070, 0.1966, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20517109334468842 tensor([0.2064, 0.2077, 0.2052, 0.1964, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20437726378440857 tensor([0.2054, 0.2081, 0.2044, 0.1945, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2053362876176834 tensor([0.2083, 0.2079, 0.2053, 0.1948, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2041260004043579 tensor([0.2043, 0.2110, 0.2041, 0.1970, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20587372779846191 tensor([0.2066, 0.2078, 0.2059, 0.1961, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20564480125904083 tensor([0.2062, 0.2082, 0.2056, 0.1965, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2065250277519226 tensor([0.2065, 0.2067, 0.2040, 0.1950, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2053103744983673 tensor([0.2081, 0.2077, 0.2053, 0.1954, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20433984696865082 tensor([0.2062, 0.2075, 0.2043, 0.1952, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20552147924900055 tensor([0.2055, 0.2067, 0.2064, 0.1975, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2054586112499237 tensor([0.2063, 0.2072, 0.2055, 0.1962, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20498351752758026 tensor([0.2056, 0.2072, 0.2050, 0.1961, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055489867925644 tensor([0.2084, 0.2069, 0.2055, 0.1945, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20440880954265594 tensor([0.2055, 0.2111, 0.2044, 0.1955, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20605631172657013 tensor([0.2077, 0.2067, 0.2061, 0.1946, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20530451834201813 tensor([0.2062, 0.2070, 0.2053, 0.1983, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20548000931739807 tensor([0.2055, 0.2071, 0.2036, 0.1953, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20613382756710052 tensor([0.2074, 0.2065, 0.2061, 0.1960, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20480918884277344 tensor([0.2052, 0.2102, 0.2048, 0.1963, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20658448338508606 tensor([0.2077, 0.2066, 0.2068, 0.1950, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20615212619304657 tensor([0.2062, 0.2073, 0.2064, 0.1976, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2049119919538498 tensor([0.2054, 0.2073, 0.2049, 0.1967, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20624294877052307 tensor([0.2076, 0.2072, 0.2062, 0.1952, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2051170915365219 tensor([0.2051, 0.2085, 0.2063, 0.1971, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20607993006706238 tensor([0.2077, 0.2074, 0.2061, 0.1950, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20456871390342712 tensor([0.2056, 0.2089, 0.2046, 0.1967, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20468507707118988 tensor([0.2051, 0.2079, 0.2047, 0.1949, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2051662653684616 tensor([0.2083, 0.2076, 0.2052, 0.1946, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2058711051940918 tensor([0.2062, 0.2073, 0.2059, 0.1967, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20624232292175293 tensor([0.2070, 0.2062, 0.2069, 0.1958, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20541264116764069 tensor([0.2061, 0.2085, 0.2054, 0.1966, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20496612787246704 tensor([0.2061, 0.2066, 0.2050, 0.1957, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2051095962524414 tensor([0.2093, 0.2072, 0.2051, 0.1936, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20456743240356445 tensor([0.2046, 0.2113, 0.2050, 0.1958, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20593902468681335 tensor([0.2068, 0.2064, 0.2059, 0.1956, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20620191097259521 tensor([0.2062, 0.2071, 0.2063, 0.1970, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20553159713745117 tensor([0.2060, 0.2078, 0.2055, 0.1960, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20568692684173584 tensor([0.2080, 0.2071, 0.2057, 0.1946, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20471414923667908 tensor([0.2052, 0.2105, 0.2047, 0.1969, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20587389171123505 tensor([0.2059, 0.2075, 0.2071, 0.1964, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2050243318080902 tensor([0.2054, 0.2087, 0.2050, 0.1972, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2056380808353424 tensor([0.2063, 0.2059, 0.2056, 0.1967, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20497532188892365 tensor([0.2093, 0.2071, 0.2050, 0.1935, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20446212589740753 tensor([0.2055, 0.2087, 0.2045, 0.1962, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.205538809299469 tensor([0.2064, 0.2078, 0.2055, 0.1967, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.205239400267601 tensor([0.2052, 0.2089, 0.2055, 0.1973, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20503373444080353 tensor([0.2050, 0.2095, 0.2056, 0.1962, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2066795378923416 tensor([0.2080, 0.2072, 0.2067, 0.1947, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2054913491010666 tensor([0.2057, 0.2073, 0.2055, 0.1962, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20605072379112244 tensor([0.2067, 0.2068, 0.2061, 0.1958, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2049688845872879 tensor([0.2050, 0.2067, 0.2058, 0.1986, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20512785017490387 tensor([0.2052, 0.2071, 0.2051, 0.1969, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2054133266210556 tensor([0.2088, 0.2070, 0.2054, 0.1944, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20502491295337677 tensor([0.2050, 0.2094, 0.2054, 0.1966, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20622046291828156 tensor([0.2072, 0.2076, 0.2062, 0.1947, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2052190601825714 tensor([0.2056, 0.2063, 0.2052, 0.1990, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20647212862968445 tensor([0.2067, 0.2065, 0.2041, 0.1953, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20541779696941376 tensor([0.2071, 0.2072, 0.2054, 0.1950, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20498806238174438 tensor([0.2053, 0.2091, 0.2050, 0.1967, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20541848242282867 tensor([0.2054, 0.2075, 0.2068, 0.1969, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2057618349790573 tensor([0.2060, 0.2071, 0.2058, 0.1982, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20549115538597107 tensor([0.2055, 0.2072, 0.2042, 0.1948, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2062642127275467 tensor([0.2072, 0.2066, 0.2063, 0.1957, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20437701046466827 tensor([0.2050, 0.2090, 0.2044, 0.1960, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20626559853553772 tensor([0.2063, 0.2076, 0.2069, 0.1961, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.205705925822258 tensor([0.2066, 0.2072, 0.2057, 0.1966, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20559203624725342 tensor([0.2057, 0.2075, 0.2056, 0.1964, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20589902997016907 tensor([0.2078, 0.2059, 0.2059, 0.1948, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20474773645401 tensor([0.2058, 0.2080, 0.2047, 0.1956, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20596297085285187 tensor([0.2071, 0.2065, 0.2060, 0.1963, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20425143837928772 tensor([0.2067, 0.2070, 0.2043, 0.1951, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20607693493366241 tensor([0.2066, 0.2061, 0.2040, 0.1949, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.204934224486351 tensor([0.2095, 0.2063, 0.2049, 0.1943, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20461800694465637 tensor([0.2054, 0.2107, 0.2046, 0.1961, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20594213902950287 tensor([0.2069, 0.2082, 0.2059, 0.1952, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20588792860507965 tensor([0.2082, 0.2070, 0.2059, 0.1949, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.206395223736763 tensor([0.2047, 0.2074, 0.2064, 0.1970, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20614579319953918 tensor([0.2082, 0.2067, 0.2061, 0.1951, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20516881346702576 tensor([0.2053, 0.2084, 0.2052, 0.1960, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20557990670204163 tensor([0.2056, 0.2083, 0.2069, 0.1968, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20631533861160278 tensor([0.2063, 0.2079, 0.2063, 0.1964, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.205248162150383 tensor([0.2053, 0.2076, 0.2052, 0.1957, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055531144142151 tensor([0.2086, 0.2075, 0.2056, 0.1944, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20492278039455414 tensor([0.2055, 0.2115, 0.2049, 0.1954, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2064022421836853 tensor([0.2064, 0.2077, 0.2067, 0.1958, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20532499253749847 tensor([0.2057, 0.2090, 0.2053, 0.1965, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20670665800571442 tensor([0.2067, 0.2068, 0.2032, 0.1941, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20569473505020142 tensor([0.2077, 0.2078, 0.2057, 0.1950, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20426779985427856 tensor([0.2049, 0.2115, 0.2043, 0.1957, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2058078795671463 tensor([0.2058, 0.2076, 0.2071, 0.1964, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2052571326494217 tensor([0.2053, 0.2077, 0.2056, 0.1980, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046116590499878 tensor([0.2067, 0.2057, 0.2046, 0.1957, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20462818443775177 tensor([0.2087, 0.2080, 0.2046, 0.1937, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20556136965751648 tensor([0.2063, 0.2087, 0.2056, 0.1948, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20679445564746857 tensor([0.2069, 0.2068, 0.2072, 0.1955, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2058170884847641 tensor([0.2058, 0.2066, 0.2059, 0.1981, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20564807951450348 tensor([0.2065, 0.2066, 0.2056, 0.1951, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20406067371368408 tensor([0.2081, 0.2061, 0.2041, 0.1946, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20436705648899078 tensor([0.2053, 0.2118, 0.2044, 0.1953, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2048570215702057 tensor([0.2059, 0.2079, 0.2049, 0.1958, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20497944951057434 tensor([0.2050, 0.2073, 0.2058, 0.1983, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20496495068073273 tensor([0.2050, 0.2076, 0.2042, 0.1945, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20533087849617004 tensor([0.2092, 0.2063, 0.2053, 0.1943, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20555247366428375 tensor([0.2056, 0.2093, 0.2059, 0.1959, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20571167767047882 tensor([0.2067, 0.2067, 0.2057, 0.1965, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20563848316669464 tensor([0.2056, 0.2077, 0.2061, 0.1969, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20492787659168243 tensor([0.2062, 0.2065, 0.2049, 0.1955, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20568236708641052 tensor([0.2076, 0.2067, 0.2057, 0.1959, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20493300259113312 tensor([0.2049, 0.2083, 0.2056, 0.1972, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.206224724650383 tensor([0.2062, 0.2071, 0.2062, 0.1965, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20635995268821716 tensor([0.2064, 0.2066, 0.2064, 0.1975, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20491264760494232 tensor([0.2063, 0.2071, 0.2049, 0.1961, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20613716542720795 tensor([0.2076, 0.2075, 0.2061, 0.1956, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20603571832180023 tensor([0.2067, 0.2074, 0.2060, 0.1962, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2058192789554596 tensor([0.2062, 0.2072, 0.2058, 0.1957, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20487862825393677 tensor([0.2049, 0.2068, 0.2049, 0.1989, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20566675066947937 tensor([0.2070, 0.2068, 0.2057, 0.1960, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20551840960979462 tensor([0.2088, 0.2070, 0.2055, 0.1940, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20531508326530457 tensor([0.2053, 0.2076, 0.2047, 0.1972, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2064068466424942 tensor([0.2064, 0.2078, 0.2067, 0.1960, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055661529302597 tensor([0.2060, 0.2067, 0.2056, 0.1982, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20550300180912018 tensor([0.2056, 0.2072, 0.2055, 0.1963, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20553112030029297 tensor([0.2081, 0.2072, 0.2055, 0.1949, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20469127595424652 tensor([0.2051, 0.2115, 0.2047, 0.1962, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20591199398040771 tensor([0.2059, 0.2069, 0.2062, 0.1971, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20470203459262848 tensor([0.2055, 0.2076, 0.2047, 0.1965, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2046012282371521 tensor([0.2078, 0.2074, 0.2046, 0.1943, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2057408094406128 tensor([0.2083, 0.2065, 0.2057, 0.1951, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20534121990203857 tensor([0.2066, 0.2091, 0.2053, 0.1951, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20621266961097717 tensor([0.2073, 0.2062, 0.2071, 0.1951, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20510081946849823 tensor([0.2051, 0.2083, 0.2051, 0.1972, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20463214814662933 tensor([0.2050, 0.2068, 0.2046, 0.1958, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20590709149837494 tensor([0.2068, 0.2065, 0.2059, 0.1960, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20470836758613586 tensor([0.2059, 0.2072, 0.2047, 0.1960, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20589357614517212 tensor([0.2059, 0.2080, 0.2064, 0.1961, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20521637797355652 tensor([0.2057, 0.2080, 0.2052, 0.1972, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20512981712818146 tensor([0.2061, 0.2073, 0.2051, 0.1959, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 2, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [2, 3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [1, 4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [0, 3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 2, 4], [3, 4], [4, 3], [3, 4], [3, 4], [0, 4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [2, 4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 4th iteration []\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 2, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [2, 3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [2, 3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [1, 4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [0, 3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 2, 4], [3, 4], [4, 3], [3, 4], [3, 4], [0, 4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [3, 4], [4, 3], [3, 2, 4], [3, 4], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [4, 3], [4, 3], [3, 4], [4, 3], [4, 3], [3, 4], [4, 3], [3, 4], [3, 4], [2, 4, 3], [4, 3], [4, 3], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [4, 3], [3, 4], [3, 4], [3, 4], [4, 3], [4, 3], [3, 4]]\n",
      "POSITION :  [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.6\n",
      "tensor([], dtype=torch.int64)\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 1.6091180992126466  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 1.6082602643966675  Accuracy on Support set:32.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6076036548614503  Accuracy on Support set:32.0\n",
      "Train_Epoch: 3  Train_Loss: 1.6069999504089356  Accuracy on Support set:40.0\n",
      "Train_Epoch: 4  Train_Loss: 1.6064271593093873  Accuracy on Support set:48.0\n",
      "Train_Epoch: 5  Train_Loss: 1.6058744382858277  Accuracy on Support set:48.0\n",
      "Train_Epoch: 6  Train_Loss: 1.6053284120559692  Accuracy on Support set:48.0\n",
      "Train_Epoch: 7  Train_Loss: 1.6047956895828248  Accuracy on Support set:48.0\n",
      "Train_Epoch: 8  Train_Loss: 1.6042737579345703  Accuracy on Support set:48.0\n",
      "Train_Epoch: 9  Train_Loss: 1.6037538719177247  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 10  Train_Loss: 1.6032432317733765  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 11  Train_Loss: 1.6027278566360474  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 12  Train_Loss: 1.6022140741348267  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 13  Train_Loss: 1.6016930294036866  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 14  Train_Loss: 1.601173858642578  Accuracy on Support set:60.0\n",
      "Train_Epoch: 15  Train_Loss: 1.600652813911438  Accuracy on Support set:60.0\n",
      "Train_Epoch: 16  Train_Loss: 1.6001159858703613  Accuracy on Support set:64.0\n",
      "Train_Epoch: 17  Train_Loss: 1.59958251953125  Accuracy on Support set:68.0\n",
      "Train_Epoch: 18  Train_Loss: 1.5990435886383056  Accuracy on Support set:68.0\n",
      "Train_Epoch: 19  Train_Loss: 1.5984921169281006  Accuracy on Support set:72.0\n",
      "Train_Epoch: 20  Train_Loss: 1.5979321718215942  Accuracy on Support set:76.0\n",
      "Train_Epoch: 21  Train_Loss: 1.5973565816879272  Accuracy on Support set:76.0\n",
      "Train_Epoch: 22  Train_Loss: 1.596781063079834  Accuracy on Support set:76.0\n",
      "Train_Epoch: 23  Train_Loss: 1.596181173324585  Accuracy on Support set:76.0\n",
      "Train_Epoch: 24  Train_Loss: 1.5955780410766602  Accuracy on Support set:76.0\n",
      "Train_Epoch: 25  Train_Loss: 1.5949580144882203  Accuracy on Support set:76.0\n",
      "Train_Epoch: 26  Train_Loss: 1.5943290710449218  Accuracy on Support set:76.0\n",
      "Train_Epoch: 27  Train_Loss: 1.593677191734314  Accuracy on Support set:76.0\n",
      "Train_Epoch: 28  Train_Loss: 1.5930184459686278  Accuracy on Support set:76.0\n",
      "Train_Epoch: 29  Train_Loss: 1.5923288297653198  Accuracy on Support set:76.0\n",
      "Train_Epoch: 30  Train_Loss: 1.5916473054885865  Accuracy on Support set:76.0\n",
      "Train_Epoch: 31  Train_Loss: 1.5909259557723998  Accuracy on Support set:80.0\n",
      "Train_Epoch: 32  Train_Loss: 1.590203561782837  Accuracy on Support set:80.0\n",
      "Train_Epoch: 33  Train_Loss: 1.5894427347183226  Accuracy on Support set:92.0\n",
      "Train_Epoch: 34  Train_Loss: 1.5886686801910401  Accuracy on Support set:96.0\n",
      "Train_Epoch: 35  Train_Loss: 1.5878692436218262  Accuracy on Support set:96.0\n",
      "Train_Epoch: 36  Train_Loss: 1.5870406866073608  Accuracy on Support set:96.0\n",
      "Train_Epoch: 37  Train_Loss: 1.5861928653717041  Accuracy on Support set:96.0\n",
      "Train_Epoch: 38  Train_Loss: 1.5853216028213502  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 1.5844197130203248  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 1.583498134613037  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 1.5825323724746705  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 1.5815573453903198  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 1.580533332824707  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 1.57948486328125  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 1.5783995389938354  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 1.5772683668136596  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 1.5760945749282838  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 1.574877429008484  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 1.573617901802063  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  69.33333333333334\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.18682019412517548 tensor([0.2066, 0.2017, 0.2129, 0.1920, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18641674518585205 tensor([0.2061, 0.2026, 0.2133, 0.1916, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18701262772083282 tensor([0.2060, 0.2019, 0.2128, 0.1923, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18732210993766785 tensor([0.2056, 0.2021, 0.2132, 0.1918, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18699942529201508 tensor([0.2063, 0.2019, 0.2127, 0.1921, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18682549893856049 tensor([0.2059, 0.2021, 0.2133, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18679583072662354 tensor([0.2063, 0.2024, 0.2132, 0.1914, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18702030181884766 tensor([0.2059, 0.2020, 0.2132, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18700295686721802 tensor([0.2059, 0.2022, 0.2131, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18685221672058105 tensor([0.2067, 0.2013, 0.2134, 0.1917, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18728598952293396 tensor([0.2057, 0.2019, 0.2134, 0.1917, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18661831319332123 tensor([0.2060, 0.2023, 0.2132, 0.1919, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1879221498966217 tensor([0.2058, 0.2015, 0.2134, 0.1914, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.186391219496727 tensor([0.2061, 0.2027, 0.2131, 0.1917, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18740618228912354 tensor([0.2059, 0.2018, 0.2134, 0.1915, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872435063123703 tensor([0.2061, 0.2017, 0.2128, 0.1921, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865604966878891 tensor([0.2066, 0.2018, 0.2131, 0.1919, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18761292099952698 tensor([0.2057, 0.2016, 0.2130, 0.1921, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18710258603096008 tensor([0.2060, 0.2021, 0.2130, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18741972744464874 tensor([0.2060, 0.2015, 0.2128, 0.1922, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18747973442077637 tensor([0.2064, 0.2015, 0.2134, 0.1912, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18712100386619568 tensor([0.2067, 0.2016, 0.2129, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18772943317890167 tensor([0.2062, 0.2014, 0.2133, 0.1913, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18694478273391724 tensor([0.2059, 0.2022, 0.2128, 0.1921, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872829645872116 tensor([0.2058, 0.2019, 0.2128, 0.1922, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865956336259842 tensor([0.2066, 0.2021, 0.2129, 0.1918, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1867840439081192 tensor([0.2063, 0.2018, 0.2133, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18787342309951782 tensor([0.2059, 0.2016, 0.2126, 0.1920, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18736135959625244 tensor([0.2062, 0.2016, 0.2134, 0.1914, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1876847892999649 tensor([0.2064, 0.2010, 0.2135, 0.1914, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18700605630874634 tensor([0.2065, 0.2013, 0.2133, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18693795800209045 tensor([0.2068, 0.2015, 0.2131, 0.1917, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1873515248298645 tensor([0.2061, 0.2019, 0.2128, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868794858455658 tensor([0.2061, 0.2019, 0.2131, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1874217689037323 tensor([0.2061, 0.2017, 0.2129, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18685707449913025 tensor([0.2066, 0.2018, 0.2133, 0.1914, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18700824677944183 tensor([0.2058, 0.2021, 0.2134, 0.1916, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1866832822561264 tensor([0.2060, 0.2018, 0.2135, 0.1920, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18700625002384186 tensor([0.2061, 0.2024, 0.2124, 0.1921, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18677270412445068 tensor([0.2060, 0.2018, 0.2132, 0.1922, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18708670139312744 tensor([0.2058, 0.2020, 0.2132, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18706244230270386 tensor([0.2056, 0.2024, 0.2130, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18743577599525452 tensor([0.2067, 0.2012, 0.2131, 0.1916, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18637888133525848 tensor([0.2055, 0.2025, 0.2138, 0.1919, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18686893582344055 tensor([0.2068, 0.2016, 0.2125, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18643924593925476 tensor([0.2067, 0.2020, 0.2128, 0.1920, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18701259791851044 tensor([0.2060, 0.2021, 0.2132, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18703877925872803 tensor([0.2060, 0.2016, 0.2135, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18731772899627686 tensor([0.2062, 0.2015, 0.2131, 0.1919, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18779444694519043 tensor([0.2059, 0.2018, 0.2130, 0.1914, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865663230419159 tensor([0.2059, 0.2023, 0.2130, 0.1922, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18670591711997986 tensor([0.2064, 0.2021, 0.2132, 0.1916, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18721938133239746 tensor([0.2062, 0.2016, 0.2133, 0.1916, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18720462918281555 tensor([0.2054, 0.2022, 0.2131, 0.1922, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18738843500614166 tensor([0.2062, 0.2017, 0.2129, 0.1918, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18753144145011902 tensor([0.2059, 0.2020, 0.2131, 0.1914, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865757256746292 tensor([0.2068, 0.2018, 0.2130, 0.1918, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18741057813167572 tensor([0.2060, 0.2015, 0.2132, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18668989837169647 tensor([0.2063, 0.2017, 0.2132, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1876658946275711 tensor([0.2059, 0.2014, 0.2130, 0.1920, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18649888038635254 tensor([0.2063, 0.2020, 0.2135, 0.1917, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1866004317998886 tensor([0.2065, 0.2022, 0.2133, 0.1914, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18724806606769562 tensor([0.2060, 0.2019, 0.2130, 0.1919, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18681465089321136 tensor([0.2055, 0.2024, 0.2132, 0.1921, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872333437204361 tensor([0.2057, 0.2021, 0.2132, 0.1917, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18687759339809418 tensor([0.2062, 0.2019, 0.2128, 0.1922, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18698593974113464 tensor([0.2058, 0.2026, 0.2129, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18689651787281036 tensor([0.2061, 0.2018, 0.2130, 0.1921, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.186870276927948 tensor([0.2061, 0.2022, 0.2130, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.186817929148674 tensor([0.2060, 0.2020, 0.2127, 0.1924, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18688929080963135 tensor([0.2061, 0.2019, 0.2132, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18652725219726562 tensor([0.2063, 0.2023, 0.2131, 0.1917, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18678171932697296 tensor([0.2059, 0.2017, 0.2136, 0.1921, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18680281937122345 tensor([0.2059, 0.2024, 0.2127, 0.1922, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18742917478084564 tensor([0.2059, 0.2019, 0.2129, 0.1919, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1870252788066864 tensor([0.2062, 0.2019, 0.2135, 0.1914, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18650753796100616 tensor([0.2061, 0.2020, 0.2134, 0.1919, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.186815544962883 tensor([0.2062, 0.2021, 0.2128, 0.1921, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18666179478168488 tensor([0.2062, 0.2017, 0.2133, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18731893599033356 tensor([0.2066, 0.2020, 0.2127, 0.1915, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18683595955371857 tensor([0.2064, 0.2020, 0.2130, 0.1918, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872168630361557 tensor([0.2061, 0.2018, 0.2134, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18695077300071716 tensor([0.2057, 0.2021, 0.2132, 0.1921, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18692156672477722 tensor([0.2064, 0.2018, 0.2128, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872725635766983 tensor([0.2060, 0.2012, 0.2135, 0.1921, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869688183069229 tensor([0.2061, 0.2016, 0.2130, 0.1923, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18690551817417145 tensor([0.2059, 0.2023, 0.2133, 0.1916, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18709217011928558 tensor([0.2060, 0.2017, 0.2129, 0.1922, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868751496076584 tensor([0.2058, 0.2027, 0.2127, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18712085485458374 tensor([0.2060, 0.2018, 0.2130, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18711549043655396 tensor([0.2060, 0.2021, 0.2130, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18663042783737183 tensor([0.2068, 0.2016, 0.2134, 0.1915, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18752877414226532 tensor([0.2064, 0.2013, 0.2132, 0.1917, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868569701910019 tensor([0.2062, 0.2018, 0.2136, 0.1915, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18761765956878662 tensor([0.2061, 0.2014, 0.2130, 0.1919, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18696624040603638 tensor([0.2062, 0.2017, 0.2130, 0.1922, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1864820122718811 tensor([0.2064, 0.2021, 0.2130, 0.1919, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18727098405361176 tensor([0.2058, 0.2019, 0.2135, 0.1916, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1870061755180359 tensor([0.2056, 0.2027, 0.2129, 0.1919, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869334578514099 tensor([0.2064, 0.2019, 0.2130, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18704754114151 tensor([0.2062, 0.2016, 0.2132, 0.1920, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18702907860279083 tensor([0.2064, 0.2020, 0.2131, 0.1914, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18719427287578583 tensor([0.2063, 0.2017, 0.2127, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868278533220291 tensor([0.2057, 0.2024, 0.2130, 0.1920, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18741504848003387 tensor([0.2068, 0.2013, 0.2127, 0.1918, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18686851859092712 tensor([0.2063, 0.2021, 0.2128, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18661901354789734 tensor([0.2061, 0.2022, 0.2132, 0.1920, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18723943829536438 tensor([0.2062, 0.2021, 0.2128, 0.1918, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18715263903141022 tensor([0.2058, 0.2019, 0.2132, 0.1919, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18748821318149567 tensor([0.2061, 0.2016, 0.2129, 0.1919, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18710167706012726 tensor([0.2058, 0.2022, 0.2131, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18695513904094696 tensor([0.2069, 0.2015, 0.2132, 0.1915, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18726709485054016 tensor([0.2064, 0.2017, 0.2129, 0.1918, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869024932384491 tensor([0.2059, 0.2024, 0.2131, 0.1918, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18764109909534454 tensor([0.2060, 0.2015, 0.2133, 0.1916, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18713070452213287 tensor([0.2060, 0.2015, 0.2130, 0.1923, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18661871552467346 tensor([0.2061, 0.2024, 0.2130, 0.1918, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.187245175242424 tensor([0.2059, 0.2018, 0.2130, 0.1921, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.187360018491745 tensor([0.2060, 0.2021, 0.2133, 0.1913, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18715521693229675 tensor([0.2061, 0.2015, 0.2133, 0.1919, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18710817396640778 tensor([0.2063, 0.2018, 0.2130, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18697956204414368 tensor([0.2062, 0.2019, 0.2137, 0.1912, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868499368429184 tensor([0.2058, 0.2021, 0.2132, 0.1921, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18734169006347656 tensor([0.2058, 0.2018, 0.2128, 0.1922, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18688613176345825 tensor([0.2061, 0.2020, 0.2131, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869352012872696 tensor([0.2068, 0.2018, 0.2129, 0.1916, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18672223389148712 tensor([0.2064, 0.2021, 0.2135, 0.1913, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872250735759735 tensor([0.2058, 0.2018, 0.2135, 0.1917, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18673117458820343 tensor([0.2061, 0.2026, 0.2128, 0.1918, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18753454089164734 tensor([0.2068, 0.2010, 0.2129, 0.1918, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18654213845729828 tensor([0.2059, 0.2022, 0.2132, 0.1922, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18666093051433563 tensor([0.2061, 0.2024, 0.2128, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18706752359867096 tensor([0.2065, 0.2023, 0.2125, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18667426705360413 tensor([0.2060, 0.2027, 0.2130, 0.1916, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18817926943302155 tensor([0.2062, 0.2010, 0.2135, 0.1911, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18683487176895142 tensor([0.2062, 0.2022, 0.2131, 0.1916, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1860034465789795 tensor([0.2064, 0.2024, 0.2129, 0.1923, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18713195621967316 tensor([0.2058, 0.2018, 0.2129, 0.1922, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18688687682151794 tensor([0.2064, 0.2017, 0.2132, 0.1917, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18685472011566162 tensor([0.2063, 0.2021, 0.2129, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1873679906129837 tensor([0.2060, 0.2017, 0.2133, 0.1916, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18666474521160126 tensor([0.2063, 0.2017, 0.2138, 0.1915, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18692150712013245 tensor([0.2061, 0.2019, 0.2130, 0.1920, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18689505755901337 tensor([0.2054, 0.2020, 0.2137, 0.1921, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18734551966190338 tensor([0.2062, 0.2019, 0.2130, 0.1916, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18711219727993011 tensor([0.2059, 0.2020, 0.2129, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18680347502231598 tensor([0.2062, 0.2023, 0.2131, 0.1916, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18704216182231903 tensor([0.2062, 0.2020, 0.2130, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1867026686668396 tensor([0.2056, 0.2020, 0.2136, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18686525523662567 tensor([0.2063, 0.2018, 0.2134, 0.1917, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18658028542995453 tensor([0.2064, 0.2021, 0.2134, 0.1915, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868642419576645 tensor([0.2061, 0.2020, 0.2133, 0.1917, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18725427985191345 tensor([0.2057, 0.2018, 0.2135, 0.1918, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1866857260465622 tensor([0.2061, 0.2025, 0.2129, 0.1918, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869925856590271 tensor([0.2065, 0.2018, 0.2125, 0.1923, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18702411651611328 tensor([0.2062, 0.2016, 0.2129, 0.1922, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18663589656352997 tensor([0.2065, 0.2022, 0.2129, 0.1918, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868119090795517 tensor([0.2063, 0.2018, 0.2129, 0.1922, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18703360855579376 tensor([0.2059, 0.2021, 0.2132, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18734095990657806 tensor([0.2059, 0.2012, 0.2132, 0.1923, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869046986103058 tensor([0.2065, 0.2012, 0.2130, 0.1924, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18657930195331573 tensor([0.2061, 0.2024, 0.2132, 0.1917, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18725013732910156 tensor([0.2061, 0.2022, 0.2125, 0.1920, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18704086542129517 tensor([0.2061, 0.2015, 0.2137, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18758618831634521 tensor([0.2061, 0.2015, 0.2129, 0.1918, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18723541498184204 tensor([0.2061, 0.2015, 0.2132, 0.1919, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18717001378536224 tensor([0.2055, 0.2022, 0.2135, 0.1916, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18742422759532928 tensor([0.2061, 0.2016, 0.2128, 0.1921, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18752814829349518 tensor([0.2059, 0.2017, 0.2130, 0.1918, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18739010393619537 tensor([0.2065, 0.2013, 0.2131, 0.1917, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18747074902057648 tensor([0.2060, 0.2017, 0.2131, 0.1917, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.186383917927742 tensor([0.2064, 0.2021, 0.2135, 0.1916, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1877928078174591 tensor([0.2062, 0.2019, 0.2130, 0.1912, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18726807832717896 tensor([0.2056, 0.2023, 0.2130, 0.1918, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18767419457435608 tensor([0.2063, 0.2014, 0.2131, 0.1916, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869034618139267 tensor([0.2062, 0.2021, 0.2129, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872672289609909 tensor([0.2061, 0.2015, 0.2135, 0.1915, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18709704279899597 tensor([0.2063, 0.2019, 0.2128, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18707101047039032 tensor([0.2063, 0.2020, 0.2129, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18700063228607178 tensor([0.2063, 0.2016, 0.2128, 0.1923, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1866687685251236 tensor([0.2059, 0.2019, 0.2135, 0.1921, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18702271580696106 tensor([0.2061, 0.2022, 0.2130, 0.1916, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1870347261428833 tensor([0.2059, 0.2021, 0.2130, 0.1920, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865713894367218 tensor([0.2060, 0.2019, 0.2134, 0.1921, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18762829899787903 tensor([0.2059, 0.2017, 0.2128, 0.1920, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18686410784721375 tensor([0.2057, 0.2021, 0.2133, 0.1921, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1866132915019989 tensor([0.2063, 0.2019, 0.2132, 0.1920, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18713979423046112 tensor([0.2061, 0.2019, 0.2131, 0.1918, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18679551780223846 tensor([0.2060, 0.2019, 0.2134, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18702499568462372 tensor([0.2065, 0.2018, 0.2130, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18707068264484406 tensor([0.2068, 0.2015, 0.2128, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18711578845977783 tensor([0.2062, 0.2023, 0.2131, 0.1914, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872492879629135 tensor([0.2064, 0.2020, 0.2125, 0.1918, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18695403635501862 tensor([0.2062, 0.2021, 0.2131, 0.1917, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18708305060863495 tensor([0.2058, 0.2026, 0.2130, 0.1916, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872943937778473 tensor([0.2062, 0.2019, 0.2130, 0.1917, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1872415691614151 tensor([0.2060, 0.2022, 0.2135, 0.1911, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1869412660598755 tensor([0.2069, 0.2014, 0.2129, 0.1919, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868012696504593 tensor([0.2058, 0.2021, 0.2134, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1875658929347992 tensor([0.2059, 0.2016, 0.2129, 0.1921, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18714015185832977 tensor([0.2070, 0.2015, 0.2129, 0.1914, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18644441664218903 tensor([0.2064, 0.2020, 0.2134, 0.1918, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18659891188144684 tensor([0.2059, 0.2022, 0.2129, 0.1923, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18663056194782257 tensor([0.2057, 0.2026, 0.2129, 0.1921, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18732303380966187 tensor([0.2059, 0.2017, 0.2135, 0.1915, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865825653076172 tensor([0.2062, 0.2020, 0.2137, 0.1916, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18691334128379822 tensor([0.2061, 0.2022, 0.2133, 0.1915, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18700134754180908 tensor([0.2054, 0.2021, 0.2131, 0.1924, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18669338524341583 tensor([0.2060, 0.2022, 0.2132, 0.1919, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18706099689006805 tensor([0.2060, 0.2015, 0.2129, 0.1926, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18757182359695435 tensor([0.2053, 0.2021, 0.2129, 0.1922, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18669529259204865 tensor([0.2064, 0.2020, 0.2133, 0.1915, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18747234344482422 tensor([0.2065, 0.2015, 0.2128, 0.1917, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18763920664787292 tensor([0.2062, 0.2019, 0.2128, 0.1914, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18746136128902435 tensor([0.2062, 0.2016, 0.2132, 0.1916, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18683189153671265 tensor([0.2064, 0.2022, 0.2130, 0.1916, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1864229291677475 tensor([0.2064, 0.2027, 0.2132, 0.1913, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18711771070957184 tensor([0.2062, 0.2019, 0.2131, 0.1917, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18641513586044312 tensor([0.2060, 0.2023, 0.2136, 0.1916, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18710492551326752 tensor([0.2059, 0.2015, 0.2134, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18723072111606598 tensor([0.2059, 0.2021, 0.2128, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18721479177474976 tensor([0.2064, 0.2014, 0.2130, 0.1920, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1870422214269638 tensor([0.2060, 0.2020, 0.2129, 0.1921, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868450939655304 tensor([0.2060, 0.2016, 0.2137, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868857592344284 tensor([0.2058, 0.2020, 0.2136, 0.1917, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1873246133327484 tensor([0.2060, 0.2018, 0.2129, 0.1920, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18636919558048248 tensor([0.2063, 0.2022, 0.2133, 0.1918, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18710798025131226 tensor([0.2060, 0.2017, 0.2130, 0.1921, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18703898787498474 tensor([0.2060, 0.2021, 0.2133, 0.1916, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18726158142089844 tensor([0.2064, 0.2015, 0.2131, 0.1918, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18672290444374084 tensor([0.2063, 0.2018, 0.2130, 0.1922, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18675251305103302 tensor([0.2057, 0.2019, 0.2137, 0.1919, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18655718863010406 tensor([0.2062, 0.2017, 0.2134, 0.1922, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18684032559394836 tensor([0.2056, 0.2020, 0.2131, 0.1926, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1876230388879776 tensor([0.2061, 0.2012, 0.2135, 0.1916, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1870122104883194 tensor([0.2062, 0.2018, 0.2128, 0.1922, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18744336068630219 tensor([0.2060, 0.2021, 0.2130, 0.1915, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18736661970615387 tensor([0.2064, 0.2018, 0.2128, 0.1917, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868293136358261 tensor([0.2058, 0.2024, 0.2129, 0.1920, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18741106986999512 tensor([0.2061, 0.2020, 0.2128, 0.1917, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18751437962055206 tensor([0.2064, 0.2015, 0.2129, 0.1916, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18654534220695496 tensor([0.2066, 0.2019, 0.2130, 0.1919, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18666067719459534 tensor([0.2059, 0.2024, 0.2129, 0.1922, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1871558129787445 tensor([0.2058, 0.2018, 0.2132, 0.1921, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18712715804576874 tensor([0.2059, 0.2022, 0.2129, 0.1919, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18720747530460358 tensor([0.2061, 0.2020, 0.2130, 0.1917, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18720220029354095 tensor([0.2065, 0.2018, 0.2130, 0.1915, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18703408539295197 tensor([0.2063, 0.2020, 0.2128, 0.1918, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18718981742858887 tensor([0.2060, 0.2023, 0.2128, 0.1917, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18737128376960754 tensor([0.2061, 0.2017, 0.2128, 0.1920, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "[[4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4]]\n",
      "[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n",
      "NL_pred of 0th iteration [[4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005092253684997558  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005092073440551758  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005091732025146485  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005091245174407959  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005090628147125244  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005089892864227295  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005089052677154541  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005088119506835937  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0050871014595031735  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0050860090255737305  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005084847927093506  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00508362865447998  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005082355499267578  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005081035614013672  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005079672813415527  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0050782737731933595  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005076841354370117  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005075379371643067  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005073894023895264  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005072384357452392  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.1931726038455963 tensor([0.2077, 0.2030, 0.2140, 0.1932, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19283506274223328 tensor([0.2072, 0.2039, 0.2145, 0.1928, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19356095790863037 tensor([0.2071, 0.2032, 0.2139, 0.1936, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929987370967865 tensor([0.2067, 0.2034, 0.2143, 0.1930, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933399885892868 tensor([0.2074, 0.2032, 0.2138, 0.1933, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313956797122955 tensor([0.2070, 0.2034, 0.2144, 0.1931, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19256915152072906 tensor([0.2074, 0.2037, 0.2143, 0.1926, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19308017194271088 tensor([0.2070, 0.2034, 0.2144, 0.1931, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19294418394565582 tensor([0.2070, 0.2036, 0.2143, 0.1929, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19295711815357208 tensor([0.2078, 0.2026, 0.2146, 0.1930, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929289698600769 tensor([0.2068, 0.2032, 0.2145, 0.1929, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313444197177887 tensor([0.2071, 0.2036, 0.2143, 0.1931, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19261102378368378 tensor([0.2069, 0.2028, 0.2146, 0.1926, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19293087720870972 tensor([0.2072, 0.2040, 0.2142, 0.1929, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19274725019931793 tensor([0.2070, 0.2031, 0.2145, 0.1927, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19336453080177307 tensor([0.2072, 0.2030, 0.2140, 0.1934, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931624412536621 tensor([0.2077, 0.2031, 0.2143, 0.1932, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932784765958786 tensor([0.2068, 0.2029, 0.2142, 0.1933, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19301113486289978 tensor([0.2071, 0.2034, 0.2141, 0.1930, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19341157376766205 tensor([0.2071, 0.2029, 0.2140, 0.1934, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1924288123846054 tensor([0.2075, 0.2028, 0.2146, 0.1924, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19291338324546814 tensor([0.2078, 0.2030, 0.2140, 0.1929, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19251112639904022 tensor([0.2073, 0.2028, 0.2145, 0.1925, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19337356090545654 tensor([0.2070, 0.2035, 0.2140, 0.1934, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933681219816208 tensor([0.2069, 0.2032, 0.2140, 0.1934, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19304321706295013 tensor([0.2077, 0.2034, 0.2141, 0.1930, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19304750859737396 tensor([0.2075, 0.2031, 0.2144, 0.1930, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932290941476822 tensor([0.2069, 0.2030, 0.2138, 0.1932, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19260643422603607 tensor([0.2073, 0.2030, 0.2145, 0.1926, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1925768256187439 tensor([0.2075, 0.2024, 0.2147, 0.1926, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313594698905945 tensor([0.2076, 0.2027, 0.2144, 0.1931, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19287161529064178 tensor([0.2079, 0.2029, 0.2143, 0.1929, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19310477375984192 tensor([0.2072, 0.2032, 0.2139, 0.1931, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19317543506622314 tensor([0.2073, 0.2032, 0.2143, 0.1932, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19312627613544464 tensor([0.2072, 0.2030, 0.2140, 0.1931, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19265417754650116 tensor([0.2077, 0.2032, 0.2144, 0.1927, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19282905757427216 tensor([0.2070, 0.2035, 0.2145, 0.1928, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19324946403503418 tensor([0.2071, 0.2031, 0.2146, 0.1932, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19337664544582367 tensor([0.2072, 0.2037, 0.2135, 0.1934, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19343885779380798 tensor([0.2071, 0.2031, 0.2143, 0.1934, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19307945668697357 tensor([0.2069, 0.2033, 0.2143, 0.1931, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19310389459133148 tensor([0.2067, 0.2038, 0.2141, 0.1931, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19281740486621857 tensor([0.2078, 0.2025, 0.2142, 0.1928, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19312447309494019 tensor([0.2065, 0.2038, 0.2149, 0.1931, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19346219301223755 tensor([0.2079, 0.2029, 0.2137, 0.1935, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19321922957897186 tensor([0.2078, 0.2033, 0.2140, 0.1932, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1928921788930893 tensor([0.2071, 0.2034, 0.2143, 0.1929, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19314344227313995 tensor([0.2071, 0.2029, 0.2146, 0.1931, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313649833202362 tensor([0.2073, 0.2028, 0.2142, 0.1931, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1926630437374115 tensor([0.2070, 0.2032, 0.2141, 0.1927, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19345863163471222 tensor([0.2070, 0.2036, 0.2141, 0.1935, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19278667867183685 tensor([0.2075, 0.2035, 0.2143, 0.1928, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19283241033554077 tensor([0.2073, 0.2030, 0.2145, 0.1928, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19339688122272491 tensor([0.2065, 0.2035, 0.2142, 0.1934, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19304221868515015 tensor([0.2073, 0.2030, 0.2140, 0.1930, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19266445934772491 tensor([0.2070, 0.2034, 0.2143, 0.1927, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19306853413581848 tensor([0.2079, 0.2031, 0.2141, 0.1931, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19311030209064484 tensor([0.2071, 0.2028, 0.2143, 0.1931, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19328653812408447 tensor([0.2074, 0.2030, 0.2144, 0.1933, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19318941235542297 tensor([0.2070, 0.2028, 0.2142, 0.1932, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19292160868644714 tensor([0.2074, 0.2034, 0.2146, 0.1929, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19257864356040955 tensor([0.2076, 0.2035, 0.2144, 0.1926, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19310985505580902 tensor([0.2071, 0.2032, 0.2142, 0.1931, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19331786036491394 tensor([0.2066, 0.2037, 0.2143, 0.1933, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19292499125003815 tensor([0.2068, 0.2034, 0.2144, 0.1929, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933935284614563 tensor([0.2073, 0.2032, 0.2140, 0.1934, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929420828819275 tensor([0.2069, 0.2039, 0.2141, 0.1929, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.193359836935997 tensor([0.2072, 0.2032, 0.2142, 0.1934, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19305135309696198 tensor([0.2071, 0.2035, 0.2142, 0.1931, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1936662644147873 tensor([0.2071, 0.2033, 0.2139, 0.1937, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313158094882965 tensor([0.2072, 0.2032, 0.2143, 0.1931, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929519921541214 tensor([0.2075, 0.2036, 0.2142, 0.1930, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19328966736793518 tensor([0.2070, 0.2030, 0.2147, 0.1933, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1934618055820465 tensor([0.2070, 0.2037, 0.2138, 0.1935, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.193146750330925 tensor([0.2070, 0.2032, 0.2140, 0.1931, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1926591545343399 tensor([0.2073, 0.2032, 0.2146, 0.1927, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931684911251068 tensor([0.2072, 0.2033, 0.2146, 0.1932, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19333359599113464 tensor([0.2073, 0.2035, 0.2139, 0.1933, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19330094754695892 tensor([0.2073, 0.2030, 0.2145, 0.1933, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1926921010017395 tensor([0.2077, 0.2033, 0.2138, 0.1927, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19298632442951202 tensor([0.2075, 0.2033, 0.2142, 0.1930, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19266191124916077 tensor([0.2072, 0.2031, 0.2146, 0.1927, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933388113975525 tensor([0.2068, 0.2033, 0.2143, 0.1933, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19323162734508514 tensor([0.2075, 0.2032, 0.2140, 0.1932, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.193289116024971 tensor([0.2070, 0.2026, 0.2147, 0.1933, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19356738030910492 tensor([0.2072, 0.2030, 0.2141, 0.1936, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19285650551319122 tensor([0.2070, 0.2036, 0.2144, 0.1929, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19347403943538666 tensor([0.2071, 0.2031, 0.2140, 0.1935, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313684105873108 tensor([0.2069, 0.2040, 0.2138, 0.1931, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19333675503730774 tensor([0.2071, 0.2031, 0.2141, 0.1933, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930198073387146 tensor([0.2071, 0.2035, 0.2141, 0.1930, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19276398420333862 tensor([0.2079, 0.2029, 0.2145, 0.1928, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19288870692253113 tensor([0.2075, 0.2026, 0.2143, 0.1929, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19272226095199585 tensor([0.2073, 0.2031, 0.2148, 0.1927, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19312171638011932 tensor([0.2072, 0.2027, 0.2142, 0.1931, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19346268475055695 tensor([0.2073, 0.2030, 0.2141, 0.1935, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19317151606082916 tensor([0.2075, 0.2035, 0.2141, 0.1932, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19285447895526886 tensor([0.2069, 0.2032, 0.2146, 0.1929, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931278109550476 tensor([0.2067, 0.2040, 0.2140, 0.1931, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19298818707466125 tensor([0.2075, 0.2032, 0.2141, 0.1930, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19321340322494507 tensor([0.2073, 0.2029, 0.2143, 0.1932, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19267958402633667 tensor([0.2075, 0.2033, 0.2142, 0.1927, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932012438774109 tensor([0.2074, 0.2031, 0.2139, 0.1932, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19326439499855042 tensor([0.2068, 0.2037, 0.2141, 0.1933, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19305890798568726 tensor([0.2079, 0.2026, 0.2139, 0.1931, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19302168488502502 tensor([0.2074, 0.2035, 0.2140, 0.1930, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932077556848526 tensor([0.2072, 0.2035, 0.2143, 0.1932, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929689347743988 tensor([0.2073, 0.2034, 0.2139, 0.1930, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19309553503990173 tensor([0.2069, 0.2033, 0.2144, 0.1931, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931121051311493 tensor([0.2072, 0.2030, 0.2141, 0.1931, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930014044046402 tensor([0.2070, 0.2035, 0.2142, 0.1930, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19269348680973053 tensor([0.2080, 0.2028, 0.2143, 0.1927, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19300679862499237 tensor([0.2075, 0.2030, 0.2141, 0.1930, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19297797977924347 tensor([0.2069, 0.2037, 0.2143, 0.1930, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19277353584766388 tensor([0.2071, 0.2028, 0.2145, 0.1928, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19354304671287537 tensor([0.2071, 0.2029, 0.2141, 0.1935, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19304510951042175 tensor([0.2072, 0.2037, 0.2142, 0.1930, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932860165834427 tensor([0.2070, 0.2031, 0.2141, 0.1933, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19251996278762817 tensor([0.2071, 0.2034, 0.2144, 0.1925, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19309119880199432 tensor([0.2072, 0.2029, 0.2145, 0.1931, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19303660094738007 tensor([0.2074, 0.2031, 0.2142, 0.1930, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19241467118263245 tensor([0.2073, 0.2032, 0.2148, 0.1924, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933385729789734 tensor([0.2069, 0.2034, 0.2143, 0.1933, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1934702843427658 tensor([0.2069, 0.2031, 0.2140, 0.1935, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19314201176166534 tensor([0.2072, 0.2033, 0.2142, 0.1931, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19287645816802979 tensor([0.2078, 0.2031, 0.2141, 0.1929, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19250093400478363 tensor([0.2076, 0.2034, 0.2146, 0.1925, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929113268852234 tensor([0.2069, 0.2031, 0.2146, 0.1929, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19303041696548462 tensor([0.2072, 0.2039, 0.2139, 0.1930, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19300967454910278 tensor([0.2079, 0.2023, 0.2141, 0.1930, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19338102638721466 tensor([0.2070, 0.2035, 0.2144, 0.1934, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19332288205623627 tensor([0.2072, 0.2037, 0.2139, 0.1933, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.192925363779068 tensor([0.2076, 0.2036, 0.2136, 0.1929, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19283288717269897 tensor([0.2071, 0.2041, 0.2141, 0.1928, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19236521422863007 tensor([0.2073, 0.2024, 0.2147, 0.1924, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1928514540195465 tensor([0.2073, 0.2035, 0.2143, 0.1929, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19356688857078552 tensor([0.2075, 0.2037, 0.2140, 0.1936, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19347061216831207 tensor([0.2069, 0.2032, 0.2141, 0.1935, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19297780096530914 tensor([0.2075, 0.2030, 0.2144, 0.1930, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.193093404173851 tensor([0.2074, 0.2034, 0.2140, 0.1931, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19282831251621246 tensor([0.2071, 0.2030, 0.2145, 0.1928, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19274434447288513 tensor([0.2074, 0.2030, 0.2150, 0.1927, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932559758424759 tensor([0.2072, 0.2033, 0.2141, 0.1933, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932746022939682 tensor([0.2065, 0.2033, 0.2148, 0.1933, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19278310239315033 tensor([0.2073, 0.2032, 0.2141, 0.1928, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19331605732440948 tensor([0.2070, 0.2033, 0.2140, 0.1933, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1928151398897171 tensor([0.2073, 0.2036, 0.2143, 0.1928, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19297660887241364 tensor([0.2073, 0.2034, 0.2141, 0.1930, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19331485033035278 tensor([0.2067, 0.2033, 0.2148, 0.1933, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929168999195099 tensor([0.2074, 0.2031, 0.2145, 0.1929, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19276678562164307 tensor([0.2075, 0.2034, 0.2145, 0.1928, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19295679032802582 tensor([0.2072, 0.2033, 0.2145, 0.1930, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19302673637866974 tensor([0.2068, 0.2031, 0.2146, 0.1930, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19297848641872406 tensor([0.2072, 0.2039, 0.2141, 0.1930, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1934874802827835 tensor([0.2076, 0.2031, 0.2136, 0.1935, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19344565272331238 tensor([0.2074, 0.2029, 0.2140, 0.1934, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19300200045108795 tensor([0.2076, 0.2035, 0.2140, 0.1930, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1934618055820465 tensor([0.2074, 0.2031, 0.2140, 0.1935, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19292570650577545 tensor([0.2071, 0.2034, 0.2143, 0.1929, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1935218870639801 tensor([0.2070, 0.2025, 0.2144, 0.1935, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19364312291145325 tensor([0.2076, 0.2025, 0.2142, 0.1936, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19293703138828278 tensor([0.2072, 0.2037, 0.2143, 0.1929, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19322051107883453 tensor([0.2072, 0.2035, 0.2137, 0.1932, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19292345643043518 tensor([0.2072, 0.2028, 0.2148, 0.1929, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19306078553199768 tensor([0.2072, 0.2029, 0.2141, 0.1931, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19312679767608643 tensor([0.2073, 0.2028, 0.2143, 0.1931, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1928679496049881 tensor([0.2066, 0.2036, 0.2146, 0.1929, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19331830739974976 tensor([0.2072, 0.2030, 0.2139, 0.1933, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19306904077529907 tensor([0.2070, 0.2031, 0.2142, 0.1931, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929674595594406 tensor([0.2076, 0.2026, 0.2142, 0.1930, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19294820725917816 tensor([0.2070, 0.2031, 0.2143, 0.1929, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19283325970172882 tensor([0.2075, 0.2034, 0.2146, 0.1928, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19237005710601807 tensor([0.2073, 0.2032, 0.2141, 0.1924, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19299913942813873 tensor([0.2067, 0.2036, 0.2142, 0.1930, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19278380274772644 tensor([0.2074, 0.2028, 0.2142, 0.1928, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19310544431209564 tensor([0.2072, 0.2034, 0.2141, 0.1931, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.192772775888443 tensor([0.2072, 0.2029, 0.2147, 0.1928, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19309118390083313 tensor([0.2074, 0.2033, 0.2139, 0.1931, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929493248462677 tensor([0.2074, 0.2034, 0.2141, 0.1929, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19357948005199432 tensor([0.2074, 0.2029, 0.2139, 0.1936, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19329993426799774 tensor([0.2070, 0.2032, 0.2146, 0.1933, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19285160303115845 tensor([0.2072, 0.2036, 0.2141, 0.1929, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19321800768375397 tensor([0.2070, 0.2034, 0.2142, 0.1932, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933416724205017 tensor([0.2071, 0.2032, 0.2145, 0.1933, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19322510063648224 tensor([0.2070, 0.2030, 0.2140, 0.1932, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19328904151916504 tensor([0.2068, 0.2034, 0.2144, 0.1933, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19322656095027924 tensor([0.2074, 0.2033, 0.2143, 0.1932, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929871290922165 tensor([0.2072, 0.2032, 0.2143, 0.1930, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19311286509037018 tensor([0.2071, 0.2032, 0.2146, 0.1931, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929645836353302 tensor([0.2076, 0.2031, 0.2141, 0.1930, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19293148815631866 tensor([0.2079, 0.2028, 0.2140, 0.1929, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1925702691078186 tensor([0.2073, 0.2036, 0.2142, 0.1926, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930171698331833 tensor([0.2076, 0.2033, 0.2137, 0.1930, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19295503199100494 tensor([0.2073, 0.2034, 0.2142, 0.1930, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19275939464569092 tensor([0.2069, 0.2039, 0.2141, 0.1928, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19289520382881165 tensor([0.2073, 0.2032, 0.2141, 0.1929, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1923467516899109 tensor([0.2071, 0.2035, 0.2146, 0.1923, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931624412536621 tensor([0.2080, 0.2027, 0.2140, 0.1932, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19315142929553986 tensor([0.2069, 0.2034, 0.2145, 0.1932, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19326332211494446 tensor([0.2070, 0.2030, 0.2140, 0.1933, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19259090721607208 tensor([0.2081, 0.2028, 0.2141, 0.1926, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19302065670490265 tensor([0.2075, 0.2033, 0.2146, 0.1930, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1935703456401825 tensor([0.2070, 0.2035, 0.2141, 0.1936, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933579295873642 tensor([0.2068, 0.2039, 0.2141, 0.1934, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19276703894138336 tensor([0.2070, 0.2030, 0.2147, 0.1928, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19282644987106323 tensor([0.2073, 0.2033, 0.2148, 0.1928, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1927197128534317 tensor([0.2072, 0.2035, 0.2144, 0.1927, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19360403716564178 tensor([0.2065, 0.2034, 0.2142, 0.1936, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930888295173645 tensor([0.2071, 0.2036, 0.2143, 0.1931, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.193807914853096 tensor([0.2071, 0.2028, 0.2140, 0.1938, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19339394569396973 tensor([0.2064, 0.2034, 0.2140, 0.1934, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.192727193236351 tensor([0.2075, 0.2034, 0.2145, 0.1927, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19295935332775116 tensor([0.2076, 0.2028, 0.2140, 0.1930, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19264553487300873 tensor([0.2073, 0.2033, 0.2140, 0.1926, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19277629256248474 tensor([0.2073, 0.2029, 0.2143, 0.1928, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19283361732959747 tensor([0.2075, 0.2035, 0.2141, 0.1928, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19252094626426697 tensor([0.2075, 0.2040, 0.2143, 0.1925, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19287478923797607 tensor([0.2073, 0.2032, 0.2143, 0.1929, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19285202026367188 tensor([0.2071, 0.2036, 0.2148, 0.1929, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933414787054062 tensor([0.2070, 0.2028, 0.2145, 0.1933, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19320587813854218 tensor([0.2070, 0.2034, 0.2139, 0.1932, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932116597890854 tensor([0.2075, 0.2028, 0.2141, 0.1932, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19331960380077362 tensor([0.2071, 0.2033, 0.2141, 0.1933, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931300163269043 tensor([0.2071, 0.2029, 0.2148, 0.1931, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929284930229187 tensor([0.2069, 0.2034, 0.2147, 0.1929, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932390183210373 tensor([0.2071, 0.2031, 0.2141, 0.1932, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930537074804306 tensor([0.2074, 0.2036, 0.2144, 0.1931, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1933714598417282 tensor([0.2071, 0.2030, 0.2142, 0.1934, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19277609884738922 tensor([0.2072, 0.2034, 0.2144, 0.1928, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19301047921180725 tensor([0.2075, 0.2028, 0.2142, 0.1930, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19342553615570068 tensor([0.2074, 0.2031, 0.2142, 0.1934, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19315789639949799 tensor([0.2068, 0.2032, 0.2149, 0.1932, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19346356391906738 tensor([0.2072, 0.2030, 0.2145, 0.1935, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1937965750694275 tensor([0.2067, 0.2033, 0.2142, 0.1938, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19280225038528442 tensor([0.2072, 0.2025, 0.2146, 0.1928, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19346381723880768 tensor([0.2073, 0.2031, 0.2139, 0.1935, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19271917641162872 tensor([0.2071, 0.2034, 0.2141, 0.1927, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19288881123065948 tensor([0.2075, 0.2031, 0.2140, 0.1929, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19324365258216858 tensor([0.2069, 0.2037, 0.2141, 0.1932, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929675191640854 tensor([0.2072, 0.2033, 0.2139, 0.1930, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19283638894557953 tensor([0.2075, 0.2029, 0.2141, 0.1928, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19314263761043549 tensor([0.2077, 0.2033, 0.2142, 0.1931, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19343562424182892 tensor([0.2070, 0.2037, 0.2140, 0.1934, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19327965378761292 tensor([0.2069, 0.2031, 0.2144, 0.1933, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19314485788345337 tensor([0.2070, 0.2035, 0.2140, 0.1931, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929554045200348 tensor([0.2072, 0.2033, 0.2142, 0.1930, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19276481866836548 tensor([0.2076, 0.2031, 0.2141, 0.1928, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930639147758484 tensor([0.2074, 0.2033, 0.2140, 0.1931, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19294852018356323 tensor([0.2071, 0.2036, 0.2140, 0.1929, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19320696592330933 tensor([0.2072, 0.2031, 0.2140, 0.1932, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 1th iteration [[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005119358062744141  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005119160652160644  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.00511878776550293  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005118253707885742  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005117577075958252  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005116774559020996  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005115854263305664  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005114832401275635  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00511371898651123  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005112523078918457  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.00511125373840332  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0051099214553833  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005108529090881347  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005107085704803466  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005105597496032715  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005104068279266357  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005102503776550293  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005100908279418945  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0050992856025695804  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005097639083862304  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20420698821544647 tensor([0.2091, 0.2042, 0.2155, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20510250329971313 tensor([0.2086, 0.2051, 0.2159, 0.1876, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20440702140331268 tensor([0.2085, 0.2044, 0.2154, 0.1883, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20459745824337006 tensor([0.2081, 0.2046, 0.2158, 0.1878, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20444586873054504 tensor([0.2088, 0.2044, 0.2153, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20453907549381256 tensor([0.2084, 0.2045, 0.2159, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2049374133348465 tensor([0.2087, 0.2049, 0.2158, 0.1874, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20457223057746887 tensor([0.2084, 0.2046, 0.2158, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20476867258548737 tensor([0.2084, 0.2048, 0.2157, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038373202085495 tensor([0.2092, 0.2038, 0.2160, 0.1877, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20443451404571533 tensor([0.2082, 0.2044, 0.2160, 0.1877, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20481081306934357 tensor([0.2085, 0.2048, 0.2158, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20402516424655914 tensor([0.2083, 0.2040, 0.2160, 0.1874, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2052316516637802 tensor([0.2086, 0.2052, 0.2157, 0.1877, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20432671904563904 tensor([0.2084, 0.2043, 0.2160, 0.1875, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20422841608524323 tensor([0.2086, 0.2042, 0.2154, 0.1881, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20432314276695251 tensor([0.2091, 0.2043, 0.2157, 0.1879, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20407775044441223 tensor([0.2082, 0.2041, 0.2156, 0.1881, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046082317829132 tensor([0.2085, 0.2046, 0.2156, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20407049357891083 tensor([0.2085, 0.2041, 0.2154, 0.1882, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2039734125137329 tensor([0.2089, 0.2040, 0.2160, 0.1872, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20418882369995117 tensor([0.2092, 0.2042, 0.2155, 0.1877, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20398439466953278 tensor([0.2087, 0.2040, 0.2160, 0.1873, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20467756688594818 tensor([0.2085, 0.2047, 0.2154, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044413983821869 tensor([0.2083, 0.2044, 0.2154, 0.1881, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20456993579864502 tensor([0.2091, 0.2046, 0.2155, 0.1878, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20427924394607544 tensor([0.2089, 0.2043, 0.2158, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20416654646396637 tensor([0.2083, 0.2042, 0.2152, 0.1880, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2041408121585846 tensor([0.2087, 0.2041, 0.2160, 0.1874, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20358482003211975 tensor([0.2089, 0.2036, 0.2161, 0.1874, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038453370332718 tensor([0.2090, 0.2038, 0.2159, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040380984544754 tensor([0.2093, 0.2040, 0.2157, 0.1876, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043631374835968 tensor([0.2087, 0.2044, 0.2153, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20437072217464447 tensor([0.2086, 0.2044, 0.2157, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20424175262451172 tensor([0.2086, 0.2042, 0.2155, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043704241514206 tensor([0.2091, 0.2044, 0.2158, 0.1874, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046448141336441 tensor([0.2084, 0.2046, 0.2160, 0.1876, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20427826046943665 tensor([0.2085, 0.2043, 0.2161, 0.1880, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20486439764499664 tensor([0.2086, 0.2049, 0.2150, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20430372655391693 tensor([0.2085, 0.2043, 0.2158, 0.1882, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20454925298690796 tensor([0.2083, 0.2045, 0.2158, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2049376368522644 tensor([0.2081, 0.2049, 0.2156, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20369234681129456 tensor([0.2092, 0.2037, 0.2157, 0.1876, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20497116446495056 tensor([0.2079, 0.2050, 0.2164, 0.1879, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20408658683300018 tensor([0.2093, 0.2041, 0.2151, 0.1882, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20449459552764893 tensor([0.2092, 0.2045, 0.2154, 0.1880, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20464223623275757 tensor([0.2085, 0.2046, 0.2158, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20414648950099945 tensor([0.2085, 0.2041, 0.2160, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040276974439621 tensor([0.2087, 0.2040, 0.2157, 0.1879, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043464481830597 tensor([0.2084, 0.2043, 0.2156, 0.1875, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20483478903770447 tensor([0.2084, 0.2048, 0.2156, 0.1882, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20465858280658722 tensor([0.2089, 0.2047, 0.2158, 0.1876, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20416311919689178 tensor([0.2087, 0.2042, 0.2159, 0.1876, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047099769115448 tensor([0.2079, 0.2047, 0.2156, 0.1882, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20421162247657776 tensor([0.2088, 0.2042, 0.2154, 0.1878, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045520693063736 tensor([0.2084, 0.2046, 0.2157, 0.1874, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20432189106941223 tensor([0.2093, 0.2043, 0.2156, 0.1878, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20402361452579498 tensor([0.2085, 0.2040, 0.2158, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20419029891490936 tensor([0.2088, 0.2042, 0.2159, 0.1880, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20396849513053894 tensor([0.2084, 0.2040, 0.2156, 0.1880, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20456469058990479 tensor([0.2088, 0.2046, 0.2160, 0.1877, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20473377406597137 tensor([0.2090, 0.2047, 0.2158, 0.1874, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20439234375953674 tensor([0.2085, 0.2044, 0.2156, 0.1879, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20489902794361115 tensor([0.2081, 0.2049, 0.2158, 0.1881, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463763177394867 tensor([0.2082, 0.2046, 0.2158, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204366534948349 tensor([0.2087, 0.2044, 0.2155, 0.1882, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20513394474983215 tensor([0.2083, 0.2051, 0.2155, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20436419546604156 tensor([0.2086, 0.2044, 0.2156, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20470665395259857 tensor([0.2086, 0.2047, 0.2156, 0.1878, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20453669130802155 tensor([0.2085, 0.2045, 0.2153, 0.1884, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20434772968292236 tensor([0.2086, 0.2043, 0.2158, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20481687784194946 tensor([0.2088, 0.2048, 0.2156, 0.1878, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042318433523178 tensor([0.2084, 0.2042, 0.2162, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20492501556873322 tensor([0.2084, 0.2049, 0.2153, 0.1882, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044045478105545 tensor([0.2084, 0.2044, 0.2155, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20439758896827698 tensor([0.2087, 0.2044, 0.2160, 0.1874, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044881284236908 tensor([0.2086, 0.2045, 0.2160, 0.1879, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20464731752872467 tensor([0.2087, 0.2046, 0.2154, 0.1881, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20417200028896332 tensor([0.2087, 0.2042, 0.2159, 0.1881, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045203596353531 tensor([0.2091, 0.2045, 0.2153, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20452558994293213 tensor([0.2089, 0.2045, 0.2156, 0.1878, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20429381728172302 tensor([0.2086, 0.2043, 0.2160, 0.1875, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20453721284866333 tensor([0.2082, 0.2045, 0.2158, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20437105000019073 tensor([0.2089, 0.2044, 0.2154, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20374225080013275 tensor([0.2085, 0.2037, 0.2161, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20415647327899933 tensor([0.2086, 0.2042, 0.2156, 0.1883, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20478735864162445 tensor([0.2084, 0.2048, 0.2159, 0.1876, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20423650741577148 tensor([0.2086, 0.2042, 0.2155, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20524999499320984 tensor([0.2083, 0.2052, 0.2153, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20431138575077057 tensor([0.2085, 0.2043, 0.2155, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20466002821922302 tensor([0.2085, 0.2047, 0.2156, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20411624014377594 tensor([0.2093, 0.2041, 0.2160, 0.1875, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2037861943244934 tensor([0.2089, 0.2038, 0.2158, 0.1877, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20431871712207794 tensor([0.2088, 0.2043, 0.2162, 0.1875, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20391139388084412 tensor([0.2086, 0.2039, 0.2156, 0.1879, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20419825613498688 tensor([0.2087, 0.2042, 0.2156, 0.1882, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20465797185897827 tensor([0.2089, 0.2047, 0.2156, 0.1879, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20440790057182312 tensor([0.2083, 0.2044, 0.2161, 0.1876, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.205228790640831 tensor([0.2081, 0.2052, 0.2154, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20440158247947693 tensor([0.2089, 0.2044, 0.2156, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20409156382083893 tensor([0.2087, 0.2041, 0.2158, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045256644487381 tensor([0.2089, 0.2045, 0.2157, 0.1874, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204269677400589 tensor([0.2088, 0.2043, 0.2153, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2049451768398285 tensor([0.2082, 0.2049, 0.2156, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20377936959266663 tensor([0.2093, 0.2038, 0.2153, 0.1878, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20467598736286163 tensor([0.2088, 0.2047, 0.2154, 0.1878, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20469558238983154 tensor([0.2086, 0.2047, 0.2157, 0.1880, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20459333062171936 tensor([0.2087, 0.2046, 0.2154, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20446845889091492 tensor([0.2083, 0.2045, 0.2158, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2041419893503189 tensor([0.2086, 0.2041, 0.2155, 0.1879, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463310182094574 tensor([0.2084, 0.2046, 0.2157, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20399077236652374 tensor([0.2094, 0.2040, 0.2158, 0.1875, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2041827142238617 tensor([0.2089, 0.2042, 0.2155, 0.1878, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20493337512016296 tensor([0.2084, 0.2049, 0.2157, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20400066673755646 tensor([0.2085, 0.2040, 0.2159, 0.1876, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040470987558365 tensor([0.2085, 0.2040, 0.2156, 0.1883, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20492616295814514 tensor([0.2086, 0.2049, 0.2156, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043047547340393 tensor([0.2084, 0.2043, 0.2155, 0.1881, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20461159944534302 tensor([0.2085, 0.2046, 0.2158, 0.1873, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040371149778366 tensor([0.2086, 0.2040, 0.2159, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20430101454257965 tensor([0.2087, 0.2043, 0.2156, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044258713722229 tensor([0.2087, 0.2044, 0.2163, 0.1872, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20457576215267181 tensor([0.2083, 0.2046, 0.2158, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042989432811737 tensor([0.2083, 0.2043, 0.2154, 0.1882, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20450539886951447 tensor([0.2086, 0.2045, 0.2157, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20427222549915314 tensor([0.2092, 0.2043, 0.2155, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20457780361175537 tensor([0.2089, 0.2046, 0.2161, 0.1873, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20432090759277344 tensor([0.2083, 0.2043, 0.2161, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20512500405311584 tensor([0.2086, 0.2051, 0.2154, 0.1878, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2035146802663803 tensor([0.2093, 0.2035, 0.2155, 0.1878, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20468084514141083 tensor([0.2084, 0.2047, 0.2158, 0.1881, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20488984882831573 tensor([0.2086, 0.2049, 0.2154, 0.1881, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20479045808315277 tensor([0.2089, 0.2048, 0.2151, 0.1877, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20525842905044556 tensor([0.2085, 0.2053, 0.2155, 0.1876, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20352891087532043 tensor([0.2087, 0.2035, 0.2161, 0.1872, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20467621088027954 tensor([0.2087, 0.2047, 0.2157, 0.1876, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20488815009593964 tensor([0.2089, 0.2049, 0.2155, 0.1883, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20438063144683838 tensor([0.2084, 0.2044, 0.2156, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20420071482658386 tensor([0.2089, 0.2042, 0.2158, 0.1878, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20460425317287445 tensor([0.2088, 0.2046, 0.2155, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204194575548172 tensor([0.2085, 0.2042, 0.2160, 0.1876, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20418906211853027 tensor([0.2088, 0.2042, 0.2164, 0.1875, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20445765554904938 tensor([0.2086, 0.2045, 0.2156, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20445312559604645 tensor([0.2079, 0.2045, 0.2163, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20438805222511292 tensor([0.2087, 0.2044, 0.2156, 0.1876, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20451608300209045 tensor([0.2084, 0.2045, 0.2154, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047967165708542 tensor([0.2087, 0.2048, 0.2157, 0.1876, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20456156134605408 tensor([0.2087, 0.2046, 0.2155, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20451796054840088 tensor([0.2081, 0.2045, 0.2162, 0.1881, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20424553751945496 tensor([0.2088, 0.2042, 0.2160, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20458629727363586 tensor([0.2089, 0.2046, 0.2159, 0.1875, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044905126094818 tensor([0.2086, 0.2045, 0.2159, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20431411266326904 tensor([0.2082, 0.2043, 0.2161, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20505794882774353 tensor([0.2086, 0.2051, 0.2155, 0.1877, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20429298281669617 tensor([0.2090, 0.2043, 0.2150, 0.1882, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20410986244678497 tensor([0.2088, 0.2041, 0.2155, 0.1882, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20472845435142517 tensor([0.2089, 0.2047, 0.2155, 0.1878, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20430812239646912 tensor([0.2088, 0.2043, 0.2155, 0.1882, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463262498378754 tensor([0.2084, 0.2046, 0.2158, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20374254882335663 tensor([0.2084, 0.2037, 0.2158, 0.1883, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20372967422008514 tensor([0.2090, 0.2037, 0.2156, 0.1884, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20491939783096313 tensor([0.2086, 0.2049, 0.2158, 0.1877, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20469585061073303 tensor([0.2086, 0.2047, 0.2151, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040310651063919 tensor([0.2086, 0.2040, 0.2162, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040349841117859 tensor([0.2086, 0.2040, 0.2156, 0.1878, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20402003824710846 tensor([0.2087, 0.2040, 0.2158, 0.1879, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20475515723228455 tensor([0.2080, 0.2048, 0.2161, 0.1876, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20416446030139923 tensor([0.2086, 0.2042, 0.2154, 0.1881, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20423650741577148 tensor([0.2084, 0.2042, 0.2156, 0.1878, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038116455078125 tensor([0.2090, 0.2038, 0.2157, 0.1877, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20425672829151154 tensor([0.2084, 0.2043, 0.2157, 0.1878, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20460931956768036 tensor([0.2089, 0.2046, 0.2161, 0.1876, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043934017419815 tensor([0.2087, 0.2044, 0.2156, 0.1872, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20479969680309296 tensor([0.2082, 0.2048, 0.2156, 0.1878, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2039649337530136 tensor([0.2088, 0.2040, 0.2157, 0.1876, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046177238225937 tensor([0.2087, 0.2046, 0.2156, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20404818654060364 tensor([0.2086, 0.2040, 0.2161, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20447486639022827 tensor([0.2088, 0.2045, 0.2154, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20454268157482147 tensor([0.2088, 0.2045, 0.2155, 0.1877, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040991336107254 tensor([0.2088, 0.2041, 0.2154, 0.1883, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20435060560703278 tensor([0.2085, 0.2044, 0.2161, 0.1881, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20473764836788177 tensor([0.2086, 0.2047, 0.2156, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20460326969623566 tensor([0.2084, 0.2046, 0.2156, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20439563691616058 tensor([0.2085, 0.2044, 0.2160, 0.1881, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20418328046798706 tensor([0.2084, 0.2042, 0.2154, 0.1880, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046370655298233 tensor([0.2082, 0.2046, 0.2159, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20446772873401642 tensor([0.2088, 0.2045, 0.2157, 0.1880, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044142484664917 tensor([0.2086, 0.2044, 0.2157, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20436599850654602 tensor([0.2085, 0.2044, 0.2160, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20428988337516785 tensor([0.2090, 0.2043, 0.2155, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040468454360962 tensor([0.2093, 0.2040, 0.2154, 0.1877, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204781636595726 tensor([0.2087, 0.2048, 0.2157, 0.1874, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20448289811611176 tensor([0.2090, 0.2045, 0.2151, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20458059012889862 tensor([0.2087, 0.2046, 0.2157, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20512670278549194 tensor([0.2083, 0.2051, 0.2155, 0.1875, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20437459647655487 tensor([0.2087, 0.2044, 0.2156, 0.1877, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047092467546463 tensor([0.2084, 0.2047, 0.2161, 0.1872, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20392294228076935 tensor([0.2093, 0.2039, 0.2154, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20457792282104492 tensor([0.2083, 0.2046, 0.2160, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20415279269218445 tensor([0.2084, 0.2042, 0.2155, 0.1880, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20397508144378662 tensor([0.2095, 0.2040, 0.2155, 0.1874, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20448973774909973 tensor([0.2089, 0.2045, 0.2160, 0.1878, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047172486782074 tensor([0.2085, 0.2047, 0.2155, 0.1883, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2051188200712204 tensor([0.2082, 0.2051, 0.2155, 0.1881, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2041902244091034 tensor([0.2084, 0.2042, 0.2161, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20443932712078094 tensor([0.2087, 0.2044, 0.2162, 0.1876, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20470532774925232 tensor([0.2086, 0.2047, 0.2159, 0.1875, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20464682579040527 tensor([0.2079, 0.2046, 0.2156, 0.1884, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20477189123630524 tensor([0.2085, 0.2048, 0.2158, 0.1879, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20401883125305176 tensor([0.2085, 0.2040, 0.2155, 0.1886, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20462340116500854 tensor([0.2078, 0.2046, 0.2154, 0.1882, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20458437502384186 tensor([0.2089, 0.2046, 0.2159, 0.1875, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20401693880558014 tensor([0.2090, 0.2040, 0.2154, 0.1877, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044396698474884 tensor([0.2087, 0.2044, 0.2154, 0.1875, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20409144461154938 tensor([0.2087, 0.2041, 0.2158, 0.1876, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20464958250522614 tensor([0.2089, 0.2046, 0.2156, 0.1876, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20520150661468506 tensor([0.2089, 0.2052, 0.2158, 0.1873, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20442992448806763 tensor([0.2087, 0.2044, 0.2157, 0.1876, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047944813966751 tensor([0.2085, 0.2048, 0.2162, 0.1876, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20401716232299805 tensor([0.2085, 0.2040, 0.2160, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463743805885315 tensor([0.2084, 0.2046, 0.2154, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20396728813648224 tensor([0.2089, 0.2040, 0.2156, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20446564257144928 tensor([0.2085, 0.2045, 0.2155, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2041168361902237 tensor([0.2085, 0.2041, 0.2162, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20454873144626617 tensor([0.2083, 0.2045, 0.2161, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042999267578125 tensor([0.2085, 0.2043, 0.2155, 0.1880, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047659009695053 tensor([0.2088, 0.2048, 0.2158, 0.1878, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042071372270584 tensor([0.2086, 0.2042, 0.2156, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045890837907791 tensor([0.2086, 0.2046, 0.2159, 0.1875, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204011008143425 tensor([0.2089, 0.2040, 0.2157, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20429307222366333 tensor([0.2088, 0.2043, 0.2156, 0.1882, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20440921187400818 tensor([0.2082, 0.2044, 0.2163, 0.1879, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042023092508316 tensor([0.2087, 0.2042, 0.2159, 0.1882, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20451563596725464 tensor([0.2081, 0.2045, 0.2156, 0.1885, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20374032855033875 tensor([0.2086, 0.2037, 0.2161, 0.1876, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042846977710724 tensor([0.2087, 0.2043, 0.2154, 0.1882, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20461830496788025 tensor([0.2085, 0.2046, 0.2156, 0.1875, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204300194978714 tensor([0.2089, 0.2043, 0.2154, 0.1876, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20492632687091827 tensor([0.2083, 0.2049, 0.2155, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045145183801651 tensor([0.2086, 0.2045, 0.2153, 0.1877, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20404787361621857 tensor([0.2089, 0.2040, 0.2155, 0.1876, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20447754859924316 tensor([0.2091, 0.2045, 0.2156, 0.1879, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20491375029087067 tensor([0.2084, 0.2049, 0.2154, 0.1882, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20427784323692322 tensor([0.2083, 0.2043, 0.2158, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047232985496521 tensor([0.2084, 0.2047, 0.2154, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045101374387741 tensor([0.2086, 0.2045, 0.2156, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043091356754303 tensor([0.2090, 0.2043, 0.2156, 0.1875, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20450076460838318 tensor([0.2088, 0.2045, 0.2154, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20474694669246674 tensor([0.2085, 0.2047, 0.2154, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204255610704422 tensor([0.2086, 0.2043, 0.2155, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 2th iteration []\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.19482608139514923 tensor([0.2038, 0.1991, 0.2005, 0.1948, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19821186363697052 tensor([0.2003, 0.2014, 0.1982, 0.1988, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19689320027828217 tensor([0.2015, 0.1986, 0.2028, 0.1969, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19740670919418335 tensor([0.1976, 0.1979, 0.2004, 0.2067, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19723652303218842 tensor([0.2026, 0.1972, 0.2011, 0.1975, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19752375781536102 tensor([0.2028, 0.1983, 0.2004, 0.1975, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19741475582122803 tensor([0.1993, 0.2052, 0.1974, 0.1977, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19709357619285583 tensor([0.2000, 0.1989, 0.2057, 0.1983, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.196674644947052 tensor([0.1967, 0.1988, 0.1993, 0.2077, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19597530364990234 tensor([0.2027, 0.1960, 0.2024, 0.1979, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19509966671466827 tensor([0.2029, 0.1991, 0.1996, 0.1951, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19648072123527527 tensor([0.1965, 0.2086, 0.1979, 0.1992, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1959962546825409 tensor([0.2020, 0.2005, 0.1990, 0.1960, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975274682044983 tensor([0.1977, 0.1998, 0.1999, 0.2052, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19505153596401215 tensor([0.2040, 0.1981, 0.2012, 0.1951, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19441692531108856 tensor([0.2036, 0.1985, 0.2000, 0.1944, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19760474562644958 tensor([0.1976, 0.2050, 0.1992, 0.1997, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1963699758052826 tensor([0.2013, 0.2018, 0.1995, 0.1964, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19855622947216034 tensor([0.2005, 0.1999, 0.2002, 0.1986, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19718033075332642 tensor([0.2022, 0.1972, 0.2000, 0.1976, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19281333684921265 tensor([0.2039, 0.2000, 0.1985, 0.1928, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19806624948978424 tensor([0.1990, 0.2046, 0.1987, 0.1981, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19542604684829712 tensor([0.2015, 0.2001, 0.2013, 0.1954, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19641652703285217 tensor([0.1971, 0.2026, 0.1977, 0.2062, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1925031840801239 tensor([0.2046, 0.1994, 0.1988, 0.1925, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19548462331295013 tensor([0.2042, 0.1996, 0.1988, 0.1955, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19689832627773285 tensor([0.1990, 0.2080, 0.1969, 0.1971, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1941995918750763 tensor([0.2039, 0.1969, 0.2024, 0.1942, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1959061473608017 tensor([0.1978, 0.1973, 0.2021, 0.2069, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19416862726211548 tensor([0.2033, 0.1992, 0.1985, 0.1942, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1979011595249176 tensor([0.2005, 0.1999, 0.2007, 0.1979, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19658425450325012 tensor([0.1966, 0.2064, 0.1985, 0.2012, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1960485726594925 tensor([0.2023, 0.1964, 0.2050, 0.1960, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19742053747177124 tensor([0.1995, 0.2013, 0.1997, 0.2021, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19807079434394836 tensor([0.2004, 0.1992, 0.2009, 0.1981, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.195220947265625 tensor([0.2022, 0.2021, 0.1980, 0.1952, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19574327766895294 tensor([0.2023, 0.2011, 0.1984, 0.1957, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19835585355758667 tensor([0.2001, 0.2029, 0.2001, 0.1984, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19714829325675964 tensor([0.1976, 0.1993, 0.1993, 0.2067, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19511887431144714 tensor([0.2019, 0.2027, 0.1996, 0.1951, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1957433968782425 tensor([0.2018, 0.2014, 0.1992, 0.1957, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19636917114257812 tensor([0.1977, 0.2091, 0.1964, 0.1982, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19601672887802124 tensor([0.2033, 0.1967, 0.2033, 0.1960, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19690488278865814 tensor([0.1981, 0.1990, 0.2003, 0.2058, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19731062650680542 tensor([0.2024, 0.1973, 0.1996, 0.1977, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979873925447464 tensor([0.2008, 0.2015, 0.2011, 0.1986, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19560810923576355 tensor([0.1956, 0.2080, 0.1975, 0.2029, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19685183465480804 tensor([0.2008, 0.1986, 0.2059, 0.1978, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19829104840755463 tensor([0.1984, 0.2016, 0.1983, 0.2029, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19424377381801605 tensor([0.2021, 0.2023, 0.1978, 0.1942, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19722789525985718 tensor([0.2009, 0.1996, 0.1989, 0.1972, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.195546954870224 tensor([0.1971, 0.2085, 0.1984, 0.2004, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19698162376880646 tensor([0.2020, 0.1998, 0.2026, 0.1970, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1966007798910141 tensor([0.1969, 0.2005, 0.1992, 0.2068, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19889001548290253 tensor([0.2019, 0.1990, 0.2005, 0.1989, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19409243762493134 tensor([0.2034, 0.1991, 0.2004, 0.1941, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19782626628875732 tensor([0.1978, 0.2051, 0.1985, 0.2007, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19772998988628387 tensor([0.2014, 0.1977, 0.2019, 0.1984, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982797533273697 tensor([0.1987, 0.2009, 0.1984, 0.2038, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969096064567566 tensor([0.2022, 0.1984, 0.1992, 0.1969, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19577515125274658 tensor([0.2029, 0.1994, 0.1994, 0.1958, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19784346222877502 tensor([0.1992, 0.2041, 0.1990, 0.1999, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19731274247169495 tensor([0.2012, 0.1973, 0.2013, 0.2000, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19831643998622894 tensor([0.1992, 0.1998, 0.1983, 0.2003, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19726639986038208 tensor([0.2007, 0.1991, 0.1977, 0.1973, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19474250078201294 tensor([0.2039, 0.1985, 0.2000, 0.1947, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19806179404258728 tensor([0.1981, 0.2049, 0.1994, 0.1991, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19611628353595734 tensor([0.2010, 0.1961, 0.2053, 0.1979, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1991838812828064 tensor([0.1997, 0.1995, 0.1992, 0.2011, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19595830142498016 tensor([0.2025, 0.1986, 0.1993, 0.1960, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.195221945643425 tensor([0.2048, 0.1983, 0.1997, 0.1952, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19454379379749298 tensor([0.1961, 0.2121, 0.1945, 0.1973, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19771353900432587 tensor([0.2009, 0.1986, 0.2046, 0.1977, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1984432339668274 tensor([0.1994, 0.2006, 0.1984, 0.2013, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19775956869125366 tensor([0.2013, 0.2007, 0.2000, 0.1978, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1925782412290573 tensor([0.2058, 0.1997, 0.1989, 0.1926, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969747096300125 tensor([0.1983, 0.2042, 0.1970, 0.1991, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19854001700878143 tensor([0.2013, 0.1988, 0.2027, 0.1985, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976756602525711 tensor([0.1981, 0.1984, 0.1990, 0.2067, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19494757056236267 tensor([0.2030, 0.2001, 0.1999, 0.1949, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19563455879688263 tensor([0.2030, 0.1983, 0.2017, 0.1956, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1964322179555893 tensor([0.1989, 0.2046, 0.1979, 0.1964, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19690436124801636 tensor([0.2006, 0.2008, 0.2028, 0.1969, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19610559940338135 tensor([0.1971, 0.1996, 0.2000, 0.2072, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932017058134079 tensor([0.2041, 0.1987, 0.1974, 0.1932, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1960677057504654 tensor([0.2026, 0.1986, 0.1992, 0.1961, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19521130621433258 tensor([0.1958, 0.2096, 0.1968, 0.2026, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19635331630706787 tensor([0.2016, 0.1993, 0.2012, 0.1964, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1969519853591919 tensor([0.1970, 0.1989, 0.1987, 0.2079, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19443956017494202 tensor([0.2024, 0.2005, 0.1977, 0.1944, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19687290489673615 tensor([0.2027, 0.1995, 0.2000, 0.1969, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19681116938591003 tensor([0.1977, 0.2085, 0.1970, 0.2001, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19702188670635223 tensor([0.2015, 0.1970, 0.2029, 0.1993, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19487740099430084 tensor([0.2033, 0.2025, 0.1979, 0.1949, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19620190560817719 tensor([0.2022, 0.2014, 0.1981, 0.1962, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19807322323322296 tensor([0.2024, 0.1985, 0.2011, 0.1981, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972505897283554 tensor([0.1991, 0.2058, 0.1973, 0.1987, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1969752311706543 tensor([0.2001, 0.1982, 0.2062, 0.1984, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972787231206894 tensor([0.1975, 0.1987, 0.1999, 0.2066, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19588688015937805 tensor([0.2032, 0.1959, 0.2015, 0.1965, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19730855524539948 tensor([0.2021, 0.2003, 0.1987, 0.1973, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1957530826330185 tensor([0.1974, 0.2075, 0.1981, 0.2012, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19669221341609955 tensor([0.2020, 0.1979, 0.2029, 0.1967, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19521957635879517 tensor([0.1968, 0.1975, 0.2006, 0.2098, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19551415741443634 tensor([0.2031, 0.1982, 0.2017, 0.1955, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19655922055244446 tensor([0.2017, 0.2005, 0.1995, 0.1966, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964579075574875 tensor([0.1974, 0.2088, 0.1975, 0.1999, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.197476327419281 tensor([0.2021, 0.1981, 0.2024, 0.1975, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19688455760478973 tensor([0.1974, 0.2009, 0.1993, 0.2055, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19632723927497864 tensor([0.2022, 0.1998, 0.1997, 0.1963, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1983720362186432 tensor([0.2006, 0.1991, 0.1990, 0.1984, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19548846781253815 tensor([0.1983, 0.2045, 0.1995, 0.2022, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19785939157009125 tensor([0.2002, 0.1979, 0.2015, 0.1998, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19857747852802277 tensor([0.1989, 0.1999, 0.2005, 0.2021, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19481196999549866 tensor([0.2031, 0.1993, 0.1981, 0.1948, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.197750061750412 tensor([0.2025, 0.1980, 0.2009, 0.1978, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19604256749153137 tensor([0.1971, 0.2045, 0.1988, 0.2036, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19424961507320404 tensor([0.2018, 0.1998, 0.2000, 0.1942, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19755831360816956 tensor([0.1976, 0.1992, 0.2012, 0.2045, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19488230347633362 tensor([0.2022, 0.2009, 0.1976, 0.1949, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19165897369384766 tensor([0.2042, 0.2009, 0.1966, 0.1917, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1974562406539917 tensor([0.1978, 0.2075, 0.1975, 0.1981, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19765205681324005 tensor([0.2009, 0.2001, 0.2026, 0.1977, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1984962373971939 tensor([0.2014, 0.1991, 0.1986, 0.1985, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1966661661863327 tensor([0.2010, 0.2007, 0.1999, 0.1967, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1949140727519989 tensor([0.2022, 0.2013, 0.2003, 0.1949, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1975567787885666 tensor([0.1987, 0.2047, 0.1976, 0.2014, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19784162938594818 tensor([0.2002, 0.1987, 0.2050, 0.1983, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19890223443508148 tensor([0.1992, 0.1995, 0.1989, 0.2033, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1945846825838089 tensor([0.2019, 0.2025, 0.1980, 0.1946, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19676052033901215 tensor([0.2021, 0.1991, 0.1985, 0.1968, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19574689865112305 tensor([0.1998, 0.2070, 0.1963, 0.1957, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19694192707538605 tensor([0.2016, 0.2024, 0.2009, 0.1969, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1962592452764511 tensor([0.1963, 0.2015, 0.1987, 0.2070, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19569949805736542 tensor([0.2026, 0.1995, 0.1994, 0.1957, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969817578792572 tensor([0.2020, 0.1984, 0.1992, 0.1970, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19827315211296082 tensor([0.1990, 0.2037, 0.1983, 0.1992, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19787132740020752 tensor([0.2015, 0.1979, 0.2033, 0.1980, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19936656951904297 tensor([0.1994, 0.2002, 0.2000, 0.2005, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19362694025039673 tensor([0.2041, 0.1978, 0.2008, 0.1936, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1947890669107437 tensor([0.2040, 0.2019, 0.1969, 0.1948, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19755487143993378 tensor([0.1989, 0.2077, 0.1976, 0.1978, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19578643143177032 tensor([0.2033, 0.1974, 0.2027, 0.1958, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19596771895885468 tensor([0.1961, 0.2041, 0.1992, 0.2046, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19669204950332642 tensor([0.2012, 0.1993, 0.1989, 0.1967, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19688840210437775 tensor([0.2018, 0.1978, 0.2007, 0.1969, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975625902414322 tensor([0.1987, 0.2078, 0.1983, 0.1976, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1948387622833252 tensor([0.2026, 0.1989, 0.2008, 0.1948, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1943567544221878 tensor([0.1986, 0.1987, 0.2019, 0.2065, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19556932151317596 tensor([0.2025, 0.1988, 0.2009, 0.1956, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969888061285019 tensor([0.2023, 0.2009, 0.1992, 0.1970, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19547761976718903 tensor([0.1976, 0.2040, 0.2011, 0.2018, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19674885272979736 tensor([0.2018, 0.1967, 0.2040, 0.1976, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19391854107379913 tensor([0.1956, 0.2014, 0.2002, 0.2089, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19460543990135193 tensor([0.2032, 0.1979, 0.1994, 0.1946, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1941537708044052 tensor([0.2034, 0.1993, 0.1986, 0.1942, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19542856514453888 tensor([0.1991, 0.2080, 0.1971, 0.1954, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1979777067899704 tensor([0.2014, 0.1987, 0.2029, 0.1980, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19763299822807312 tensor([0.1976, 0.1995, 0.2008, 0.2032, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19260621070861816 tensor([0.2053, 0.1961, 0.1994, 0.1926, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19796858727931976 tensor([0.2000, 0.2020, 0.1980, 0.1990, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19590012729167938 tensor([0.1959, 0.2080, 0.1969, 0.2016, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19734938442707062 tensor([0.2009, 0.1980, 0.2042, 0.1973, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19779334962368011 tensor([0.1998, 0.1997, 0.1997, 0.2030, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19675882160663605 tensor([0.2018, 0.1998, 0.1983, 0.1968, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19305051863193512 tensor([0.2034, 0.2021, 0.1980, 0.1931, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19699329137802124 tensor([0.1980, 0.2063, 0.1999, 0.1989, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1976359635591507 tensor([0.2009, 0.1976, 0.2032, 0.1985, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19537323713302612 tensor([0.1965, 0.1997, 0.2003, 0.2081, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19274947047233582 tensor([0.2040, 0.2002, 0.1978, 0.1927, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970839500427246 tensor([0.2020, 0.1992, 0.1994, 0.1971, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19556336104869843 tensor([0.1971, 0.2100, 0.1980, 0.1994, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19764696061611176 tensor([0.2018, 0.1976, 0.2029, 0.1977, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19683265686035156 tensor([0.1968, 0.1975, 0.1997, 0.2085, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19363968074321747 tensor([0.2035, 0.2005, 0.1986, 0.1936, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1984405517578125 tensor([0.2017, 0.1999, 0.1999, 0.1984, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19639837741851807 tensor([0.1976, 0.2036, 0.1999, 0.2024, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1992122083902359 tensor([0.2002, 0.2005, 0.1994, 0.2006, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19630183279514313 tensor([0.1966, 0.2008, 0.1987, 0.2076, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1956026554107666 tensor([0.2041, 0.1970, 0.2005, 0.1956, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19475002586841583 tensor([0.2045, 0.1985, 0.1993, 0.1948, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969423145055771 tensor([0.1991, 0.2016, 0.1975, 0.1969, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19687460362911224 tensor([0.1989, 0.1983, 0.2051, 0.2008, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19813892245292664 tensor([0.2000, 0.1981, 0.1999, 0.2018, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975296586751938 tensor([0.2022, 0.1987, 0.2000, 0.1975, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724500179290771 tensor([0.2010, 0.2006, 0.1979, 0.1972, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19771523773670197 tensor([0.1984, 0.2044, 0.1983, 0.2012, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19465689361095428 tensor([0.2034, 0.1989, 0.2023, 0.1947, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19560638070106506 tensor([0.1971, 0.2013, 0.2006, 0.2053, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1948758214712143 tensor([0.2021, 0.2005, 0.1987, 0.1949, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1928676962852478 tensor([0.2040, 0.2009, 0.2006, 0.1929, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19623668491840363 tensor([0.1995, 0.2077, 0.1962, 0.1970, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1953565925359726 tensor([0.2023, 0.1996, 0.2010, 0.1954, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19676125049591064 tensor([0.1968, 0.1992, 0.2000, 0.2068, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19772081077098846 tensor([0.2008, 0.2003, 0.1996, 0.1977, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19419178366661072 tensor([0.2034, 0.2008, 0.1997, 0.1942, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19647353887557983 tensor([0.1972, 0.2053, 0.1984, 0.2026, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19705082476139069 tensor([0.2031, 0.1983, 0.2017, 0.1971, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19712956249713898 tensor([0.1971, 0.2001, 0.1986, 0.2068, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19459304213523865 tensor([0.2021, 0.2015, 0.1983, 0.1946, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19484539330005646 tensor([0.2045, 0.1980, 0.2011, 0.1948, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19685615599155426 tensor([0.1969, 0.2078, 0.1975, 0.2007, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19869792461395264 tensor([0.1987, 0.1989, 0.2039, 0.1997, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19643884897232056 tensor([0.1967, 0.1975, 0.2010, 0.2084, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19444037973880768 tensor([0.2035, 0.1994, 0.1979, 0.1944, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19585202634334564 tensor([0.1997, 0.2035, 0.1978, 0.1959, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19712474942207336 tensor([0.1979, 0.2039, 0.1971, 0.2001, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1973745971918106 tensor([0.2012, 0.1983, 0.2011, 0.1974, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975170075893402 tensor([0.1976, 0.2030, 0.1981, 0.2037, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19313649833202362 tensor([0.2047, 0.2003, 0.1981, 0.1931, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19749344885349274 tensor([0.2015, 0.1993, 0.1995, 0.1975, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19487899541854858 tensor([0.1992, 0.2074, 0.1949, 0.1972, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19777852296829224 tensor([0.2016, 0.1978, 0.2031, 0.1981, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19900941848754883 tensor([0.1993, 0.2001, 0.1990, 0.2015, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19221122562885284 tensor([0.2044, 0.1989, 0.1978, 0.1922, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19387058913707733 tensor([0.2042, 0.1998, 0.1989, 0.1939, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19615811109542847 tensor([0.1971, 0.2042, 0.1989, 0.2037, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19581063091754913 tensor([0.2015, 0.2027, 0.1982, 0.1958, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1959811896085739 tensor([0.1974, 0.2010, 0.2012, 0.2045, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19673533737659454 tensor([0.2018, 0.2012, 0.1989, 0.1967, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1950635015964508 tensor([0.2015, 0.2017, 0.2000, 0.1951, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19734574854373932 tensor([0.2001, 0.2031, 0.1995, 0.1973, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19681969285011292 tensor([0.2008, 0.1986, 0.2049, 0.1968, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1947840303182602 tensor([0.1972, 0.1998, 0.2011, 0.2072, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1981281042098999 tensor([0.2003, 0.1991, 0.1981, 0.1984, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19544006884098053 tensor([0.2018, 0.2007, 0.1989, 0.1954, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19436654448509216 tensor([0.1955, 0.2090, 0.1982, 0.2030, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19646668434143066 tensor([0.2014, 0.1999, 0.2033, 0.1965, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19796709716320038 tensor([0.1990, 0.1980, 0.2000, 0.2046, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19680821895599365 tensor([0.2018, 0.1998, 0.2003, 0.1968, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19554120302200317 tensor([0.2031, 0.2003, 0.2008, 0.1955, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19718962907791138 tensor([0.2010, 0.2008, 0.1995, 0.1972, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1970890611410141 tensor([0.2007, 0.1998, 0.2002, 0.1971, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1968451887369156 tensor([0.1997, 0.2002, 0.2006, 0.2027, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1962229609489441 tensor([0.2025, 0.1999, 0.1992, 0.1962, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19348061084747314 tensor([0.2047, 0.1975, 0.2003, 0.1935, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19829517602920532 tensor([0.1983, 0.2048, 0.1992, 0.1988, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19690768420696259 tensor([0.2009, 0.2005, 0.2027, 0.1969, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1973995864391327 tensor([0.1974, 0.1989, 0.1991, 0.2051, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1958293616771698 tensor([0.2017, 0.2024, 0.1992, 0.1958, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19298526644706726 tensor([0.2051, 0.1996, 0.1985, 0.1930, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19701328873634338 tensor([0.1984, 0.2065, 0.1970, 0.1991, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978314071893692 tensor([0.1997, 0.1997, 0.2043, 0.1985, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19682076573371887 tensor([0.1976, 0.2005, 0.1982, 0.2069, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1946706622838974 tensor([0.2035, 0.1980, 0.2004, 0.1947, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1969461441040039 tensor([0.2023, 0.2015, 0.1984, 0.1969, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19461649656295776 tensor([0.2003, 0.2056, 0.1968, 0.1946, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19735591113567352 tensor([0.2017, 0.2003, 0.2032, 0.1975, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19754767417907715 tensor([0.1985, 0.1990, 0.1995, 0.2054, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1956017166376114 tensor([0.2025, 0.2001, 0.1987, 0.1956, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [2], [3], [4], [1], [3], [2], [4], [0], [1], [3], [0], [3], [4], [3], [3], [0], [3], [3], [1], [3], [3], [3], [4], [3], [3], [2], [3], [4], [3], [3], [0], [3], [4], [3], [3], [3], [3], [4], [3], [3], [2], [3], [4], [1], [4], [0], [4], [2], [3], [3], [4], [3], [4], [3], [3], [0], [1], [4], [3], [3], [4], [1], [2], [3], [3], [0], [1], [2], [3], [3], [2], [3], [2], [3], [3], [2], [3], [4], [3], [3], [3], [3], [4], [3], [3], [4], [3], [0], [3], [3], [4], [1], [3], [3], [3], [2], [4], [4], [1], [3], [4], [3], [4], [3], [3], [4], [3], [4], [3], [3], [4], [1], [4], [3], [3], [4], [3], [4], [3], [3], [2], [3], [3], [3], [3], [2], [4], [2], [3], [3], [3], [3], [0], [3], [3], [2], [1], [0], [3], [3], [2], [3], [4], [3], [3], [3], [3], [4], [3], [3], [4], [1], [4], [3], [3], [3], [3], [0], [3], [2], [0], [3], [4], [3], [3], [4], [1], [4], [3], [3], [4], [1], [0], [3], [3], [4], [4], [4], [3], [3], [3], [4], [1], [3], [3], [4], [3], [4], [3], [3], [2], [3], [0], [3], [3], [4], [3], [0], [3], [3], [0], [0], [4], [3], [3], [2], [3], [4], [3], [3], [2], [1], [2], [3], [3], [4], [3], [4], [3], [3], [3], [3], [4], [2], [3], [4], [3], [1], [3], [3], [3], [3], [4], [3], [3], [0], [3], [0], [3], [3], [2], [4], [4], [3], [3], [3], [4], [4], [3]]\n",
      "[[0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4]]\n",
      "NL_pred of 0th iteration [[3], [2], [3], [4], [1], [3], [2], [4], [0], [1], [3], [0], [3], [4], [3], [3], [0], [3], [3], [1], [3], [3], [3], [4], [3], [3], [2], [3], [4], [3], [3], [0], [3], [4], [3], [3], [3], [3], [4], [3], [3], [2], [3], [4], [1], [4], [0], [4], [2], [3], [3], [4], [3], [4], [3], [3], [0], [1], [4], [3], [3], [4], [1], [2], [3], [3], [0], [1], [2], [3], [3], [2], [3], [2], [3], [3], [2], [3], [4], [3], [3], [3], [3], [4], [3], [3], [4], [3], [0], [3], [3], [4], [1], [3], [3], [3], [2], [4], [4], [1], [3], [4], [3], [4], [3], [3], [4], [3], [4], [3], [3], [4], [1], [4], [3], [3], [4], [3], [4], [3], [3], [2], [3], [3], [3], [3], [2], [4], [2], [3], [3], [3], [3], [0], [3], [3], [2], [1], [0], [3], [3], [2], [3], [4], [3], [3], [3], [3], [4], [3], [3], [4], [1], [4], [3], [3], [3], [3], [0], [3], [2], [0], [3], [4], [3], [3], [4], [1], [4], [3], [3], [4], [1], [0], [3], [3], [4], [4], [4], [3], [3], [3], [4], [1], [3], [3], [4], [3], [4], [3], [3], [2], [3], [0], [3], [3], [4], [3], [0], [3], [3], [0], [0], [4], [3], [3], [2], [3], [4], [3], [3], [2], [1], [2], [3], [3], [4], [3], [4], [3], [3], [3], [3], [4], [2], [3], [4], [3], [1], [3], [3], [3], [3], [4], [3], [3], [0], [3], [0], [3], [3], [2], [4], [4], [3], [3], [3], [4], [4], [3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.00513387393951416  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005133822917938232  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005133721828460693  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005133578300476074  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005133396148681641  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005133179664611817  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005132931709289551  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0051326556205749515  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0051323542594909665  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005132031917572021  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.0051316885948181155  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005131327152252198  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005130949974060058  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005130558490753174  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005130154609680176  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005129738807678223  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005129313468933106  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005128878116607666  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005128436088562012  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00512798547744751  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20021860301494598 tensor([0.2050, 0.2002, 0.2011, 0.1918, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.195768341422081 tensor([0.2014, 0.2025, 0.1989, 0.1958, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19971305131912231 tensor([0.2027, 0.1997, 0.2035, 0.1938, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19874660670757294 tensor([0.1987, 0.1991, 0.2011, 0.2035, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.194438636302948 tensor([0.2038, 0.1984, 0.2017, 0.1944, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19943204522132874 tensor([0.2040, 0.1994, 0.2011, 0.1944, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19459037482738495 tensor([0.2004, 0.2064, 0.1981, 0.1946, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1951749473810196 tensor([0.2012, 0.2001, 0.2064, 0.1952, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19772404432296753 tensor([0.1978, 0.1999, 0.2000, 0.2046, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.194835364818573 tensor([0.2039, 0.1971, 0.2031, 0.1948, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20014417171478271 tensor([0.2040, 0.2001, 0.2003, 0.1921, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1961466372013092 tensor([0.1976, 0.2098, 0.1986, 0.1961, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1996745616197586 tensor([0.2032, 0.2017, 0.1997, 0.1930, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19883809983730316 tensor([0.1988, 0.2009, 0.2006, 0.2020, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.199247345328331 tensor([0.2051, 0.1992, 0.2018, 0.1920, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19961696863174438 tensor([0.2048, 0.1996, 0.2006, 0.1914, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19657789170742035 tensor([0.1988, 0.2062, 0.1999, 0.1966, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200206458568573 tensor([0.2025, 0.2030, 0.2002, 0.1933, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20089927315711975 tensor([0.2016, 0.2010, 0.2009, 0.1954, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19455061852931976 tensor([0.2034, 0.1983, 0.2006, 0.1946, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19912096858024597 tensor([0.2050, 0.2011, 0.1991, 0.1899, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19934387505054474 tensor([0.2001, 0.2058, 0.1993, 0.1950, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20122286677360535 tensor([0.2027, 0.2012, 0.2019, 0.1924, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1982719600200653 tensor([0.1983, 0.2038, 0.1984, 0.2031, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19946609437465668 tensor([0.2057, 0.2005, 0.1995, 0.1895, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19945882260799408 tensor([0.2054, 0.2007, 0.1995, 0.1924, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19404299557209015 tensor([0.2002, 0.2091, 0.1975, 0.1940, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19801312685012817 tensor([0.2051, 0.1980, 0.2030, 0.1912, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19845156371593475 tensor([0.1990, 0.1985, 0.2028, 0.2037, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19918031990528107 tensor([0.2044, 0.2003, 0.1992, 0.1912, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20104378461837769 tensor([0.2017, 0.2011, 0.2013, 0.1948, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19743609428405762 tensor([0.1977, 0.2076, 0.1992, 0.1981, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1975509226322174 tensor([0.2034, 0.1976, 0.2056, 0.1930, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1989736109972 tensor([0.2007, 0.2025, 0.2004, 0.1990, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2003953456878662 tensor([0.2016, 0.2004, 0.2016, 0.1950, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19868606328964233 tensor([0.2034, 0.2032, 0.1987, 0.1922, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19902579486370087 tensor([0.2035, 0.2023, 0.1990, 0.1927, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19858640432357788 tensor([0.2013, 0.2041, 0.2008, 0.1953, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19873248040676117 tensor([0.1987, 0.2005, 0.2000, 0.2036, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20022062957286835 tensor([0.2031, 0.2038, 0.2002, 0.1921, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19983352720737457 tensor([0.2030, 0.2026, 0.1998, 0.1927, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19518724083900452 tensor([0.1988, 0.2103, 0.1970, 0.1952, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19781909883022308 tensor([0.2045, 0.1978, 0.2040, 0.1930, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1992359757423401 tensor([0.1992, 0.2001, 0.2010, 0.2026, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1946340799331665 tensor([0.2036, 0.1984, 0.2003, 0.1946, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19547393918037415 tensor([0.2020, 0.2027, 0.2018, 0.1955, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1961095929145813 tensor([0.1967, 0.2092, 0.1982, 0.1998, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19476498663425446 tensor([0.2020, 0.1998, 0.2066, 0.1948, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19884687662124634 tensor([0.1996, 0.2028, 0.1990, 0.1998, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19843992590904236 tensor([0.2033, 0.2035, 0.1984, 0.1912, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19953882694244385 tensor([0.2021, 0.2008, 0.1995, 0.1942, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19733349978923798 tensor([0.1983, 0.2098, 0.1990, 0.1973, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19877785444259644 tensor([0.2032, 0.2009, 0.2033, 0.1939, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19809243083000183 tensor([0.1981, 0.2016, 0.1999, 0.2037, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19977398216724396 tensor([0.2031, 0.2001, 0.2012, 0.1958, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20026074349880219 tensor([0.2046, 0.2003, 0.2011, 0.1910, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976371556520462 tensor([0.1990, 0.2063, 0.1992, 0.1976, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19530946016311646 tensor([0.2026, 0.1989, 0.2025, 0.1953, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19906775653362274 tensor([0.1998, 0.2021, 0.1991, 0.2007, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19953559339046478 tensor([0.2034, 0.1995, 0.1998, 0.1938, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000349909067154 tensor([0.2041, 0.2006, 0.2000, 0.1927, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19685308635234833 tensor([0.2003, 0.2053, 0.1996, 0.1969, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19693146646022797 tensor([0.2024, 0.1985, 0.2020, 0.1969, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1971815973520279 tensor([0.2003, 0.2010, 0.1990, 0.1972, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1983480453491211 tensor([0.2019, 0.2003, 0.1983, 0.1942, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19960269331932068 tensor([0.2051, 0.1996, 0.2006, 0.1917, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19603776931762695 tensor([0.1992, 0.2061, 0.2001, 0.1960, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19478969275951385 tensor([0.2022, 0.1972, 0.2060, 0.1948, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19799202680587769 tensor([0.2009, 0.2006, 0.1999, 0.1980, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19978152215480804 tensor([0.2037, 0.1998, 0.1999, 0.1929, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19941042363643646 tensor([0.2060, 0.1994, 0.2003, 0.1922, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19429655373096466 tensor([0.1973, 0.2133, 0.1952, 0.1943, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19830505549907684 tensor([0.2020, 0.1997, 0.2053, 0.1947, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19815079867839813 tensor([0.2006, 0.2018, 0.1991, 0.1982, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2003629207611084 tensor([0.2024, 0.2018, 0.2007, 0.1947, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19958065450191498 tensor([0.2070, 0.2008, 0.1996, 0.1896, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1959681212902069 tensor([0.1995, 0.2054, 0.1976, 0.1960, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19866931438446045 tensor([0.2025, 0.2000, 0.2034, 0.1954, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1993311494588852 tensor([0.1993, 0.1996, 0.1997, 0.2036, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20058593153953552 tensor([0.2042, 0.2012, 0.2006, 0.1919, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19942986965179443 tensor([0.2042, 0.1994, 0.2024, 0.1926, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1985873430967331 tensor([0.2001, 0.2058, 0.1986, 0.1934, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1989501565694809 tensor([0.2018, 0.2019, 0.2035, 0.1938, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19823496043682098 tensor([0.1982, 0.2008, 0.2007, 0.2041, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19809992611408234 tensor([0.2053, 0.1998, 0.1981, 0.1902, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19968654215335846 tensor([0.2037, 0.1997, 0.1999, 0.1930, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19693046808242798 tensor([0.1969, 0.2109, 0.1974, 0.1995, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20040208101272583 tensor([0.2028, 0.2004, 0.2019, 0.1933, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19770444929599762 tensor([0.1981, 0.2001, 0.1994, 0.2047, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19833967089653015 tensor([0.2036, 0.2016, 0.1983, 0.1914, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2006250023841858 tensor([0.2038, 0.2007, 0.2006, 0.1938, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19699391722679138 tensor([0.1988, 0.2097, 0.1976, 0.1970, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19618111848831177 tensor([0.2027, 0.1982, 0.2036, 0.1962, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19854304194450378 tensor([0.2044, 0.2036, 0.1985, 0.1918, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19871878623962402 tensor([0.2033, 0.2026, 0.1987, 0.1932, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19962356984615326 tensor([0.2036, 0.1996, 0.2018, 0.1950, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19559936225414276 tensor([0.2002, 0.2070, 0.1979, 0.1956, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.195376917719841 tensor([0.2013, 0.1994, 0.2069, 0.1954, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.198709174990654 tensor([0.1987, 0.1998, 0.2006, 0.2035, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19344516098499298 tensor([0.2044, 0.1970, 0.2022, 0.1934, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19933295249938965 tensor([0.2032, 0.2015, 0.1993, 0.1942, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19812403619289398 tensor([0.1985, 0.2088, 0.1988, 0.1981, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19905367493629456 tensor([0.2032, 0.1991, 0.2036, 0.1936, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1980358362197876 tensor([0.1980, 0.1987, 0.2014, 0.2066, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1993165910243988 tensor([0.2043, 0.1993, 0.2024, 0.1924, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20012302696704865 tensor([0.2029, 0.2017, 0.2001, 0.1935, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1968223750591278 tensor([0.1985, 0.2100, 0.1981, 0.1968, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1992516815662384 tensor([0.2033, 0.1993, 0.2031, 0.1944, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19862499833106995 tensor([0.1986, 0.2020, 0.2000, 0.2024, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20035558938980103 tensor([0.2034, 0.2010, 0.2004, 0.1932, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19968919456005096 tensor([0.2018, 0.2003, 0.1997, 0.1953, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19912390410900116 tensor([0.1995, 0.2056, 0.2002, 0.1991, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19668026268482208 tensor([0.2013, 0.1990, 0.2022, 0.1967, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19902633130550385 tensor([0.2001, 0.2011, 0.2012, 0.1990, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19873081147670746 tensor([0.2043, 0.2004, 0.1987, 0.1918, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19908460974693298 tensor([0.2037, 0.1991, 0.2016, 0.1947, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19829526543617249 tensor([0.1983, 0.2057, 0.1995, 0.2004, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20066994428634644 tensor([0.2030, 0.2009, 0.2007, 0.1912, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19875256717205048 tensor([0.1988, 0.2004, 0.2019, 0.2013, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1982535719871521 tensor([0.2034, 0.2020, 0.1983, 0.1919, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19728076457977295 tensor([0.2053, 0.2020, 0.1973, 0.1888, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19500605762004852 tensor([0.1989, 0.2087, 0.1981, 0.1950, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19884614646434784 tensor([0.2021, 0.2013, 0.2032, 0.1946, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19930359721183777 tensor([0.2025, 0.2002, 0.1993, 0.1954, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20052999258041382 tensor([0.2021, 0.2018, 0.2005, 0.1936, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20092734694480896 tensor([0.2034, 0.2025, 0.2009, 0.1919, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19765043258666992 tensor([0.1999, 0.2059, 0.1982, 0.1983, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19523964822292328 tensor([0.2013, 0.1999, 0.2057, 0.1952, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1992332637310028 tensor([0.2004, 0.2007, 0.1996, 0.2001, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19859780371189117 tensor([0.2031, 0.2036, 0.1986, 0.1916, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19915072619915009 tensor([0.2033, 0.2003, 0.1992, 0.1937, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19691497087478638 tensor([0.2009, 0.2082, 0.1969, 0.1927, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19817619025707245 tensor([0.2028, 0.2036, 0.2016, 0.1939, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668614864349365 tensor([0.1974, 0.2027, 0.1994, 0.2038, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20007142424583435 tensor([0.2038, 0.2007, 0.2001, 0.1926, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19948908686637878 tensor([0.2032, 0.1995, 0.1998, 0.1939, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1960993856191635 tensor([0.2002, 0.2049, 0.1989, 0.1961, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19490744173526764 tensor([0.2027, 0.1990, 0.2040, 0.1949, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19737982749938965 tensor([0.2005, 0.2013, 0.2006, 0.1974, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1989167332649231 tensor([0.2052, 0.1989, 0.2015, 0.1906, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19752022624015808 tensor([0.2052, 0.2031, 0.1975, 0.1917, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19467362761497498 tensor([0.2001, 0.2089, 0.1982, 0.1947, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19857825338840485 tensor([0.2045, 0.1986, 0.2034, 0.1927, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19730915129184723 tensor([0.1973, 0.2053, 0.1998, 0.2014, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19960524141788483 tensor([0.2024, 0.2004, 0.1996, 0.1936, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19895237684249878 tensor([0.2029, 0.1990, 0.2014, 0.1938, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19766142964363098 tensor([0.1999, 0.2090, 0.1990, 0.1945, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20004452764987946 tensor([0.2037, 0.2000, 0.2014, 0.1918, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1997455507516861 tensor([0.1997, 0.1998, 0.2026, 0.2033, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19996888935565948 tensor([0.2037, 0.2000, 0.2015, 0.1925, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19987554848194122 tensor([0.2035, 0.2021, 0.1999, 0.1939, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19871459901332855 tensor([0.1987, 0.2052, 0.2018, 0.1987, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19453051686286926 tensor([0.2029, 0.1979, 0.2046, 0.1945, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19673459231853485 tensor([0.1967, 0.2026, 0.2009, 0.2058, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1990005075931549 tensor([0.2044, 0.1990, 0.2000, 0.1916, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19923453032970428 tensor([0.2045, 0.2005, 0.1992, 0.1911, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19778943061828613 tensor([0.2002, 0.2092, 0.1978, 0.1924, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19925613701343536 tensor([0.2025, 0.1998, 0.2035, 0.1949, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19895623624324799 tensor([0.1988, 0.2007, 0.2015, 0.2001, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19719533622264862 tensor([0.2064, 0.1972, 0.2000, 0.1896, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19588586688041687 tensor([0.2012, 0.2031, 0.1986, 0.1959, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197590634226799 tensor([0.1970, 0.2092, 0.1976, 0.1985, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19919106364250183 tensor([0.2021, 0.1992, 0.2049, 0.1943, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.199859619140625 tensor([0.2010, 0.2009, 0.2004, 0.1999, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19891341030597687 tensor([0.2030, 0.2009, 0.1989, 0.1937, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19863037765026093 tensor([0.2045, 0.2033, 0.1986, 0.1901, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19580376148223877 tensor([0.1991, 0.2075, 0.2006, 0.1958, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19535566866397858 tensor([0.2021, 0.1988, 0.2039, 0.1954, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19765445590019226 tensor([0.1977, 0.2009, 0.2010, 0.2050, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1984168142080307 tensor([0.2052, 0.2013, 0.1984, 0.1898, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20010487735271454 tensor([0.2032, 0.2003, 0.2001, 0.1940, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19629336893558502 tensor([0.1982, 0.2112, 0.1987, 0.1963, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19461071491241455 tensor([0.2030, 0.1988, 0.2036, 0.1946, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19750215113162994 tensor([0.1980, 0.1986, 0.2004, 0.2055, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19922475516796112 tensor([0.2047, 0.2016, 0.1992, 0.1906, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2000337541103363 tensor([0.2029, 0.2011, 0.2006, 0.1953, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19875766336917877 tensor([0.1988, 0.2048, 0.2006, 0.1993, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975019872188568 tensor([0.2014, 0.2017, 0.2001, 0.1975, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1977330893278122 tensor([0.1977, 0.2020, 0.1994, 0.2045, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19815023243427277 tensor([0.2053, 0.1982, 0.2011, 0.1925, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19968387484550476 tensor([0.2056, 0.1997, 0.1999, 0.1917, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19819463789463043 tensor([0.2002, 0.2028, 0.1982, 0.1939, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19774112105369568 tensor([0.2001, 0.1994, 0.2058, 0.1977, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19869551062583923 tensor([0.2012, 0.1993, 0.2006, 0.1987, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19981621205806732 tensor([0.2034, 0.1998, 0.2007, 0.1944, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1985083371400833 tensor([0.2022, 0.2018, 0.1985, 0.1942, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19812189042568207 tensor([0.1995, 0.2056, 0.1989, 0.1981, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20007330179214478 tensor([0.2046, 0.2001, 0.2030, 0.1916, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19830647110939026 tensor([0.1983, 0.2025, 0.2013, 0.2022, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19940608739852905 tensor([0.2033, 0.2016, 0.1994, 0.1919, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20124724507331848 tensor([0.2052, 0.2020, 0.2012, 0.1899, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1939031183719635 tensor([0.2007, 0.2089, 0.1969, 0.1939, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20078255236148834 tensor([0.2035, 0.2008, 0.2016, 0.1923, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19738589227199554 tensor([0.1979, 0.2004, 0.2007, 0.2036, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200276717543602 tensor([0.2020, 0.2015, 0.2003, 0.1947, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20038630068302155 tensor([0.2045, 0.2020, 0.2004, 0.1912, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983841061592102 tensor([0.1984, 0.2065, 0.1990, 0.1995, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19941957294940948 tensor([0.2042, 0.1994, 0.2024, 0.1940, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19735805690288544 tensor([0.1983, 0.2013, 0.1993, 0.2037, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19896627962589264 tensor([0.2033, 0.2026, 0.1990, 0.1916, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19909875094890594 tensor([0.2057, 0.1991, 0.2018, 0.1918, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19726057350635529 tensor([0.1980, 0.2090, 0.1982, 0.1976, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19655242562294006 tensor([0.1999, 0.2000, 0.2046, 0.1966, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19786974787712097 tensor([0.1979, 0.1986, 0.2017, 0.2053, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19851048290729523 tensor([0.2047, 0.2006, 0.1985, 0.1914, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19846874475479126 tensor([0.2009, 0.2047, 0.1985, 0.1928, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19699478149414062 tensor([0.1990, 0.2051, 0.1978, 0.1970, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19945713877677917 tensor([0.2023, 0.1995, 0.2017, 0.1943, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1987573504447937 tensor([0.1988, 0.2042, 0.1988, 0.2006, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19880342483520508 tensor([0.2058, 0.2014, 0.1988, 0.1901, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20020316541194916 tensor([0.2027, 0.2004, 0.2002, 0.1944, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19415497779846191 tensor([0.2003, 0.2085, 0.1955, 0.1942, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1950269639492035 tensor([0.2028, 0.1989, 0.2037, 0.1950, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19834133982658386 tensor([0.2004, 0.2013, 0.1997, 0.1983, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1984594762325287 tensor([0.2055, 0.2001, 0.1985, 0.1893, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19959400594234467 tensor([0.2054, 0.2010, 0.1996, 0.1908, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1982930600643158 tensor([0.1983, 0.2054, 0.1995, 0.2005, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19880814850330353 tensor([0.2027, 0.2039, 0.1988, 0.1927, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19854636490345 tensor([0.1985, 0.2021, 0.2019, 0.2014, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19951029121875763 tensor([0.2030, 0.2024, 0.1995, 0.1936, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2006751298904419 tensor([0.2027, 0.2028, 0.2007, 0.1920, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20005890727043152 tensor([0.2013, 0.2042, 0.2001, 0.1943, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19889776408672333 tensor([0.2020, 0.1998, 0.2056, 0.1937, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19836345314979553 tensor([0.1984, 0.2010, 0.2018, 0.2040, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.195353165268898 tensor([0.2015, 0.2003, 0.1988, 0.1954, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19956906139850616 tensor([0.2029, 0.2019, 0.1996, 0.1924, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19662775099277496 tensor([0.1966, 0.2102, 0.1989, 0.1999, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19893178343772888 tensor([0.2026, 0.2011, 0.2040, 0.1934, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19857308268547058 tensor([0.2001, 0.1991, 0.2007, 0.2015, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20090824365615845 tensor([0.2030, 0.2009, 0.2010, 0.1937, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20035415887832642 tensor([0.2043, 0.2014, 0.2015, 0.1925, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001659870147705 tensor([0.2022, 0.2019, 0.2002, 0.1941, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20091530680656433 tensor([0.2019, 0.2010, 0.2009, 0.1940, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1995750516653061 tensor([0.2009, 0.2013, 0.2013, 0.1996, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19984887540340424 tensor([0.2037, 0.2011, 0.1998, 0.1932, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1986451894044876 tensor([0.2059, 0.1986, 0.2009, 0.1904, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1957274228334427 tensor([0.1995, 0.2060, 0.1998, 0.1957, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19914638996124268 tensor([0.2021, 0.2016, 0.2033, 0.1938, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19964586198329926 tensor([0.1986, 0.2000, 0.1998, 0.2020, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19983583688735962 tensor([0.2029, 0.2036, 0.1998, 0.1928, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19911248981952667 tensor([0.2062, 0.2007, 0.1991, 0.1900, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1960044503211975 tensor([0.1996, 0.2077, 0.1977, 0.1960, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19540484249591827 tensor([0.2008, 0.2008, 0.2050, 0.1954, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19877175986766815 tensor([0.1988, 0.2017, 0.1989, 0.2037, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1991725116968155 tensor([0.2047, 0.1992, 0.2010, 0.1916, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19908426702022552 tensor([0.2035, 0.2026, 0.1991, 0.1939, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1974252313375473 tensor([0.2015, 0.2067, 0.1974, 0.1916, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19436895847320557 tensor([0.2029, 0.2014, 0.2039, 0.1944, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19972428679466248 tensor([0.1997, 0.2002, 0.2002, 0.2023, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993357092142105 tensor([0.2037, 0.2012, 0.1993, 0.1925, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [2, 3], [3, 1], [4, 0], [1, 3], [3, 1], [2, 3], [4, 3], [0, 4], [1, 3], [3], [0, 3], [3, 2], [4, 0], [3, 1], [3, 1], [0, 3], [3], [3], [1, 3], [3, 2], [3, 2], [3], [4, 0], [3, 2], [3, 2], [2, 3], [3, 1], [4, 1], [3, 2], [3], [0, 4], [3, 1], [4, 3], [3], [3, 2], [3, 2], [3, 4], [4, 0], [3], [3, 2], [2, 3], [3, 1], [4, 0], [1, 3], [4, 3], [0, 4], [4, 3], [2, 4], [3, 2], [3, 2], [4, 3], [3, 4], [4, 0], [3, 4], [3], [0, 3], [1, 3], [4, 2], [3, 1], [3], [4, 3], [1, 3], [2, 3], [3, 2], [3, 1], [0, 3], [1, 3], [2, 3], [3, 1], [3, 1], [2, 3], [3, 4], [2, 3], [3], [3, 2], [2, 3], [3, 4], [4, 0], [3], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2], [3, 1], [4, 0], [3], [0, 4], [3, 2], [3], [4, 3], [1, 3], [3, 2], [3, 2], [3, 1], [2, 3], [4, 3], [4, 0], [1, 3], [3, 2], [4, 3], [3, 1], [4, 0], [3, 1], [3], [4, 3], [3, 1], [4, 0], [3], [3, 2], [4, 3], [1, 3], [4, 3], [3, 2], [3, 1], [4, 0], [3], [4, 0], [3, 2], [3, 2], [2, 3], [3, 4], [3, 2], [3], [3], [2, 4], [4, 3], [2, 4], [3, 2], [3, 2], [3, 2], [3, 4], [0, 4], [3], [3, 1], [2, 3], [1, 3], [0, 3], [3, 1], [3, 2], [2, 3], [3, 1], [4, 0], [3, 2], [3, 1], [3, 4], [3], [4, 0], [3, 1], [3, 2], [4, 0], [1, 3], [4, 0], [3, 1], [3, 2], [3, 2], [3, 4], [0, 4], [3, 1], [2, 3], [0, 2], [3, 1], [4, 3], [3, 2], [3, 2], [4, 3], [1, 3], [4, 0], [3, 2], [3], [4, 3], [1, 3], [0, 4], [3, 2], [3], [4, 0], [4, 3], [4, 0], [3, 1], [3, 1], [3, 2], [4, 3], [1, 3], [3, 1], [3, 2], [4, 3], [3], [4, 0], [3, 2], [3], [2, 3], [3], [0, 4], [3], [3], [4, 0], [3, 1], [0, 4], [3, 2], [3, 1], [0, 4], [0, 3], [4, 0], [3, 2], [3, 2], [2, 3], [3, 1], [4, 0], [3, 2], [3], [2, 3], [1, 3], [2, 3], [3, 2], [3, 2], [4, 0], [3, 2], [4, 0], [3, 2], [3], [3], [3, 4], [4, 0], [2, 3], [3, 2], [4, 0], [3, 4], [1, 4], [3], [3], [3], [3], [4, 3], [3, 2], [3, 1], [0, 3], [3, 4], [0, 4], [3, 2], [3, 2], [2, 3], [4, 3], [4, 0], [3, 1], [3, 2], [3, 2], [4, 3], [4, 0], [3, 2]]\n",
      "[[0, 1, 2, 4], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 2, 4], [0, 1, 2, 4], [1, 2, 4], [0, 1, 4], [1, 2, 3], [0, 2, 4], [0, 2, 4], [1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [0, 2, 3], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 2, 4], [0, 1, 2], [0, 1, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 2], [0, 1, 2], [1, 2, 3], [0, 1, 2], [0, 1, 2, 4], [1, 2, 4], [0, 2, 4], [0, 1, 3], [0, 2, 4], [0, 1, 2, 4], [0, 1, 2], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 2], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 2, 4], [0, 1, 4], [0, 1, 2], [0, 2, 4], [1, 2, 3], [0, 2, 4], [0, 1, 2, 4], [0, 1, 2], [0, 2, 4], [1, 2, 3], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2], [0, 2, 4], [0, 1, 2], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3], [0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 2, 4], [1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 2], [0, 1, 2, 4], [1, 2, 3], [0, 2, 4], [0, 1, 4], [1, 2, 3], [0, 2, 4], [1, 2, 3], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 2, 4], [0, 1, 4], [1, 3, 4], [0, 2, 4], [0, 1, 2], [0, 1, 4], [0, 1, 4], [0, 1, 2], [0, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 2], [0, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [0, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3], [0, 2, 4], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 2, 3], [0, 1, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [0, 1, 2], [0, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2], [0, 1, 4], [0, 2, 4], [1, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4]]\n",
      "NL_pred of 1th iteration [[2, 3], [3, 1], [4, 0], [1, 3], [3, 1], [2, 3], [4, 3], [0, 4], [1, 3], [0, 3], [3, 2], [4, 0], [3, 1], [3, 1], [0, 3], [1, 3], [3, 2], [3, 2], [4, 0], [3, 2], [3, 2], [2, 3], [3, 1], [4, 1], [3, 2], [0, 4], [3, 1], [4, 3], [3, 2], [3, 2], [3, 4], [4, 0], [3, 2], [2, 3], [3, 1], [4, 0], [1, 3], [4, 3], [0, 4], [4, 3], [2, 4], [3, 2], [3, 2], [4, 3], [3, 4], [4, 0], [3, 4], [0, 3], [1, 3], [4, 2], [3, 1], [4, 3], [1, 3], [2, 3], [3, 2], [3, 1], [0, 3], [1, 3], [2, 3], [3, 1], [3, 1], [2, 3], [3, 4], [2, 3], [3, 2], [2, 3], [3, 4], [4, 0], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2], [3, 1], [4, 0], [0, 4], [3, 2], [4, 3], [1, 3], [3, 2], [3, 2], [3, 1], [2, 3], [4, 3], [4, 0], [1, 3], [3, 2], [4, 3], [3, 1], [4, 0], [3, 1], [4, 3], [3, 1], [4, 0], [3, 2], [4, 3], [1, 3], [4, 3], [3, 2], [3, 1], [4, 0], [4, 0], [3, 2], [3, 2], [2, 3], [3, 4], [3, 2], [2, 4], [4, 3], [2, 4], [3, 2], [3, 2], [3, 2], [3, 4], [0, 4], [3, 1], [2, 3], [1, 3], [0, 3], [3, 1], [3, 2], [2, 3], [3, 1], [4, 0], [3, 2], [3, 1], [3, 4], [4, 0], [3, 1], [3, 2], [4, 0], [1, 3], [4, 0], [3, 1], [3, 2], [3, 2], [3, 4], [0, 4], [3, 1], [2, 3], [0, 2], [3, 1], [4, 3], [3, 2], [3, 2], [4, 3], [1, 3], [4, 0], [3, 2], [4, 3], [1, 3], [0, 4], [3, 2], [4, 0], [4, 3], [4, 0], [3, 1], [3, 1], [3, 2], [4, 3], [1, 3], [3, 1], [3, 2], [4, 3], [4, 0], [3, 2], [2, 3], [0, 4], [4, 0], [3, 1], [0, 4], [3, 2], [3, 1], [0, 4], [0, 3], [4, 0], [3, 2], [3, 2], [2, 3], [3, 1], [4, 0], [3, 2], [2, 3], [1, 3], [2, 3], [3, 2], [3, 2], [4, 0], [3, 2], [4, 0], [3, 2], [3, 4], [4, 0], [2, 3], [3, 2], [4, 0], [3, 4], [1, 4], [4, 3], [3, 2], [3, 1], [0, 3], [3, 4], [0, 4], [3, 2], [3, 2], [2, 3], [4, 3], [4, 0], [3, 1], [3, 2], [3, 2], [4, 3], [4, 0], [3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005977873469507971  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005977863489195358  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.00597784352857013  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005977815805479537  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005977780319923578  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005977738180825877  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005977689388186432  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005977636159852494  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005977577386900436  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005977514732715695  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005977447088374648  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005977376671724541  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005977302373841751  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005977225303649902  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005977146015610806  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005977064509724462  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005976981340452682  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005976895398871843  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.00597680834836738  Accuracy on Support set:0.0\n",
      "torch.Size([215, 2048]) torch.Size([215])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00597671908001567  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20055599510669708 tensor([0.2055, 0.2006, 0.2006, 0.1907, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2019629329442978 tensor([0.2020, 0.2029, 0.1983, 0.1947, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2010301649570465 tensor([0.2032, 0.2001, 0.2029, 0.1928, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1994246244430542 tensor([0.1993, 0.1994, 0.2006, 0.2025, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20118264853954315 tensor([0.2044, 0.1987, 0.2012, 0.1934, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20055294036865234 tensor([0.2045, 0.1998, 0.2006, 0.1934, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20096120238304138 tensor([0.2010, 0.2068, 0.1975, 0.1936, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2004270851612091 tensor([0.2017, 0.2004, 0.2058, 0.1941, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994411051273346 tensor([0.1983, 0.2003, 0.1994, 0.2035, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2017229050397873 tensor([0.2045, 0.1974, 0.2026, 0.1938, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997096985578537 tensor([0.2046, 0.2005, 0.1997, 0.1911, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19806721806526184 tensor([0.1981, 0.2102, 0.1981, 0.1951, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.201992005109787 tensor([0.2037, 0.2020, 0.1991, 0.1920, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20004604756832123 tensor([0.1994, 0.2013, 0.2000, 0.2010, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2012670338153839 tensor([0.2056, 0.1996, 0.2013, 0.1910, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000778615474701 tensor([0.2054, 0.2000, 0.2001, 0.1903, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19931665062904358 tensor([0.1993, 0.2065, 0.1993, 0.1955, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19965428113937378 tensor([0.2030, 0.2033, 0.1997, 0.1923, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2003481686115265 tensor([0.2022, 0.2014, 0.2003, 0.1944, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20008273422718048 tensor([0.2039, 0.1987, 0.2001, 0.1935, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20140109956264496 tensor([0.2055, 0.2014, 0.1986, 0.1889, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2004844695329666 tensor([0.2006, 0.2062, 0.1988, 0.1939, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20135049521923065 tensor([0.2032, 0.2016, 0.2014, 0.1914, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19783461093902588 tensor([0.1988, 0.2042, 0.1978, 0.2020, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20082291960716248 tensor([0.2063, 0.2008, 0.1989, 0.1885, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20108231902122498 tensor([0.2059, 0.2011, 0.1989, 0.1914, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19978231191635132 tensor([0.2007, 0.2095, 0.1970, 0.1930, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20249585807323456 tensor([0.2056, 0.1983, 0.2025, 0.1902, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19954974949359894 tensor([0.1995, 0.1988, 0.2023, 0.2027, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20064249634742737 tensor([0.2049, 0.2006, 0.1986, 0.1902, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20078592002391815 tensor([0.2022, 0.2014, 0.2008, 0.1938, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19703033566474915 tensor([0.1982, 0.2080, 0.1986, 0.1970, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20106521248817444 tensor([0.2040, 0.1979, 0.2051, 0.1920, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19983379542827606 tensor([0.2012, 0.2029, 0.1998, 0.1979, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20074665546417236 tensor([0.2021, 0.2007, 0.2011, 0.1939, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20318512618541718 tensor([0.2039, 0.2036, 0.1981, 0.1912, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20261427760124207 tensor([0.2040, 0.2026, 0.1985, 0.1917, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2002415657043457 tensor([0.2018, 0.2044, 0.2002, 0.1942, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19946636259555817 tensor([0.1992, 0.2009, 0.1995, 0.2025, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19966168701648712 tensor([0.2036, 0.2042, 0.1997, 0.1910, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20259909331798553 tensor([0.2035, 0.2029, 0.1993, 0.1917, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19932806491851807 tensor([0.1993, 0.2107, 0.1965, 0.1941, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20147080719470978 tensor([0.2050, 0.1981, 0.2034, 0.1920, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2004673182964325 tensor([0.1998, 0.2005, 0.2005, 0.2016, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997235119342804 tensor([0.2041, 0.1988, 0.1997, 0.1936, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20122171938419342 tensor([0.2025, 0.2030, 0.2012, 0.1944, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19762977957725525 tensor([0.1973, 0.2096, 0.1976, 0.1988, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20011933147907257 tensor([0.2025, 0.2001, 0.2060, 0.1937, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19878476858139038 tensor([0.2001, 0.2031, 0.1984, 0.1988, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20378078520298004 tensor([0.2038, 0.2038, 0.1979, 0.1902, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2010936141014099 tensor([0.2026, 0.2011, 0.1990, 0.1931, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19849537312984467 tensor([0.1988, 0.2102, 0.1985, 0.1963, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20124515891075134 tensor([0.2037, 0.2012, 0.2027, 0.1929, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19938938319683075 tensor([0.1986, 0.2020, 0.1994, 0.2026, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20048213005065918 tensor([0.2036, 0.2005, 0.2007, 0.1947, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2005336731672287 tensor([0.2051, 0.2006, 0.2005, 0.1900, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1986151486635208 tensor([0.1995, 0.2067, 0.1986, 0.1966, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20143693685531616 tensor([0.2031, 0.1992, 0.2020, 0.1943, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19960305094718933 tensor([0.2004, 0.2024, 0.1985, 0.1996, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19929422438144684 tensor([0.2039, 0.1999, 0.1993, 0.1928, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19947735965251923 tensor([0.2046, 0.2009, 0.1995, 0.1917, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19910117983818054 tensor([0.2009, 0.2056, 0.1991, 0.1958, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20088830590248108 tensor([0.2030, 0.1988, 0.2015, 0.1959, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20084264874458313 tensor([0.2008, 0.2013, 0.1984, 0.1961, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20062090456485748 tensor([0.2024, 0.2006, 0.1978, 0.1932, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20006145536899567 tensor([0.2056, 0.1999, 0.2001, 0.1907, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19928987324237823 tensor([0.1997, 0.2065, 0.1995, 0.1950, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2005140483379364 tensor([0.2028, 0.1975, 0.2054, 0.1938, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2009880542755127 tensor([0.2014, 0.2010, 0.1993, 0.1970, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19939425587654114 tensor([0.2042, 0.2001, 0.1994, 0.1919, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19978369772434235 tensor([0.2065, 0.1997, 0.1998, 0.1912, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19777105748653412 tensor([0.1978, 0.2136, 0.1946, 0.1933, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20005176961421967 tensor([0.2026, 0.2001, 0.2047, 0.1936, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2010519802570343 tensor([0.2011, 0.2022, 0.1986, 0.1971, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20016330480575562 tensor([0.2030, 0.2022, 0.2002, 0.1936, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2011331021785736 tensor([0.2075, 0.2011, 0.1990, 0.1886, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20000894367694855 tensor([0.2000, 0.2057, 0.1971, 0.1949, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20034043490886688 tensor([0.2031, 0.2003, 0.2028, 0.1944, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19915708899497986 tensor([0.1998, 0.2000, 0.1992, 0.2026, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20002557337284088 tensor([0.2047, 0.2016, 0.2000, 0.1909, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20180293917655945 tensor([0.2047, 0.1998, 0.2018, 0.1916, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20057955384254456 tensor([0.2006, 0.2062, 0.1980, 0.1924, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20226331055164337 tensor([0.2023, 0.2023, 0.2029, 0.1928, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20013479888439178 tensor([0.1987, 0.2012, 0.2001, 0.2031, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20016013085842133 tensor([0.2058, 0.2002, 0.1975, 0.1892, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993107795715332 tensor([0.2043, 0.2000, 0.1993, 0.1920, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19692331552505493 tensor([0.1974, 0.2113, 0.1969, 0.1984, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20074191689491272 tensor([0.2033, 0.2007, 0.2013, 0.1923, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19884362816810608 tensor([0.1986, 0.2004, 0.1988, 0.2037, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20195932686328888 tensor([0.2041, 0.2020, 0.1978, 0.1904, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20006713271141052 tensor([0.2044, 0.2010, 0.2001, 0.1928, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19711165130138397 tensor([0.1993, 0.2100, 0.1971, 0.1959, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2000805139541626 tensor([0.2032, 0.1985, 0.2030, 0.1951, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20227186381816864 tensor([0.2050, 0.2040, 0.1980, 0.1908, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20290279388427734 tensor([0.2038, 0.2029, 0.1982, 0.1921, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20067355036735535 tensor([0.2041, 0.2000, 0.2012, 0.1940, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19997240602970123 tensor([0.2007, 0.2074, 0.1974, 0.1946, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1997021734714508 tensor([0.2018, 0.1997, 0.2064, 0.1943, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20012548565864563 tensor([0.1992, 0.2001, 0.2001, 0.2024, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20165684819221497 tensor([0.2049, 0.1973, 0.2017, 0.1924, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20185142755508423 tensor([0.2038, 0.2019, 0.1988, 0.1932, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19822441041469574 tensor([0.1991, 0.2091, 0.1982, 0.1971, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20132240653038025 tensor([0.2037, 0.1994, 0.2030, 0.1926, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19908539950847626 tensor([0.1985, 0.1991, 0.2009, 0.2056, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20184814929962158 tensor([0.2049, 0.1996, 0.2018, 0.1914, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19956418871879578 tensor([0.2034, 0.2020, 0.1996, 0.1925, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19757628440856934 tensor([0.1990, 0.2104, 0.1976, 0.1958, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20070624351501465 tensor([0.2038, 0.1996, 0.2025, 0.1934, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19948594272136688 tensor([0.1991, 0.2024, 0.1995, 0.2013, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19980314373970032 tensor([0.2039, 0.2013, 0.1998, 0.1922, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2006031721830368 tensor([0.2023, 0.2006, 0.1991, 0.1943, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19967801868915558 tensor([0.2000, 0.2060, 0.1997, 0.1981, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20143567025661469 tensor([0.2019, 0.1993, 0.2017, 0.1956, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20060572028160095 tensor([0.2006, 0.2014, 0.2006, 0.1980, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2007480263710022 tensor([0.2048, 0.2007, 0.1982, 0.1908, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20106011629104614 tensor([0.2042, 0.1994, 0.2011, 0.1937, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19895663857460022 tensor([0.1988, 0.2060, 0.1990, 0.1994, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001098394393921 tensor([0.2035, 0.2013, 0.2001, 0.1902, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20028622448444366 tensor([0.1993, 0.2007, 0.2014, 0.2003, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20236773788928986 tensor([0.2039, 0.2024, 0.1977, 0.1909, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20238524675369263 tensor([0.2058, 0.2024, 0.1967, 0.1879, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19943977892398834 tensor([0.1994, 0.2091, 0.1976, 0.1940, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016068994998932 tensor([0.2026, 0.2016, 0.2027, 0.1936, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2005310356616974 tensor([0.2031, 0.2005, 0.1988, 0.1944, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19998446106910706 tensor([0.2027, 0.2022, 0.2000, 0.1926, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20036602020263672 tensor([0.2039, 0.2028, 0.2004, 0.1908, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19727347791194916 tensor([0.2004, 0.2063, 0.1977, 0.1973, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20020869374275208 tensor([0.2019, 0.2002, 0.2051, 0.1942, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19908621907234192 tensor([0.2009, 0.2010, 0.1991, 0.1991, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20360246300697327 tensor([0.2036, 0.2040, 0.1980, 0.1906, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20060408115386963 tensor([0.2038, 0.2006, 0.1986, 0.1927, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20144928991794586 tensor([0.2014, 0.2086, 0.1964, 0.1917, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20103225111961365 tensor([0.2033, 0.2040, 0.2010, 0.1928, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19884033501148224 tensor([0.1979, 0.2031, 0.1988, 0.2028, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19951704144477844 tensor([0.2043, 0.2010, 0.1995, 0.1916, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1992865949869156 tensor([0.2037, 0.1998, 0.1993, 0.1929, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20058801770210266 tensor([0.2007, 0.2053, 0.1984, 0.1951, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20008109509944916 tensor([0.2033, 0.1993, 0.2034, 0.1939, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20009227097034454 tensor([0.2011, 0.2017, 0.2001, 0.1963, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20092330873012543 tensor([0.2057, 0.1992, 0.2009, 0.1896, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20318558812141418 tensor([0.2057, 0.2034, 0.1970, 0.1907, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19885209202766418 tensor([0.2006, 0.2093, 0.1977, 0.1936, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20150327682495117 tensor([0.2050, 0.1989, 0.2028, 0.1917, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19932499527931213 tensor([0.1978, 0.2057, 0.1993, 0.2004, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20073114335536957 tensor([0.2029, 0.2007, 0.1990, 0.1926, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2008436918258667 tensor([0.2035, 0.1993, 0.2008, 0.1928, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19840934872627258 tensor([0.2004, 0.2094, 0.1984, 0.1934, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20036935806274414 tensor([0.2043, 0.2004, 0.2009, 0.1908, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2002062350511551 tensor([0.2003, 0.2002, 0.2021, 0.2023, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200992614030838 tensor([0.2042, 0.2003, 0.2010, 0.1915, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20136095583438873 tensor([0.2040, 0.2024, 0.1993, 0.1929, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19769151508808136 tensor([0.1992, 0.2056, 0.2013, 0.1977, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20071518421173096 tensor([0.2035, 0.1982, 0.2041, 0.1935, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20037613809108734 tensor([0.1973, 0.2030, 0.2004, 0.2047, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994933784008026 tensor([0.2049, 0.1993, 0.1995, 0.1906, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20079869031906128 tensor([0.2051, 0.2008, 0.1987, 0.1901, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2007250338792801 tensor([0.2007, 0.2095, 0.1972, 0.1914, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20013679563999176 tensor([0.2031, 0.2001, 0.2030, 0.1938, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19904214143753052 tensor([0.1993, 0.2011, 0.2009, 0.1990, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19945020973682404 tensor([0.2069, 0.1975, 0.1995, 0.1887, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2017229050397873 tensor([0.2017, 0.2035, 0.1981, 0.1948, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974429041147232 tensor([0.1975, 0.2096, 0.1971, 0.1974, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20019720494747162 tensor([0.2026, 0.1995, 0.2044, 0.1933, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1998622566461563 tensor([0.2015, 0.2013, 0.1999, 0.1988, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2012699544429779 tensor([0.2035, 0.2013, 0.1984, 0.1927, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2036077231168747 tensor([0.2051, 0.2036, 0.1981, 0.1891, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19962885975837708 tensor([0.1996, 0.2078, 0.2000, 0.1948, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20058543980121613 tensor([0.2026, 0.1991, 0.2033, 0.1943, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20045700669288635 tensor([0.1982, 0.2013, 0.2005, 0.2039, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20165780186653137 tensor([0.2057, 0.2017, 0.1979, 0.1888, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19955477118492126 tensor([0.2037, 0.2006, 0.1996, 0.1930, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19815802574157715 tensor([0.1987, 0.2116, 0.1982, 0.1953, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20074722170829773 tensor([0.2035, 0.1991, 0.2030, 0.1936, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19894981384277344 tensor([0.1985, 0.1989, 0.1999, 0.2044, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20194126665592194 tensor([0.2052, 0.2019, 0.1987, 0.1896, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20006217062473297 tensor([0.2034, 0.2015, 0.2001, 0.1943, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19828496873378754 tensor([0.1993, 0.2052, 0.2001, 0.1983, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19954201579093933 tensor([0.2020, 0.2020, 0.1995, 0.1965, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19885173439979553 tensor([0.1982, 0.2024, 0.1989, 0.2034, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20056238770484924 tensor([0.2058, 0.1985, 0.2006, 0.1915, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19937565922737122 tensor([0.2062, 0.2000, 0.1994, 0.1907, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20074404776096344 tensor([0.2007, 0.2031, 0.1976, 0.1929, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1997617930173874 tensor([0.2007, 0.1998, 0.2052, 0.1967, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000136822462082 tensor([0.2017, 0.1996, 0.2000, 0.1977, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20013652741909027 tensor([0.2039, 0.2002, 0.2001, 0.1934, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20212966203689575 tensor([0.2027, 0.2021, 0.1980, 0.1931, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19840706884860992 tensor([0.2000, 0.2060, 0.1984, 0.1971, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20041394233703613 tensor([0.2051, 0.2004, 0.2024, 0.1906, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007896900177002 tensor([0.1988, 0.2029, 0.2008, 0.2012, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20197391510009766 tensor([0.2038, 0.2020, 0.1989, 0.1909, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20068961381912231 tensor([0.2057, 0.2023, 0.2007, 0.1889, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20035713911056519 tensor([0.2012, 0.2092, 0.1963, 0.1929, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2010752409696579 tensor([0.2040, 0.2011, 0.2011, 0.1913, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20015257596969604 tensor([0.1985, 0.2007, 0.2002, 0.2026, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19972428679466248 tensor([0.2025, 0.2018, 0.1997, 0.1936, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19982580840587616 tensor([0.2051, 0.2023, 0.1998, 0.1902, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19849568605422974 tensor([0.1989, 0.2069, 0.1985, 0.1985, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20070074498653412 tensor([0.2048, 0.1997, 0.2018, 0.1930, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19878274202346802 tensor([0.1988, 0.2017, 0.1988, 0.2026, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2029489427804947 tensor([0.2038, 0.2029, 0.1984, 0.1906, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20122258365154266 tensor([0.2062, 0.1994, 0.2012, 0.1908, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19652022421360016 tensor([0.1985, 0.2094, 0.1976, 0.1965, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19968096911907196 tensor([0.2004, 0.2004, 0.2040, 0.1955, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19895601272583008 tensor([0.1984, 0.1990, 0.2012, 0.2042, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2009292095899582 tensor([0.2052, 0.2009, 0.1979, 0.1903, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20141176879405975 tensor([0.2014, 0.2050, 0.1979, 0.1918, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19953548908233643 tensor([0.1995, 0.2055, 0.1972, 0.1959, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20119225978851318 tensor([0.2029, 0.1998, 0.2012, 0.1933, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19828829169273376 tensor([0.1993, 0.2046, 0.1983, 0.1995, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20174391567707062 tensor([0.2063, 0.2017, 0.1983, 0.1892, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19965031743049622 tensor([0.2032, 0.2007, 0.1997, 0.1934, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20083081722259521 tensor([0.2008, 0.2089, 0.1950, 0.1931, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20023521780967712 tensor([0.2033, 0.1993, 0.2032, 0.1940, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20093397796154022 tensor([0.2010, 0.2017, 0.1991, 0.1973, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20043769478797913 tensor([0.2060, 0.2004, 0.1979, 0.1883, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20129913091659546 tensor([0.2059, 0.2013, 0.1990, 0.1898, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19901150465011597 tensor([0.1988, 0.2058, 0.1990, 0.1995, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2025955766439438 tensor([0.2032, 0.2043, 0.1983, 0.1917, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20034635066986084 tensor([0.1991, 0.2025, 0.2013, 0.2003, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20217826962471008 tensor([0.2035, 0.2028, 0.1990, 0.1926, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20012390613555908 tensor([0.2032, 0.2032, 0.2001, 0.1910, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19958378374576569 tensor([0.2018, 0.2046, 0.1996, 0.1932, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20011600852012634 tensor([0.2025, 0.2001, 0.2050, 0.1927, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20126554369926453 tensor([0.1989, 0.2013, 0.2013, 0.2030, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20061266422271729 tensor([0.2020, 0.2006, 0.1982, 0.1943, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20221926271915436 tensor([0.2034, 0.2022, 0.1990, 0.1914, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1983301192522049 tensor([0.1971, 0.2106, 0.1983, 0.1988, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20142275094985962 tensor([0.2031, 0.2014, 0.2034, 0.1924, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20012450218200684 tensor([0.2007, 0.1995, 0.2001, 0.2004, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2004462480545044 tensor([0.2035, 0.2013, 0.2004, 0.1927, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2009040266275406 tensor([0.2048, 0.2018, 0.2009, 0.1915, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1996137499809265 tensor([0.2027, 0.2023, 0.1996, 0.1931, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2003568857908249 tensor([0.2024, 0.2013, 0.2004, 0.1930, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20077326893806458 tensor([0.2014, 0.2017, 0.2008, 0.1985, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20142897963523865 tensor([0.2042, 0.2014, 0.1993, 0.1921, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20036864280700684 tensor([0.2064, 0.1990, 0.2004, 0.1894, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19927944242954254 tensor([0.2000, 0.2064, 0.1993, 0.1947, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019311934709549 tensor([0.2026, 0.2019, 0.2028, 0.1928, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19926413893699646 tensor([0.1991, 0.2004, 0.1993, 0.2009, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20167502760887146 tensor([0.2034, 0.2039, 0.1993, 0.1917, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20102962851524353 tensor([0.2067, 0.2010, 0.1986, 0.1891, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19977009296417236 tensor([0.2001, 0.2081, 0.1971, 0.1950, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20117560029029846 tensor([0.2014, 0.2012, 0.2045, 0.1944, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19840313494205475 tensor([0.1993, 0.2021, 0.1984, 0.2027, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20046550035476685 tensor([0.2052, 0.1995, 0.2005, 0.1906, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20162907242774963 tensor([0.2040, 0.2030, 0.1985, 0.1928, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20198693871498108 tensor([0.2020, 0.2071, 0.1969, 0.1906, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20178820192813873 tensor([0.2034, 0.2018, 0.2033, 0.1933, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19970372319221497 tensor([0.2002, 0.2005, 0.1997, 0.2012, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20153899490833282 tensor([0.2043, 0.2015, 0.1988, 0.1915, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [2, 3], [3, 1], [4, 0, 1], [1, 3], [3, 1], [2, 3], [4, 3], [0, 4, 2], [1, 3], [3, 2], [0, 3, 2], [3, 2], [4, 0], [3, 1], [3, 1], [0, 3, 4], [3, 2], [3], [1, 3], [3, 2], [3, 2], [3], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1], [4, 1, 0], [3, 2], [3], [0, 4, 3], [3, 1], [4, 3, 2], [3], [3, 2], [3, 2], [3, 4], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1], [4, 0], [1, 3, 2], [4, 3], [0, 4, 2], [4, 3], [2, 4, 3], [3, 2], [3, 2], [4, 3, 2], [3, 4], [4, 0, 2], [3, 4], [3], [0, 3, 4], [1, 3], [4, 2, 3], [3, 1, 2], [3, 2], [4, 3, 2], [1, 3], [2, 3], [3, 2], [3, 1], [0, 3, 4], [1, 3], [2, 3], [3, 1, 2], [3, 1, 2], [2, 3, 0], [3, 4], [2, 3], [3], [3, 2], [2, 3], [3, 4], [4, 0, 2], [3], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2], [3, 1, 2], [4, 0, 2], [3], [0, 4, 2], [3, 2], [3], [4, 3, 2], [1, 3], [3, 2], [3, 2], [3, 1], [2, 3, 4], [4, 3, 1], [4, 0], [1, 3], [3, 2], [4, 3, 2], [3, 1], [4, 0, 1], [3, 1], [3, 2], [4, 3, 2], [3, 1], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3], [4, 3], [3, 2], [3, 1], [4, 0, 2], [3], [4, 0], [3, 2], [3, 2], [2, 3, 0], [3, 4], [3, 2], [3, 2], [3], [2, 4, 3], [4, 3], [2, 4, 3], [3, 2], [3, 2], [3, 2], [3, 4], [0, 4, 2], [3, 2], [3, 1, 2], [2, 3], [1, 3], [0, 3], [3, 1], [3, 2], [2, 3, 4], [3, 1], [4, 0, 2], [3, 2], [3, 1], [3, 4, 2], [3], [4, 0], [3, 1], [3, 2], [4, 0, 3], [1, 3], [4, 0], [3, 1, 2], [3, 2], [3, 2], [3, 4], [0, 4, 3], [3, 1, 2], [2, 3], [0, 2, 3], [3, 1], [4, 3, 2], [3, 2], [3, 2], [4, 3, 0], [1, 3], [4, 0], [3, 2], [3, 2], [4, 3, 2], [1, 3], [0, 4, 1], [3, 2], [3], [4, 0, 3], [4, 3, 2], [4, 0, 2], [3, 1], [3, 1, 2], [3, 2], [4, 3, 1], [1, 3], [3, 1], [3, 2], [4, 3, 2], [3], [4, 0], [3, 2], [3], [2, 3], [3], [0, 4], [3, 2], [3, 2], [4, 0, 3], [3, 1], [0, 4, 2], [3, 2], [3, 1], [0, 4, 3], [0, 3, 4], [4, 0, 1], [3, 2], [3, 2], [2, 3, 0], [3, 1], [4, 0, 2], [3, 2], [3, 2], [2, 3], [1, 3], [2, 3], [3, 2], [3, 2], [4, 0, 2], [3, 2], [4, 0], [3, 2], [3], [3, 2], [3, 4], [4, 0], [2, 3], [3, 2], [4, 0, 2], [3, 4], [1, 4], [3], [3], [3, 2], [3], [4, 3], [3, 2], [3, 1], [0, 3, 2], [3, 4], [0, 4, 2], [3, 2], [3, 2], [2, 3, 4], [4, 3], [4, 0, 2], [3, 1], [3, 2], [3, 2], [4, 3], [4, 0, 2], [3, 2]]\n",
      "[[0, 1, 2, 4], [0, 1, 4], [0, 2, 4], [2, 3], [0, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 2, 4], [0, 1, 4], [1, 4], [0, 1, 4], [1, 2, 3], [0, 2, 4], [0, 2, 4], [1, 2], [0, 1, 4], [0, 1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 2, 4], [2, 3], [0, 1, 4], [0, 1, 2, 4], [1, 2], [0, 2, 4], [0, 1], [0, 1, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 2, 4], [1, 2, 3], [0, 4], [0, 1, 2], [1, 3], [0, 1, 2], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 2], [1, 3], [0, 1, 2], [0, 1, 2, 4], [1, 2], [0, 2, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [1, 2], [0, 2, 4], [0, 1, 4], [0, 4], [0, 4], [1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 4], [1, 3], [0, 1, 2, 4], [1, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [0, 1], [0, 2], [1, 2, 3], [0, 2, 4], [0, 1, 4], [0, 1], [0, 2, 4], [2, 3], [0, 2, 4], [0, 1, 4], [0, 1], [0, 2, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 2, 4], [0, 1, 2], [0, 1, 4], [0, 2, 4], [1, 3], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 4], [0, 1, 2, 4], [0, 1], [0, 1, 2], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 2, 4], [1, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1], [0, 2, 4], [1, 3], [0, 1, 4], [0, 2, 4], [0, 1], [0, 1, 2, 4], [1, 2, 3], [0, 2, 4], [0, 1, 4], [1, 2], [0, 2, 4], [1, 2, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2], [0, 4], [0, 1, 4], [1, 4], [0, 2, 4], [0, 1], [0, 1, 4], [0, 1, 4], [1, 2], [0, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 2, 4], [2, 3], [0, 1, 4], [0, 1, 2, 4], [1, 2], [0, 1], [1, 3], [0, 2, 4], [0, 4], [0, 1, 4], [0, 2], [0, 2, 4], [0, 2, 4], [0, 1, 4], [0, 1], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2], [0, 2, 4], [1, 3], [0, 1, 4], [0, 2, 4], [1, 2], [1, 2], [2, 3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 2, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 2], [0, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2, 4], [0, 1, 2], [0, 1, 4], [0, 2, 4], [1, 4], [0, 1, 2], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 2], [1, 3], [0, 2, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 1, 4]]\n",
      "NL_pred of 2th iteration [[4, 0, 1], [0, 4, 2], [3, 2], [0, 3, 2], [0, 3, 4], [3, 2], [4, 0, 2], [2, 3, 4], [4, 1, 0], [0, 4, 3], [4, 3, 2], [4, 0, 2], [3, 2], [2, 3, 4], [1, 3, 2], [0, 4, 2], [2, 4, 3], [4, 3, 2], [4, 0, 2], [0, 3, 4], [4, 2, 3], [3, 1, 2], [3, 2], [4, 3, 2], [0, 3, 4], [3, 1, 2], [3, 1, 2], [2, 3, 0], [4, 0, 2], [3, 1, 2], [4, 0, 2], [0, 4, 2], [4, 3, 2], [2, 3, 4], [4, 3, 1], [4, 3, 2], [4, 0, 1], [3, 2], [4, 3, 2], [4, 0, 2], [3, 2], [4, 3, 2], [4, 0, 2], [2, 3, 0], [3, 2], [2, 4, 3], [2, 4, 3], [0, 4, 2], [3, 2], [3, 1, 2], [2, 3, 4], [4, 0, 2], [3, 4, 2], [4, 0, 3], [3, 1, 2], [0, 4, 3], [3, 1, 2], [0, 2, 3], [4, 3, 2], [4, 3, 0], [3, 2], [4, 3, 2], [0, 4, 1], [4, 0, 3], [4, 3, 2], [4, 0, 2], [3, 1, 2], [4, 3, 1], [4, 3, 2], [3, 2], [3, 2], [4, 0, 3], [0, 4, 2], [0, 4, 3], [0, 3, 4], [4, 0, 1], [2, 3, 0], [4, 0, 2], [3, 2], [4, 0, 2], [3, 2], [4, 0, 2], [3, 2], [0, 3, 2], [0, 4, 2], [2, 3, 4], [4, 0, 2], [4, 0, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.014618858695030212  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.014618650078773499  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.014618253166025335  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014617685567248951  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.014616964892907576  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.014616110108115456  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.014615130695429716  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.014614042910662565  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.014612857591022144  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.014611581509763544  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.014610226858745922  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.014608804475177418  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01460731706836007  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.014605775475502014  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.014604181051254272  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.014602543278173967  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.014600867574865168  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.014599155295978893  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.014597411860119213  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.014595639976588163  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.19709618389606476 tensor([0.2066, 0.2018, 0.1971, 0.1910, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20294229686260223 tensor([0.2029, 0.2041, 0.1949, 0.1950, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19940626621246338 tensor([0.2043, 0.2013, 0.1994, 0.1930, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971578150987625 tensor([0.2003, 0.2006, 0.1972, 0.2028, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19771164655685425 tensor([0.2054, 0.1999, 0.1977, 0.1937, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19711105525493622 tensor([0.2055, 0.2010, 0.1971, 0.1937, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2019336074590683 tensor([0.2019, 0.2080, 0.1941, 0.1938, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20165248215198517 tensor([0.2028, 0.2017, 0.2023, 0.1944, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20154055953025818 tensor([0.1993, 0.2015, 0.1960, 0.2037, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19906610250473022 tensor([0.2055, 0.1986, 0.1991, 0.1941, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016766220331192 tensor([0.2056, 0.2017, 0.1963, 0.1913, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19947560131549835 tensor([0.1991, 0.2115, 0.1946, 0.1953, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2032238095998764 tensor([0.2047, 0.2032, 0.1957, 0.1922, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1965779960155487 tensor([0.2003, 0.2026, 0.1966, 0.2012, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19781023263931274 tensor([0.2067, 0.2008, 0.1978, 0.1913, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1966092437505722 tensor([0.2064, 0.2012, 0.1966, 0.1906, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19584660232067108 tensor([0.2003, 0.2078, 0.1958, 0.1958, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20272845029830933 tensor([0.2040, 0.2046, 0.1962, 0.1925, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.196864515542984 tensor([0.2032, 0.2026, 0.1969, 0.1947, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19662871956825256 tensor([0.2049, 0.1999, 0.1966, 0.1938, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20261700451374054 tensor([0.2065, 0.2026, 0.1951, 0.1892, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2014455795288086 tensor([0.2016, 0.2074, 0.1953, 0.1942, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19787409901618958 tensor([0.2042, 0.2028, 0.1979, 0.1917, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.202247753739357 tensor([0.1998, 0.2055, 0.1944, 0.2022, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20203723013401031 tensor([0.2072, 0.2020, 0.1955, 0.1888, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20231541991233826 tensor([0.2069, 0.2023, 0.1954, 0.1916, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2016964703798294 tensor([0.2017, 0.2108, 0.1936, 0.1932, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19904839992523193 tensor([0.2067, 0.1995, 0.1990, 0.1904, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19880074262619019 tensor([0.2006, 0.2000, 0.1988, 0.2029, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20185871422290802 tensor([0.2059, 0.2019, 0.1952, 0.1904, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19733493030071259 tensor([0.2032, 0.2027, 0.1973, 0.1941, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1951732486486435 tensor([0.1992, 0.2093, 0.1952, 0.1973, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20161089301109314 tensor([0.2050, 0.1991, 0.2016, 0.1922, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20219019055366516 tensor([0.2022, 0.2041, 0.1964, 0.1982, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19758948683738708 tensor([0.2031, 0.2020, 0.1976, 0.1942, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20413583517074585 tensor([0.2049, 0.2048, 0.1947, 0.1914, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20385661721229553 tensor([0.2050, 0.2039, 0.1950, 0.1919, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19679167866706848 tensor([0.2028, 0.2057, 0.1968, 0.1945, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20210327208042145 tensor([0.2002, 0.2021, 0.1960, 0.2028, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20246686041355133 tensor([0.2046, 0.2054, 0.1962, 0.1913, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20356765389442444 tensor([0.2045, 0.2042, 0.1958, 0.1919, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20031028985977173 tensor([0.2003, 0.2120, 0.1931, 0.1944, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19997453689575195 tensor([0.2060, 0.1993, 0.2000, 0.1922, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19701150059700012 tensor([0.2008, 0.2017, 0.1970, 0.2018, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2048037350177765 tensor([0.2050, 0.2000, 0.1963, 0.1939, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19774962961673737 tensor([0.2036, 0.2043, 0.1977, 0.1947, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1989917755126953 tensor([0.1982, 0.2108, 0.1942, 0.1990, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20131279528141022 tensor([0.2036, 0.2013, 0.2025, 0.1940, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20108798146247864 tensor([0.2011, 0.2044, 0.1950, 0.1990, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20476439595222473 tensor([0.2048, 0.2051, 0.1944, 0.1904, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20232385396957397 tensor([0.2036, 0.2023, 0.1955, 0.1934, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19975440204143524 tensor([0.1998, 0.2115, 0.1950, 0.1965, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19922687113285065 tensor([0.2047, 0.2025, 0.1992, 0.1932, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20289140939712524 tensor([0.1996, 0.2033, 0.1959, 0.2029, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19718870520591736 tensor([0.2047, 0.2017, 0.1972, 0.1950, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1970653235912323 tensor([0.2061, 0.2018, 0.1971, 0.1903, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19517867267131805 tensor([0.2005, 0.2079, 0.1952, 0.1968, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.198512464761734 tensor([0.2041, 0.2004, 0.1985, 0.1945, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20135290920734406 tensor([0.2014, 0.2037, 0.1951, 0.1998, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20490135252475739 tensor([0.2049, 0.2011, 0.1958, 0.1931, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20215483009815216 tensor([0.2056, 0.2022, 0.1960, 0.1919, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20184679329395294 tensor([0.2018, 0.2069, 0.1956, 0.1961, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19796742498874664 tensor([0.2040, 0.2000, 0.1980, 0.1962, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20182721316814423 tensor([0.2018, 0.2025, 0.1950, 0.1964, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018430531024933 tensor([0.2033, 0.2018, 0.1944, 0.1934, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1966000199317932 tensor([0.2066, 0.2012, 0.1966, 0.1910, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19606393575668335 tensor([0.2007, 0.2077, 0.1961, 0.1953, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2014930695295334 tensor([0.2038, 0.1987, 0.2019, 0.1940, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20222195982933044 tensor([0.2024, 0.2022, 0.1959, 0.1972, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20518624782562256 tensor([0.2052, 0.2013, 0.1960, 0.1921, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20376908779144287 tensor([0.2075, 0.2009, 0.1963, 0.1914, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20155610144138336 tensor([0.1988, 0.2149, 0.1913, 0.1935, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20124422013759613 tensor([0.2036, 0.2013, 0.2012, 0.1939, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2020285725593567 tensor([0.2021, 0.2034, 0.1951, 0.1974, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19671258330345154 tensor([0.2039, 0.2034, 0.1967, 0.1939, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20237411558628082 tensor([0.2085, 0.2024, 0.1956, 0.1888, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2009834498167038 tensor([0.2010, 0.2070, 0.1937, 0.1952, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19935743510723114 tensor([0.2041, 0.2016, 0.1994, 0.1947, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20122618973255157 tensor([0.2008, 0.2012, 0.1957, 0.2028, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1965649574995041 tensor([0.2057, 0.2028, 0.1966, 0.1912, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19835476577281952 tensor([0.2058, 0.2009, 0.1984, 0.1918, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2015485018491745 tensor([0.2015, 0.2074, 0.1946, 0.1926, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19943879544734955 tensor([0.2033, 0.2035, 0.1994, 0.1931, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19669519364833832 tensor([0.1997, 0.2024, 0.1967, 0.2033, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20137988030910492 tensor([0.2068, 0.2014, 0.1941, 0.1894, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20524264872074127 tensor([0.2052, 0.2012, 0.1959, 0.1923, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19866503775119781 tensor([0.1984, 0.2125, 0.1935, 0.1987, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19784323871135712 tensor([0.2043, 0.2020, 0.1978, 0.1925, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016526609659195 tensor([0.1996, 0.2017, 0.1954, 0.2039, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20320281386375427 tensor([0.2050, 0.2032, 0.1944, 0.1906, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19659464061260223 tensor([0.2054, 0.2023, 0.1966, 0.1930, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2003132849931717 tensor([0.2003, 0.2113, 0.1937, 0.1962, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19954976439476013 tensor([0.2043, 0.1997, 0.1995, 0.1954, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20324066281318665 tensor([0.2059, 0.2052, 0.1946, 0.1910, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2038944512605667 tensor([0.2048, 0.2041, 0.1948, 0.1924, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19777396321296692 tensor([0.2052, 0.2012, 0.1978, 0.1942, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2017003744840622 tensor([0.2017, 0.2086, 0.1939, 0.1948, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20284141600131989 tensor([0.2028, 0.2009, 0.2029, 0.1946, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1966649740934372 tensor([0.2002, 0.2014, 0.1967, 0.2027, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1982215791940689 tensor([0.2059, 0.1985, 0.1982, 0.1927, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2030850201845169 tensor([0.2048, 0.2031, 0.1953, 0.1934, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2000531256198883 tensor([0.2001, 0.2104, 0.1948, 0.1973, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19951178133487701 tensor([0.2047, 0.2006, 0.1995, 0.1929, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19741985201835632 tensor([0.1995, 0.2003, 0.1974, 0.2059, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19840648770332336 tensor([0.2059, 0.2009, 0.1984, 0.1917, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20327891409397125 tensor([0.2044, 0.2033, 0.1961, 0.1927, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19996993243694305 tensor([0.2000, 0.2117, 0.1941, 0.1960, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19904233515262604 tensor([0.2048, 0.2008, 0.1990, 0.1937, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20156516134738922 tensor([0.2001, 0.2037, 0.1960, 0.2016, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202586367726326 tensor([0.2049, 0.2026, 0.1963, 0.1925, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20183207094669342 tensor([0.2033, 0.2018, 0.1957, 0.1945, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20098371803760529 tensor([0.2010, 0.2073, 0.1962, 0.1983, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19819818437099457 tensor([0.2029, 0.2006, 0.1982, 0.1959, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971507966518402 tensor([0.2016, 0.2027, 0.1972, 0.1982, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20199035108089447 tensor([0.2057, 0.2020, 0.1948, 0.1910, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976342350244522 tensor([0.2052, 0.2006, 0.1976, 0.1940, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19961872696876526 tensor([0.1998, 0.2073, 0.1955, 0.1996, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19666226208209991 tensor([0.2045, 0.2025, 0.1967, 0.1905, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19789306819438934 tensor([0.2003, 0.2020, 0.1979, 0.2006, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2035810351371765 tensor([0.2049, 0.2036, 0.1943, 0.1911, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20361986756324768 tensor([0.2067, 0.2036, 0.1934, 0.1882, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2009141445159912 tensor([0.2004, 0.2103, 0.1941, 0.1942, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1992282122373581 tensor([0.2036, 0.2028, 0.1992, 0.1938, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017444223165512 tensor([0.2041, 0.2017, 0.1953, 0.1946, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2034105509519577 tensor([0.2037, 0.2034, 0.1965, 0.1928, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19689977169036865 tensor([0.2049, 0.2041, 0.1969, 0.1911, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20138442516326904 tensor([0.2014, 0.2076, 0.1943, 0.1975, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2014482319355011 tensor([0.2029, 0.2014, 0.2016, 0.1945, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2019093930721283 tensor([0.2019, 0.2023, 0.1956, 0.1993, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20459046959877014 tensor([0.2046, 0.2052, 0.1946, 0.1908, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20184603333473206 tensor([0.2048, 0.2018, 0.1952, 0.1929, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20242688059806824 tensor([0.2024, 0.2098, 0.1930, 0.1919, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19758199155330658 tensor([0.2043, 0.2052, 0.1976, 0.1931, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20304714143276215 tensor([0.1989, 0.2043, 0.1954, 0.2030, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20221249759197235 tensor([0.2053, 0.2022, 0.1961, 0.1919, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2046886384487152 tensor([0.2047, 0.2010, 0.1958, 0.1931, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20154312252998352 tensor([0.2017, 0.2065, 0.1950, 0.1953, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19993945956230164 tensor([0.2043, 0.2006, 0.1999, 0.1942, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19661901891231537 tensor([0.2021, 0.2029, 0.1966, 0.1966, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19745834171772003 tensor([0.2068, 0.2005, 0.1975, 0.1899, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20413494110107422 tensor([0.2067, 0.2046, 0.1936, 0.1910, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20157812535762787 tensor([0.2016, 0.2105, 0.1942, 0.1939, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993413120508194 tensor([0.2061, 0.2001, 0.1993, 0.1920, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20064574480056763 tensor([0.1988, 0.2070, 0.1959, 0.2006, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20195363461971283 tensor([0.2039, 0.2020, 0.1956, 0.1929, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19741423428058624 tensor([0.2045, 0.2005, 0.1974, 0.1931, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2013537585735321 tensor([0.2014, 0.2107, 0.1949, 0.1937, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19741462171077728 tensor([0.2053, 0.2016, 0.1974, 0.1911, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19863031804561615 tensor([0.2013, 0.2014, 0.1986, 0.2026, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1975407749414444 tensor([0.2052, 0.2015, 0.1975, 0.1917, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20232631266117096 tensor([0.2050, 0.2037, 0.1959, 0.1931, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19779829680919647 tensor([0.2002, 0.2068, 0.1978, 0.1980, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20058555901050568 tensor([0.2045, 0.1994, 0.2006, 0.1938, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969255656003952 tensor([0.1982, 0.2043, 0.1969, 0.2050, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20593614876270294 tensor([0.2059, 0.2005, 0.1961, 0.1909, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20201639831066132 tensor([0.2060, 0.2020, 0.1952, 0.1904, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20170246064662933 tensor([0.2017, 0.2108, 0.1938, 0.1916, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19949117302894592 tensor([0.2041, 0.2014, 0.1995, 0.1941, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197440505027771 tensor([0.2003, 0.2023, 0.1974, 0.1993, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2079174816608429 tensor([0.2079, 0.1987, 0.1960, 0.1890, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20270799100399017 tensor([0.2027, 0.2048, 0.1946, 0.1951, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1993192881345749 tensor([0.1985, 0.2109, 0.1936, 0.1977, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2009054720401764 tensor([0.2037, 0.2007, 0.2009, 0.1935, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20248407125473022 tensor([0.2025, 0.2025, 0.1964, 0.1991, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20250536501407623 tensor([0.2045, 0.2025, 0.1949, 0.1929, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2048448771238327 tensor([0.2060, 0.2048, 0.1946, 0.1893, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19654908776283264 tensor([0.2006, 0.2091, 0.1965, 0.1950, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19981640577316284 tensor([0.2037, 0.2004, 0.1998, 0.1946, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197010338306427 tensor([0.1992, 0.2025, 0.1970, 0.2042, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20287400484085083 tensor([0.2067, 0.2029, 0.1945, 0.1890, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20184916257858276 tensor([0.2047, 0.2018, 0.1961, 0.1933, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19969654083251953 tensor([0.1997, 0.2129, 0.1947, 0.1955, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19953681528568268 tensor([0.2045, 0.2003, 0.1995, 0.1939, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.196487694978714 tensor([0.1995, 0.2001, 0.1965, 0.2047, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2031712532043457 tensor([0.2062, 0.2032, 0.1952, 0.1898, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19659370183944702 tensor([0.2044, 0.2027, 0.1966, 0.1946, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19660188257694244 tensor([0.2003, 0.2065, 0.1966, 0.1985, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20294992625713348 tensor([0.2029, 0.2033, 0.1961, 0.1967, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20364928245544434 tensor([0.1992, 0.2036, 0.1954, 0.2037, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19709783792495728 tensor([0.2069, 0.1997, 0.1971, 0.1918, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20474131405353546 tensor([0.2072, 0.2012, 0.1959, 0.1909, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20171332359313965 tensor([0.2017, 0.2044, 0.1942, 0.1931, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2017049789428711 tensor([0.2017, 0.2010, 0.2017, 0.1970, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19654519855976105 tensor([0.2027, 0.2009, 0.1965, 0.1979, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19666364789009094 tensor([0.2049, 0.2014, 0.1967, 0.1937, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20336760580539703 tensor([0.2037, 0.2034, 0.1945, 0.1934, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20102782547473907 tensor([0.2010, 0.2073, 0.1950, 0.1973, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19893759489059448 tensor([0.2061, 0.2017, 0.1989, 0.1909, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19732242822647095 tensor([0.1998, 0.2041, 0.1973, 0.2014, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20319852232933044 tensor([0.2047, 0.2032, 0.1954, 0.1911, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19724251329898834 tensor([0.2067, 0.2036, 0.1972, 0.1892, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2012949138879776 tensor([0.2022, 0.2105, 0.1930, 0.1931, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19758810102939606 tensor([0.2050, 0.2024, 0.1976, 0.1915, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1966794729232788 tensor([0.1995, 0.2020, 0.1967, 0.2028, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20304669439792633 tensor([0.2035, 0.2030, 0.1963, 0.1939, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20353426039218903 tensor([0.2060, 0.2035, 0.1964, 0.1904, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1951025128364563 tensor([0.1999, 0.2081, 0.1951, 0.1987, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19836483895778656 tensor([0.2058, 0.2010, 0.1984, 0.1932, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20288808643817902 tensor([0.1998, 0.2029, 0.1953, 0.2029, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20419223606586456 tensor([0.2047, 0.2042, 0.1950, 0.1908, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1977727860212326 tensor([0.2073, 0.2006, 0.1978, 0.1910, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19416776299476624 tensor([0.1995, 0.2107, 0.1942, 0.1967, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20052777230739594 tensor([0.2014, 0.2016, 0.2005, 0.1958, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19775442779064178 tensor([0.1994, 0.2001, 0.1978, 0.2045, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20216111838817596 tensor([0.2062, 0.2022, 0.1945, 0.1906, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20239070057868958 tensor([0.2024, 0.2063, 0.1945, 0.1920, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20278023183345795 tensor([0.2005, 0.2067, 0.1938, 0.1962, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19771644473075867 tensor([0.2039, 0.2010, 0.1977, 0.1935, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19978134334087372 tensor([0.2003, 0.2059, 0.1948, 0.1998, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20299793779850006 tensor([0.2072, 0.2030, 0.1948, 0.1894, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20196658372879028 tensor([0.2042, 0.2020, 0.1962, 0.1937, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20183080434799194 tensor([0.2018, 0.2101, 0.1916, 0.1934, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19968342781066895 tensor([0.2044, 0.2005, 0.1997, 0.1943, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2018967568874359 tensor([0.2020, 0.2029, 0.1957, 0.1975, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20166921615600586 tensor([0.2069, 0.2017, 0.1945, 0.1886, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20253919064998627 tensor([0.2069, 0.2025, 0.1956, 0.1901, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19971297681331635 tensor([0.1998, 0.2070, 0.1956, 0.1997, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20355366170406342 tensor([0.2042, 0.2055, 0.1948, 0.1919, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19783854484558105 tensor([0.2001, 0.2038, 0.1978, 0.2006, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20314227044582367 tensor([0.2045, 0.2040, 0.1955, 0.1928, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19667713344097137 tensor([0.2042, 0.2044, 0.1967, 0.1913, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20172490179538727 tensor([0.2028, 0.2059, 0.1961, 0.1935, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.201359361410141 tensor([0.2036, 0.2014, 0.2015, 0.1930, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19780166447162628 tensor([0.1999, 0.2026, 0.1978, 0.2033, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20183613896369934 tensor([0.2030, 0.2018, 0.1948, 0.1946, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20345184206962585 tensor([0.2044, 0.2035, 0.1956, 0.1917, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19906014204025269 tensor([0.1981, 0.2119, 0.1949, 0.1991, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19993336498737335 tensor([0.2042, 0.2027, 0.1999, 0.1927, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1966775506734848 tensor([0.2017, 0.2007, 0.1967, 0.2007, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19696810841560364 tensor([0.2045, 0.2025, 0.1970, 0.1929, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19743512570858002 tensor([0.2058, 0.2030, 0.1974, 0.1917, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2033134251832962 tensor([0.2037, 0.2035, 0.1962, 0.1933, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19687935709953308 tensor([0.2034, 0.2026, 0.1969, 0.1932, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19729682803153992 tensor([0.2024, 0.2029, 0.1973, 0.1988, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20267260074615479 tensor([0.2052, 0.2027, 0.1958, 0.1924, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969243437051773 tensor([0.2074, 0.2002, 0.1969, 0.1897, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20066511631011963 tensor([0.2009, 0.2076, 0.1958, 0.1949, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19930002093315125 tensor([0.2036, 0.2032, 0.1993, 0.1931, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20115715265274048 tensor([0.2001, 0.2016, 0.1958, 0.2012, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20261695981025696 tensor([0.2044, 0.2051, 0.1958, 0.1920, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022670954465866 tensor([0.2077, 0.2023, 0.1951, 0.1893, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20106223225593567 tensor([0.2011, 0.2093, 0.1937, 0.1952, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2009652853012085 tensor([0.2024, 0.2024, 0.2010, 0.1946, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20289698243141174 tensor([0.2003, 0.2033, 0.1950, 0.2029, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19700729846954346 tensor([0.2063, 0.2007, 0.1970, 0.1908, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20260119438171387 tensor([0.2050, 0.2042, 0.1951, 0.1931, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20296719670295715 tensor([0.2030, 0.2083, 0.1934, 0.1908, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1998496651649475 tensor([0.2045, 0.2030, 0.1998, 0.1936, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20145763456821442 tensor([0.2013, 0.2018, 0.1962, 0.2015, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2027703821659088 tensor([0.2053, 0.2028, 0.1953, 0.1918, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 2], [2, 3], [3, 1, 2], [4, 0, 1, 2], [1, 3, 2], [3, 1, 2], [2, 3], [4, 3], [0, 4, 2], [1, 3, 2], [3, 2], [0, 3, 2, 4], [3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [0, 3, 4, 2], [3, 2], [3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 1, 0, 2], [3, 2], [3, 2], [0, 4, 3, 2], [3, 1], [4, 3, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [1, 3, 2], [4, 3, 2], [0, 4, 2, 3], [4, 3], [2, 4, 3], [3, 2], [3, 2], [4, 3, 2, 0], [3, 4, 2], [4, 0, 2], [3, 4, 2], [3, 2], [0, 3, 4, 2], [1, 3, 2], [4, 2, 3], [3, 1, 2], [3, 2], [4, 3, 2], [1, 3, 2], [2, 3], [3, 2], [3, 1, 2], [0, 3, 4, 2], [1, 3], [2, 3], [3, 1, 2], [3, 1, 2], [2, 3, 0], [3, 4], [2, 3], [3, 2], [3, 2], [2, 3], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 1, 2], [2, 3, 4], [4, 3, 1], [4, 0, 2], [1, 3, 2], [3, 2], [4, 3, 2], [3, 1, 2], [4, 0, 1, 2], [3, 1, 2], [3, 2], [4, 3, 2, 0], [3, 1, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 0], [3, 4, 2], [3, 2], [3, 2], [3, 2], [2, 4, 3], [4, 3], [2, 4, 3], [3, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 1, 2], [2, 3], [1, 3, 2], [0, 3, 2], [3, 1, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 4, 2], [3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [4, 0, 3, 2], [1, 3], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 3, 2], [3, 1, 2], [2, 3], [0, 2, 3, 4], [3, 1], [4, 3, 2], [3, 2], [3, 2], [4, 3, 0, 2], [1, 3, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2, 0], [1, 3, 2], [0, 4, 1, 2], [3, 2], [3, 2], [4, 0, 3, 2], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [3, 2], [4, 3, 1], [1, 3, 2], [3, 1, 2], [3, 2], [4, 3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 0, 3, 2], [3, 1, 2], [0, 4, 2], [3, 2], [3, 1, 2], [0, 4, 3, 2], [0, 3, 4], [4, 0, 1, 2], [3, 2], [3, 2], [2, 3, 0], [3, 1, 2], [4, 0, 2, 3], [3, 2], [3, 2], [2, 3], [1, 3, 2], [2, 3], [3, 2], [3, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [3, 2], [3, 4], [4, 0, 2], [2, 3], [3, 2], [4, 0, 2, 3], [3, 4, 2], [1, 4, 2], [3, 2], [3, 2], [3, 2], [3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [0, 3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 2], [2, 3, 4], [4, 3], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [4, 3, 2], [4, 0, 2], [3, 2]]\n",
      "[[0, 1, 4], [0, 1, 4], [0, 4], [3], [0, 4], [0, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 2, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 4], [0, 1], [1], [0, 1, 2], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 1], [0, 1, 4], [1], [0, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [1], [0, 2, 4], [0, 1, 4], [0, 4], [0, 4], [1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1], [0, 2], [1, 3], [0, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 2], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 4], [1, 4], [0, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [0, 1], [0, 1, 4], [1, 3], [0, 4], [0, 1, 4], [1], [0, 2, 4], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1], [0, 4], [0, 1, 4], [1], [0, 2, 4], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 4], [0, 4], [0, 1, 4], [0, 2], [0, 4], [0, 4], [0, 1, 4], [0, 1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [1], [1, 2], [3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [0, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 4], [0, 4], [1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 2], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4]]\n",
      "NL_pred of 3th iteration [[3, 2], [3, 1, 2], [4, 0, 1, 2], [1, 3, 2], [3, 1, 2], [1, 3, 2], [0, 3, 2, 4], [4, 0, 2], [3, 1, 2], [3, 1, 2], [0, 3, 4, 2], [3, 2], [1, 3, 2], [3, 2], [3, 1, 2], [4, 1, 0, 2], [3, 2], [0, 4, 3, 2], [3, 2], [3, 4, 2], [3, 1, 2], [4, 0, 2], [4, 3, 2], [0, 4, 2, 3], [4, 3, 2, 0], [3, 4, 2], [3, 4, 2], [3, 2], [0, 3, 4, 2], [1, 3, 2], [1, 3, 2], [3, 1, 2], [0, 3, 4, 2], [3, 2], [3, 4, 2], [3, 2], [3, 1, 2], [3, 4, 2], [4, 0, 2], [4, 0, 2, 3], [3, 2], [3, 2], [1, 3, 2], [3, 1, 2], [4, 0, 2], [1, 3, 2], [3, 1, 2], [4, 0, 1, 2], [3, 1, 2], [4, 3, 2, 0], [3, 1, 2], [1, 3, 2], [4, 3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 4, 2], [3, 2], [3, 4, 2], [1, 3, 2], [0, 3, 2], [3, 1, 2], [3, 1, 2], [3, 1, 2], [3, 2], [4, 0, 2], [3, 1, 2], [4, 0, 3, 2], [4, 0, 2], [3, 4, 2], [0, 4, 3, 2], [0, 2, 3, 4], [4, 3, 0, 2], [1, 3, 2], [4, 0, 2], [4, 3, 2, 0], [1, 3, 2], [0, 4, 1, 2], [3, 2], [4, 0, 3, 2], [3, 1, 2], [1, 3, 2], [3, 1, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [0, 4, 2], [4, 0, 3, 2], [3, 1, 2], [3, 1, 2], [0, 4, 3, 2], [4, 0, 1, 2], [3, 1, 2], [4, 0, 2, 3], [1, 3, 2], [4, 0, 2, 3], [4, 0, 2], [3, 2], [4, 0, 2], [4, 0, 2, 3], [3, 4, 2], [1, 4, 2], [3, 2], [3, 2], [3, 2], [4, 3, 2], [3, 1, 2], [3, 4, 2], [3, 1, 2], [4, 3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01147410592862538  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.01147370891911643  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.011472958539213454  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.011471886719976152  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.011470529649938856  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.011468912873949324  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.011467066194329942  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.011465011962822505  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.011462773595537459  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.011460370251110621  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.011457818959440504  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.011455134621688299  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.011452335332121168  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.011449431734425681  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.011446434472288405  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.011443356318133218  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.011440205786909376  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.011436990329197474  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.011433720588684082  Accuracy on Support set:0.0\n",
      "torch.Size([112, 2048]) torch.Size([112])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.011430400822843825  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20360805094242096 tensor([0.2077, 0.2036, 0.1917, 0.1916, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.204021617770195 tensor([0.2040, 0.2059, 0.1897, 0.1956, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20374234020709991 tensor([0.2054, 0.2031, 0.1940, 0.1937, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.205065056681633 tensor([0.2066, 0.2017, 0.1924, 0.1943, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20442551374435425 tensor([0.2066, 0.2028, 0.1918, 0.1943, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20304471254348755 tensor([0.2030, 0.2098, 0.1889, 0.1944, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969090700149536 tensor([0.2040, 0.2035, 0.1969, 0.1951, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20342619717121124 tensor([0.2005, 0.2034, 0.1908, 0.2043, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20443876087665558 tensor([0.2067, 0.2005, 0.1937, 0.1947, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2034776657819748 tensor([0.2067, 0.2035, 0.1910, 0.1920, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20503579080104828 tensor([0.2058, 0.2050, 0.1904, 0.1929, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20188681781291962 tensor([0.2015, 0.2044, 0.1913, 0.2019, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20509539544582367 tensor([0.2079, 0.2025, 0.1925, 0.1920, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20697718858718872 tensor([0.2075, 0.2030, 0.1913, 0.1912, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20444612205028534 tensor([0.2052, 0.2064, 0.1909, 0.1931, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20431765913963318 tensor([0.2043, 0.2045, 0.1915, 0.1953, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2061355710029602 tensor([0.2061, 0.2016, 0.1913, 0.1944, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044059932231903 tensor([0.2076, 0.2044, 0.1898, 0.1898, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20279112458229065 tensor([0.2028, 0.2093, 0.1901, 0.1948, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463073253631592 tensor([0.2054, 0.2046, 0.1925, 0.1923, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20286372303962708 tensor([0.2009, 0.2073, 0.1892, 0.2029, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20384074747562408 tensor([0.2083, 0.2038, 0.1903, 0.1895, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20413321256637573 tensor([0.2080, 0.2041, 0.1902, 0.1923, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20281903445720673 tensor([0.2028, 0.2126, 0.1884, 0.1938, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20614075660705566 tensor([0.2079, 0.2013, 0.1937, 0.1910, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20365473628044128 tensor([0.2070, 0.2037, 0.1899, 0.1910, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2043876349925995 tensor([0.2044, 0.2045, 0.1920, 0.1947, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19624799489974976 tensor([0.2063, 0.2008, 0.1962, 0.1929, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20335587859153748 tensor([0.2034, 0.2059, 0.1911, 0.1988, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20382295548915863 tensor([0.2042, 0.2038, 0.1923, 0.1948, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20584052801132202 tensor([0.2060, 0.2066, 0.1895, 0.1920, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20567186176776886 tensor([0.2061, 0.2057, 0.1898, 0.1925, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2039836347103119 tensor([0.2040, 0.2075, 0.1915, 0.1951, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2033938616514206 tensor([0.2014, 0.2039, 0.1907, 0.2034, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20418010652065277 tensor([0.2058, 0.2073, 0.1909, 0.1919, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20527629554271698 tensor([0.2056, 0.2060, 0.1905, 0.1926, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20141756534576416 tensor([0.2014, 0.2138, 0.1879, 0.1950, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2041359692811966 tensor([0.2073, 0.2011, 0.1946, 0.1929, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20247742533683777 tensor([0.2019, 0.2036, 0.1917, 0.2025, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20614954829216003 tensor([0.2061, 0.2018, 0.1910, 0.1945, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20471647381782532 tensor([0.2047, 0.2061, 0.1924, 0.1953, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971161663532257 tensor([0.2048, 0.2032, 0.1971, 0.1947, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20222115516662598 tensor([0.2022, 0.2062, 0.1898, 0.1997, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20588013529777527 tensor([0.2059, 0.2069, 0.1892, 0.1910, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2041235864162445 tensor([0.2047, 0.2041, 0.1902, 0.1940, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20431405305862427 tensor([0.2060, 0.2043, 0.1939, 0.1938, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20351842045783997 tensor([0.2007, 0.2050, 0.1907, 0.2035, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2035442739725113 tensor([0.2059, 0.2035, 0.1919, 0.1956, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20361903309822083 tensor([0.2073, 0.2036, 0.1917, 0.1909, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20415782928466797 tensor([0.2053, 0.2022, 0.1932, 0.1952, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20245467126369476 tensor([0.2025, 0.2055, 0.1898, 0.2005, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.206021249294281 tensor([0.2060, 0.2029, 0.1906, 0.1937, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2039727121591568 tensor([0.2067, 0.2040, 0.1907, 0.1925, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20297564566135406 tensor([0.2030, 0.2087, 0.1904, 0.1967, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20358127355575562 tensor([0.2052, 0.2019, 0.1926, 0.1968, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20295678079128265 tensor([0.2030, 0.2044, 0.1897, 0.1970, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20363572239875793 tensor([0.2044, 0.2036, 0.1892, 0.1941, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20635102689266205 tensor([0.2078, 0.2030, 0.1913, 0.1916, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1965431123971939 tensor([0.2051, 0.2005, 0.1965, 0.1947, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20354478061199188 tensor([0.2035, 0.2040, 0.1906, 0.1978, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20632250607013702 tensor([0.2063, 0.2031, 0.1907, 0.1928, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20547032356262207 tensor([0.2087, 0.2027, 0.1910, 0.1921, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20311513543128967 tensor([0.1999, 0.2166, 0.1863, 0.1941, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1958562135696411 tensor([0.2048, 0.2031, 0.1959, 0.1946, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20321796834468842 tensor([0.2032, 0.2052, 0.1898, 0.1980, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2037566751241684 tensor([0.2051, 0.2053, 0.1914, 0.1945, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20419177412986755 tensor([0.2096, 0.2042, 0.1903, 0.1895, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20208939909934998 tensor([0.2021, 0.2088, 0.1884, 0.1958, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20342248678207397 tensor([0.2053, 0.2034, 0.1940, 0.1953, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20301081240177155 tensor([0.2019, 0.2030, 0.1905, 0.2034, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046245038509369 tensor([0.2068, 0.2046, 0.1912, 0.1918, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20477910339832306 tensor([0.2070, 0.2027, 0.1930, 0.1925, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20265382528305054 tensor([0.2027, 0.2092, 0.1894, 0.1932, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20451869070529938 tensor([0.2045, 0.2054, 0.1941, 0.1937, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20396064221858978 tensor([0.2009, 0.2042, 0.1914, 0.2040, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2031831294298172 tensor([0.2078, 0.2032, 0.1889, 0.1901, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20633918046951294 tensor([0.2063, 0.2030, 0.1907, 0.1929, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20378245413303375 tensor([0.2055, 0.2038, 0.1925, 0.1931, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20342834293842316 tensor([0.2008, 0.2034, 0.1902, 0.2045, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20502373576164246 tensor([0.2061, 0.2050, 0.1891, 0.1912, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20409011840820312 tensor([0.2065, 0.2041, 0.1913, 0.1937, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20142649114131927 tensor([0.2014, 0.2132, 0.1885, 0.1968, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2027410864830017 tensor([0.2055, 0.2015, 0.1942, 0.1961, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20495718717575073 tensor([0.2070, 0.2071, 0.1893, 0.1916, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20555229485034943 tensor([0.2059, 0.2059, 0.1896, 0.1930, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20336218178272247 tensor([0.2063, 0.2030, 0.1924, 0.1949, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20281147956848145 tensor([0.2028, 0.2105, 0.1887, 0.1954, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19746586680412292 tensor([0.2040, 0.2028, 0.1975, 0.1953, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20318762958049774 tensor([0.2014, 0.2032, 0.1914, 0.2034, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20628826320171356 tensor([0.2071, 0.2003, 0.1929, 0.1934, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2048930823802948 tensor([0.2059, 0.2049, 0.1900, 0.1941, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20118844509124756 tensor([0.2012, 0.2123, 0.1896, 0.1979, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20399606227874756 tensor([0.2059, 0.2024, 0.1941, 0.1935, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2048545628786087 tensor([0.2070, 0.2027, 0.1931, 0.1923, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2050979882478714 tensor([0.2055, 0.2051, 0.1908, 0.1933, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20336107909679413 tensor([0.2060, 0.2026, 0.1937, 0.1943, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20220644772052765 tensor([0.2013, 0.2055, 0.1908, 0.2022, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2044186294078827 tensor([0.2061, 0.2044, 0.1910, 0.1931, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20363633334636688 tensor([0.2045, 0.2036, 0.1904, 0.1951, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20212547481060028 tensor([0.2021, 0.2091, 0.1910, 0.1990, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20410092175006866 tensor([0.2041, 0.2024, 0.1928, 0.1966, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20276124775409698 tensor([0.2028, 0.2045, 0.1918, 0.1989, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20381496846675873 tensor([0.2068, 0.2038, 0.1895, 0.1917, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20420598983764648 tensor([0.2064, 0.2024, 0.1924, 0.1946, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204281285405159 tensor([0.2056, 0.2043, 0.1914, 0.1911, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.201199471950531 tensor([0.2014, 0.2038, 0.1926, 0.2012, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20536400377750397 tensor([0.2060, 0.2054, 0.1891, 0.1917, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20543478429317474 tensor([0.2077, 0.2054, 0.1882, 0.1888, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2025877684354782 tensor([0.2015, 0.2122, 0.1889, 0.1948, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20467448234558105 tensor([0.2048, 0.2047, 0.1939, 0.1945, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2035321742296219 tensor([0.2052, 0.2035, 0.1901, 0.1953, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20483601093292236 tensor([0.2048, 0.2052, 0.1913, 0.1935, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20471034944057465 tensor([0.2061, 0.2059, 0.1916, 0.1917, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2024984508752823 tensor([0.2025, 0.2094, 0.1890, 0.1981, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19618570804595947 tensor([0.2041, 0.2033, 0.1962, 0.1951, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20305143296718597 tensor([0.2031, 0.2041, 0.1903, 0.2000, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20572085678577423 tensor([0.2057, 0.2070, 0.1893, 0.1914, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20367251336574554 tensor([0.2059, 0.2037, 0.1899, 0.1935, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2035577893257141 tensor([0.2036, 0.2116, 0.1879, 0.1926, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20538653433322906 tensor([0.2054, 0.2071, 0.1923, 0.1937, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20366452634334564 tensor([0.2000, 0.2061, 0.1902, 0.2037, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20402391254901886 tensor([0.2065, 0.2040, 0.1908, 0.1925, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20582215487957 tensor([0.2058, 0.2028, 0.1906, 0.1938, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20277950167655945 tensor([0.2028, 0.2083, 0.1897, 0.1959, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2027292549610138 tensor([0.2055, 0.2024, 0.1946, 0.1948, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2035011202096939 tensor([0.2032, 0.2048, 0.1913, 0.1972, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20713543891906738 tensor([0.2080, 0.2023, 0.1921, 0.1905, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20578761398792267 tensor([0.2078, 0.2065, 0.1883, 0.1916, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20269888639450073 tensor([0.2027, 0.2124, 0.1890, 0.1945, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20417293906211853 tensor([0.2072, 0.2019, 0.1940, 0.1926, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20128725469112396 tensor([0.1999, 0.2089, 0.1907, 0.2013, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20375025272369385 tensor([0.2050, 0.2038, 0.1903, 0.1935, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20562024414539337 tensor([0.2056, 0.2023, 0.1921, 0.1937, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20248308777809143 tensor([0.2025, 0.2126, 0.1897, 0.1943, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20336663722991943 tensor([0.2065, 0.2034, 0.1921, 0.1917, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20325273275375366 tensor([0.2025, 0.2033, 0.1933, 0.2033, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20568406581878662 tensor([0.2063, 0.2034, 0.1922, 0.1924, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2040364295244217 tensor([0.2062, 0.2055, 0.1906, 0.1937, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19519278407096863 tensor([0.2057, 0.2013, 0.1952, 0.1944, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20564182102680206 tensor([0.1994, 0.2061, 0.1916, 0.2056, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20712144672870636 tensor([0.2071, 0.2023, 0.1908, 0.1915, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038189023733139 tensor([0.2072, 0.2038, 0.1900, 0.1910, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20282408595085144 tensor([0.2028, 0.2127, 0.1886, 0.1922, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20320343971252441 tensor([0.2053, 0.2032, 0.1941, 0.1948, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20904847979545593 tensor([0.2090, 0.2005, 0.1908, 0.1896, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20383793115615845 tensor([0.2038, 0.2066, 0.1894, 0.1957, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19554653763771057 tensor([0.2049, 0.2025, 0.1955, 0.1942, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2036401331424713 tensor([0.2036, 0.2043, 0.1911, 0.1997, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20431120693683624 tensor([0.2056, 0.2043, 0.1897, 0.1935, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066524177789688 tensor([0.2072, 0.2067, 0.1894, 0.1899, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20328941941261292 tensor([0.2048, 0.2022, 0.1944, 0.1953, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043541967868805 tensor([0.2003, 0.2044, 0.1917, 0.2048, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046702355146408 tensor([0.2078, 0.2047, 0.1893, 0.1897, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20365217328071594 tensor([0.2058, 0.2037, 0.1908, 0.1939, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20343017578125 tensor([0.2057, 0.2022, 0.1942, 0.1945, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20498092472553253 tensor([0.2073, 0.2050, 0.1899, 0.1905, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20341086387634277 tensor([0.2056, 0.2045, 0.1913, 0.1952, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2040937840938568 tensor([0.2041, 0.2051, 0.1908, 0.1974, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20429062843322754 tensor([0.2004, 0.2055, 0.1901, 0.2043, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20627900958061218 tensor([0.2080, 0.2015, 0.1918, 0.1924, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20646913349628448 tensor([0.2083, 0.2031, 0.1906, 0.1915, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20282351970672607 tensor([0.2028, 0.2062, 0.1890, 0.1937, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19629386067390442 tensor([0.2029, 0.2028, 0.1963, 0.1976, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.203657329082489 tensor([0.2039, 0.2027, 0.1912, 0.1986, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20510824024677277 tensor([0.2061, 0.2032, 0.1913, 0.1943, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20484158396720886 tensor([0.2048, 0.2052, 0.1893, 0.1940, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2021709531545639 tensor([0.2022, 0.2091, 0.1897, 0.1979, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20351248979568481 tensor([0.2073, 0.2035, 0.1936, 0.1915, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20205058157444 tensor([0.2010, 0.2060, 0.1920, 0.2021, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20499606430530548 tensor([0.2058, 0.2050, 0.1902, 0.1918, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20502802729606628 tensor([0.2078, 0.2054, 0.1919, 0.1898, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20293883979320526 tensor([0.2033, 0.2122, 0.1878, 0.1937, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20421190559864044 tensor([0.2062, 0.2042, 0.1922, 0.1921, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20344235002994537 tensor([0.2006, 0.2038, 0.1914, 0.2034, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2045707106590271 tensor([0.2046, 0.2049, 0.1910, 0.1945, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2053355723619461 tensor([0.2072, 0.2053, 0.1911, 0.1911, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20332546532154083 tensor([0.2070, 0.2028, 0.1930, 0.1939, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2034672200679779 tensor([0.2010, 0.2048, 0.1901, 0.2035, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.205791637301445 tensor([0.2058, 0.2060, 0.1898, 0.1915, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20494559407234192 tensor([0.2085, 0.2024, 0.1925, 0.1917, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19511812925338745 tensor([0.2026, 0.2034, 0.1951, 0.1965, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20397138595581055 tensor([0.2073, 0.2040, 0.1892, 0.1912, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20350974798202515 tensor([0.2035, 0.2081, 0.1892, 0.1926, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20440642535686493 tensor([0.2017, 0.2085, 0.1886, 0.1968, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2050536721944809 tensor([0.2051, 0.2028, 0.1924, 0.1942, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20483294129371643 tensor([0.2083, 0.2048, 0.1896, 0.1901, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20376136898994446 tensor([0.2053, 0.2038, 0.1909, 0.1943, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20299093425273895 tensor([0.2030, 0.2119, 0.1865, 0.1939, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20294402539730072 tensor([0.2055, 0.2023, 0.1943, 0.1949, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.203092560172081 tensor([0.2031, 0.2047, 0.1904, 0.1982, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20347745716571808 tensor([0.2080, 0.2035, 0.1894, 0.1893, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20437264442443848 tensor([0.2080, 0.2044, 0.1904, 0.1907, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2052544504404068 tensor([0.2053, 0.2074, 0.1895, 0.1926, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20127038657665253 tensor([0.2012, 0.2056, 0.1925, 0.2013, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2048482447862625 tensor([0.2056, 0.2058, 0.1902, 0.1935, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20516914129257202 tensor([0.2053, 0.2062, 0.1914, 0.1919, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20342940092086792 tensor([0.2040, 0.2077, 0.1908, 0.1941, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19608914852142334 tensor([0.2048, 0.2032, 0.1961, 0.1936, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2039371132850647 tensor([0.2010, 0.2044, 0.1925, 0.2039, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2036285698413849 tensor([0.2041, 0.2036, 0.1896, 0.1952, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20526288449764252 tensor([0.2055, 0.2053, 0.1903, 0.1923, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20449316501617432 tensor([0.2054, 0.2045, 0.1945, 0.1933, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20131056010723114 tensor([0.2028, 0.2025, 0.1914, 0.2013, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20432190597057343 tensor([0.2057, 0.2043, 0.1916, 0.1936, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20372320711612701 tensor([0.2070, 0.2048, 0.1921, 0.1924, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20480170845985413 tensor([0.2048, 0.2053, 0.1909, 0.1940, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20441733300685883 tensor([0.2046, 0.2044, 0.1915, 0.1938, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.203572079539299 tensor([0.2036, 0.2048, 0.1920, 0.1994, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20450231432914734 tensor([0.2064, 0.2045, 0.1905, 0.1930, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20753301680088043 tensor([0.2085, 0.2020, 0.1916, 0.1903, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20234538614749908 tensor([0.2020, 0.2095, 0.1906, 0.1956, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20480214059352875 tensor([0.2048, 0.2050, 0.1939, 0.1937, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20178435742855072 tensor([0.2012, 0.2035, 0.1905, 0.2018, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20431146025657654 tensor([0.2056, 0.2070, 0.1905, 0.1926, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040945291519165 tensor([0.2089, 0.2041, 0.1899, 0.1900, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20217935740947723 tensor([0.2022, 0.2112, 0.1885, 0.1958, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19554774463176727 tensor([0.2035, 0.2043, 0.1955, 0.1953, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20350146293640137 tensor([0.2014, 0.2051, 0.1898, 0.2035, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20686671137809753 tensor([0.2075, 0.2025, 0.1917, 0.1915, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20432636141777039 tensor([0.2061, 0.2060, 0.1898, 0.1937, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2041180282831192 tensor([0.2041, 0.2102, 0.1882, 0.1914, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20487336814403534 tensor([0.2057, 0.2049, 0.1945, 0.1943, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20209117233753204 tensor([0.2024, 0.2035, 0.1910, 0.2021, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20458625257015228 tensor([0.2064, 0.2046, 0.1900, 0.1924, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 2], [2, 3], [3, 1, 2], [4, 0, 1, 2], [1, 3, 2], [3, 1, 2], [2, 3], [4, 3, 2], [0, 4, 2], [1, 3, 2], [3, 2], [0, 3, 2, 4], [3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [0, 3, 4, 2], [3, 2], [3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 1, 0, 2], [3, 2], [3, 2], [0, 4, 3, 2], [3, 1, 2], [4, 3, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [1, 3, 2], [4, 3, 2], [0, 4, 2, 3], [4, 3, 2], [2, 4, 3], [3, 2], [3, 2], [4, 3, 2, 0], [3, 4, 2], [4, 0, 2], [3, 4, 2], [3, 2], [0, 3, 4, 2], [1, 3, 2], [4, 2, 3], [3, 1, 2], [3, 2], [4, 3, 2], [1, 3, 2], [2, 3], [3, 2], [3, 1, 2], [0, 3, 4, 2], [1, 3, 2], [2, 3], [3, 1, 2], [3, 1, 2], [2, 3, 0], [3, 4, 2], [2, 3], [3, 2], [3, 2], [2, 3], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 1, 2], [2, 3, 4], [4, 3, 1, 2], [4, 0, 2], [1, 3, 2], [3, 2], [4, 3, 2], [3, 1, 2], [4, 0, 1, 2], [3, 1, 2], [3, 2], [4, 3, 2, 0], [3, 1, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 0], [3, 4, 2], [3, 2], [3, 2], [3, 2], [2, 4, 3], [4, 3, 2], [2, 4, 3], [3, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 1, 2], [2, 3], [1, 3, 2], [0, 3, 2], [3, 1, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 4, 2], [3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [4, 0, 3, 2], [1, 3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 3, 2], [3, 1, 2], [2, 3], [0, 2, 3, 4], [3, 1, 2], [4, 3, 2], [3, 2], [3, 2], [4, 3, 0, 2], [1, 3, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2, 0], [1, 3, 2], [0, 4, 1, 2], [3, 2], [3, 2], [4, 0, 3, 2], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [3, 2], [4, 3, 1, 2], [1, 3, 2], [3, 1, 2], [3, 2], [4, 3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 0, 3, 2], [3, 1, 2], [0, 4, 2], [3, 2], [3, 1, 2], [0, 4, 3, 2], [0, 3, 4, 2], [4, 0, 1, 2], [3, 2], [3, 2], [2, 3, 0], [3, 1, 2], [4, 0, 2, 3], [3, 2], [3, 2], [2, 3], [1, 3, 2], [2, 3], [3, 2], [3, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [2, 3], [3, 2], [4, 0, 2, 3], [3, 4, 2], [1, 4, 2], [3, 2], [3, 2], [3, 2], [3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [0, 3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 2], [2, 3, 4], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [4, 3, 2], [4, 0, 2], [3, 2]]\n",
      "[[0, 1, 4], [0, 1, 4], [0, 4], [3], [0, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 4], [0, 1], [1], [0, 1], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 1], [0, 1, 4], [1], [0, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [1], [0, 4], [0, 1, 4], [0, 4], [0, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1], [0], [1, 3], [0, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 4], [1, 4], [0, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [0, 1], [0, 1, 4], [1, 3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1], [0, 4], [0, 1, 4], [1], [0, 4], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 4], [0, 4], [0, 1, 4], [0], [0, 4], [0, 4], [0, 1, 4], [0, 1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [1], [1], [3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [0, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 4], [0, 4], [1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4]]\n",
      "NL_pred of 4th iteration [[4, 3, 2], [3, 1, 2], [4, 3, 2], [1, 3, 2], [3, 4, 2], [4, 3, 1, 2], [4, 3, 2], [1, 3, 2], [3, 1, 2], [4, 3, 1, 2], [0, 3, 4, 2], [3, 4, 2], [4, 3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.09871185742891751  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.09870745585514949  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.09869908369504489  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.09868714442619911  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.09867202318631686  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.0986540225835947  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.09863344522622916  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0986105937224168  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.09858567898090069  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.09855894859020527  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.09853056760934684  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.09850072860717773  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.09846961498260498  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.09843731843508206  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.098404022363516  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.09836982763730563  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.09833486263568585  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.09829917320838341  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.09826288773463322  Accuracy on Support set:0.0\n",
      "torch.Size([13, 2048]) torch.Size([13])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.09822608874394344  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20537512004375458 tensor([0.2090, 0.2054, 0.1859, 0.1926, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2052135467529297 tensor([0.2052, 0.2077, 0.1840, 0.1966, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20566540956497192 tensor([0.2068, 0.2049, 0.1880, 0.1946, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20694880187511444 tensor([0.2078, 0.2035, 0.1865, 0.1952, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20627416670322418 tensor([0.2078, 0.2046, 0.1860, 0.1953, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20423012971878052 tensor([0.2042, 0.2116, 0.1832, 0.1953, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20533765852451324 tensor([0.2053, 0.2054, 0.1908, 0.1961, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20509065687656403 tensor([0.2017, 0.2052, 0.1852, 0.2051, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20632067322731018 tensor([0.2080, 0.2022, 0.1878, 0.1957, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20524291694164276 tensor([0.2079, 0.2052, 0.1851, 0.1930, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681215822696686 tensor([0.2070, 0.2068, 0.1846, 0.1938, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2028820812702179 tensor([0.2027, 0.2062, 0.1854, 0.2029, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20688919723033905 tensor([0.2092, 0.2043, 0.1867, 0.1930, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20879147946834564 tensor([0.2088, 0.2048, 0.1854, 0.1922, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20629453659057617 tensor([0.2064, 0.2082, 0.1850, 0.1940, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2055920660495758 tensor([0.2056, 0.2063, 0.1856, 0.1963, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20741228759288788 tensor([0.2074, 0.2034, 0.1855, 0.1953, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20615409314632416 tensor([0.2089, 0.2062, 0.1841, 0.1908, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2041134536266327 tensor([0.2041, 0.2110, 0.1843, 0.1957, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20641665160655975 tensor([0.2067, 0.2064, 0.1866, 0.1932, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2038080096244812 tensor([0.2021, 0.2090, 0.1836, 0.2038, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20560994744300842 tensor([0.2095, 0.2056, 0.1846, 0.1905, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20588871836662292 tensor([0.2093, 0.2059, 0.1844, 0.1932, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20413915812969208 tensor([0.2041, 0.2142, 0.1828, 0.1947, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20802398025989532 tensor([0.2092, 0.2031, 0.1877, 0.1920, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20541103184223175 tensor([0.2082, 0.2054, 0.1842, 0.1920, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20566989481449127 tensor([0.2057, 0.2062, 0.1862, 0.1957, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20567204058170319 tensor([0.2077, 0.2026, 0.1902, 0.1939, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20465871691703796 tensor([0.2047, 0.2077, 0.1853, 0.1997, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20553536713123322 tensor([0.2055, 0.2056, 0.1864, 0.1958, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20721453428268433 tensor([0.2072, 0.2084, 0.1837, 0.1930, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.207303985953331 tensor([0.2073, 0.2074, 0.1840, 0.1935, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20526188611984253 tensor([0.2053, 0.2093, 0.1857, 0.1961, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042984664440155 tensor([0.2026, 0.2058, 0.1850, 0.2043, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20602215826511383 tensor([0.2071, 0.2091, 0.1850, 0.1928, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20688962936401367 tensor([0.2069, 0.2077, 0.1847, 0.1936, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20266297459602356 tensor([0.2027, 0.2156, 0.1823, 0.1959, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20596438646316528 tensor([0.2086, 0.2029, 0.1887, 0.1938, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2034633308649063 tensor([0.2032, 0.2053, 0.1859, 0.2035, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2073606252670288 tensor([0.2074, 0.2035, 0.1853, 0.1955, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20600567758083344 tensor([0.2060, 0.2079, 0.1866, 0.1963, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20499011874198914 tensor([0.2061, 0.2050, 0.1911, 0.1957, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20347772538661957 tensor([0.2035, 0.2079, 0.1840, 0.2006, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20709292590618134 tensor([0.2071, 0.2086, 0.1835, 0.1920, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20587027072906494 tensor([0.2060, 0.2059, 0.1844, 0.1950, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20611149072647095 tensor([0.2073, 0.2061, 0.1880, 0.1948, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2044934183359146 tensor([0.2020, 0.2068, 0.1850, 0.2045, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20532594621181488 tensor([0.2071, 0.2053, 0.1860, 0.1966, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2053871601819992 tensor([0.2086, 0.2054, 0.1858, 0.1918, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20606499910354614 tensor([0.2066, 0.2040, 0.1872, 0.1962, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20365606248378754 tensor([0.2037, 0.2073, 0.1840, 0.2014, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20723575353622437 tensor([0.2072, 0.2047, 0.1847, 0.1947, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20574139058589935 tensor([0.2080, 0.2057, 0.1848, 0.1935, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20422525703907013 tensor([0.2042, 0.2105, 0.1846, 0.1977, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20545624196529388 tensor([0.2065, 0.2036, 0.1867, 0.1978, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20421220362186432 tensor([0.2042, 0.2061, 0.1839, 0.1980, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20538006722927094 tensor([0.2056, 0.2054, 0.1835, 0.1950, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20820356905460358 tensor([0.2090, 0.2047, 0.1855, 0.1926, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20519773662090302 tensor([0.2064, 0.2023, 0.1905, 0.1957, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2048000991344452 tensor([0.2048, 0.2058, 0.1848, 0.1988, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20754024386405945 tensor([0.2075, 0.2049, 0.1849, 0.1937, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20730604231357574 tensor([0.2100, 0.2045, 0.1852, 0.1931, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20476143062114716 tensor([0.2012, 0.2182, 0.1808, 0.1950, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20494447648525238 tensor([0.2061, 0.2049, 0.1898, 0.1956, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20445995032787323 tensor([0.2045, 0.2070, 0.1840, 0.1989, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20560914278030396 tensor([0.2063, 0.2071, 0.1856, 0.1955, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2059457004070282 tensor([0.2109, 0.2059, 0.1845, 0.1904, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2033155858516693 tensor([0.2033, 0.2105, 0.1827, 0.1967, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20524904131889343 tensor([0.2066, 0.2052, 0.1880, 0.1963, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20428308844566345 tensor([0.2032, 0.2048, 0.1850, 0.2043, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20638327300548553 tensor([0.2081, 0.2064, 0.1854, 0.1927, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20663072168827057 tensor([0.2083, 0.2045, 0.1872, 0.1934, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20392632484436035 tensor([0.2039, 0.2110, 0.1836, 0.1942, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20582303404808044 tensor([0.2058, 0.2072, 0.1881, 0.1947, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20495954155921936 tensor([0.2021, 0.2060, 0.1857, 0.2050, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204945906996727 tensor([0.2090, 0.2049, 0.1832, 0.1910, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2075612097978592 tensor([0.2076, 0.2048, 0.1849, 0.1938, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20555508136749268 tensor([0.2068, 0.2056, 0.1866, 0.1941, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20520231127738953 tensor([0.2020, 0.2052, 0.1846, 0.2054, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067846655845642 tensor([0.2073, 0.2068, 0.1833, 0.1922, 0.2104], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2058609127998352 tensor([0.2077, 0.2059, 0.1855, 0.1946, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20266105234622955 tensor([0.2027, 0.2149, 0.1829, 0.1977, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20460723340511322 tensor([0.2068, 0.2033, 0.1882, 0.1971, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2067829668521881 tensor([0.2083, 0.2089, 0.1836, 0.1925, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20719373226165771 tensor([0.2072, 0.2077, 0.1838, 0.1940, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2052161991596222 tensor([0.2076, 0.2048, 0.1865, 0.1958, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20406535267829895 tensor([0.2041, 0.2122, 0.1831, 0.1963, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042929083108902 tensor([0.2027, 0.2049, 0.1856, 0.2043, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20811958611011505 tensor([0.2084, 0.2021, 0.1871, 0.1943, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20663954317569733 tensor([0.2072, 0.2066, 0.1842, 0.1950, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20245860517024994 tensor([0.2025, 0.2140, 0.1838, 0.1988, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20588219165802002 tensor([0.2073, 0.2043, 0.1881, 0.1945, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20669503509998322 tensor([0.2083, 0.2044, 0.1872, 0.1933, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20681101083755493 tensor([0.2068, 0.2069, 0.1849, 0.1943, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20521335303783417 tensor([0.2072, 0.2044, 0.1878, 0.1953, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20318588614463806 tensor([0.2025, 0.2072, 0.1850, 0.2032, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20620177686214447 tensor([0.2073, 0.2062, 0.1852, 0.1940, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20539213716983795 tensor([0.2057, 0.2054, 0.1846, 0.1961, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2033972442150116 tensor([0.2034, 0.2109, 0.1852, 0.1999, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2053966373205185 tensor([0.2054, 0.2041, 0.1869, 0.1975, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20402923226356506 tensor([0.2040, 0.2063, 0.1859, 0.1998, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20557717978954315 tensor([0.2080, 0.2056, 0.1838, 0.1927, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20602639019489288 tensor([0.2076, 0.2042, 0.1866, 0.1956, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20605257153511047 tensor([0.2068, 0.2061, 0.1855, 0.1921, 0.2095], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20219555497169495 tensor([0.2027, 0.2056, 0.1867, 0.2022, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070857733488083 tensor([0.2072, 0.2071, 0.1834, 0.1927, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20720823109149933 tensor([0.2089, 0.2072, 0.1826, 0.1898, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2043868899345398 tensor([0.2027, 0.2140, 0.1831, 0.1958, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2061024308204651 tensor([0.2061, 0.2065, 0.1879, 0.1955, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20530126988887787 tensor([0.2064, 0.2053, 0.1843, 0.1962, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20613043010234833 tensor([0.2061, 0.2070, 0.1854, 0.1944, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20655636489391327 tensor([0.2074, 0.2077, 0.1857, 0.1927, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20374354720115662 tensor([0.2037, 0.2111, 0.1833, 0.1990, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20513981580734253 tensor([0.2054, 0.2051, 0.1901, 0.1961, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20431391894817352 tensor([0.2043, 0.2059, 0.1844, 0.2009, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20694616436958313 tensor([0.2069, 0.2088, 0.1835, 0.1924, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2054501473903656 tensor([0.2071, 0.2055, 0.1842, 0.1945, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20478831231594086 tensor([0.2048, 0.2133, 0.1823, 0.1935, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2066119909286499 tensor([0.2066, 0.2089, 0.1864, 0.1947, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2046227604150772 tensor([0.2013, 0.2079, 0.1845, 0.2046, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2057909518480301 tensor([0.2077, 0.2058, 0.1850, 0.1935, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20707069337368011 tensor([0.2071, 0.2046, 0.1847, 0.1947, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20401644706726074 tensor([0.2040, 0.2101, 0.1840, 0.1969, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20460498332977295 tensor([0.2069, 0.2041, 0.1886, 0.1958, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20534497499465942 tensor([0.2045, 0.2065, 0.1854, 0.1982, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2089800089597702 tensor([0.2093, 0.2040, 0.1863, 0.1915, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2075539231300354 tensor([0.2091, 0.2082, 0.1826, 0.1926, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20394198596477509 tensor([0.2039, 0.2142, 0.1833, 0.1954, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20604780316352844 tensor([0.2085, 0.2037, 0.1881, 0.1936, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20226454734802246 tensor([0.2011, 0.2107, 0.1848, 0.2023, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20547933876514435 tensor([0.2063, 0.2055, 0.1846, 0.1945, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20684203505516052 tensor([0.2068, 0.2040, 0.1863, 0.1947, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20374076068401337 tensor([0.2037, 0.2144, 0.1839, 0.1952, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20514021813869476 tensor([0.2078, 0.2051, 0.1863, 0.1927, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20431183278560638 tensor([0.2038, 0.2050, 0.1874, 0.2043, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2075445055961609 tensor([0.2076, 0.2051, 0.1864, 0.1934, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2058771848678589 tensor([0.2074, 0.2072, 0.1848, 0.1947, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2053309828042984 tensor([0.2070, 0.2031, 0.1891, 0.1954, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20664753019809723 tensor([0.2006, 0.2078, 0.1859, 0.2066, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20842260122299194 tensor([0.2084, 0.2041, 0.1850, 0.1924, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20557361841201782 tensor([0.2084, 0.2056, 0.1842, 0.1919, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20408689975738525 tensor([0.2041, 0.2144, 0.1829, 0.1931, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20498880743980408 tensor([0.2067, 0.2050, 0.1882, 0.1958, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21023724973201752 tensor([0.2102, 0.2023, 0.1850, 0.1907, 0.2118], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20509399473667145 tensor([0.2051, 0.2083, 0.1836, 0.1966, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20473337173461914 tensor([0.2063, 0.2043, 0.1895, 0.1952, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20492391288280487 tensor([0.2049, 0.2061, 0.1853, 0.2007, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20607049763202667 tensor([0.2068, 0.2061, 0.1839, 0.1944, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20839886367321014 tensor([0.2084, 0.2084, 0.1836, 0.1909, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20519202947616577 tensor([0.2062, 0.2040, 0.1884, 0.1962, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20579668879508972 tensor([0.2016, 0.2061, 0.1860, 0.2058, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20642311871051788 tensor([0.2090, 0.2064, 0.1836, 0.1907, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20542095601558685 tensor([0.2071, 0.2054, 0.1851, 0.1949, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20529115200042725 tensor([0.2070, 0.2040, 0.1882, 0.1955, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20675241947174072 tensor([0.2085, 0.2068, 0.1842, 0.1915, 0.2091], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20526225864887238 tensor([0.2069, 0.2063, 0.1854, 0.1962, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20535720884799957 tensor([0.2054, 0.2069, 0.1850, 0.1983, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2051871120929718 tensor([0.2016, 0.2073, 0.1845, 0.2052, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20810183882713318 tensor([0.2093, 0.2033, 0.1859, 0.1934, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20830461382865906 tensor([0.2096, 0.2048, 0.1848, 0.1925, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20405930280685425 tensor([0.2041, 0.2078, 0.1834, 0.1946, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20515486598014832 tensor([0.2052, 0.2045, 0.1854, 0.1995, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2069733440876007 tensor([0.2074, 0.2050, 0.1855, 0.1952, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2060958743095398 tensor([0.2061, 0.2069, 0.1835, 0.1949, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20338869094848633 tensor([0.2034, 0.2109, 0.1840, 0.1988, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2053227722644806 tensor([0.2086, 0.2053, 0.1876, 0.1924, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20304031670093536 tensor([0.2022, 0.2078, 0.1862, 0.2030, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20672908425331116 tensor([0.2070, 0.2067, 0.1845, 0.1927, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20684723556041718 tensor([0.2091, 0.2072, 0.1861, 0.1907, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2045096606016159 tensor([0.2045, 0.2140, 0.1822, 0.1947, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2060166448354721 tensor([0.2075, 0.2060, 0.1863, 0.1931, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2044120877981186 tensor([0.2019, 0.2056, 0.1855, 0.2044, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2057468742132187 tensor([0.2057, 0.2066, 0.1853, 0.1955, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20709970593452454 tensor([0.2084, 0.2071, 0.1853, 0.1920, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2051525115966797 tensor([0.2083, 0.2045, 0.1872, 0.1949, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20434844493865967 tensor([0.2022, 0.2065, 0.1844, 0.2043, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20693427324295044 tensor([0.2069, 0.2078, 0.1841, 0.1925, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2067694067955017 tensor([0.2098, 0.2042, 0.1867, 0.1927, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20573748648166656 tensor([0.2086, 0.2057, 0.1834, 0.1921, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20473922789096832 tensor([0.2047, 0.2099, 0.1835, 0.1936, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2061377614736557 tensor([0.2030, 0.2102, 0.1829, 0.1977, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20635133981704712 tensor([0.2064, 0.2046, 0.1865, 0.1951, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20661041140556335 tensor([0.2095, 0.2066, 0.1839, 0.1911, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20550429821014404 tensor([0.2066, 0.2055, 0.1851, 0.1953, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20428286492824554 tensor([0.2043, 0.2135, 0.1809, 0.1949, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20484283566474915 tensor([0.2069, 0.2042, 0.1883, 0.1959, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2043057233095169 tensor([0.2043, 0.2065, 0.1846, 0.1991, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20523503422737122 tensor([0.2092, 0.2052, 0.1837, 0.1904, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20615838468074799 tensor([0.2092, 0.2062, 0.1846, 0.1917, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20648467540740967 tensor([0.2065, 0.2092, 0.1838, 0.1935, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20228034257888794 tensor([0.2025, 0.2074, 0.1866, 0.2023, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2066754400730133 tensor([0.2069, 0.2076, 0.1844, 0.1944, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20653949677944183 tensor([0.2065, 0.2080, 0.1856, 0.1928, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2052599936723709 tensor([0.2053, 0.2095, 0.1849, 0.1950, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20507179200649261 tensor([0.2061, 0.2051, 0.1900, 0.1946, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20497353374958038 tensor([0.2023, 0.2062, 0.1867, 0.2050, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.205311119556427 tensor([0.2054, 0.2053, 0.1838, 0.1962, 0.2093], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20680423080921173 tensor([0.2068, 0.2070, 0.1846, 0.1932, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20632298290729523 tensor([0.2067, 0.2063, 0.1885, 0.1943, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2022332102060318 tensor([0.2041, 0.2043, 0.1856, 0.2022, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20609928667545319 tensor([0.2069, 0.2061, 0.1858, 0.1945, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20556291937828064 tensor([0.2083, 0.2066, 0.1862, 0.1933, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20605836808681488 tensor([0.2061, 0.2071, 0.1850, 0.1949, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20588678121566772 tensor([0.2059, 0.2062, 0.1856, 0.1948, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20485301315784454 tensor([0.2049, 0.2066, 0.1861, 0.2004, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20627926290035248 tensor([0.2077, 0.2063, 0.1847, 0.1939, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.209370955824852 tensor([0.2097, 0.2038, 0.1858, 0.1913, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20414766669273376 tensor([0.2033, 0.2113, 0.1848, 0.1965, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2061181217432022 tensor([0.2061, 0.2068, 0.1880, 0.1947, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20268023014068604 tensor([0.2024, 0.2053, 0.1848, 0.2027, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20613783597946167 tensor([0.2068, 0.2088, 0.1847, 0.1935, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20587539672851562 tensor([0.2101, 0.2059, 0.1841, 0.1909, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20341594517230988 tensor([0.2034, 0.2129, 0.1828, 0.1967, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20485785603523254 tensor([0.2049, 0.2062, 0.1894, 0.1963, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.204324409365654 tensor([0.2027, 0.2069, 0.1842, 0.2043, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2086830884218216 tensor([0.2087, 0.2043, 0.1859, 0.1925, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20618204772472382 tensor([0.2073, 0.2078, 0.1840, 0.1947, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2053864449262619 tensor([0.2054, 0.2119, 0.1825, 0.1923, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20670796930789948 tensor([0.2070, 0.2067, 0.1885, 0.1952, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20307187736034393 tensor([0.2037, 0.2053, 0.1852, 0.2031, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206344872713089 tensor([0.2077, 0.2063, 0.1842, 0.1933, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 2], [2, 3], [3, 1, 2], [4, 0, 1, 2], [1, 3, 2], [3, 1, 2], [2, 3], [4, 3, 2], [0, 4, 2], [1, 3, 2], [3, 2], [0, 3, 2, 4], [3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [0, 3, 4, 2], [3, 2], [3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 1, 0, 2], [3, 2], [3, 2], [0, 4, 3, 2], [3, 1, 2], [4, 3, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [1, 3, 2], [4, 3, 2], [0, 4, 2, 3], [4, 3, 2], [2, 4, 3], [3, 2], [3, 2], [4, 3, 2, 0], [3, 4, 2], [4, 0, 2], [3, 4, 2], [3, 2], [0, 3, 4, 2], [1, 3, 2], [4, 2, 3], [3, 1, 2], [3, 2], [4, 3, 2], [1, 3, 2], [2, 3], [3, 2], [3, 1, 2], [0, 3, 4, 2], [1, 3, 2], [2, 3], [3, 1, 2], [3, 1, 2], [2, 3, 0], [3, 4, 2], [2, 3], [3, 2], [3, 2], [2, 3], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 1, 2], [2, 3, 4], [4, 3, 1, 2], [4, 0, 2], [1, 3, 2], [3, 2], [4, 3, 2], [3, 1, 2], [4, 0, 1, 2], [3, 1, 2], [3, 2], [4, 3, 2, 0], [3, 1, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 0], [3, 4, 2], [3, 2], [3, 2], [3, 2], [2, 4, 3], [4, 3, 2], [2, 4, 3], [3, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 1, 2], [2, 3], [1, 3, 2], [0, 3, 2], [3, 1, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 4, 2], [3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [4, 0, 3, 2], [1, 3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 3, 2], [3, 1, 2], [2, 3], [0, 2, 3, 4], [3, 1, 2], [4, 3, 2], [3, 2], [3, 2], [4, 3, 0, 2], [1, 3, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2, 0], [1, 3, 2], [0, 4, 1, 2], [3, 2], [3, 2], [4, 0, 3, 2], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [3, 2], [4, 3, 1, 2], [1, 3, 2], [3, 1, 2], [3, 2], [4, 3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 0, 3, 2], [3, 1, 2], [0, 4, 2], [3, 2], [3, 1, 2], [0, 4, 3, 2], [0, 3, 4, 2], [4, 0, 1, 2], [3, 2], [3, 2], [2, 3, 0], [3, 1, 2], [4, 0, 2, 3], [3, 2], [3, 2], [2, 3], [1, 3, 2], [2, 3], [3, 2], [3, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [2, 3], [3, 2], [4, 0, 2, 3], [3, 4, 2], [1, 4, 2], [3, 2], [3, 2], [3, 2], [3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [0, 3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 2], [2, 3, 4], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [4, 3, 2], [4, 0, 2], [3, 2]]\n",
      "[[0, 1, 4], [0, 1, 4], [0, 4], [3], [0, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 4], [0, 1], [1], [0, 1], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 1], [0, 1, 4], [1], [0, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [1], [0, 4], [0, 1, 4], [0, 4], [0, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1], [0], [1, 3], [0, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 4], [1, 4], [0, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [0, 1], [0, 1, 4], [1, 3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1], [0, 4], [0, 1, 4], [1], [0, 4], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 4], [0, 4], [0, 1, 4], [0], [0, 4], [0, 4], [0, 1, 4], [0, 1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [1], [1], [3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [0, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 4], [0, 4], [1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4]]\n",
      "NL_pred of 5th iteration []\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[3, 2], [2, 3], [3, 1, 2], [4, 0, 1, 2], [1, 3, 2], [3, 1, 2], [2, 3], [4, 3, 2], [0, 4, 2], [1, 3, 2], [3, 2], [0, 3, 2, 4], [3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [0, 3, 4, 2], [3, 2], [3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 1, 0, 2], [3, 2], [3, 2], [0, 4, 3, 2], [3, 1, 2], [4, 3, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [1, 3, 2], [4, 3, 2], [0, 4, 2, 3], [4, 3, 2], [2, 4, 3], [3, 2], [3, 2], [4, 3, 2, 0], [3, 4, 2], [4, 0, 2], [3, 4, 2], [3, 2], [0, 3, 4, 2], [1, 3, 2], [4, 2, 3], [3, 1, 2], [3, 2], [4, 3, 2], [1, 3, 2], [2, 3], [3, 2], [3, 1, 2], [0, 3, 4, 2], [1, 3, 2], [2, 3], [3, 1, 2], [3, 1, 2], [2, 3, 0], [3, 4, 2], [2, 3], [3, 2], [3, 2], [2, 3], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [3, 4, 2], [4, 0, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [3, 2], [3, 2], [3, 1, 2], [2, 3, 4], [4, 3, 1, 2], [4, 0, 2], [1, 3, 2], [3, 2], [4, 3, 2], [3, 1, 2], [4, 0, 1, 2], [3, 1, 2], [3, 2], [4, 3, 2, 0], [3, 1, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2], [1, 3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3, 0], [3, 4, 2], [3, 2], [3, 2], [3, 2], [2, 4, 3], [4, 3, 2], [2, 4, 3], [3, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 1, 2], [2, 3], [1, 3, 2], [0, 3, 2], [3, 1, 2], [3, 2], [2, 3, 4], [3, 1, 2], [4, 0, 2], [3, 2], [3, 1, 2], [3, 4, 2], [3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [4, 0, 3, 2], [1, 3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [3, 4, 2], [0, 4, 3, 2], [3, 1, 2], [2, 3], [0, 2, 3, 4], [3, 1, 2], [4, 3, 2], [3, 2], [3, 2], [4, 3, 0, 2], [1, 3, 2], [4, 0, 2], [3, 2], [3, 2], [4, 3, 2, 0], [1, 3, 2], [0, 4, 1, 2], [3, 2], [3, 2], [4, 0, 3, 2], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 1, 2], [3, 2], [4, 3, 1, 2], [1, 3, 2], [3, 1, 2], [3, 2], [4, 3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2], [2, 3], [3, 2], [0, 4, 2], [3, 2], [3, 2], [4, 0, 3, 2], [3, 1, 2], [0, 4, 2], [3, 2], [3, 1, 2], [0, 4, 3, 2], [0, 3, 4, 2], [4, 0, 1, 2], [3, 2], [3, 2], [2, 3, 0], [3, 1, 2], [4, 0, 2, 3], [3, 2], [3, 2], [2, 3], [1, 3, 2], [2, 3], [3, 2], [3, 2], [4, 0, 2, 3], [3, 2], [4, 0, 2], [3, 2], [3, 2], [3, 2], [3, 4, 2], [4, 0, 2], [2, 3], [3, 2], [4, 0, 2, 3], [3, 4, 2], [1, 4, 2], [3, 2], [3, 2], [3, 2], [3, 2], [4, 3, 2], [3, 2], [3, 1, 2], [0, 3, 2], [3, 4, 2], [0, 4, 2], [3, 2], [3, 2], [2, 3, 4], [4, 3, 2], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2], [4, 3, 2], [4, 0, 2], [3, 2]]\n",
      "POSITION :  [[0, 1, 4], [0, 1, 4], [0, 4], [3], [0, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 4], [0, 1], [1], [0, 1], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 1], [0, 1, 4], [1], [0, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [1], [0, 4], [0, 1, 4], [0, 4], [0, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1], [0], [1, 3], [0, 4], [0, 1, 4], [0, 1], [0, 4], [3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 4], [0, 1], [0, 1, 4], [0, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1], [0, 1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 4], [1, 4], [0, 4], [0, 1, 4], [0, 1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [0, 1], [0, 1, 4], [1, 3], [0, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1], [0, 4], [0, 1, 4], [1], [0, 4], [0, 1], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [1, 3], [0, 4], [0, 4], [0, 1, 4], [0], [0, 4], [0, 4], [0, 1, 4], [0, 1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 4], [1, 3], [0, 1, 4], [0, 4], [1], [1], [3], [0, 1, 4], [0, 1, 4], [1, 4], [0, 4], [1], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [1], [0, 1, 4], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [1], [0, 1], [0, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1, 4], [0, 4], [1, 4], [0, 1], [1, 3], [0, 1, 4], [0, 1, 4], [0, 1], [0, 1], [1, 3], [0, 4], [0, 1, 4], [0, 1, 4], [0, 1], [1, 3], [0, 1, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.724\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "[0, 1, 4]\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.0\n",
      "Epoch: 1  Loss: 0.0\n",
      "Epoch: 2  Loss: 0.0\n",
      "Epoch: 3  Loss: 0.0\n",
      "Epoch: 4  Loss: 0.0\n",
      "Epoch: 5  Loss: 0.0\n",
      "Epoch: 6  Loss: 0.0\n",
      "Epoch: 7  Loss: 0.0\n",
      "Epoch: 8  Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 2/150 [01:31<1:51:07, 45.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 0.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_15028\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 1.5999255895614624  Accuracy on Support set:48.0\n",
      "Train_Epoch: 1  Train_Loss: 1.5984222030639648  Accuracy on Support set:48.0\n",
      "Train_Epoch: 2  Train_Loss: 1.5970264768600464  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 3  Train_Loss: 1.5956387948989867  Accuracy on Support set:60.0\n",
      "Train_Epoch: 4  Train_Loss: 1.594246234893799  Accuracy on Support set:68.0\n",
      "Train_Epoch: 5  Train_Loss: 1.592845687866211  Accuracy on Support set:68.0\n",
      "Train_Epoch: 6  Train_Loss: 1.591428656578064  Accuracy on Support set:72.0\n",
      "Train_Epoch: 7  Train_Loss: 1.5899770116806031  Accuracy on Support set:72.0\n",
      "Train_Epoch: 8  Train_Loss: 1.5885137367248534  Accuracy on Support set:72.0\n",
      "Train_Epoch: 9  Train_Loss: 1.5870020389556885  Accuracy on Support set:72.0\n",
      "Train_Epoch: 10  Train_Loss: 1.5854579734802245  Accuracy on Support set:72.0\n",
      "Train_Epoch: 11  Train_Loss: 1.583930149078369  Accuracy on Support set:72.0\n",
      "Train_Epoch: 12  Train_Loss: 1.582315583229065  Accuracy on Support set:72.0\n",
      "Train_Epoch: 13  Train_Loss: 1.5807008600234986  Accuracy on Support set:72.0\n",
      "Train_Epoch: 14  Train_Loss: 1.5789944076538085  Accuracy on Support set:72.0\n",
      "Train_Epoch: 15  Train_Loss: 1.5772575855255127  Accuracy on Support set:72.0\n",
      "Train_Epoch: 16  Train_Loss: 1.5754385089874268  Accuracy on Support set:72.0\n",
      "Train_Epoch: 17  Train_Loss: 1.573577971458435  Accuracy on Support set:76.0\n",
      "Train_Epoch: 18  Train_Loss: 1.5716188955307007  Accuracy on Support set:76.0\n",
      "Train_Epoch: 19  Train_Loss: 1.5695799684524536  Accuracy on Support set:76.0\n",
      "Train_Epoch: 20  Train_Loss: 1.5675049209594727  Accuracy on Support set:76.0\n",
      "Train_Epoch: 21  Train_Loss: 1.565310516357422  Accuracy on Support set:76.0\n",
      "Train_Epoch: 22  Train_Loss: 1.5630370044708253  Accuracy on Support set:76.0\n",
      "Train_Epoch: 23  Train_Loss: 1.5606545734405517  Accuracy on Support set:76.0\n",
      "Train_Epoch: 24  Train_Loss: 1.5582089471817016  Accuracy on Support set:76.0\n",
      "Train_Epoch: 25  Train_Loss: 1.5555850601196288  Accuracy on Support set:76.0\n",
      "Train_Epoch: 26  Train_Loss: 1.5528773164749146  Accuracy on Support set:76.0\n",
      "Train_Epoch: 27  Train_Loss: 1.5500608444213868  Accuracy on Support set:80.0\n",
      "Train_Epoch: 28  Train_Loss: 1.5470351552963257  Accuracy on Support set:80.0\n",
      "Train_Epoch: 29  Train_Loss: 1.5439382982254028  Accuracy on Support set:80.0\n",
      "Train_Epoch: 30  Train_Loss: 1.540636224746704  Accuracy on Support set:80.0\n",
      "Train_Epoch: 31  Train_Loss: 1.5371759510040284  Accuracy on Support set:80.0\n",
      "Train_Epoch: 32  Train_Loss: 1.533525857925415  Accuracy on Support set:80.0\n",
      "Train_Epoch: 33  Train_Loss: 1.5297324323654176  Accuracy on Support set:80.0\n",
      "Train_Epoch: 34  Train_Loss: 1.5256614589691162  Accuracy on Support set:88.0\n",
      "Train_Epoch: 35  Train_Loss: 1.5214238214492797  Accuracy on Support set:88.0\n",
      "Train_Epoch: 36  Train_Loss: 1.5169779348373413  Accuracy on Support set:92.0\n",
      "Train_Epoch: 37  Train_Loss: 1.512250337600708  Accuracy on Support set:92.0\n",
      "Train_Epoch: 38  Train_Loss: 1.5072649717330933  Accuracy on Support set:92.0\n",
      "Train_Epoch: 39  Train_Loss: 1.5020205783843994  Accuracy on Support set:92.0\n",
      "Train_Epoch: 40  Train_Loss: 1.4964430665969848  Accuracy on Support set:92.0\n",
      "Train_Epoch: 41  Train_Loss: 1.4905968475341798  Accuracy on Support set:96.0\n",
      "Train_Epoch: 42  Train_Loss: 1.4843995666503906  Accuracy on Support set:96.0\n",
      "Train_Epoch: 43  Train_Loss: 1.4778237771987914  Accuracy on Support set:96.0\n",
      "Train_Epoch: 44  Train_Loss: 1.4708921098709107  Accuracy on Support set:96.0\n",
      "Train_Epoch: 45  Train_Loss: 1.4635550689697265  Accuracy on Support set:96.0\n",
      "Train_Epoch: 46  Train_Loss: 1.4558194255828858  Accuracy on Support set:96.0\n",
      "Train_Epoch: 47  Train_Loss: 1.4475963592529297  Accuracy on Support set:96.0\n",
      "Train_Epoch: 48  Train_Loss: 1.4389931774139404  Accuracy on Support set:96.0\n",
      "Train_Epoch: 49  Train_Loss: 1.4298043060302734  Accuracy on Support set:92.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  61.33333333333333\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.18351657688617706 tensor([0.2089, 0.2043, 0.2155, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18340544402599335 tensor([0.2089, 0.2044, 0.2153, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18300220370292664 tensor([0.2085, 0.2051, 0.2157, 0.1877, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18352894484996796 tensor([0.2083, 0.2047, 0.2156, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18350853025913239 tensor([0.2084, 0.2044, 0.2159, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18393675982952118 tensor([0.2084, 0.2042, 0.2153, 0.1882, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18336620926856995 tensor([0.2086, 0.2048, 0.2155, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18370495736598969 tensor([0.2084, 0.2045, 0.2159, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18314823508262634 tensor([0.2083, 0.2049, 0.2155, 0.1881, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18345767259597778 tensor([0.2082, 0.2047, 0.2160, 0.1877, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18334637582302094 tensor([0.2085, 0.2044, 0.2158, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18297381699085236 tensor([0.2084, 0.2050, 0.2157, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833650767803192 tensor([0.2085, 0.2049, 0.2154, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18299634754657745 tensor([0.2086, 0.2049, 0.2159, 0.1876, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834888607263565 tensor([0.2085, 0.2040, 0.2159, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1838933676481247 tensor([0.2086, 0.2046, 0.2153, 0.1876, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18335101008415222 tensor([0.2088, 0.2046, 0.2155, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18347348272800446 tensor([0.2086, 0.2045, 0.2158, 0.1876, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18339300155639648 tensor([0.2085, 0.2049, 0.2158, 0.1874, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183427095413208 tensor([0.2084, 0.2044, 0.2156, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18323098123073578 tensor([0.2083, 0.2047, 0.2154, 0.1883, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18365153670310974 tensor([0.2091, 0.2042, 0.2150, 0.1880, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18335826694965363 tensor([0.2085, 0.2043, 0.2157, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18324200809001923 tensor([0.2089, 0.2041, 0.2158, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18378522992134094 tensor([0.2087, 0.2042, 0.2158, 0.1876, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1842634379863739 tensor([0.2087, 0.2037, 0.2157, 0.1876, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18324898183345795 tensor([0.2087, 0.2041, 0.2155, 0.1885, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18315619230270386 tensor([0.2089, 0.2042, 0.2158, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1835244745016098 tensor([0.2083, 0.2044, 0.2160, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18358759582042694 tensor([0.2086, 0.2041, 0.2160, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18296188116073608 tensor([0.2085, 0.2044, 0.2162, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18324583768844604 tensor([0.2089, 0.2044, 0.2154, 0.1881, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18330056965351105 tensor([0.2087, 0.2045, 0.2162, 0.1874, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833259016275406 tensor([0.2087, 0.2046, 0.2158, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18358668684959412 tensor([0.2084, 0.2042, 0.2156, 0.1882, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183489128947258 tensor([0.2084, 0.2044, 0.2155, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18325106799602509 tensor([0.2087, 0.2042, 0.2158, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18364474177360535 tensor([0.2083, 0.2040, 0.2163, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1830342710018158 tensor([0.2088, 0.2047, 0.2155, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18346840143203735 tensor([0.2085, 0.2045, 0.2159, 0.1876, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1836032271385193 tensor([0.2088, 0.2043, 0.2155, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18347246944904327 tensor([0.2086, 0.2041, 0.2158, 0.1880, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18370278179645538 tensor([0.2085, 0.2044, 0.2159, 0.1876, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18356719613075256 tensor([0.2082, 0.2047, 0.2157, 0.1879, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18381761014461517 tensor([0.2086, 0.2040, 0.2161, 0.1875, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183180034160614 tensor([0.2091, 0.2044, 0.2155, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18329231441020966 tensor([0.2091, 0.2043, 0.2154, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18329857289791107 tensor([0.2089, 0.2048, 0.2156, 0.1875, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18339422345161438 tensor([0.2083, 0.2049, 0.2157, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18316315114498138 tensor([0.2090, 0.2041, 0.2159, 0.1878, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18371914327144623 tensor([0.2091, 0.2042, 0.2154, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1836429238319397 tensor([0.2084, 0.2047, 0.2154, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833883374929428 tensor([0.2091, 0.2043, 0.2151, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833287477493286 tensor([0.2088, 0.2051, 0.2152, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18353985249996185 tensor([0.2084, 0.2043, 0.2157, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18330077826976776 tensor([0.2087, 0.2042, 0.2156, 0.1882, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1837032288312912 tensor([0.2083, 0.2043, 0.2162, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183152437210083 tensor([0.2088, 0.2042, 0.2158, 0.1881, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18283887207508087 tensor([0.2086, 0.2048, 0.2159, 0.1877, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18379782140254974 tensor([0.2083, 0.2043, 0.2160, 0.1875, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18361400067806244 tensor([0.2087, 0.2042, 0.2155, 0.1879, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183360755443573 tensor([0.2088, 0.2040, 0.2159, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18362973630428314 tensor([0.2087, 0.2045, 0.2154, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1830466240644455 tensor([0.2085, 0.2042, 0.2162, 0.1880, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18388688564300537 tensor([0.2087, 0.2043, 0.2156, 0.1875, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834588348865509 tensor([0.2085, 0.2043, 0.2156, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18348895013332367 tensor([0.2088, 0.2042, 0.2157, 0.1877, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18382437527179718 tensor([0.2091, 0.2040, 0.2154, 0.1877, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18322500586509705 tensor([0.2089, 0.2044, 0.2155, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18371433019638062 tensor([0.2087, 0.2042, 0.2157, 0.1876, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1831686645746231 tensor([0.2090, 0.2040, 0.2156, 0.1882, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1837577223777771 tensor([0.2081, 0.2044, 0.2156, 0.1882, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834045797586441 tensor([0.2093, 0.2047, 0.2155, 0.1872, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18360662460327148 tensor([0.2084, 0.2048, 0.2157, 0.1874, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18329483270645142 tensor([0.2086, 0.2047, 0.2158, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18355132639408112 tensor([0.2088, 0.2044, 0.2153, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18316298723220825 tensor([0.2085, 0.2046, 0.2159, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18328212201595306 tensor([0.2088, 0.2043, 0.2156, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18321876227855682 tensor([0.2083, 0.2049, 0.2156, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18317344784736633 tensor([0.2086, 0.2044, 0.2157, 0.1882, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1829824447631836 tensor([0.2082, 0.2048, 0.2153, 0.1887, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18337760865688324 tensor([0.2088, 0.2043, 0.2156, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1837720423936844 tensor([0.2090, 0.2047, 0.2155, 0.1870, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1830437332391739 tensor([0.2086, 0.2044, 0.2156, 0.1883, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18386660516262054 tensor([0.2088, 0.2043, 0.2155, 0.1875, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18343707919120789 tensor([0.2087, 0.2042, 0.2153, 0.1883, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18287794291973114 tensor([0.2085, 0.2047, 0.2158, 0.1881, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18332110345363617 tensor([0.2083, 0.2048, 0.2158, 0.1878, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18319998681545258 tensor([0.2084, 0.2047, 0.2156, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1839839220046997 tensor([0.2084, 0.2045, 0.2154, 0.1878, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18327513337135315 tensor([0.2083, 0.2050, 0.2152, 0.1882, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18398801982402802 tensor([0.2085, 0.2041, 0.2158, 0.1876, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18313677608966827 tensor([0.2091, 0.2046, 0.2157, 0.1875, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18360190093517303 tensor([0.2081, 0.2045, 0.2161, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18360954523086548 tensor([0.2082, 0.2043, 0.2160, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18375997245311737 tensor([0.2081, 0.2045, 0.2158, 0.1878, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834252029657364 tensor([0.2084, 0.2047, 0.2157, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1836586743593216 tensor([0.2083, 0.2049, 0.2159, 0.1873, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18346311151981354 tensor([0.2080, 0.2044, 0.2160, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18391473591327667 tensor([0.2089, 0.2042, 0.2153, 0.1877, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1832735538482666 tensor([0.2082, 0.2048, 0.2161, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1831934005022049 tensor([0.2087, 0.2045, 0.2159, 0.1877, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834591180086136 tensor([0.2087, 0.2042, 0.2158, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18295696377754211 tensor([0.2084, 0.2051, 0.2157, 0.1878, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1839243471622467 tensor([0.2085, 0.2041, 0.2156, 0.1879, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18343447148799896 tensor([0.2087, 0.2044, 0.2158, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18319179117679596 tensor([0.2090, 0.2043, 0.2160, 0.1875, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18335647881031036 tensor([0.2090, 0.2039, 0.2160, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18351919949054718 tensor([0.2086, 0.2049, 0.2152, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18377931416034698 tensor([0.2084, 0.2042, 0.2160, 0.1877, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18338556587696075 tensor([0.2084, 0.2043, 0.2160, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1831633448600769 tensor([0.2088, 0.2046, 0.2158, 0.1877, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18325036764144897 tensor([0.2087, 0.2048, 0.2160, 0.1873, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183444544672966 tensor([0.2086, 0.2047, 0.2156, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834954172372818 tensor([0.2087, 0.2039, 0.2161, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18385545909404755 tensor([0.2089, 0.2043, 0.2154, 0.1875, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833583265542984 tensor([0.2088, 0.2041, 0.2157, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833670437335968 tensor([0.2085, 0.2045, 0.2158, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18305042386054993 tensor([0.2082, 0.2046, 0.2160, 0.1882, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18327851593494415 tensor([0.2087, 0.2046, 0.2154, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1837063580751419 tensor([0.2092, 0.2039, 0.2157, 0.1876, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1832164078950882 tensor([0.2087, 0.2043, 0.2159, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18331359326839447 tensor([0.2089, 0.2047, 0.2155, 0.1876, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18307264149188995 tensor([0.2086, 0.2044, 0.2156, 0.1883, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18389631807804108 tensor([0.2085, 0.2038, 0.2159, 0.1878, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18344762921333313 tensor([0.2091, 0.2036, 0.2160, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18296968936920166 tensor([0.2088, 0.2046, 0.2157, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18319444358348846 tensor([0.2086, 0.2044, 0.2161, 0.1878, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18334756791591644 tensor([0.2088, 0.2040, 0.2157, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1836674064397812 tensor([0.2085, 0.2040, 0.2159, 0.1879, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18353284895420074 tensor([0.2086, 0.2046, 0.2151, 0.1883, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18308961391448975 tensor([0.2086, 0.2048, 0.2156, 0.1879, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18359796702861786 tensor([0.2090, 0.2041, 0.2158, 0.1875, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18374723196029663 tensor([0.2084, 0.2044, 0.2158, 0.1877, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18335315585136414 tensor([0.2089, 0.2042, 0.2155, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18337148427963257 tensor([0.2086, 0.2045, 0.2155, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18305343389511108 tensor([0.2091, 0.2045, 0.2157, 0.1877, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18303550779819489 tensor([0.2087, 0.2045, 0.2155, 0.1883, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1835317611694336 tensor([0.2085, 0.2044, 0.2157, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18334273993968964 tensor([0.2083, 0.2045, 0.2160, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18387067317962646 tensor([0.2083, 0.2042, 0.2160, 0.1876, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1832921952009201 tensor([0.2081, 0.2050, 0.2157, 0.1880, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833149939775467 tensor([0.2091, 0.2042, 0.2156, 0.1877, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18324580788612366 tensor([0.2086, 0.2047, 0.2155, 0.1879, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18322186172008514 tensor([0.2087, 0.2043, 0.2157, 0.1881, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18371902406215668 tensor([0.2086, 0.2040, 0.2154, 0.1882, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18319816887378693 tensor([0.2083, 0.2045, 0.2159, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1832047551870346 tensor([0.2089, 0.2044, 0.2155, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18282970786094666 tensor([0.2087, 0.2046, 0.2161, 0.1877, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1835782825946808 tensor([0.2085, 0.2043, 0.2157, 0.1879, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18346597254276276 tensor([0.2089, 0.2040, 0.2157, 0.1880, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18298229575157166 tensor([0.2085, 0.2050, 0.2160, 0.1875, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18360887467861176 tensor([0.2089, 0.2045, 0.2158, 0.1872, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18345563113689423 tensor([0.2087, 0.2045, 0.2159, 0.1874, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18378232419490814 tensor([0.2082, 0.2042, 0.2159, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18400190770626068 tensor([0.2085, 0.2042, 0.2159, 0.1874, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18312610685825348 tensor([0.2085, 0.2047, 0.2156, 0.1880, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833217740058899 tensor([0.2089, 0.2046, 0.2158, 0.1874, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18336057662963867 tensor([0.2082, 0.2052, 0.2153, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1832839697599411 tensor([0.2087, 0.2042, 0.2158, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18349868059158325 tensor([0.2085, 0.2045, 0.2156, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1835426539182663 tensor([0.2084, 0.2046, 0.2154, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18353672325611115 tensor([0.2089, 0.2048, 0.2155, 0.1873, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18325713276863098 tensor([0.2083, 0.2048, 0.2155, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18370184302330017 tensor([0.2087, 0.2042, 0.2156, 0.1877, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18309944868087769 tensor([0.2085, 0.2038, 0.2159, 0.1887, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18321016430854797 tensor([0.2088, 0.2042, 0.2156, 0.1881, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18390533328056335 tensor([0.2083, 0.2044, 0.2157, 0.1878, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18324467539787292 tensor([0.2083, 0.2046, 0.2157, 0.1882, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183637797832489 tensor([0.2090, 0.2040, 0.2157, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18349230289459229 tensor([0.2090, 0.2041, 0.2154, 0.1881, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18344731628894806 tensor([0.2082, 0.2047, 0.2156, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18312709033489227 tensor([0.2089, 0.2047, 0.2157, 0.1875, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18339335918426514 tensor([0.2083, 0.2047, 0.2156, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18334393203258514 tensor([0.2086, 0.2042, 0.2161, 0.1878, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18363375961780548 tensor([0.2084, 0.2044, 0.2153, 0.1882, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18365676701068878 tensor([0.2084, 0.2043, 0.2157, 0.1879, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18388040363788605 tensor([0.2088, 0.2040, 0.2159, 0.1873, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18373870849609375 tensor([0.2080, 0.2045, 0.2162, 0.1875, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18342386186122894 tensor([0.2082, 0.2046, 0.2158, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18299224972724915 tensor([0.2085, 0.2044, 0.2160, 0.1882, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834527850151062 tensor([0.2089, 0.2042, 0.2156, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18342545628547668 tensor([0.2089, 0.2042, 0.2157, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18300053477287292 tensor([0.2087, 0.2047, 0.2157, 0.1879, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18390056490898132 tensor([0.2087, 0.2046, 0.2156, 0.1872, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18369324505329132 tensor([0.2088, 0.2039, 0.2159, 0.1877, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18398329615592957 tensor([0.2083, 0.2043, 0.2156, 0.1878, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18348273634910583 tensor([0.2082, 0.2048, 0.2162, 0.1873, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834675818681717 tensor([0.2088, 0.2046, 0.2155, 0.1876, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18357498943805695 tensor([0.2085, 0.2039, 0.2160, 0.1880, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18338914215564728 tensor([0.2087, 0.2043, 0.2156, 0.1880, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18309979140758514 tensor([0.2090, 0.2042, 0.2157, 0.1880, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18341393768787384 tensor([0.2084, 0.2044, 0.2159, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1836032122373581 tensor([0.2082, 0.2046, 0.2159, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18377509713172913 tensor([0.2084, 0.2040, 0.2160, 0.1878, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183014914393425 tensor([0.2085, 0.2048, 0.2157, 0.1880, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18371888995170593 tensor([0.2084, 0.2043, 0.2155, 0.1881, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18351523578166962 tensor([0.2088, 0.2041, 0.2158, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18360400199890137 tensor([0.2085, 0.2047, 0.2156, 0.1876, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1838397979736328 tensor([0.2088, 0.2040, 0.2159, 0.1875, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18351849913597107 tensor([0.2084, 0.2040, 0.2157, 0.1884, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18359418213367462 tensor([0.2084, 0.2046, 0.2157, 0.1878, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18333539366722107 tensor([0.2089, 0.2043, 0.2159, 0.1875, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183231383562088 tensor([0.2082, 0.2043, 0.2156, 0.1886, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18337175250053406 tensor([0.2088, 0.2045, 0.2152, 0.1881, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1832454949617386 tensor([0.2090, 0.2043, 0.2151, 0.1884, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18321771919727325 tensor([0.2091, 0.2041, 0.2158, 0.1878, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18331442773342133 tensor([0.2089, 0.2041, 0.2158, 0.1879, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18297262489795685 tensor([0.2085, 0.2045, 0.2162, 0.1878, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1839837282896042 tensor([0.2085, 0.2037, 0.2162, 0.1876, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18383638560771942 tensor([0.2090, 0.2043, 0.2152, 0.1878, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1829623281955719 tensor([0.2085, 0.2047, 0.2157, 0.1882, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18337149918079376 tensor([0.2090, 0.2040, 0.2161, 0.1875, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18308942019939423 tensor([0.2085, 0.2049, 0.2155, 0.1881, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18358546495437622 tensor([0.2085, 0.2044, 0.2157, 0.1879, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18353883922100067 tensor([0.2085, 0.2040, 0.2158, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18317486345767975 tensor([0.2087, 0.2045, 0.2156, 0.1880, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18345409631729126 tensor([0.2085, 0.2044, 0.2157, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18370163440704346 tensor([0.2085, 0.2045, 0.2157, 0.1876, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18324418365955353 tensor([0.2087, 0.2042, 0.2160, 0.1877, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18374788761138916 tensor([0.2090, 0.2042, 0.2157, 0.1873, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18341104686260223 tensor([0.2084, 0.2044, 0.2156, 0.1882, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18335740268230438 tensor([0.2095, 0.2042, 0.2154, 0.1876, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18354174494743347 tensor([0.2084, 0.2044, 0.2158, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183705672621727 tensor([0.2088, 0.2042, 0.2158, 0.1874, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18382589519023895 tensor([0.2086, 0.2040, 0.2157, 0.1878, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1830834150314331 tensor([0.2085, 0.2051, 0.2156, 0.1878, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18343517184257507 tensor([0.2085, 0.2046, 0.2159, 0.1877, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18338340520858765 tensor([0.2089, 0.2046, 0.2159, 0.1872, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18356946110725403 tensor([0.2088, 0.2042, 0.2157, 0.1877, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18377946317195892 tensor([0.2091, 0.2039, 0.2153, 0.1879, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18280130624771118 tensor([0.2087, 0.2045, 0.2158, 0.1881, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1835068315267563 tensor([0.2082, 0.2045, 0.2161, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1835804134607315 tensor([0.2083, 0.2047, 0.2158, 0.1876, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18339768052101135 tensor([0.2086, 0.2043, 0.2159, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18329589068889618 tensor([0.2084, 0.2045, 0.2157, 0.1881, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1827147752046585 tensor([0.2089, 0.2050, 0.2158, 0.1876, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18321502208709717 tensor([0.2089, 0.2043, 0.2159, 0.1878, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834956705570221 tensor([0.2086, 0.2044, 0.2156, 0.1879, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18349438905715942 tensor([0.2088, 0.2041, 0.2160, 0.1876, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834743469953537 tensor([0.2086, 0.2043, 0.2155, 0.1882, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1830524504184723 tensor([0.2087, 0.2052, 0.2154, 0.1877, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18277572095394135 tensor([0.2093, 0.2044, 0.2156, 0.1879, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.183522030711174 tensor([0.2086, 0.2043, 0.2156, 0.1880, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18344609439373016 tensor([0.2087, 0.2044, 0.2156, 0.1878, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18302488327026367 tensor([0.2087, 0.2047, 0.2154, 0.1882, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833784282207489 tensor([0.2089, 0.2043, 0.2155, 0.1879, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18398407101631165 tensor([0.2085, 0.2048, 0.2157, 0.1870, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1834901124238968 tensor([0.2080, 0.2047, 0.2160, 0.1878, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18363167345523834 tensor([0.2089, 0.2040, 0.2154, 0.1881, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "[[4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4]]\n",
      "[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n",
      "NL_pred of 0th iteration [[4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4], [4]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005076216220855713  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005076042652130127  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005075711727142334  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005075241088867187  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0050746450424194334  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005073934555053711  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0050731229782104495  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005072221279144287  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005071237087249756  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0050701813697814945  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005069059371948242  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005067880630493164  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005066649436950683  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005065372943878174  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005064056873321533  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0050627031326293944  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005061317920684814  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005059905052185058  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005058466911315918  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005057008266448975  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.18895122408866882 tensor([0.2100, 0.2056, 0.2166, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.189112588763237 tensor([0.2100, 0.2057, 0.2164, 0.1891, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18882396817207336 tensor([0.2096, 0.2064, 0.2168, 0.1888, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18906903266906738 tensor([0.2094, 0.2060, 0.2167, 0.1891, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18889929354190826 tensor([0.2095, 0.2057, 0.2170, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18932116031646729 tensor([0.2095, 0.2055, 0.2164, 0.1893, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889008730649948 tensor([0.2097, 0.2061, 0.2166, 0.1889, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18862731754779816 tensor([0.2095, 0.2058, 0.2171, 0.1886, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18929827213287354 tensor([0.2094, 0.2061, 0.2167, 0.1893, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18884824216365814 tensor([0.2093, 0.2059, 0.2171, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18909958004951477 tensor([0.2095, 0.2057, 0.2169, 0.1891, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18909317255020142 tensor([0.2095, 0.2062, 0.2168, 0.1891, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18892233073711395 tensor([0.2096, 0.2062, 0.2165, 0.1889, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18871065974235535 tensor([0.2097, 0.2062, 0.2170, 0.1887, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18930909037590027 tensor([0.2096, 0.2052, 0.2170, 0.1893, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887226402759552 tensor([0.2097, 0.2059, 0.2164, 0.1887, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18897122144699097 tensor([0.2099, 0.2058, 0.2166, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887783408164978 tensor([0.2097, 0.2057, 0.2169, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1885852813720703 tensor([0.2096, 0.2062, 0.2169, 0.1886, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18928413093090057 tensor([0.2095, 0.2057, 0.2167, 0.1893, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18945686519145966 tensor([0.2094, 0.2061, 0.2165, 0.1895, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1891055703163147 tensor([0.2102, 0.2055, 0.2161, 0.1891, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18918977677822113 tensor([0.2096, 0.2056, 0.2168, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18907378613948822 tensor([0.2100, 0.2054, 0.2169, 0.1891, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1886928677558899 tensor([0.2098, 0.2055, 0.2169, 0.1887, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18876855075359344 tensor([0.2098, 0.2050, 0.2168, 0.1888, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18968917429447174 tensor([0.2097, 0.2054, 0.2166, 0.1897, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890944242477417 tensor([0.2099, 0.2055, 0.2170, 0.1891, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18895718455314636 tensor([0.2093, 0.2057, 0.2171, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18888600170612335 tensor([0.2097, 0.2054, 0.2171, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890152096748352 tensor([0.2096, 0.2057, 0.2173, 0.1890, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18923430144786835 tensor([0.2099, 0.2057, 0.2165, 0.1892, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18850548565387726 tensor([0.2098, 0.2058, 0.2173, 0.1885, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18880373239517212 tensor([0.2098, 0.2059, 0.2169, 0.1888, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18932832777500153 tensor([0.2095, 0.2055, 0.2167, 0.1893, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18932323157787323 tensor([0.2095, 0.2057, 0.2166, 0.1893, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18904541432857513 tensor([0.2098, 0.2056, 0.2170, 0.1890, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18884558975696564 tensor([0.2094, 0.2053, 0.2174, 0.1888, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903066217899323 tensor([0.2099, 0.2060, 0.2166, 0.1890, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18874244391918182 tensor([0.2096, 0.2058, 0.2171, 0.1887, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18896028399467468 tensor([0.2098, 0.2056, 0.2167, 0.1890, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18917076289653778 tensor([0.2097, 0.2054, 0.2169, 0.1892, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18869930505752563 tensor([0.2096, 0.2057, 0.2170, 0.1887, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903233110904694 tensor([0.2093, 0.2060, 0.2168, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18866005539894104 tensor([0.2097, 0.2053, 0.2172, 0.1887, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18904688954353333 tensor([0.2102, 0.2057, 0.2166, 0.1890, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18906572461128235 tensor([0.2101, 0.2056, 0.2165, 0.1891, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18861345946788788 tensor([0.2100, 0.2060, 0.2167, 0.1886, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18884260952472687 tensor([0.2094, 0.2062, 0.2169, 0.1888, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18892037868499756 tensor([0.2101, 0.2054, 0.2170, 0.1889, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18866439163684845 tensor([0.2102, 0.2055, 0.2165, 0.1887, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18897204101085663 tensor([0.2095, 0.2060, 0.2166, 0.1890, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18918061256408691 tensor([0.2102, 0.2056, 0.2162, 0.1892, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887945681810379 tensor([0.2098, 0.2063, 0.2163, 0.1888, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18919788300991058 tensor([0.2095, 0.2056, 0.2169, 0.1892, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892728954553604 tensor([0.2098, 0.2055, 0.2167, 0.1893, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18858857452869415 tensor([0.2094, 0.2056, 0.2174, 0.1886, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18922211229801178 tensor([0.2098, 0.2055, 0.2169, 0.1892, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18888534605503082 tensor([0.2097, 0.2061, 0.2171, 0.1889, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18867971003055573 tensor([0.2094, 0.2057, 0.2171, 0.1887, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18902158737182617 tensor([0.2098, 0.2056, 0.2167, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18901427090168 tensor([0.2099, 0.2053, 0.2171, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887967586517334 tensor([0.2098, 0.2058, 0.2166, 0.1888, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18911993503570557 tensor([0.2096, 0.2055, 0.2173, 0.1891, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18869397044181824 tensor([0.2098, 0.2056, 0.2167, 0.1887, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18931922316551208 tensor([0.2095, 0.2056, 0.2167, 0.1893, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18884290754795074 tensor([0.2099, 0.2055, 0.2169, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888001710176468 tensor([0.2101, 0.2053, 0.2165, 0.1888, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18911296129226685 tensor([0.2100, 0.2057, 0.2166, 0.1891, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18878185749053955 tensor([0.2098, 0.2055, 0.2168, 0.1888, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18937145173549652 tensor([0.2101, 0.2053, 0.2167, 0.1894, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18928179144859314 tensor([0.2092, 0.2057, 0.2167, 0.1893, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1883108764886856 tensor([0.2104, 0.2060, 0.2166, 0.1883, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18857663869857788 tensor([0.2096, 0.2060, 0.2168, 0.1886, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888321340084076 tensor([0.2097, 0.2060, 0.2169, 0.1888, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18910670280456543 tensor([0.2099, 0.2057, 0.2164, 0.1891, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18899238109588623 tensor([0.2095, 0.2059, 0.2170, 0.1890, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18918895721435547 tensor([0.2098, 0.2055, 0.2168, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18906138837337494 tensor([0.2093, 0.2062, 0.2167, 0.1891, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18934853374958038 tensor([0.2097, 0.2056, 0.2168, 0.1893, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18983259797096252 tensor([0.2093, 0.2061, 0.2164, 0.1898, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903669714927673 tensor([0.2099, 0.2056, 0.2167, 0.1890, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1881590038537979 tensor([0.2101, 0.2060, 0.2166, 0.1882, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1894828826189041 tensor([0.2097, 0.2057, 0.2167, 0.1895, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18865405023097992 tensor([0.2099, 0.2057, 0.2166, 0.1887, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1894526183605194 tensor([0.2098, 0.2055, 0.2164, 0.1895, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892055869102478 tensor([0.2096, 0.2060, 0.2169, 0.1892, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889488697052002 tensor([0.2094, 0.2061, 0.2169, 0.1889, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18918199837207794 tensor([0.2095, 0.2060, 0.2167, 0.1892, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18890459835529327 tensor([0.2095, 0.2057, 0.2166, 0.1889, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18934950232505798 tensor([0.2093, 0.2063, 0.2164, 0.1893, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18872176110744476 tensor([0.2095, 0.2054, 0.2170, 0.1887, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18857572972774506 tensor([0.2102, 0.2059, 0.2168, 0.1886, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18893033266067505 tensor([0.2092, 0.2058, 0.2172, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18900124728679657 tensor([0.2092, 0.2056, 0.2172, 0.1890, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18894155323505402 tensor([0.2092, 0.2058, 0.2169, 0.1889, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18885870277881622 tensor([0.2095, 0.2060, 0.2169, 0.1889, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1884627342224121 tensor([0.2094, 0.2061, 0.2170, 0.1885, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18919898569583893 tensor([0.2091, 0.2057, 0.2172, 0.1892, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888345181941986 tensor([0.2100, 0.2055, 0.2164, 0.1888, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18880130350589752 tensor([0.2093, 0.2061, 0.2172, 0.1888, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18887175619602203 tensor([0.2098, 0.2058, 0.2170, 0.1889, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.189043790102005 tensor([0.2098, 0.2055, 0.2169, 0.1890, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18896014988422394 tensor([0.2095, 0.2064, 0.2168, 0.1890, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18905703723430634 tensor([0.2096, 0.2053, 0.2167, 0.1891, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18885870277881622 tensor([0.2098, 0.2057, 0.2169, 0.1889, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18866375088691711 tensor([0.2101, 0.2056, 0.2171, 0.1887, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888473927974701 tensor([0.2101, 0.2052, 0.2171, 0.1888, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18893730640411377 tensor([0.2097, 0.2061, 0.2163, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888333410024643 tensor([0.2094, 0.2055, 0.2171, 0.1888, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18901799619197845 tensor([0.2095, 0.2056, 0.2171, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18880264461040497 tensor([0.2099, 0.2059, 0.2169, 0.1888, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18838587403297424 tensor([0.2098, 0.2061, 0.2172, 0.1884, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18880479037761688 tensor([0.2097, 0.2060, 0.2167, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18904055655002594 tensor([0.2098, 0.2051, 0.2172, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.188591867685318 tensor([0.2100, 0.2056, 0.2165, 0.1886, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18912740051746368 tensor([0.2099, 0.2054, 0.2168, 0.1891, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18897245824337006 tensor([0.2095, 0.2058, 0.2169, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892939805984497 tensor([0.2093, 0.2059, 0.2171, 0.1893, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18915244936943054 tensor([0.2098, 0.2059, 0.2165, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18870101869106293 tensor([0.2103, 0.2052, 0.2168, 0.1887, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18902373313903809 tensor([0.2098, 0.2056, 0.2170, 0.1890, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18873713910579681 tensor([0.2100, 0.2060, 0.2166, 0.1887, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18944936990737915 tensor([0.2097, 0.2057, 0.2167, 0.1894, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889963150024414 tensor([0.2096, 0.2051, 0.2171, 0.1890, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18909728527069092 tensor([0.2101, 0.2049, 0.2171, 0.1891, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890614926815033 tensor([0.2099, 0.2059, 0.2169, 0.1891, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18890899419784546 tensor([0.2096, 0.2057, 0.2172, 0.1889, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18919573724269867 tensor([0.2099, 0.2053, 0.2168, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18899837136268616 tensor([0.2096, 0.2053, 0.2170, 0.1890, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18939632177352905 tensor([0.2096, 0.2059, 0.2162, 0.1894, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890677958726883 tensor([0.2097, 0.2061, 0.2167, 0.1891, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1886107176542282 tensor([0.2101, 0.2054, 0.2169, 0.1886, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18882611393928528 tensor([0.2095, 0.2057, 0.2169, 0.1888, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18912169337272644 tensor([0.2100, 0.2055, 0.2167, 0.1891, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18916119635105133 tensor([0.2097, 0.2058, 0.2167, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888497769832611 tensor([0.2102, 0.2057, 0.2168, 0.1888, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1894593983888626 tensor([0.2098, 0.2057, 0.2166, 0.1895, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889263540506363 tensor([0.2096, 0.2057, 0.2169, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18899087607860565 tensor([0.2094, 0.2058, 0.2171, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18871112167835236 tensor([0.2094, 0.2055, 0.2171, 0.1887, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18910685181617737 tensor([0.2092, 0.2063, 0.2168, 0.1891, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.188852921128273 tensor([0.2102, 0.2055, 0.2168, 0.1889, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18900494277477264 tensor([0.2097, 0.2060, 0.2166, 0.1890, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18924973905086517 tensor([0.2098, 0.2056, 0.2168, 0.1892, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18937258422374725 tensor([0.2097, 0.2053, 0.2166, 0.1894, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18914785981178284 tensor([0.2094, 0.2058, 0.2170, 0.1891, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18908679485321045 tensor([0.2100, 0.2057, 0.2167, 0.1891, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1888027936220169 tensor([0.2098, 0.2059, 0.2172, 0.1888, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890634000301361 tensor([0.2096, 0.2056, 0.2168, 0.1891, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890980452299118 tensor([0.2100, 0.2053, 0.2168, 0.1891, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18867303431034088 tensor([0.2095, 0.2063, 0.2172, 0.1887, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18831856548786163 tensor([0.2100, 0.2058, 0.2169, 0.1883, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1885775923728943 tensor([0.2098, 0.2058, 0.2171, 0.1886, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903273344039917 tensor([0.2093, 0.2055, 0.2171, 0.1890, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1885373741388321 tensor([0.2096, 0.2055, 0.2170, 0.1885, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18916364014148712 tensor([0.2096, 0.2061, 0.2167, 0.1892, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18849705159664154 tensor([0.2100, 0.2058, 0.2170, 0.1885, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18907730281352997 tensor([0.2093, 0.2065, 0.2164, 0.1891, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18922032415866852 tensor([0.2097, 0.2055, 0.2169, 0.1892, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18898724019527435 tensor([0.2096, 0.2058, 0.2168, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18920163810253143 tensor([0.2094, 0.2059, 0.2165, 0.1892, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18837912380695343 tensor([0.2100, 0.2061, 0.2166, 0.1884, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18926741182804108 tensor([0.2094, 0.2061, 0.2166, 0.1893, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18889418244361877 tensor([0.2098, 0.2055, 0.2167, 0.1889, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18985149264335632 tensor([0.2096, 0.2051, 0.2170, 0.1899, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892990916967392 tensor([0.2099, 0.2055, 0.2167, 0.1893, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18887321650981903 tensor([0.2094, 0.2057, 0.2168, 0.1889, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1893191784620285 tensor([0.2094, 0.2059, 0.2168, 0.1893, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18886540830135345 tensor([0.2101, 0.2053, 0.2168, 0.1889, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18920040130615234 tensor([0.2101, 0.2054, 0.2165, 0.1892, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18915285170078278 tensor([0.2093, 0.2060, 0.2168, 0.1892, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1886121928691864 tensor([0.2100, 0.2060, 0.2169, 0.1886, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18919558823108673 tensor([0.2094, 0.2060, 0.2167, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18889303505420685 tensor([0.2097, 0.2055, 0.2172, 0.1889, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18931002914905548 tensor([0.2095, 0.2058, 0.2164, 0.1893, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18907251954078674 tensor([0.2095, 0.2056, 0.2168, 0.1891, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18843843042850494 tensor([0.2099, 0.2053, 0.2171, 0.1884, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1886822134256363 tensor([0.2091, 0.2058, 0.2173, 0.1887, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1891549527645111 tensor([0.2093, 0.2058, 0.2169, 0.1892, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18934254348278046 tensor([0.2096, 0.2057, 0.2171, 0.1893, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18889418244361877 tensor([0.2100, 0.2055, 0.2167, 0.1889, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18882843852043152 tensor([0.2100, 0.2055, 0.2168, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18908245861530304 tensor([0.2098, 0.2060, 0.2168, 0.1891, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18829645216464996 tensor([0.2098, 0.2059, 0.2168, 0.1883, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18884359300136566 tensor([0.2099, 0.2052, 0.2170, 0.1888, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18891489505767822 tensor([0.2094, 0.2056, 0.2167, 0.1889, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18840566277503967 tensor([0.2093, 0.2061, 0.2173, 0.1884, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18872085213661194 tensor([0.2100, 0.2059, 0.2166, 0.1887, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1891418695449829 tensor([0.2096, 0.2052, 0.2171, 0.1891, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18913598358631134 tensor([0.2098, 0.2056, 0.2167, 0.1891, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1891086846590042 tensor([0.2101, 0.2055, 0.2168, 0.1891, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18898555636405945 tensor([0.2095, 0.2057, 0.2170, 0.1890, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889258325099945 tensor([0.2093, 0.2059, 0.2170, 0.1889, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18893742561340332 tensor([0.2095, 0.2053, 0.2171, 0.1889, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.189138725399971 tensor([0.2096, 0.2061, 0.2168, 0.1891, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18922939896583557 tensor([0.2095, 0.2056, 0.2166, 0.1892, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889360249042511 tensor([0.2099, 0.2054, 0.2169, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18870042264461517 tensor([0.2096, 0.2060, 0.2168, 0.1887, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18865734338760376 tensor([0.2099, 0.2052, 0.2170, 0.1887, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18953466415405273 tensor([0.2095, 0.2053, 0.2168, 0.1895, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889495849609375 tensor([0.2094, 0.2059, 0.2168, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1886555552482605 tensor([0.2100, 0.2056, 0.2170, 0.1887, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18978440761566162 tensor([0.2093, 0.2056, 0.2167, 0.1898, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892249435186386 tensor([0.2099, 0.2058, 0.2164, 0.1892, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18957111239433289 tensor([0.2100, 0.2056, 0.2162, 0.1896, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18895074725151062 tensor([0.2101, 0.2054, 0.2169, 0.1890, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890440583229065 tensor([0.2100, 0.2054, 0.2169, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889430433511734 tensor([0.2096, 0.2058, 0.2173, 0.1889, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887311190366745 tensor([0.2096, 0.2050, 0.2173, 0.1887, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.188903346657753 tensor([0.2101, 0.2056, 0.2163, 0.1889, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18931284546852112 tensor([0.2096, 0.2060, 0.2168, 0.1893, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18863728642463684 tensor([0.2101, 0.2053, 0.2172, 0.1886, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892169713973999 tensor([0.2096, 0.2062, 0.2166, 0.1892, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18898911774158478 tensor([0.2096, 0.2057, 0.2168, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1893698275089264 tensor([0.2096, 0.2053, 0.2169, 0.1894, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890992522239685 tensor([0.2098, 0.2058, 0.2167, 0.1891, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18904371559619904 tensor([0.2096, 0.2058, 0.2168, 0.1890, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18875999748706818 tensor([0.2096, 0.2058, 0.2168, 0.1888, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18887946009635925 tensor([0.2098, 0.2055, 0.2171, 0.1889, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18844254314899445 tensor([0.2101, 0.2055, 0.2168, 0.1884, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18937809765338898 tensor([0.2095, 0.2057, 0.2167, 0.1894, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18870943784713745 tensor([0.2106, 0.2055, 0.2165, 0.1887, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18905147910118103 tensor([0.2095, 0.2057, 0.2169, 0.1891, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18858423829078674 tensor([0.2099, 0.2055, 0.2169, 0.1886, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18887202441692352 tensor([0.2097, 0.2053, 0.2169, 0.1889, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18889282643795013 tensor([0.2096, 0.2064, 0.2168, 0.1889, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887989491224289 tensor([0.2096, 0.2059, 0.2170, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18835878372192383 tensor([0.2100, 0.2059, 0.2170, 0.1884, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18888640403747559 tensor([0.2099, 0.2055, 0.2168, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903759121894836 tensor([0.2102, 0.2052, 0.2164, 0.1890, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892450600862503 tensor([0.2098, 0.2058, 0.2169, 0.1892, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18889233469963074 tensor([0.2093, 0.2058, 0.2172, 0.1889, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887529343366623 tensor([0.2094, 0.2060, 0.2169, 0.1888, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18899038434028625 tensor([0.2097, 0.2055, 0.2170, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18922393023967743 tensor([0.2095, 0.2058, 0.2168, 0.1892, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887180060148239 tensor([0.2100, 0.2063, 0.2169, 0.1887, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18890953063964844 tensor([0.2099, 0.2056, 0.2170, 0.1889, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903836607933044 tensor([0.2097, 0.2057, 0.2168, 0.1890, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1887938529253006 tensor([0.2099, 0.2054, 0.2171, 0.1888, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18929234147071838 tensor([0.2096, 0.2056, 0.2167, 0.1893, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18883897364139557 tensor([0.2098, 0.2064, 0.2165, 0.1888, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889745593070984 tensor([0.2104, 0.2058, 0.2167, 0.1890, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18915310502052307 tensor([0.2097, 0.2056, 0.2167, 0.1892, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18900062143802643 tensor([0.2097, 0.2057, 0.2167, 0.1890, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1893138587474823 tensor([0.2098, 0.2060, 0.2165, 0.1893, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18899710476398468 tensor([0.2100, 0.2057, 0.2166, 0.1890, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18810416758060455 tensor([0.2096, 0.2061, 0.2169, 0.1881, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1889728456735611 tensor([0.2091, 0.2060, 0.2171, 0.1890, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18927370011806488 tensor([0.2099, 0.2052, 0.2165, 0.1893, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 1th iteration [[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005101174831390381  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005100985527038574  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005100624561309814  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005100111484527588  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005099459171295166  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005098685264587403  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005097798824310303  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0050968146324157715  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005095742225646973  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005094589710235596  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005093368053436279  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005092082023620606  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005090741634368896  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005089352130889893  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005087918758392334  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005086446762084961  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005084940910339356  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005083404064178467  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.00508184289932251  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005080257892608643  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20673738420009613 tensor([0.2114, 0.2067, 0.2180, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20684148371219635 tensor([0.2114, 0.2068, 0.2178, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20755724608898163 tensor([0.2110, 0.2076, 0.2182, 0.1838, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20712146162986755 tensor([0.2107, 0.2071, 0.2181, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20688515901565552 tensor([0.2109, 0.2069, 0.2185, 0.1838, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20663267374038696 tensor([0.2109, 0.2066, 0.2178, 0.1842, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2072548121213913 tensor([0.2110, 0.2073, 0.2180, 0.1839, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20696790516376495 tensor([0.2108, 0.2070, 0.2185, 0.1836, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20730023086071014 tensor([0.2108, 0.2073, 0.2181, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20710057020187378 tensor([0.2107, 0.2071, 0.2185, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20692113041877747 tensor([0.2109, 0.2069, 0.2183, 0.1840, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2074039876461029 tensor([0.2108, 0.2074, 0.2182, 0.1841, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20737719535827637 tensor([0.2109, 0.2074, 0.2180, 0.1839, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.207376167178154 tensor([0.2111, 0.2074, 0.2184, 0.1837, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20638692378997803 tensor([0.2110, 0.2064, 0.2184, 0.1842, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070339024066925 tensor([0.2111, 0.2070, 0.2178, 0.1837, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20698237419128418 tensor([0.2113, 0.2070, 0.2180, 0.1839, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20689550042152405 tensor([0.2111, 0.2069, 0.2183, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20733076333999634 tensor([0.2110, 0.2073, 0.2183, 0.1835, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20684073865413666 tensor([0.2109, 0.2068, 0.2181, 0.1842, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20722268521785736 tensor([0.2108, 0.2072, 0.2179, 0.1844, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669949054718018 tensor([0.2116, 0.2067, 0.2175, 0.1841, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20680367946624756 tensor([0.2110, 0.2068, 0.2182, 0.1841, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2065902203321457 tensor([0.2114, 0.2066, 0.2183, 0.1840, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20661787688732147 tensor([0.2111, 0.2066, 0.2183, 0.1837, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20611588656902313 tensor([0.2112, 0.2061, 0.2182, 0.1838, 0.1807], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20653478801250458 tensor([0.2111, 0.2065, 0.2180, 0.1846, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20665377378463745 tensor([0.2113, 0.2067, 0.2184, 0.1840, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206865131855011 tensor([0.2107, 0.2069, 0.2185, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20653875172138214 tensor([0.2110, 0.2065, 0.2185, 0.1839, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068473845720291 tensor([0.2110, 0.2068, 0.2187, 0.1840, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20687702298164368 tensor([0.2113, 0.2069, 0.2179, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069343775510788 tensor([0.2111, 0.2069, 0.2187, 0.1835, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20700126886367798 tensor([0.2112, 0.2070, 0.2183, 0.1838, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20665571093559265 tensor([0.2109, 0.2067, 0.2181, 0.1843, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20688652992248535 tensor([0.2109, 0.2069, 0.2180, 0.1843, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20671716332435608 tensor([0.2112, 0.2067, 0.2184, 0.1840, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20640972256660461 tensor([0.2108, 0.2064, 0.2188, 0.1838, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20720504224300385 tensor([0.2113, 0.2072, 0.2180, 0.1840, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20693561434745789 tensor([0.2110, 0.2069, 0.2185, 0.1837, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20673705637454987 tensor([0.2112, 0.2067, 0.2181, 0.1839, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20659904181957245 tensor([0.2110, 0.2066, 0.2183, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681193470954895 tensor([0.2110, 0.2068, 0.2184, 0.1837, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20715270936489105 tensor([0.2106, 0.2072, 0.2182, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064339518547058 tensor([0.2111, 0.2064, 0.2186, 0.1837, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20683974027633667 tensor([0.2115, 0.2068, 0.2180, 0.1840, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20679378509521484 tensor([0.2115, 0.2068, 0.2179, 0.1840, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20718072354793549 tensor([0.2114, 0.2072, 0.2181, 0.1836, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20732787251472473 tensor([0.2108, 0.2073, 0.2183, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2065563201904297 tensor([0.2115, 0.2066, 0.2185, 0.1839, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20668314397335052 tensor([0.2115, 0.2067, 0.2179, 0.1837, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20716452598571777 tensor([0.2108, 0.2072, 0.2180, 0.1839, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067326009273529 tensor([0.2116, 0.2067, 0.2176, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20749303698539734 tensor([0.2112, 0.2075, 0.2177, 0.1838, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067147195339203 tensor([0.2109, 0.2067, 0.2183, 0.1842, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20668314397335052 tensor([0.2111, 0.2067, 0.2181, 0.1843, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206752210855484 tensor([0.2108, 0.2068, 0.2188, 0.1835, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066711187362671 tensor([0.2112, 0.2067, 0.2183, 0.1842, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20729732513427734 tensor([0.2111, 0.2073, 0.2184, 0.1838, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068070024251938 tensor([0.2108, 0.2068, 0.2185, 0.1836, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20671652257442474 tensor([0.2112, 0.2067, 0.2181, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20647959411144257 tensor([0.2113, 0.2065, 0.2184, 0.1840, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20699413120746613 tensor([0.2112, 0.2070, 0.2180, 0.1838, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669029653072357 tensor([0.2110, 0.2067, 0.2187, 0.1841, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20676229894161224 tensor([0.2112, 0.2068, 0.2181, 0.1836, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20675484836101532 tensor([0.2109, 0.2068, 0.2181, 0.1843, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20667338371276855 tensor([0.2113, 0.2067, 0.2183, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20651479065418243 tensor([0.2115, 0.2065, 0.2179, 0.1838, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068493515253067 tensor([0.2114, 0.2068, 0.2180, 0.1841, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669445395469666 tensor([0.2112, 0.2067, 0.2182, 0.1838, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20650556683540344 tensor([0.2114, 0.2065, 0.2181, 0.1843, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681411027908325 tensor([0.2106, 0.2068, 0.2181, 0.1842, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20710013806819916 tensor([0.2117, 0.2071, 0.2180, 0.1833, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071974277496338 tensor([0.2109, 0.2072, 0.2182, 0.1835, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20712171494960785 tensor([0.2110, 0.2071, 0.2183, 0.1838, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20687206089496613 tensor([0.2113, 0.2069, 0.2178, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20703533291816711 tensor([0.2109, 0.2070, 0.2184, 0.1840, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067078948020935 tensor([0.2112, 0.2067, 0.2182, 0.1841, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20741145312786102 tensor([0.2107, 0.2074, 0.2181, 0.1840, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067861109972 tensor([0.2111, 0.2068, 0.2182, 0.1843, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2073141634464264 tensor([0.2107, 0.2073, 0.2178, 0.1848, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20674577355384827 tensor([0.2113, 0.2067, 0.2181, 0.1840, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071409523487091 tensor([0.2114, 0.2071, 0.2181, 0.1832, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068663090467453 tensor([0.2111, 0.2069, 0.2181, 0.1844, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20682713389396667 tensor([0.2113, 0.2068, 0.2180, 0.1836, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20664478838443756 tensor([0.2112, 0.2066, 0.2178, 0.1844, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20715244114398956 tensor([0.2110, 0.2072, 0.2183, 0.1841, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20721733570098877 tensor([0.2108, 0.2072, 0.2183, 0.1839, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20718874037265778 tensor([0.2109, 0.2072, 0.2181, 0.1841, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20691141486167908 tensor([0.2108, 0.2069, 0.2180, 0.1839, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20745934545993805 tensor([0.2107, 0.2075, 0.2178, 0.1843, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20659933984279633 tensor([0.2109, 0.2066, 0.2184, 0.1837, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20700134336948395 tensor([0.2115, 0.2070, 0.2183, 0.1836, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20694297552108765 tensor([0.2105, 0.2069, 0.2186, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067791223526001 tensor([0.2106, 0.2068, 0.2185, 0.1840, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069544792175293 tensor([0.2106, 0.2070, 0.2183, 0.1839, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20711003243923187 tensor([0.2109, 0.2071, 0.2183, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20728035271167755 tensor([0.2107, 0.2073, 0.2184, 0.1834, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20688848197460175 tensor([0.2104, 0.2069, 0.2186, 0.1842, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20666815340518951 tensor([0.2114, 0.2067, 0.2178, 0.1838, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20727847516536713 tensor([0.2106, 0.2073, 0.2186, 0.1837, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20696434378623962 tensor([0.2112, 0.2070, 0.2184, 0.1838, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20663762092590332 tensor([0.2112, 0.2066, 0.2183, 0.1840, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075577825307846 tensor([0.2109, 0.2076, 0.2182, 0.1840, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20650240778923035 tensor([0.2110, 0.2065, 0.2181, 0.1840, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681554079055786 tensor([0.2112, 0.2068, 0.2183, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206753209233284 tensor([0.2115, 0.2068, 0.2185, 0.1836, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20638339221477509 tensor([0.2115, 0.2064, 0.2185, 0.1838, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20729988813400269 tensor([0.2111, 0.2073, 0.2178, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20664212107658386 tensor([0.2108, 0.2066, 0.2185, 0.1838, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206772580742836 tensor([0.2109, 0.2068, 0.2186, 0.1840, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20702731609344482 tensor([0.2113, 0.2070, 0.2183, 0.1838, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20719808340072632 tensor([0.2111, 0.2072, 0.2186, 0.1834, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20716682076454163 tensor([0.2110, 0.2072, 0.2181, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20630860328674316 tensor([0.2111, 0.2063, 0.2186, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20679770410060883 tensor([0.2114, 0.2068, 0.2179, 0.1836, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20658716559410095 tensor([0.2113, 0.2066, 0.2182, 0.1841, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20696330070495605 tensor([0.2109, 0.2070, 0.2183, 0.1839, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20703533291816711 tensor([0.2107, 0.2070, 0.2185, 0.1842, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070092409849167 tensor([0.2112, 0.2070, 0.2179, 0.1841, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20632512867450714 tensor([0.2116, 0.2063, 0.2182, 0.1837, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20675918459892273 tensor([0.2112, 0.2068, 0.2184, 0.1840, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20711813867092133 tensor([0.2113, 0.2071, 0.2180, 0.1837, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068544328212738 tensor([0.2110, 0.2069, 0.2181, 0.1844, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20629645884037018 tensor([0.2110, 0.2063, 0.2184, 0.1839, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2059987485408783 tensor([0.2115, 0.2060, 0.2185, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20702902972698212 tensor([0.2113, 0.2070, 0.2182, 0.1840, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20684316754341125 tensor([0.2110, 0.2068, 0.2186, 0.1839, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064775675535202 tensor([0.2113, 0.2065, 0.2183, 0.1841, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20650461316108704 tensor([0.2110, 0.2065, 0.2184, 0.1839, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20706883072853088 tensor([0.2110, 0.2071, 0.2176, 0.1843, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20725232362747192 tensor([0.2110, 0.2073, 0.2182, 0.1840, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20657393336296082 tensor([0.2115, 0.2066, 0.2183, 0.1836, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20685520768165588 tensor([0.2109, 0.2069, 0.2183, 0.1838, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20668986439704895 tensor([0.2114, 0.2067, 0.2181, 0.1841, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069483995437622 tensor([0.2110, 0.2069, 0.2181, 0.1841, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20689210295677185 tensor([0.2116, 0.2069, 0.2182, 0.1838, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20688949525356293 tensor([0.2112, 0.2069, 0.2180, 0.1844, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20689894258975983 tensor([0.2110, 0.2069, 0.2183, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20692682266235352 tensor([0.2108, 0.2069, 0.2185, 0.1839, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669806003570557 tensor([0.2107, 0.2067, 0.2185, 0.1837, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2074444591999054 tensor([0.2105, 0.2074, 0.2182, 0.1841, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20667527616024017 tensor([0.2116, 0.2067, 0.2182, 0.1838, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071712762117386 tensor([0.2111, 0.2072, 0.2180, 0.1840, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20677176117897034 tensor([0.2112, 0.2068, 0.2182, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064703404903412 tensor([0.2111, 0.2065, 0.2180, 0.1843, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069748491048813 tensor([0.2108, 0.2070, 0.2184, 0.1841, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20682427287101746 tensor([0.2114, 0.2068, 0.2181, 0.1841, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20709767937660217 tensor([0.2112, 0.2071, 0.2186, 0.1838, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20678897202014923 tensor([0.2110, 0.2068, 0.2182, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064104676246643 tensor([0.2114, 0.2064, 0.2182, 0.1840, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20740240812301636 tensor([0.2109, 0.2074, 0.2186, 0.1836, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20693373680114746 tensor([0.2113, 0.2069, 0.2183, 0.1833, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20693372189998627 tensor([0.2112, 0.2069, 0.2185, 0.1835, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20662343502044678 tensor([0.2107, 0.2066, 0.2185, 0.1840, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20670025050640106 tensor([0.2109, 0.2067, 0.2184, 0.1835, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20722834765911102 tensor([0.2110, 0.2072, 0.2181, 0.1841, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20700021088123322 tensor([0.2114, 0.2070, 0.2184, 0.1835, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20761211216449738 tensor([0.2107, 0.2076, 0.2178, 0.1840, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066967636346817 tensor([0.2111, 0.2067, 0.2183, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069678157567978 tensor([0.2110, 0.2070, 0.2182, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20709815621376038 tensor([0.2108, 0.2071, 0.2179, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20724862813949585 tensor([0.2114, 0.2072, 0.2180, 0.1834, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2072446495294571 tensor([0.2108, 0.2072, 0.2180, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20666766166687012 tensor([0.2112, 0.2067, 0.2182, 0.1838, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20629124343395233 tensor([0.2109, 0.2063, 0.2184, 0.1848, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20665544271469116 tensor([0.2113, 0.2067, 0.2181, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20682299137115479 tensor([0.2108, 0.2068, 0.2182, 0.1839, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20703686773777008 tensor([0.2108, 0.2070, 0.2182, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20642036199569702 tensor([0.2114, 0.2064, 0.2182, 0.1838, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20653118193149567 tensor([0.2114, 0.2065, 0.2179, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071445882320404 tensor([0.2107, 0.2071, 0.2182, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20714734494686127 tensor([0.2114, 0.2071, 0.2183, 0.1836, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20715491473674774 tensor([0.2108, 0.2072, 0.2181, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066977173089981 tensor([0.2110, 0.2067, 0.2186, 0.1838, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20694030821323395 tensor([0.2109, 0.2069, 0.2178, 0.1843, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068021297454834 tensor([0.2108, 0.2068, 0.2182, 0.1840, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064739465713501 tensor([0.2113, 0.2065, 0.2185, 0.1834, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20699326694011688 tensor([0.2105, 0.2070, 0.2187, 0.1837, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20699895918369293 tensor([0.2107, 0.2070, 0.2183, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068185806274414 tensor([0.2109, 0.2068, 0.2185, 0.1843, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20665577054023743 tensor([0.2114, 0.2067, 0.2182, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066735178232193 tensor([0.2114, 0.2067, 0.2182, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071322351694107 tensor([0.2112, 0.2071, 0.2182, 0.1840, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20707841217517853 tensor([0.2111, 0.2071, 0.2181, 0.1833, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20637871325016022 tensor([0.2112, 0.2064, 0.2184, 0.1838, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20672479271888733 tensor([0.2108, 0.2067, 0.2182, 0.1839, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20722682774066925 tensor([0.2107, 0.2072, 0.2187, 0.1834, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20703773200511932 tensor([0.2113, 0.2070, 0.2180, 0.1837, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20634524524211884 tensor([0.2110, 0.2063, 0.2185, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206757053732872 tensor([0.2112, 0.2068, 0.2181, 0.1841, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20667500793933868 tensor([0.2115, 0.2067, 0.2182, 0.1841, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20688383281230927 tensor([0.2109, 0.2069, 0.2184, 0.1839, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070140689611435 tensor([0.2106, 0.2070, 0.2184, 0.1839, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20645980536937714 tensor([0.2108, 0.2065, 0.2185, 0.1839, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20722445845603943 tensor([0.2110, 0.2072, 0.2182, 0.1841, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067449986934662 tensor([0.2109, 0.2067, 0.2180, 0.1841, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20658300817012787 tensor([0.2112, 0.2066, 0.2183, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071375995874405 tensor([0.2110, 0.2071, 0.2182, 0.1837, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20639890432357788 tensor([0.2113, 0.2064, 0.2184, 0.1836, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20647846162319183 tensor([0.2108, 0.2065, 0.2182, 0.1845, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20702822506427765 tensor([0.2108, 0.2070, 0.2182, 0.1839, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067449986934662 tensor([0.2114, 0.2067, 0.2184, 0.1836, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20680534839630127 tensor([0.2107, 0.2068, 0.2181, 0.1847, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20697297155857086 tensor([0.2112, 0.2070, 0.2177, 0.1842, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067117542028427 tensor([0.2114, 0.2067, 0.2176, 0.1845, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20653314888477325 tensor([0.2115, 0.2065, 0.2183, 0.1839, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20655852556228638 tensor([0.2114, 0.2066, 0.2183, 0.1840, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20697537064552307 tensor([0.2109, 0.2070, 0.2187, 0.1839, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206192746758461 tensor([0.2110, 0.2062, 0.2187, 0.1837, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20673559606075287 tensor([0.2114, 0.2067, 0.2177, 0.1839, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20718009769916534 tensor([0.2110, 0.2072, 0.2182, 0.1843, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20645615458488464 tensor([0.2114, 0.2065, 0.2186, 0.1837, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.207328200340271 tensor([0.2109, 0.2073, 0.2180, 0.1842, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068626582622528 tensor([0.2109, 0.2069, 0.2182, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20644019544124603 tensor([0.2110, 0.2064, 0.2183, 0.1843, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069786936044693 tensor([0.2112, 0.2070, 0.2181, 0.1840, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20692294836044312 tensor([0.2109, 0.2069, 0.2182, 0.1840, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069389373064041 tensor([0.2110, 0.2069, 0.2182, 0.1837, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669522881507874 tensor([0.2112, 0.2067, 0.2185, 0.1838, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669566094875336 tensor([0.2115, 0.2067, 0.2182, 0.1834, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20685908198356628 tensor([0.2109, 0.2069, 0.2181, 0.1843, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066708505153656 tensor([0.2120, 0.2067, 0.2179, 0.1837, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068398892879486 tensor([0.2108, 0.2068, 0.2183, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20665237307548523 tensor([0.2113, 0.2067, 0.2183, 0.1835, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20651255548000336 tensor([0.2111, 0.2065, 0.2183, 0.1838, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20751072466373444 tensor([0.2109, 0.2075, 0.2182, 0.1839, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20700521767139435 tensor([0.2109, 0.2070, 0.2184, 0.1838, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20705409348011017 tensor([0.2113, 0.2071, 0.2184, 0.1834, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20668978989124298 tensor([0.2112, 0.2067, 0.2182, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2063436061143875 tensor([0.2116, 0.2063, 0.2178, 0.1840, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20699352025985718 tensor([0.2112, 0.2070, 0.2184, 0.1842, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069023847579956 tensor([0.2106, 0.2069, 0.2186, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20717766880989075 tensor([0.2108, 0.2072, 0.2183, 0.1837, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20669056475162506 tensor([0.2111, 0.2067, 0.2185, 0.1839, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20695175230503082 tensor([0.2109, 0.2070, 0.2182, 0.1842, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20746353268623352 tensor([0.2114, 0.2075, 0.2183, 0.1837, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20675736665725708 tensor([0.2113, 0.2068, 0.2184, 0.1838, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20687833428382874 tensor([0.2110, 0.2069, 0.2182, 0.1840, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2065136879682541 tensor([0.2113, 0.2065, 0.2185, 0.1838, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20674681663513184 tensor([0.2110, 0.2067, 0.2181, 0.1842, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20756953954696655 tensor([0.2111, 0.2076, 0.2179, 0.1838, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069024294614792 tensor([0.2118, 0.2069, 0.2181, 0.1840, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067372351884842 tensor([0.2111, 0.2067, 0.2181, 0.1841, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068827599287033 tensor([0.2111, 0.2069, 0.2181, 0.1840, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071223258972168 tensor([0.2112, 0.2071, 0.2179, 0.1842, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681525766849518 tensor([0.2113, 0.2068, 0.2180, 0.1840, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20726126432418823 tensor([0.2109, 0.2073, 0.2182, 0.1831, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20715691149234772 tensor([0.2105, 0.2072, 0.2185, 0.1839, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20640717446804047 tensor([0.2113, 0.2064, 0.2180, 0.1842, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3], [4, 3]]\n",
      "[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
      "NL_pred of 2th iteration []\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.17226065695285797 tensor([0.2120, 0.2059, 0.2008, 0.1723, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892608255147934 tensor([0.2058, 0.2031, 0.1996, 0.1893, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18056844174861908 tensor([0.1966, 0.2088, 0.2080, 0.2061, 0.1806], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16375300288200378 tensor([0.1765, 0.2042, 0.2069, 0.2487, 0.1638], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15923890471458435 tensor([0.2196, 0.1965, 0.1904, 0.1592, 0.2343], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1679566204547882 tensor([0.2230, 0.1934, 0.2023, 0.1680, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17805983126163483 tensor([0.1883, 0.2162, 0.2054, 0.2121, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17769406735897064 tensor([0.1871, 0.2148, 0.2054, 0.2150, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1930169314146042 tensor([0.1985, 0.2059, 0.2009, 0.2017, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1643180102109909 tensor([0.2186, 0.1911, 0.1917, 0.1643, 0.2343], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18487341701984406 tensor([0.2118, 0.2002, 0.2123, 0.1849, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17618028819561005 tensor([0.1861, 0.2262, 0.2013, 0.2102, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.174941286444664 tensor([0.1908, 0.2075, 0.2106, 0.2162, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16151754558086395 tensor([0.1791, 0.2080, 0.2129, 0.2385, 0.1615], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16124486923217773 tensor([0.2187, 0.1961, 0.1910, 0.1612, 0.2329], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17520460486412048 tensor([0.2147, 0.1925, 0.1996, 0.1752, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16074955463409424 tensor([0.1705, 0.2199, 0.2006, 0.2482, 0.1607], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17009739577770233 tensor([0.1858, 0.2048, 0.2105, 0.2288, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1713721603155136 tensor([0.1791, 0.2052, 0.2019, 0.2424, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1629679948091507 tensor([0.2176, 0.1934, 0.1904, 0.1630, 0.2356], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1951204538345337 tensor([0.2038, 0.2013, 0.2003, 0.1951, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18100181221961975 tensor([0.2090, 0.2024, 0.1981, 0.1810, 0.2095], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1845686137676239 tensor([0.1986, 0.2058, 0.2089, 0.2020, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16665245592594147 tensor([0.1757, 0.2236, 0.2033, 0.2308, 0.1667], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16232724487781525 tensor([0.2147, 0.2012, 0.1903, 0.1623, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1710025817155838 tensor([0.2175, 0.1966, 0.1931, 0.1710, 0.2218], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18368194997310638 tensor([0.1957, 0.2071, 0.2058, 0.2077, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18535874783992767 tensor([0.1975, 0.2105, 0.2051, 0.2016, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17467080056667328 tensor([0.2142, 0.1985, 0.1982, 0.1747, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1465773582458496 tensor([0.2226, 0.1849, 0.1789, 0.1466, 0.2670], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17718854546546936 tensor([0.2139, 0.1953, 0.1998, 0.1772, 0.2138], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17693622410297394 tensor([0.1804, 0.2261, 0.1959, 0.2207, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1711905598640442 tensor([0.1829, 0.2134, 0.2066, 0.2259, 0.1712], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15601536631584167 tensor([0.1663, 0.2072, 0.2031, 0.2674, 0.1560], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15145088732242584 tensor([0.2196, 0.1905, 0.1820, 0.1515, 0.2564], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1661236733198166 tensor([0.2210, 0.1937, 0.1930, 0.1661, 0.2262], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17290069162845612 tensor([0.2166, 0.1947, 0.2007, 0.1729, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1887596696615219 tensor([0.2020, 0.2048, 0.2087, 0.1957, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16900275647640228 tensor([0.1753, 0.2140, 0.2005, 0.2411, 0.1690], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17309075593948364 tensor([0.2102, 0.1998, 0.1896, 0.1731, 0.2273], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17164115607738495 tensor([0.2181, 0.1943, 0.1967, 0.1716, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1702580749988556 tensor([0.1744, 0.2220, 0.1980, 0.2354, 0.1703], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18487423658370972 tensor([0.1943, 0.2077, 0.2045, 0.2087, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16413544118404388 tensor([0.1752, 0.2009, 0.2059, 0.2539, 0.1641], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15615026652812958 tensor([0.2213, 0.1893, 0.1892, 0.1562, 0.2440], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16671647131443024 tensor([0.2185, 0.1985, 0.2001, 0.1667, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16879326105117798 tensor([0.2149, 0.2057, 0.1986, 0.1688, 0.2120], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1751450002193451 tensor([0.1895, 0.2060, 0.2098, 0.2195, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15757086873054504 tensor([0.1675, 0.2083, 0.2025, 0.2641, 0.1576], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1502600610256195 tensor([0.2237, 0.1913, 0.1834, 0.1503, 0.2514], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17624691128730774 tensor([0.2144, 0.1931, 0.1958, 0.1762, 0.2204], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18258900940418243 tensor([0.2053, 0.2064, 0.1927, 0.1826, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1802518367767334 tensor([0.1917, 0.2125, 0.2060, 0.2095, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1782335340976715 tensor([0.1845, 0.2088, 0.2020, 0.2265, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16019268333911896 tensor([0.2183, 0.1987, 0.1892, 0.1602, 0.2337], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16188180446624756 tensor([0.2252, 0.1922, 0.1984, 0.1619, 0.2224], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17009645700454712 tensor([0.2151, 0.1991, 0.1955, 0.1701, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19480332732200623 tensor([0.2002, 0.2004, 0.2062, 0.1984, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18997998535633087 tensor([0.1938, 0.2132, 0.2023, 0.2006, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15551252663135529 tensor([0.2227, 0.1936, 0.1903, 0.1555, 0.2379], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17846013605594635 tensor([0.2150, 0.1962, 0.2024, 0.1785, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19195392727851868 tensor([0.1971, 0.2156, 0.1989, 0.1965, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17564453184604645 tensor([0.1868, 0.2150, 0.2088, 0.2138, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.192641019821167 tensor([0.1966, 0.2155, 0.1980, 0.1926, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17678874731063843 tensor([0.2065, 0.2080, 0.1956, 0.1768, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17461678385734558 tensor([0.2173, 0.1949, 0.2006, 0.1746, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17913900315761566 tensor([0.2069, 0.2068, 0.1965, 0.1791, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16659340262413025 tensor([0.1819, 0.2167, 0.2071, 0.2278, 0.1666], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16528646647930145 tensor([0.1733, 0.2087, 0.2014, 0.2513, 0.1653], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18077072501182556 tensor([0.2095, 0.1998, 0.1978, 0.1808, 0.2121], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1664137840270996 tensor([0.2218, 0.1973, 0.1992, 0.1664, 0.2152], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15634049475193024 tensor([0.2246, 0.1915, 0.1946, 0.1563, 0.2330], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18879328668117523 tensor([0.1977, 0.2106, 0.2051, 0.1979, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16151107847690582 tensor([0.1748, 0.2031, 0.2083, 0.2524, 0.1615], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1635999083518982 tensor([0.2172, 0.1918, 0.1872, 0.1636, 0.2402], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16929778456687927 tensor([0.2190, 0.1975, 0.1994, 0.1693, 0.2149], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17600111663341522 tensor([0.2089, 0.2050, 0.1943, 0.1760, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17492976784706116 tensor([0.1824, 0.2096, 0.2058, 0.2273, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16075648367404938 tensor([0.1748, 0.2136, 0.2083, 0.2425, 0.1608], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15862102806568146 tensor([0.2222, 0.1927, 0.1927, 0.1586, 0.2338], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19357965886592865 tensor([0.2022, 0.2018, 0.2055, 0.1969, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1791887879371643 tensor([0.1813, 0.2260, 0.1954, 0.2181, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18930663168430328 tensor([0.1948, 0.2046, 0.2032, 0.2081, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1872158795595169 tensor([0.2053, 0.2031, 0.1997, 0.1872, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15033721923828125 tensor([0.2272, 0.1869, 0.1893, 0.1503, 0.2463], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17873017489910126 tensor([0.2141, 0.1985, 0.2067, 0.1787, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18174396455287933 tensor([0.2072, 0.2039, 0.2004, 0.1817, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18548381328582764 tensor([0.1984, 0.2114, 0.2091, 0.1956, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16014224290847778 tensor([0.1706, 0.2106, 0.2030, 0.2557, 0.1601], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1598263382911682 tensor([0.2192, 0.1931, 0.1863, 0.1598, 0.2416], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16853199899196625 tensor([0.2148, 0.2009, 0.1921, 0.1685, 0.2236], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17858634889125824 tensor([0.1899, 0.2118, 0.2047, 0.2150, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17954184114933014 tensor([0.1934, 0.2139, 0.2073, 0.2059, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16601590812206268 tensor([0.1763, 0.2156, 0.2046, 0.2376, 0.1660], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16839127242565155 tensor([0.2118, 0.1945, 0.1849, 0.1684, 0.2405], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17487183213233948 tensor([0.2138, 0.1937, 0.1981, 0.1749, 0.2195], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16221031546592712 tensor([0.2193, 0.2007, 0.1950, 0.1622, 0.2229], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1894066482782364 tensor([0.1970, 0.2049, 0.2069, 0.2018, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19520583748817444 tensor([0.2061, 0.1985, 0.2032, 0.1952, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15527649223804474 tensor([0.2193, 0.1941, 0.1862, 0.1553, 0.2451], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18772770464420319 tensor([0.2073, 0.2023, 0.2048, 0.1877, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16477277874946594 tensor([0.1741, 0.2252, 0.1987, 0.2372, 0.1648], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17749926447868347 tensor([0.1833, 0.2062, 0.2035, 0.2295, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1622193306684494 tensor([0.1731, 0.2055, 0.2044, 0.2549, 0.1622], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15944944322109222 tensor([0.2187, 0.1987, 0.1899, 0.1594, 0.2332], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16340260207653046 tensor([0.2238, 0.1925, 0.1977, 0.1634, 0.2226], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1823946237564087 tensor([0.1927, 0.2137, 0.2058, 0.2054, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1863214671611786 tensor([0.1984, 0.2006, 0.2080, 0.2067, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16513046622276306 tensor([0.1726, 0.2135, 0.2006, 0.2482, 0.1651], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15937669575214386 tensor([0.2176, 0.1967, 0.1881, 0.1594, 0.2383], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1930939108133316 tensor([0.2000, 0.2072, 0.1958, 0.1931, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17061804234981537 tensor([0.1771, 0.2209, 0.2002, 0.2312, 0.1706], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17499664425849915 tensor([0.1843, 0.2231, 0.2054, 0.2122, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1577940583229065 tensor([0.1688, 0.2035, 0.2044, 0.2655, 0.1578], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14704935252666473 tensor([0.2243, 0.1865, 0.1813, 0.1470, 0.2608], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1659097671508789 tensor([0.2225, 0.1907, 0.1968, 0.1659, 0.2241], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18311536312103271 tensor([0.1918, 0.2175, 0.2013, 0.2064, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1754458099603653 tensor([0.1915, 0.2105, 0.2095, 0.2130, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18889346718788147 tensor([0.1940, 0.2061, 0.2041, 0.2069, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1696081906557083 tensor([0.2123, 0.1954, 0.1893, 0.1696, 0.2334], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15866073966026306 tensor([0.2248, 0.1924, 0.1904, 0.1587, 0.2338], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1881234496831894 tensor([0.2056, 0.2040, 0.2035, 0.1881, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1907912790775299 tensor([0.2001, 0.2019, 0.2064, 0.2008, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18653813004493713 tensor([0.1956, 0.2079, 0.2046, 0.2053, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1534881293773651 tensor([0.2229, 0.1860, 0.1825, 0.1535, 0.2552], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.177406907081604 tensor([0.2178, 0.1947, 0.2015, 0.1774, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18384812772274017 tensor([0.1908, 0.2166, 0.1997, 0.2090, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1884852796792984 tensor([0.2018, 0.2087, 0.2021, 0.1885, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18532533943653107 tensor([0.1970, 0.2048, 0.2085, 0.2044, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1391630619764328 tensor([0.2277, 0.1835, 0.1776, 0.1392, 0.2721], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1880526840686798 tensor([0.2080, 0.2045, 0.2071, 0.1923, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1643420159816742 tensor([0.1734, 0.2281, 0.1992, 0.2349, 0.1643], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18094323575496674 tensor([0.1930, 0.2039, 0.2089, 0.2133, 0.1809], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15742547810077667 tensor([0.1677, 0.2162, 0.2022, 0.2565, 0.1574], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1594049483537674 tensor([0.2195, 0.1921, 0.1879, 0.1594, 0.2411], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19090107083320618 tensor([0.1987, 0.2079, 0.2015, 0.2010, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16534939408302307 tensor([0.1723, 0.2211, 0.2000, 0.2413, 0.1653], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18602655827999115 tensor([0.1990, 0.2104, 0.2053, 0.1992, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1894831508398056 tensor([0.2040, 0.2062, 0.2015, 0.1895, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17033720016479492 tensor([0.2138, 0.1966, 0.1913, 0.1703, 0.2280], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1616089940071106 tensor([0.2253, 0.1917, 0.1972, 0.1616, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1908857226371765 tensor([0.2025, 0.2091, 0.2014, 0.1909, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17126911878585815 tensor([0.1871, 0.2084, 0.2109, 0.2224, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16137813031673431 tensor([0.1755, 0.2046, 0.2080, 0.2505, 0.1614], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16099174320697784 tensor([0.2173, 0.1933, 0.1888, 0.1610, 0.2396], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16989006102085114 tensor([0.2156, 0.2002, 0.1982, 0.1699, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19121180474758148 tensor([0.1941, 0.2126, 0.1995, 0.2026, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1898544728755951 tensor([0.1997, 0.2003, 0.2088, 0.2013, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1558481603860855 tensor([0.1702, 0.2100, 0.2071, 0.2568, 0.1558], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14642265439033508 tensor([0.2226, 0.1929, 0.1807, 0.1464, 0.2574], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17110614478588104 tensor([0.2133, 0.2049, 0.1959, 0.1711, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18514354526996613 tensor([0.1997, 0.2136, 0.1964, 0.1851, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1873713582754135 tensor([0.2013, 0.2086, 0.2111, 0.1916, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17267809808254242 tensor([0.1857, 0.2104, 0.2076, 0.2237, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1459585428237915 tensor([0.2257, 0.1897, 0.1827, 0.1460, 0.2559], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17070013284683228 tensor([0.2175, 0.1964, 0.1949, 0.1707, 0.2206], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1695723980665207 tensor([0.1804, 0.2174, 0.2042, 0.2285, 0.1696], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16912968456745148 tensor([0.1849, 0.2108, 0.2118, 0.2233, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1657448559999466 tensor([0.1714, 0.2142, 0.1998, 0.2488, 0.1657], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15882553160190582 tensor([0.2187, 0.1920, 0.1876, 0.1588, 0.2429], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16091060638427734 tensor([0.2237, 0.1925, 0.1950, 0.1609, 0.2279], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.185110941529274 tensor([0.2003, 0.2026, 0.2081, 0.2039, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16682274639606476 tensor([0.2151, 0.2054, 0.2005, 0.1668, 0.2121], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16238003969192505 tensor([0.1713, 0.2059, 0.2022, 0.2581, 0.1624], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14733199775218964 tensor([0.2228, 0.1941, 0.1827, 0.1473, 0.2531], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18352603912353516 tensor([0.2064, 0.2063, 0.2006, 0.1835, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18989349901676178 tensor([0.1975, 0.2113, 0.1927, 0.1899, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17546966671943665 tensor([0.2082, 0.2079, 0.1953, 0.1755, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16988684237003326 tensor([0.1735, 0.2103, 0.1999, 0.2464, 0.1699], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16850079596042633 tensor([0.2165, 0.1945, 0.1938, 0.1685, 0.2267], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17337030172348022 tensor([0.2182, 0.1945, 0.1981, 0.1734, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1879584789276123 tensor([0.2061, 0.2010, 0.1997, 0.1880, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17296183109283447 tensor([0.1851, 0.2125, 0.2082, 0.2212, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15123523771762848 tensor([0.1641, 0.2099, 0.2051, 0.2698, 0.1512], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16025689244270325 tensor([0.2192, 0.1919, 0.1857, 0.1603, 0.2430], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18017512559890747 tensor([0.2147, 0.1966, 0.2040, 0.1802, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1715308427810669 tensor([0.2137, 0.2019, 0.1947, 0.1715, 0.2181], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.192881241440773 tensor([0.2024, 0.2046, 0.2045, 0.1929, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1926795393228531 tensor([0.1952, 0.2050, 0.2010, 0.2061, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14623041450977325 tensor([0.2239, 0.1928, 0.1833, 0.1462, 0.2537], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15720225870609283 tensor([0.2281, 0.1893, 0.1953, 0.1572, 0.2301], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17108124494552612 tensor([0.1838, 0.2183, 0.2045, 0.2223, 0.1711], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1875784695148468 tensor([0.1972, 0.2028, 0.2079, 0.2046, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16702505946159363 tensor([0.1743, 0.2164, 0.2008, 0.2415, 0.1670], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1486784815788269 tensor([0.2291, 0.1862, 0.1890, 0.1487, 0.2470], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1790221929550171 tensor([0.2125, 0.2023, 0.2021, 0.1790, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18060384690761566 tensor([0.2082, 0.2011, 0.1962, 0.1806, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1786530762910843 tensor([0.2072, 0.2060, 0.1996, 0.1787, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17327702045440674 tensor([0.1874, 0.2059, 0.2099, 0.2235, 0.1733], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15968571603298187 tensor([0.2140, 0.1977, 0.1833, 0.1597, 0.2453], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18397511541843414 tensor([0.2134, 0.1957, 0.2019, 0.1840, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18073946237564087 tensor([0.2128, 0.1976, 0.2010, 0.1807, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15951183438301086 tensor([0.1782, 0.2097, 0.2130, 0.2396, 0.1595], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16964603960514069 tensor([0.1858, 0.2034, 0.2091, 0.2321, 0.1696], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17108304798603058 tensor([0.2131, 0.1999, 0.1928, 0.1711, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1931520402431488 tensor([0.2042, 0.2014, 0.2006, 0.1932, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18077640235424042 tensor([0.2044, 0.2103, 0.1947, 0.1808, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17367000877857208 tensor([0.1873, 0.2078, 0.2095, 0.2217, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15986813604831696 tensor([0.1684, 0.2102, 0.2016, 0.2599, 0.1599], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16131873428821564 tensor([0.2206, 0.1888, 0.1901, 0.1613, 0.2391], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1707385629415512 tensor([0.2173, 0.2001, 0.1994, 0.1707, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16867071390151978 tensor([0.1779, 0.2122, 0.2040, 0.2372, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17201924324035645 tensor([0.1864, 0.2119, 0.2090, 0.2207, 0.1720], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18949225544929504 tensor([0.1953, 0.2059, 0.2025, 0.2069, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15001548826694489 tensor([0.2240, 0.1899, 0.1853, 0.1500, 0.2508], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16718606650829315 tensor([0.2201, 0.1925, 0.1998, 0.1672, 0.2204], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15973100066184998 tensor([0.2191, 0.1922, 0.1895, 0.1597, 0.2395], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18335478007793427 tensor([0.1949, 0.2039, 0.2089, 0.2089, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19111564755439758 tensor([0.2003, 0.2087, 0.2056, 0.1943, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16988340020179749 tensor([0.2135, 0.1933, 0.1912, 0.1699, 0.2322], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17621904611587524 tensor([0.2185, 0.1966, 0.1990, 0.1762, 0.2097], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18667852878570557 tensor([0.2062, 0.2017, 0.1998, 0.1867, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1804109364748001 tensor([0.1908, 0.2097, 0.2066, 0.2125, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1577606201171875 tensor([0.1703, 0.2204, 0.2032, 0.2483, 0.1578], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15348301827907562 tensor([0.2241, 0.1918, 0.1890, 0.1535, 0.2416], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1658574491739273 tensor([0.2236, 0.1918, 0.2001, 0.1659, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18523646891117096 tensor([0.1867, 0.2133, 0.1966, 0.2182, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18323169648647308 tensor([0.1907, 0.2126, 0.2025, 0.2110, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17432823777198792 tensor([0.1864, 0.2092, 0.2084, 0.2217, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1592155396938324 tensor([0.2181, 0.1940, 0.1876, 0.1592, 0.2411], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17153002321720123 tensor([0.2175, 0.1944, 0.1942, 0.1715, 0.2223], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17964139580726624 tensor([0.1863, 0.2171, 0.2014, 0.2156, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17471523582935333 tensor([0.1907, 0.2111, 0.2088, 0.2146, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17137251794338226 tensor([0.1861, 0.2070, 0.2083, 0.2272, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16373516619205475 tensor([0.2159, 0.1954, 0.1890, 0.1637, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1928718239068985 tensor([0.2031, 0.2033, 0.2049, 0.1959, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17779557406902313 tensor([0.1868, 0.2215, 0.2023, 0.2117, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18632419407367706 tensor([0.1967, 0.2143, 0.2046, 0.1981, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18718542158603668 tensor([0.1916, 0.2035, 0.2017, 0.2159, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15927299857139587 tensor([0.2175, 0.1932, 0.1858, 0.1593, 0.2443], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17190316319465637 tensor([0.2149, 0.2011, 0.1977, 0.1719, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1791718304157257 tensor([0.1898, 0.2164, 0.2057, 0.2090, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1844990849494934 tensor([0.1986, 0.2091, 0.2082, 0.1995, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1541568487882614 tensor([0.1648, 0.2029, 0.2029, 0.2753, 0.1542], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1551365852355957 tensor([0.2189, 0.1949, 0.1849, 0.1551, 0.2463], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17768187820911407 tensor([0.2160, 0.1946, 0.2015, 0.1777, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1647617518901825 tensor([0.1768, 0.2199, 0.2043, 0.2342, 0.1648], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19008664786815643 tensor([0.2019, 0.2066, 0.2061, 0.1901, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1484479308128357 tensor([0.1623, 0.2069, 0.2041, 0.2783, 0.1484], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14817261695861816 tensor([0.2214, 0.1886, 0.1817, 0.1482, 0.2601], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15802694857120514 tensor([0.2283, 0.1903, 0.1949, 0.1580, 0.2285], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16748622059822083 tensor([0.1752, 0.2278, 0.2005, 0.2290, 0.1675], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1791861355304718 tensor([0.1936, 0.2136, 0.2082, 0.2055, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16620320081710815 tensor([0.2140, 0.2027, 0.1943, 0.1662, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16229867935180664 tensor([0.2186, 0.1969, 0.1893, 0.1623, 0.2329], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16431397199630737 tensor([0.2222, 0.1938, 0.1978, 0.1643, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18102777004241943 tensor([0.2100, 0.2018, 0.2001, 0.1810, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18320007622241974 tensor([0.2067, 0.2066, 0.2031, 0.1832, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18506741523742676 tensor([0.1851, 0.2154, 0.1974, 0.2164, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1531558632850647 tensor([0.2206, 0.1897, 0.1807, 0.1532, 0.2558], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [3], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [3], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [4], [4], [4], [3], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [3], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [3], [4], [3], [4], [4], [4], [4], [3], [4], [4], [4], [3], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [3], [4], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3], [3], [3], [3], [4], [3], [3], [4], [4], [4], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [4], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [4], [3], [3], [3], [3], [3], [0], [3]]\n",
      "[[0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4]]\n",
      "NL_pred of 0th iteration [[3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [3], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [3], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [4], [4], [4], [3], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [3], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [3], [4], [3], [4], [4], [4], [4], [3], [4], [4], [4], [3], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [3], [4], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3], [3], [3], [3], [4], [3], [3], [4], [4], [4], [3], [3], [3], [3], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [3], [4], [4], [3], [3], [3], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [4], [4], [3], [4], [4], [4], [4], [3], [3], [4], [4], [4], [3], [3], [4], [3], [4], [3], [3], [4], [4], [3], [3], [3], [3], [3], [0], [3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0050269713401794435  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005026856899261475  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005026640415191651  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005026331424713135  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005025940418243408  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005025475025177002  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.00502494192123413  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005024351119995117  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005023705005645752  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005023010730743408  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005022274017333984  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005021498203277588  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.0050206871032714845  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005019845962524414  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005018978595733642  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005018085479736328  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005017170906066895  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005016236305236816  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005015285015106201  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005014318943023682  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.20265579223632812 tensor([0.2146, 0.2077, 0.2027, 0.1681, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20014244318008423 tensor([0.2084, 0.2050, 0.2015, 0.1849, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19903436303138733 tensor([0.1990, 0.2108, 0.2101, 0.2015, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1787581443786621 tensor([0.1788, 0.2064, 0.2091, 0.2438, 0.1620], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19215787947177887 tensor([0.2224, 0.1982, 0.1922, 0.1553, 0.2319], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1951187700033188 tensor([0.2257, 0.1951, 0.2041, 0.1639, 0.2112], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19065119326114655 tensor([0.1907, 0.2183, 0.2075, 0.2075, 0.1760], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18940454721450806 tensor([0.1894, 0.2169, 0.2075, 0.2104, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19724047183990479 tensor([0.2010, 0.2079, 0.2029, 0.1972, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1928674727678299 tensor([0.2214, 0.1929, 0.1934, 0.1602, 0.2321], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1888234168291092 tensor([0.2143, 0.2020, 0.2142, 0.1806, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18839064240455627 tensor([0.1884, 0.2284, 0.2033, 0.2057, 0.1742], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19312039017677307 tensor([0.1931, 0.2095, 0.2127, 0.2116, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1812516301870346 tensor([0.1813, 0.2102, 0.2151, 0.2337, 0.1598], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19278302788734436 tensor([0.2215, 0.1979, 0.1928, 0.1573, 0.2306], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1942836046218872 tensor([0.2175, 0.1943, 0.2015, 0.1709, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17272204160690308 tensor([0.1727, 0.2222, 0.2028, 0.2433, 0.1589], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18814660608768463 tensor([0.1881, 0.2070, 0.2127, 0.2240, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18147777020931244 tensor([0.1815, 0.2074, 0.2041, 0.2375, 0.1695], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1921154260635376 tensor([0.2204, 0.1952, 0.1921, 0.1589, 0.2334], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974811553955078 tensor([0.2064, 0.2032, 0.2023, 0.1906, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19999894499778748 tensor([0.2116, 0.2043, 0.2000, 0.1768, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975792497396469 tensor([0.2011, 0.2078, 0.2110, 0.1976, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1778300404548645 tensor([0.1778, 0.2259, 0.2054, 0.2261, 0.1647], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19205938279628754 tensor([0.2174, 0.2030, 0.1921, 0.1583, 0.2292], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19487763941287994 tensor([0.2202, 0.1984, 0.1949, 0.1669, 0.2196], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1981772780418396 tensor([0.1982, 0.2091, 0.2079, 0.2032, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19716517627239227 tensor([0.1999, 0.2125, 0.2071, 0.1972, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20008501410484314 tensor([0.2169, 0.2003, 0.2001, 0.1705, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18047697842121124 tensor([0.2254, 0.1864, 0.1805, 0.1429, 0.2647], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19707360863685608 tensor([0.2166, 0.1971, 0.2017, 0.1729, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18263579905033112 tensor([0.1826, 0.2284, 0.1980, 0.2160, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18518587946891785 tensor([0.1852, 0.2156, 0.2088, 0.2212, 0.1692], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.168426513671875 tensor([0.1684, 0.2095, 0.2054, 0.2624, 0.1543], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1836780458688736 tensor([0.2224, 0.1922, 0.1837, 0.1477, 0.2540], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19484113156795502 tensor([0.2238, 0.1954, 0.1948, 0.1620, 0.2239], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19648022949695587 tensor([0.2193, 0.1965, 0.2026, 0.1688, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19137068092823029 tensor([0.2045, 0.2067, 0.2108, 0.1914, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17756737768650055 tensor([0.1776, 0.2163, 0.2027, 0.2362, 0.1671], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.191400408744812 tensor([0.2129, 0.2017, 0.1914, 0.1690, 0.2250], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19609303772449493 tensor([0.2208, 0.1961, 0.1985, 0.1675, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17666208744049072 tensor([0.1767, 0.2243, 0.2001, 0.2306, 0.1683], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19671034812927246 tensor([0.1967, 0.2097, 0.2065, 0.2042, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1774289608001709 tensor([0.1774, 0.2031, 0.2082, 0.2489, 0.1624], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19097501039505005 tensor([0.2242, 0.1910, 0.1910, 0.1523, 0.2416], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2002418339252472 tensor([0.2212, 0.2002, 0.2019, 0.1627, 0.2140], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2004317343235016 tensor([0.2175, 0.2075, 0.2004, 0.1648, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19189894199371338 tensor([0.1919, 0.2081, 0.2119, 0.2149, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16966666281223297 tensor([0.1697, 0.2106, 0.2048, 0.2591, 0.1559], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.185054212808609 tensor([0.2266, 0.1930, 0.1851, 0.1464, 0.2489], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1949329972267151 tensor([0.2172, 0.1949, 0.1976, 0.1720, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19452670216560364 tensor([0.2080, 0.2083, 0.1945, 0.1783, 0.2109], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1940818727016449 tensor([0.1941, 0.2146, 0.2081, 0.2050, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18683218955993652 tensor([0.1868, 0.2109, 0.2042, 0.2218, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19088104367256165 tensor([0.2211, 0.2005, 0.1909, 0.1562, 0.2314], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1938571184873581 tensor([0.2280, 0.1939, 0.2001, 0.1579, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19734348356723785 tensor([0.2178, 0.2010, 0.1973, 0.1660, 0.2178], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19401749968528748 tensor([0.2027, 0.2023, 0.2083, 0.1940, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19612111151218414 tensor([0.1963, 0.2153, 0.2043, 0.1961, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1919832080602646 tensor([0.2255, 0.1953, 0.1920, 0.1517, 0.2356], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19803711771965027 tensor([0.2177, 0.1980, 0.2043, 0.1742, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19207210838794708 tensor([0.1996, 0.2177, 0.2009, 0.1921, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1890854835510254 tensor([0.1891, 0.2171, 0.2109, 0.2093, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19509902596473694 tensor([0.1991, 0.2176, 0.1999, 0.1883, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19740846753120422 tensor([0.2091, 0.2099, 0.1974, 0.1726, 0.2109], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1967231035232544 tensor([0.2200, 0.1967, 0.2025, 0.1704, 0.2104], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1983141303062439 tensor([0.2095, 0.2087, 0.1983, 0.1750, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1841682493686676 tensor([0.1842, 0.2189, 0.2092, 0.2230, 0.1647], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17563310265541077 tensor([0.1756, 0.2110, 0.2036, 0.2463, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19967713952064514 tensor([0.2123, 0.2017, 0.1997, 0.1764, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19904974102973938 tensor([0.2245, 0.1990, 0.2010, 0.1624, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19315363466739655 tensor([0.2273, 0.1932, 0.1963, 0.1525, 0.2306], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19350974261760712 tensor([0.2001, 0.2126, 0.2071, 0.1935, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17705567181110382 tensor([0.1771, 0.2053, 0.2106, 0.2474, 0.1597], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18893292546272278 tensor([0.2200, 0.1935, 0.1889, 0.1596, 0.2379], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1992221474647522 tensor([0.2217, 0.1992, 0.2012, 0.1652, 0.2127], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19612561166286469 tensor([0.2116, 0.2069, 0.1961, 0.1718, 0.2136], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18472082912921906 tensor([0.1847, 0.2118, 0.2080, 0.2226, 0.1729], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17696893215179443 tensor([0.1770, 0.2158, 0.2105, 0.2378, 0.1589], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19440197944641113 tensor([0.2250, 0.1944, 0.1945, 0.1547, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1923786997795105 tensor([0.2047, 0.2038, 0.2075, 0.1924, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18361008167266846 tensor([0.1836, 0.2283, 0.1974, 0.2135, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19728228449821472 tensor([0.1973, 0.2067, 0.2053, 0.2036, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20164765417575836 tensor([0.2079, 0.2050, 0.2016, 0.1830, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18852408230304718 tensor([0.2300, 0.1885, 0.1909, 0.1466, 0.2439], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19984328746795654 tensor([0.2168, 0.2003, 0.2086, 0.1745, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20233245193958282 tensor([0.2098, 0.2057, 0.2023, 0.1775, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19128234684467316 tensor([0.2008, 0.2134, 0.2111, 0.1913, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.172831192612648 tensor([0.1728, 0.2129, 0.2053, 0.2507, 0.1583], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18801414966583252 tensor([0.2220, 0.1948, 0.1880, 0.1558, 0.2393], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19390133023262024 tensor([0.2175, 0.2027, 0.1939, 0.1645, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1922539472579956 tensor([0.1923, 0.2139, 0.2068, 0.2105, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19572854042053223 tensor([0.1957, 0.2159, 0.2094, 0.2014, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17850585281848907 tensor([0.1785, 0.2178, 0.2068, 0.2327, 0.1642], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18668383359909058 tensor([0.2145, 0.1963, 0.1867, 0.1643, 0.2382], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.195493683218956 tensor([0.2165, 0.1955, 0.2000, 0.1707, 0.2174], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1967492252588272 tensor([0.2220, 0.2025, 0.1967, 0.1583, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19732007384300232 tensor([0.1995, 0.2069, 0.2090, 0.1973, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19482387602329254 tensor([0.2087, 0.2004, 0.2052, 0.1908, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18789303302764893 tensor([0.2221, 0.1959, 0.1879, 0.1514, 0.2427], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1957862377166748 tensor([0.2099, 0.2042, 0.2067, 0.1833, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17631494998931885 tensor([0.1763, 0.2276, 0.2008, 0.2324, 0.1629], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18565380573272705 tensor([0.1857, 0.2083, 0.2057, 0.2248, 0.1755], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17534857988357544 tensor([0.1753, 0.2077, 0.2067, 0.2498, 0.1604], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19160835444927216 tensor([0.2214, 0.2004, 0.1916, 0.1556, 0.2309], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19421923160552979 tensor([0.2265, 0.1942, 0.1995, 0.1595, 0.2203], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1951219141483307 tensor([0.1951, 0.2158, 0.2079, 0.2008, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20083367824554443 tensor([0.2008, 0.2026, 0.2101, 0.2023, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17478571832180023 tensor([0.1748, 0.2159, 0.2028, 0.2433, 0.1633], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.189803346991539 tensor([0.2203, 0.1985, 0.1898, 0.1555, 0.2359], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19770051538944244 tensor([0.2026, 0.2092, 0.1977, 0.1887, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17931047081947327 tensor([0.1793, 0.2232, 0.2023, 0.2264, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1865503489971161 tensor([0.1866, 0.2253, 0.2074, 0.2077, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1709967702627182 tensor([0.1710, 0.2058, 0.2067, 0.2605, 0.1561], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18292124569416046 tensor([0.2272, 0.1882, 0.1829, 0.1433, 0.2583], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19243919849395752 tensor([0.2253, 0.1924, 0.1986, 0.1619, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19415709376335144 tensor([0.1942, 0.2196, 0.2033, 0.2018, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.193848118185997 tensor([0.1938, 0.2126, 0.2116, 0.2084, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1964290738105774 tensor([0.1964, 0.2081, 0.2062, 0.2024, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19107766449451447 tensor([0.2151, 0.1972, 0.1911, 0.1655, 0.2311], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1920749396085739 tensor([0.2276, 0.1941, 0.1921, 0.1547, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19665838778018951 tensor([0.2082, 0.2059, 0.2054, 0.1838, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19639240205287933 tensor([0.2026, 0.2039, 0.2085, 0.1964, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19804851710796356 tensor([0.1980, 0.2100, 0.2067, 0.2008, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18414829671382904 tensor([0.2257, 0.1876, 0.1841, 0.1497, 0.2529], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1965208798646927 tensor([0.2205, 0.1965, 0.2034, 0.1732, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19318543374538422 tensor([0.1932, 0.2188, 0.2018, 0.2045, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1966840624809265 tensor([0.2043, 0.2107, 0.2041, 0.1842, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1994667500257492 tensor([0.1995, 0.2068, 0.2106, 0.1998, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17912785708904266 tensor([0.2304, 0.1851, 0.1791, 0.1359, 0.2695], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1879194676876068 tensor([0.2106, 0.2064, 0.2090, 0.1879, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17562982439994812 tensor([0.1756, 0.2305, 0.2013, 0.2301, 0.1625], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1953507661819458 tensor([0.1954, 0.2059, 0.2111, 0.2088, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1698375791311264 tensor([0.1698, 0.2186, 0.2044, 0.2515, 0.1556], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18959268927574158 tensor([0.2222, 0.1939, 0.1896, 0.1555, 0.2388], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1965160220861435 tensor([0.2012, 0.2099, 0.2035, 0.1965, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.174530029296875 tensor([0.1745, 0.2234, 0.2021, 0.2365, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19475656747817993 tensor([0.2015, 0.2125, 0.2073, 0.1948, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19669334590435028 tensor([0.2066, 0.2082, 0.2034, 0.1852, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19312889873981476 tensor([0.2165, 0.1984, 0.1931, 0.1662, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19339676201343536 tensor([0.2281, 0.1934, 0.1989, 0.1577, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19395969808101654 tensor([0.2051, 0.2111, 0.2033, 0.1865, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1894253045320511 tensor([0.1894, 0.2105, 0.2131, 0.2177, 0.1693], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17773881554603577 tensor([0.1777, 0.2068, 0.2103, 0.2455, 0.1596], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19051773846149445 tensor([0.2201, 0.1951, 0.1905, 0.1570, 0.2373], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19999852776527405 tensor([0.2184, 0.2020, 0.2000, 0.1658, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1965021938085556 tensor([0.1965, 0.2147, 0.2016, 0.1982, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19691330194473267 tensor([0.2022, 0.2023, 0.2109, 0.1969, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.172406867146492 tensor([0.1724, 0.2123, 0.2094, 0.2518, 0.1541], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18240706622600555 tensor([0.2254, 0.1946, 0.1824, 0.1428, 0.2549], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976630985736847 tensor([0.2160, 0.2067, 0.1977, 0.1670, 0.2126], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19828379154205322 tensor([0.2021, 0.2157, 0.1983, 0.1809, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18737223744392395 tensor([0.2038, 0.2106, 0.2130, 0.1874, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1880815178155899 tensor([0.1881, 0.2125, 0.2097, 0.2189, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1843174695968628 tensor([0.2286, 0.1914, 0.1843, 0.1423, 0.2534], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1967022866010666 tensor([0.2203, 0.1982, 0.1967, 0.1665, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18270692229270935 tensor([0.1827, 0.2196, 0.2063, 0.2237, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18707063794136047 tensor([0.1871, 0.2130, 0.2140, 0.2188, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17358513176441193 tensor([0.1736, 0.2166, 0.2020, 0.2439, 0.1640], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1893458515405655 tensor([0.2215, 0.1938, 0.1893, 0.1549, 0.2405], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19423986971378326 tensor([0.2265, 0.1942, 0.1968, 0.1569, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.199368417263031 tensor([0.2028, 0.2045, 0.2102, 0.1994, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2022561877965927 tensor([0.2176, 0.2072, 0.2023, 0.1631, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17351782321929932 tensor([0.1735, 0.2082, 0.2045, 0.2532, 0.1606], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18431739509105682 tensor([0.2257, 0.1958, 0.1843, 0.1436, 0.2506], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20108413696289062 tensor([0.2090, 0.2082, 0.2025, 0.1793, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19462311267852783 tensor([0.2000, 0.2133, 0.1946, 0.1856, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19712942838668823 tensor([0.2108, 0.2098, 0.1971, 0.1714, 0.2109], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17568975687026978 tensor([0.1757, 0.2126, 0.2021, 0.2415, 0.1680], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19560518860816956 tensor([0.2193, 0.1963, 0.1956, 0.1644, 0.2244], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19622084498405457 tensor([0.2210, 0.1962, 0.1999, 0.1692, 0.2137], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20163917541503906 tensor([0.2087, 0.2029, 0.2016, 0.1836, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1873752474784851 tensor([0.1874, 0.2147, 0.2104, 0.2166, 0.1709], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16625703871250153 tensor([0.1663, 0.2122, 0.2073, 0.2646, 0.1496], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1874314248561859 tensor([0.2220, 0.1936, 0.1874, 0.1563, 0.2407], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19840891659259796 tensor([0.2173, 0.1984, 0.2059, 0.1759, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19652356207370758 tensor([0.2164, 0.2038, 0.1965, 0.1674, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1934555321931839 tensor([0.2049, 0.2066, 0.2065, 0.1885, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19765152037143707 tensor([0.1977, 0.2070, 0.2031, 0.2016, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18494753539562225 tensor([0.2267, 0.1945, 0.1849, 0.1427, 0.2512], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19100932776927948 tensor([0.2310, 0.1910, 0.1970, 0.1533, 0.2278], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1860870122909546 tensor([0.1861, 0.2205, 0.2066, 0.2176, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1996375471353531 tensor([0.1996, 0.2048, 0.2100, 0.2001, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1765148788690567 tensor([0.1765, 0.2187, 0.2030, 0.2367, 0.1652], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1878339946269989 tensor([0.2318, 0.1878, 0.1907, 0.1451, 0.2446], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20186065137386322 tensor([0.2152, 0.2042, 0.2040, 0.1748, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19808456301689148 tensor([0.2109, 0.2029, 0.1981, 0.1763, 0.2118], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20148546993732452 tensor([0.2098, 0.2080, 0.2015, 0.1745, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18974637985229492 tensor([0.1897, 0.2080, 0.2121, 0.2188, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18498767912387848 tensor([0.2168, 0.1995, 0.1850, 0.1557, 0.2430], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19747358560562134 tensor([0.2161, 0.1975, 0.2038, 0.1797, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1993998885154724 tensor([0.2155, 0.1994, 0.2029, 0.1765, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1804780513048172 tensor([0.1805, 0.2119, 0.2152, 0.2347, 0.1578], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1881379783153534 tensor([0.1881, 0.2055, 0.2113, 0.2273, 0.1678], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19453038275241852 tensor([0.2158, 0.2017, 0.1945, 0.1669, 0.2210], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19855856895446777 tensor([0.2069, 0.2034, 0.2025, 0.1887, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19661423563957214 tensor([0.2069, 0.2122, 0.1966, 0.1766, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18967469036579132 tensor([0.1897, 0.2099, 0.2117, 0.2170, 0.1717], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17051127552986145 tensor([0.1705, 0.2126, 0.2039, 0.2549, 0.1581], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19056114554405212 tensor([0.2234, 0.1906, 0.1919, 0.1573, 0.2368], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2012358456850052 tensor([0.2200, 0.2019, 0.2012, 0.1666, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18023473024368286 tensor([0.1802, 0.2145, 0.2062, 0.2323, 0.1668], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1887514740228653 tensor([0.1888, 0.2140, 0.2112, 0.2161, 0.1700], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1977793574333191 tensor([0.1978, 0.2080, 0.2045, 0.2023, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18690413236618042 tensor([0.2267, 0.1916, 0.1869, 0.1464, 0.2484], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1942397803068161 tensor([0.2229, 0.1942, 0.2016, 0.1631, 0.2181], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19126468896865845 tensor([0.2219, 0.1939, 0.1913, 0.1558, 0.2371], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19737508893013 tensor([0.1974, 0.2059, 0.2110, 0.2044, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18999624252319336 tensor([0.2027, 0.2106, 0.2076, 0.1900, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19299937784671783 tensor([0.2162, 0.1951, 0.1930, 0.1658, 0.2299], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19840240478515625 tensor([0.2213, 0.1984, 0.2008, 0.1720, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20175541937351227 tensor([0.2088, 0.2036, 0.2018, 0.1823, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19314128160476685 tensor([0.1931, 0.2118, 0.2088, 0.2080, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17242814600467682 tensor([0.1724, 0.2228, 0.2054, 0.2434, 0.1560], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19071488082408905 tensor([0.2269, 0.1935, 0.1907, 0.1496, 0.2392], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19355548918247223 tensor([0.2263, 0.1936, 0.2019, 0.1619, 0.2164], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18908759951591492 tensor([0.1891, 0.2155, 0.1986, 0.2136, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19307571649551392 tensor([0.1931, 0.2147, 0.2046, 0.2064, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18873049318790436 tensor([0.1887, 0.2113, 0.2106, 0.2170, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18934020400047302 tensor([0.2209, 0.1958, 0.1893, 0.1553, 0.2388], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1960601806640625 tensor([0.2203, 0.1962, 0.1961, 0.1674, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1887003779411316 tensor([0.1887, 0.2193, 0.2035, 0.2109, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19306084513664246 tensor([0.1931, 0.2132, 0.2109, 0.2101, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18845582008361816 tensor([0.1885, 0.2091, 0.2105, 0.2225, 0.1695], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1907612681388855 tensor([0.2187, 0.1972, 0.1908, 0.1597, 0.2337], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19149284064769745 tensor([0.2056, 0.2052, 0.2069, 0.1915, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18909874558448792 tensor([0.1891, 0.2237, 0.2044, 0.2071, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19371630251407623 tensor([0.1991, 0.2164, 0.2066, 0.1937, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19411076605319977 tensor([0.1941, 0.2056, 0.2039, 0.2112, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18751536309719086 tensor([0.2203, 0.1949, 0.1875, 0.1553, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19953322410583496 tensor([0.2175, 0.2029, 0.1995, 0.1679, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19210922718048096 tensor([0.1921, 0.2185, 0.2078, 0.2044, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1951608657836914 tensor([0.2010, 0.2112, 0.2103, 0.1952, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16676054894924164 tensor([0.1668, 0.2049, 0.2051, 0.2707, 0.1525], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18653401732444763 tensor([0.2216, 0.1966, 0.1865, 0.1513, 0.2439], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19642570614814758 tensor([0.2187, 0.1964, 0.2034, 0.1734, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17909479141235352 tensor([0.1791, 0.2221, 0.2065, 0.2294, 0.1629], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19311514496803284 tensor([0.2044, 0.2086, 0.2081, 0.1858, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1643516570329666 tensor([0.1644, 0.2091, 0.2064, 0.2733, 0.1469], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18335355818271637 tensor([0.2243, 0.1903, 0.1834, 0.1444, 0.2577], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1919519454240799 tensor([0.2312, 0.1920, 0.1966, 0.1541, 0.2262], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.177434504032135 tensor([0.1774, 0.2301, 0.2026, 0.2242, 0.1656], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1959490180015564 tensor([0.1959, 0.2156, 0.2102, 0.2011, 0.1771], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19608907401561737 tensor([0.2167, 0.2045, 0.1961, 0.1622, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19103556871414185 tensor([0.2215, 0.1987, 0.1910, 0.1583, 0.2306], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1955161988735199 tensor([0.2249, 0.1955, 0.1996, 0.1603, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20197290182113647 tensor([0.2126, 0.2036, 0.2020, 0.1767, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982448399066925 tensor([0.2093, 0.2085, 0.2050, 0.1790, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18366305530071259 tensor([0.1874, 0.2177, 0.1995, 0.2117, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1823173463344574 tensor([0.2235, 0.1914, 0.1823, 0.1492, 0.2535], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [3], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 3], [3, 1], [3, 4], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 4], [3, 2], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 3], [3], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 2], [3, 1], [4, 3], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3], [3], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 3], [4, 3], [3, 2], [3, 1], [4, 3], [4, 0], [3, 4], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1], [3, 1], [4, 3], [4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 1], [4, 3], [4, 0], [4, 0], [3], [3, 1], [3, 4], [3], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 3], [3, 4], [3, 2], [3, 4], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 2], [3, 4], [4, 3], [4, 0], [3, 2], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2], [4, 3], [4, 0], [4, 0], [4, 0], [3, 2], [4, 3], [4, 0], [4, 3], [3, 4], [3, 2], [3, 1], [3, 4], [4, 0], [4, 0], [3, 2], [3, 2], [4, 0], [4, 3], [4, 0], [3, 2], [3, 2], [3, 2], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 3], [3], [4, 0], [3, 2], [3], [3, 2], [3, 2], [4, 0], [3, 2], [3, 1], [3], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 1], [3], [3, 2], [3], [4, 0], [3, 2], [3, 1], [3, 1], [4, 0], [4, 0], [3, 2], [3, 4], [3, 2], [4, 0], [4, 0], [3, 1], [3], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 3], [3, 2], [3, 1], [3], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [4, 3], [4, 0], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 3], [4, 0], [3, 2], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [3, 2], [3, 2], [3, 1], [3], [3, 4], [0, 4], [3, 2]]\n",
      "[[0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 2], [0, 2, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 2], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [0, 1, 2], [0, 1, 2, 4], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2], [0, 1, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 2, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [0, 1, 2, 4], [0, 2, 4], [0, 1, 2], [0, 1, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1, 2], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2], [0, 1, 2], [0, 1, 4], [0, 2, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 2], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 4], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 2, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 2], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 2, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [0, 1, 2], [0, 1, 4], [0, 2, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 1, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 4], [0, 2, 4], [0, 1, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4]]\n",
      "NL_pred of 1th iteration [[4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 3], [3, 1], [3, 4], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 4], [3, 2], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 3], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 2], [3, 1], [4, 3], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 3], [4, 3], [3, 2], [3, 1], [4, 3], [4, 0], [3, 4], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1], [3, 1], [4, 3], [4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 1], [4, 3], [4, 0], [4, 0], [3, 1], [3, 4], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 3], [3, 4], [3, 2], [3, 4], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 2], [3, 4], [4, 3], [4, 0], [3, 2], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2], [4, 3], [4, 0], [4, 0], [4, 0], [3, 2], [4, 3], [4, 0], [4, 3], [3, 4], [3, 2], [3, 1], [3, 4], [4, 0], [4, 0], [3, 2], [3, 2], [4, 0], [4, 3], [4, 0], [3, 2], [3, 2], [3, 2], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [4, 3], [4, 0], [3, 2], [3, 2], [3, 2], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 1], [3, 2], [4, 0], [3, 2], [3, 1], [3, 1], [4, 0], [4, 0], [3, 2], [3, 4], [3, 2], [4, 0], [4, 0], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 3], [3, 2], [3, 1], [4, 0], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2], [4, 3], [4, 0], [4, 3], [4, 0], [3, 2], [3, 2], [4, 0], [4, 3], [4, 0], [3, 2], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2], [3, 1], [4, 0], [4, 0], [3, 2], [3, 2], [3, 1], [3, 4], [0, 4], [3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005454296230250954  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00545425751270392  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005454184662582528  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005454080227093819  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005453947772327651  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005453788317166842  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0054536079749082905  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0054534062361105895  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005453187176305004  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005452952323815761  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005452700659760043  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005452437278551933  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005452160142425798  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005451874855237129  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005451579379220294  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005451274733258109  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0054509624456747984  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005450644554235997  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005450320040058886  Accuracy on Support set:0.0\n",
      "torch.Size([234, 2048]) torch.Size([234])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005449989922026284  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.20143648982048035 tensor([0.2122, 0.2087, 0.2014, 0.1698, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20030082762241364 tensor([0.2060, 0.2060, 0.2003, 0.1867, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20352673530578613 tensor([0.1965, 0.2118, 0.2089, 0.2035, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2072269171476364 tensor([0.1763, 0.2072, 0.2078, 0.2461, 0.1625], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19918693602085114 tensor([0.2199, 0.1992, 0.1909, 0.1568, 0.2332], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029363363981247 tensor([0.2233, 0.1960, 0.2029, 0.1655, 0.2123], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20620901882648468 tensor([0.1882, 0.2193, 0.2062, 0.2096, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20627477765083313 tensor([0.1870, 0.2179, 0.2063, 0.2125, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19851279258728027 tensor([0.1985, 0.2089, 0.2017, 0.1992, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1921852082014084 tensor([0.2190, 0.1937, 0.1922, 0.1618, 0.2333], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20298446714878082 tensor([0.2119, 0.2030, 0.2130, 0.1824, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20209543406963348 tensor([0.1860, 0.2294, 0.2021, 0.2077, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.210496187210083 tensor([0.1907, 0.2105, 0.2114, 0.2137, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21109750866889954 tensor([0.1789, 0.2111, 0.2139, 0.2359, 0.1602], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1987883448600769 tensor([0.2191, 0.1988, 0.1916, 0.1588, 0.2318], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20022350549697876 tensor([0.2150, 0.1952, 0.2002, 0.1726, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2015506476163864 tensor([0.1704, 0.2230, 0.2016, 0.2455, 0.1595], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2078704684972763 tensor([0.1857, 0.2079, 0.2114, 0.2262, 0.1688], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20279985666275024 tensor([0.1791, 0.2083, 0.2028, 0.2397, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19608181715011597 tensor([0.2179, 0.1961, 0.1909, 0.1605, 0.2346], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20104795694351196 tensor([0.2040, 0.2041, 0.2010, 0.1925, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20520351827144623 tensor([0.2091, 0.2052, 0.1988, 0.1785, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19864250719547272 tensor([0.1986, 0.2088, 0.2097, 0.1995, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20417171716690063 tensor([0.1755, 0.2268, 0.2042, 0.2283, 0.1653], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20398759841918945 tensor([0.2149, 0.2040, 0.1908, 0.1599, 0.2304], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19930307567119598 tensor([0.2178, 0.1993, 0.1937, 0.1685, 0.2207], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2051817774772644 tensor([0.1957, 0.2101, 0.2067, 0.2052, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19746133685112 tensor([0.1975, 0.2135, 0.2059, 0.1991, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19887132942676544 tensor([0.2144, 0.2012, 0.1989, 0.1722, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18722927570343018 tensor([0.2230, 0.1872, 0.1793, 0.1442, 0.2662], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20045015215873718 tensor([0.2142, 0.1980, 0.2005, 0.1747, 0.2127], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19674639403820038 tensor([0.1802, 0.2293, 0.1967, 0.2181, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20752619206905365 tensor([0.1828, 0.2165, 0.2075, 0.2233, 0.1698], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20407474040985107 tensor([0.1661, 0.2103, 0.2041, 0.2648, 0.1547], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19307486712932587 tensor([0.2200, 0.1931, 0.1825, 0.1491, 0.2554], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19634884595870972 tensor([0.2213, 0.1963, 0.1936, 0.1636, 0.2251], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2013740837574005 tensor([0.2169, 0.1974, 0.2014, 0.1704, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20202568173408508 tensor([0.2020, 0.2077, 0.2095, 0.1933, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20143947005271912 tensor([0.1751, 0.2173, 0.2014, 0.2385, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20263682305812836 tensor([0.2104, 0.2026, 0.1902, 0.1706, 0.2261], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19734500348567963 tensor([0.2183, 0.1970, 0.1973, 0.1692, 0.2181], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19884027540683746 tensor([0.1743, 0.2252, 0.1988, 0.2328, 0.1689], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20529940724372864 tensor([0.1942, 0.2107, 0.2053, 0.2063, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20391568541526794 tensor([0.1750, 0.2039, 0.2069, 0.2512, 0.1629], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19188998639583588 tensor([0.2217, 0.1919, 0.1897, 0.1537, 0.2429], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20066659152507782 tensor([0.2188, 0.2012, 0.2007, 0.1643, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19923292100429535 tensor([0.2150, 0.2085, 0.1992, 0.1664, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20908311009407043 tensor([0.1894, 0.2091, 0.2107, 0.2170, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20350198447704315 tensor([0.1673, 0.2114, 0.2035, 0.2614, 0.1563], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19387198984622955 tensor([0.2241, 0.1939, 0.1838, 0.1479, 0.2503], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19643038511276245 tensor([0.2147, 0.1958, 0.1964, 0.1737, 0.2193], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20549936592578888 tensor([0.2055, 0.2092, 0.1933, 0.1800, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20687304437160492 tensor([0.1916, 0.2156, 0.2069, 0.2070, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2029147446155548 tensor([0.1844, 0.2119, 0.2029, 0.2240, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20140662789344788 tensor([0.2186, 0.2014, 0.1897, 0.1578, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19891536235809326 tensor([0.2255, 0.1948, 0.1989, 0.1594, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20190389454364777 tensor([0.2154, 0.2019, 0.1961, 0.1676, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2002602517604828 tensor([0.2003, 0.2033, 0.2070, 0.1959, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19388212263584137 tensor([0.1939, 0.2163, 0.2031, 0.1980, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19619572162628174 tensor([0.2230, 0.1962, 0.1908, 0.1531, 0.2368], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20308655500411987 tensor([0.2152, 0.1989, 0.2031, 0.1759, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19710081815719604 tensor([0.1971, 0.2187, 0.1996, 0.1940, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20963646471500397 tensor([0.1867, 0.2181, 0.2096, 0.2113, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1966308206319809 tensor([0.1966, 0.2186, 0.1987, 0.1901, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20668233931064606 tensor([0.2067, 0.2109, 0.1962, 0.1743, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20126840472221375 tensor([0.2176, 0.1977, 0.2013, 0.1721, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20706315338611603 tensor([0.2071, 0.2097, 0.1971, 0.1767, 0.2095], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20796121656894684 tensor([0.1817, 0.2199, 0.2080, 0.2252, 0.1653], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20233097672462463 tensor([0.1732, 0.2118, 0.2023, 0.2486, 0.1640], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20262941718101501 tensor([0.2098, 0.2026, 0.1984, 0.1782, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19983230531215668 tensor([0.2221, 0.2000, 0.1998, 0.1640, 0.2141], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19514255225658417 tensor([0.2249, 0.1940, 0.1951, 0.1540, 0.2319], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1976568102836609 tensor([0.1977, 0.2136, 0.2059, 0.1954, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20611104369163513 tensor([0.1747, 0.2061, 0.2093, 0.2498, 0.1602], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19441774487495422 tensor([0.2176, 0.1944, 0.1877, 0.1611, 0.2392], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000146061182022 tensor([0.2192, 0.2001, 0.2000, 0.1668, 0.2138], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20788916945457458 tensor([0.2091, 0.2079, 0.1949, 0.1735, 0.2146], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20670516788959503 tensor([0.1823, 0.2127, 0.2067, 0.2248, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20924833416938782 tensor([0.1746, 0.2167, 0.2092, 0.2400, 0.1594], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19327303767204285 tensor([0.2225, 0.1953, 0.1933, 0.1562, 0.2327], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20229558646678925 tensor([0.2023, 0.2047, 0.2063, 0.1942, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19621197879314423 tensor([0.1812, 0.2293, 0.1962, 0.2155, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20402874052524567 tensor([0.1948, 0.2076, 0.2040, 0.2056, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200428768992424 tensor([0.2055, 0.2059, 0.2004, 0.1847, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1897217035293579 tensor([0.2276, 0.1894, 0.1897, 0.1480, 0.2453], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2012096494436264 tensor([0.2143, 0.2012, 0.2074, 0.1762, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2010982781648636 tensor([0.2074, 0.2067, 0.2011, 0.1792, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19841748476028442 tensor([0.1984, 0.2143, 0.2099, 0.1931, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20395168662071228 tensor([0.1704, 0.2137, 0.2040, 0.2531, 0.1588], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19574783742427826 tensor([0.2195, 0.1957, 0.1868, 0.1574, 0.2406], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20358911156654358 tensor([0.2151, 0.2036, 0.1927, 0.1661, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2055126577615738 tensor([0.1898, 0.2149, 0.2055, 0.2125, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20341968536376953 tensor([0.1933, 0.2170, 0.2082, 0.2034, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20547205209732056 tensor([0.1761, 0.2188, 0.2055, 0.2350, 0.1647], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19718629121780396 tensor([0.2121, 0.1972, 0.1855, 0.1659, 0.2394], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19876664876937866 tensor([0.2140, 0.1964, 0.1988, 0.1723, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20342297852039337 tensor([0.2195, 0.2034, 0.1955, 0.1598, 0.2217], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19703686237335205 tensor([0.1970, 0.2079, 0.2077, 0.1993, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2013624608516693 tensor([0.2062, 0.2014, 0.2040, 0.1927, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19678018987178802 tensor([0.2197, 0.1968, 0.1867, 0.1529, 0.2440], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20520003139972687 tensor([0.2074, 0.2052, 0.2055, 0.1852, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19955962896347046 tensor([0.1739, 0.2285, 0.1996, 0.2345, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2044324278831482 tensor([0.1832, 0.2093, 0.2044, 0.2270, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20537634193897247 tensor([0.1729, 0.2086, 0.2054, 0.2522, 0.1609], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20133523643016815 tensor([0.2191, 0.2013, 0.1904, 0.1571, 0.2322], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19825638830661774 tensor([0.2241, 0.1951, 0.1983, 0.1610, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20278728008270264 tensor([0.1927, 0.2168, 0.2066, 0.2028, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983463317155838 tensor([0.1983, 0.2035, 0.2088, 0.2043, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20151987671852112 tensor([0.1724, 0.2167, 0.2015, 0.2455, 0.1638], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19938965141773224 tensor([0.2179, 0.1994, 0.1886, 0.1570, 0.2372], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20012766122817993 tensor([0.2001, 0.2101, 0.1965, 0.1905, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20107461512088776 tensor([0.1769, 0.2242, 0.2011, 0.2285, 0.1693], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2061571329832077 tensor([0.1841, 0.2263, 0.2062, 0.2097, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20542381703853607 tensor([0.1686, 0.2066, 0.2054, 0.2629, 0.1565], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18901918828487396 tensor([0.2248, 0.1890, 0.1817, 0.1447, 0.2598], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19736798107624054 tensor([0.2228, 0.1933, 0.1974, 0.1635, 0.2230], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20206992328166962 tensor([0.1917, 0.2206, 0.2021, 0.2038, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.210394486784935 tensor([0.1914, 0.2136, 0.2104, 0.2105, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20436841249465942 tensor([0.1940, 0.2091, 0.2049, 0.2044, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.198138028383255 tensor([0.2126, 0.1981, 0.1898, 0.1671, 0.2323], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19498299062252045 tensor([0.2251, 0.1950, 0.1909, 0.1562, 0.2328], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20422179996967316 tensor([0.2057, 0.2069, 0.2042, 0.1856, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2001359462738037 tensor([0.2001, 0.2048, 0.2072, 0.1983, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20281660556793213 tensor([0.1956, 0.2109, 0.2054, 0.2028, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18846215307712555 tensor([0.2233, 0.1885, 0.1829, 0.1511, 0.2542], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20219087600708008 tensor([0.2181, 0.1974, 0.2022, 0.1749, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20052124559879303 tensor([0.1907, 0.2198, 0.2005, 0.2065, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20188815891742706 tensor([0.2019, 0.2117, 0.2029, 0.1860, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20184966921806335 tensor([0.1970, 0.2078, 0.2093, 0.2018, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18582797050476074 tensor([0.2281, 0.1858, 0.1780, 0.1370, 0.2711], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20739096403121948 tensor([0.2081, 0.2074, 0.2078, 0.1898, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20008280873298645 tensor([0.1733, 0.2314, 0.2001, 0.2322, 0.1631], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20686832070350647 tensor([0.1929, 0.2069, 0.2098, 0.2109, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.203121617436409 tensor([0.1675, 0.2194, 0.2031, 0.2538, 0.1561], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19474753737449646 tensor([0.2198, 0.1947, 0.1884, 0.1570, 0.2401], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19874922931194305 tensor([0.1987, 0.2109, 0.2023, 0.1984, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20087288320064545 tensor([0.1721, 0.2243, 0.2009, 0.2387, 0.1640], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19903549551963806 tensor([0.1990, 0.2134, 0.2061, 0.1966, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2021932303905487 tensor([0.2041, 0.2091, 0.2022, 0.1870, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19930244982242584 tensor([0.2141, 0.1993, 0.1919, 0.1678, 0.2269], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19772756099700928 tensor([0.2256, 0.1943, 0.1977, 0.1592, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20210722088813782 tensor([0.2026, 0.2121, 0.2021, 0.1883, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21142005920410156 tensor([0.1870, 0.2114, 0.2118, 0.2199, 0.1699], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20769579708576202 tensor([0.1753, 0.2077, 0.2090, 0.2479, 0.1601], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19597607851028442 tensor([0.2176, 0.1960, 0.1893, 0.1585, 0.2386], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20291854441165924 tensor([0.2159, 0.2029, 0.1988, 0.1674, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20010769367218018 tensor([0.1940, 0.2156, 0.2003, 0.2001, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19973328709602356 tensor([0.1997, 0.2033, 0.2096, 0.1989, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20808535814285278 tensor([0.1700, 0.2132, 0.2081, 0.2542, 0.1546], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19548696279525757 tensor([0.2229, 0.1955, 0.1812, 0.1441, 0.2563], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20768889784812927 tensor([0.2136, 0.2077, 0.1965, 0.1686, 0.2137], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19973504543304443 tensor([0.1997, 0.2166, 0.1971, 0.1826, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20135730504989624 tensor([0.2014, 0.2115, 0.2118, 0.1892, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20845840871334076 tensor([0.1856, 0.2134, 0.2085, 0.2211, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19224366545677185 tensor([0.2262, 0.1922, 0.1831, 0.1436, 0.2549], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1990765482187271 tensor([0.2178, 0.1991, 0.1955, 0.1682, 0.2195], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2050330489873886 tensor([0.1802, 0.2206, 0.2050, 0.2259, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21271130442619324 tensor([0.1847, 0.2139, 0.2127, 0.2209, 0.1678], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20074041187763214 tensor([0.1712, 0.2174, 0.2007, 0.2461, 0.1645], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19465792179107666 tensor([0.2190, 0.1947, 0.1881, 0.1564, 0.2418], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19557015597820282 tensor([0.2240, 0.1952, 0.1956, 0.1584, 0.2268], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20033343136310577 tensor([0.2003, 0.2055, 0.2089, 0.2013, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20108161866664886 tensor([0.2152, 0.2082, 0.2011, 0.1646, 0.2109], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20320947468280792 tensor([0.1711, 0.2090, 0.2032, 0.2556, 0.1611], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19667619466781616 tensor([0.2232, 0.1967, 0.1831, 0.1450, 0.2520], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2012338936328888 tensor([0.2066, 0.2091, 0.2012, 0.1810, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1976434886455536 tensor([0.1976, 0.2143, 0.1934, 0.1873, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2083226442337036 tensor([0.2083, 0.2108, 0.1959, 0.1730, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20084550976753235 tensor([0.1733, 0.2135, 0.2008, 0.2438, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19724462926387787 tensor([0.2168, 0.1972, 0.1944, 0.1660, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19869861006736755 tensor([0.2185, 0.1971, 0.1987, 0.1709, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2004087120294571 tensor([0.2063, 0.2038, 0.2004, 0.1854, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20914560556411743 tensor([0.1849, 0.2156, 0.2091, 0.2187, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20604734122753143 tensor([0.1639, 0.2130, 0.2060, 0.2670, 0.1500], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19450779259204865 tensor([0.2195, 0.1945, 0.1862, 0.1578, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20348399877548218 tensor([0.2149, 0.1993, 0.2047, 0.1776, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20473483204841614 tensor([0.2139, 0.2047, 0.1953, 0.1690, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20243556797504425 tensor([0.2024, 0.2076, 0.2053, 0.1904, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20184572041034698 tensor([0.1952, 0.2079, 0.2018, 0.2036, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1954118311405182 tensor([0.2243, 0.1954, 0.1837, 0.1440, 0.2526], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19581112265586853 tensor([0.2285, 0.1919, 0.1958, 0.1548, 0.2290], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.205363467335701 tensor([0.1836, 0.2215, 0.2054, 0.2197, 0.1698], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2021293044090271 tensor([0.1972, 0.2058, 0.2087, 0.2021, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20169053971767426 tensor([0.1741, 0.2196, 0.2017, 0.2389, 0.1657], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18946371972560883 tensor([0.2295, 0.1887, 0.1895, 0.1464, 0.2460], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20278698205947876 tensor([0.2127, 0.2052, 0.2028, 0.1765, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038743793964386 tensor([0.2084, 0.2039, 0.1968, 0.1780, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20025205612182617 tensor([0.2073, 0.2089, 0.2003, 0.1762, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20895230770111084 tensor([0.1873, 0.2090, 0.2108, 0.2210, 0.1720], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2003822773694992 tensor([0.2144, 0.2004, 0.1838, 0.1572, 0.2442], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20261555910110474 tensor([0.2136, 0.1984, 0.2026, 0.1815, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2016569972038269 tensor([0.2130, 0.2003, 0.2017, 0.1782, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21277768909931183 tensor([0.1781, 0.2128, 0.2140, 0.2369, 0.1582], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20636940002441406 tensor([0.1857, 0.2064, 0.2101, 0.2295, 0.1684], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20260947942733765 tensor([0.2134, 0.2026, 0.1933, 0.1685, 0.2222], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20130108296871185 tensor([0.2044, 0.2043, 0.2013, 0.1905, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20447319746017456 tensor([0.2045, 0.2132, 0.1954, 0.1783, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21042436361312866 tensor([0.1872, 0.2109, 0.2104, 0.2192, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20259085297584534 tensor([0.1682, 0.2134, 0.2026, 0.2573, 0.1585], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1906670480966568 tensor([0.2209, 0.1914, 0.1907, 0.1589, 0.2381], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000313103199005 tensor([0.2176, 0.2028, 0.2000, 0.1683, 0.2113], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2049172967672348 tensor([0.1778, 0.2154, 0.2049, 0.2346, 0.1674], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2098885029554367 tensor([0.1863, 0.2150, 0.2099, 0.2182, 0.1707], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20326046645641327 tensor([0.1953, 0.2089, 0.2033, 0.2042, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1924692690372467 tensor([0.2244, 0.1925, 0.1857, 0.1477, 0.2498], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20042170584201813 tensor([0.2204, 0.1952, 0.2004, 0.1648, 0.2193], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1948007494211197 tensor([0.2195, 0.1948, 0.1900, 0.1573, 0.2384], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.206424742937088 tensor([0.1949, 0.2069, 0.2098, 0.2064, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20030124485492706 tensor([0.2003, 0.2116, 0.2064, 0.1918, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19596512615680695 tensor([0.2138, 0.1960, 0.1918, 0.1674, 0.2310], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19961649179458618 tensor([0.2188, 0.1993, 0.1996, 0.1737, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20051635801792145 tensor([0.2063, 0.2045, 0.2005, 0.1841, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20748572051525116 tensor([0.1907, 0.2128, 0.2075, 0.2100, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2041357159614563 tensor([0.1701, 0.2236, 0.2041, 0.2457, 0.1565], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19442223012447357 tensor([0.2244, 0.1944, 0.1895, 0.1511, 0.2406], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007167488336563 tensor([0.2238, 0.1944, 0.2007, 0.1635, 0.2175], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1973695158958435 tensor([0.1866, 0.2164, 0.1974, 0.2156, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20334315299987793 tensor([0.1906, 0.2157, 0.2033, 0.2084, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2092808336019516 tensor([0.1863, 0.2123, 0.2093, 0.2192, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19664815068244934 tensor([0.2184, 0.1966, 0.1881, 0.1567, 0.2401], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19706419110298157 tensor([0.2178, 0.1971, 0.1948, 0.1690, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20221658051013947 tensor([0.1862, 0.2202, 0.2022, 0.2130, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20968849956989288 tensor([0.1906, 0.2142, 0.2097, 0.2121, 0.1734], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20920349657535553 tensor([0.1860, 0.2100, 0.2092, 0.2246, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19810481369495392 tensor([0.2162, 0.1981, 0.1895, 0.1613, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20315571129322052 tensor([0.2032, 0.2061, 0.2057, 0.1933, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20312297344207764 tensor([0.1866, 0.2247, 0.2031, 0.2091, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19662845134735107 tensor([0.1966, 0.2174, 0.2054, 0.1956, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20258913934230804 tensor([0.1916, 0.2066, 0.2026, 0.2133, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1958101987838745 tensor([0.2179, 0.1958, 0.1863, 0.1568, 0.2432], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20385845005512238 tensor([0.2151, 0.2039, 0.1983, 0.1695, 0.2132], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2064150720834732 tensor([0.1897, 0.2195, 0.2065, 0.2064, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19855466485023499 tensor([0.1986, 0.2121, 0.2090, 0.1971, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20384572446346283 tensor([0.1644, 0.2057, 0.2038, 0.2731, 0.1529], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1975187361240387 tensor([0.2192, 0.1975, 0.1853, 0.1527, 0.2452], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20221884548664093 tensor([0.2162, 0.1974, 0.2022, 0.1752, 0.2091], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2051902711391449 tensor([0.1767, 0.2231, 0.2052, 0.2316, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20200417935848236 tensor([0.2020, 0.2095, 0.2069, 0.1876, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20509526133537292 tensor([0.1620, 0.2100, 0.2051, 0.2757, 0.1472], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19112810492515564 tensor([0.2219, 0.1911, 0.1821, 0.1458, 0.2591], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19543613493442535 tensor([0.2287, 0.1929, 0.1954, 0.1556, 0.2274], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20131146907806396 tensor([0.1751, 0.2310, 0.2013, 0.2263, 0.1662], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20307238399982452 tensor([0.1935, 0.2166, 0.2090, 0.2031, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20549894869327545 tensor([0.2143, 0.2055, 0.1949, 0.1638, 0.2216], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19961777329444885 tensor([0.2190, 0.1996, 0.1898, 0.1598, 0.2318], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19839000701904297 tensor([0.2225, 0.1964, 0.1984, 0.1619, 0.2208], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007521539926529 tensor([0.2102, 0.2046, 0.2008, 0.1785, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038053274154663 tensor([0.2068, 0.2095, 0.2038, 0.1807, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19823527336120605 tensor([0.1850, 0.2186, 0.1982, 0.2138, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19230066239833832 tensor([0.2210, 0.1923, 0.1811, 0.1507, 0.2548], grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [3], [4, 0], [4, 0], [3, 2, 1], [3, 1], [4, 0], [4, 0], [4, 3, 0], [3, 1, 2], [3, 4], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 4], [3, 2], [4, 3, 0], [4, 0], [3, 2], [3, 2, 1], [4, 0], [4, 3, 0], [3, 2], [3, 2, 1], [3, 1], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [3, 1], [4, 3], [4, 0], [3, 2], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1, 2], [3, 2], [4, 3], [4, 3, 0], [3, 2, 1], [3, 1], [4, 3, 0], [4, 0], [3, 4, 0], [3, 2], [3, 1], [3, 2], [4, 0], [4, 0], [3, 2], [3, 1, 2], [3, 1, 2], [4, 3, 0], [4, 0], [3, 2, 1], [3, 1], [3, 2], [4, 0], [4, 0], [3, 1, 2], [4, 3], [4, 0, 2], [4, 0], [3], [3, 1, 2], [3, 4], [3], [4, 3, 0], [4, 0], [3, 2, 1], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2], [4, 3, 0], [3, 4], [3, 2, 1], [3, 4], [4, 0, 2], [4, 0], [4, 0], [3, 2], [3, 1, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [3, 4], [4, 3], [4, 0], [3, 2, 1], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2, 1], [4, 3], [4, 0], [4, 0], [4, 0], [3, 2, 1], [4, 3, 0], [4, 0], [4, 3, 0], [3, 4], [3, 2, 1], [3, 1, 2], [3, 4], [4, 0], [4, 0], [3, 2, 1], [3, 2], [4, 0], [4, 3, 0], [4, 0], [3, 2, 1], [3, 2], [3, 2, 0], [4, 3], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 3], [3], [4, 0], [3, 2, 1], [3], [3, 2, 0], [3, 2], [4, 0], [3, 2, 1], [3, 1, 2], [3], [4, 0], [4, 0], [3, 2, 1], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [4, 0], [3, 1, 2], [3], [3, 2], [3], [4, 0], [3, 2], [3, 1], [3, 1], [4, 0], [4, 0], [3, 2], [3, 4], [3, 2], [4, 0], [4, 0], [3, 1, 2], [3], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1], [3, 2, 1], [4, 0], [4, 3], [3, 2, 1], [3, 1, 2], [3], [4, 0], [4, 0], [3, 2, 1], [3, 1], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 0], [4, 0], [3, 2, 1], [4, 3], [4, 0], [4, 3, 0], [4, 0], [3, 2, 1], [3, 2], [4, 0], [4, 3, 0], [4, 0], [3, 2, 1], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [3, 2], [3, 2, 1], [3, 1, 2], [3], [3, 4], [0, 4, 2], [3, 2, 1]]\n",
      "[[0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2], [0, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2], [0, 1, 4], [1, 2], [1, 2, 3], [0, 1, 4], [0, 4], [1, 2, 3], [1, 2], [0, 1, 4], [0, 4], [0, 2, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 2, 4], [0, 1, 2], [1, 2, 3], [0, 1, 4], [0, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 4], [0, 1, 4], [0, 1, 2], [1, 2], [0, 4], [0, 2, 4], [1, 2], [1, 2, 3], [1, 2], [0, 1, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 4], [0, 4], [1, 2], [1, 2, 3], [0, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2], [1, 3], [1, 2, 3], [0, 1, 2, 4], [0, 4], [0, 1, 2], [0, 1, 2, 4], [1, 2], [1, 2, 3], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2], [0, 1, 2], [0, 4], [0, 1, 2], [1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 1, 2], [0, 1, 2], [1, 2, 3], [0, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [1, 2], [1, 2, 3], [1, 2], [0, 1, 2], [0, 4], [0, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2], [1, 2, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 1, 2], [0, 1, 2, 4], [1, 2, 3], [0, 4], [0, 1, 2, 4], [1, 4], [0, 1, 4], [1, 2, 3], [0, 4], [0, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2, 4], [0, 1, 4], [0, 1, 2, 4], [1, 2, 3], [0, 1, 4], [0, 2, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 2], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [0, 4], [1, 2, 3], [0, 1, 2], [0, 4], [0, 4], [0, 1, 2, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2], [1, 2, 3], [1, 2], [1, 2, 3], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2], [1, 2, 3], [0, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 4], [0, 4], [0, 1, 2, 4], [0, 1, 2], [1, 3], [0, 4]]\n",
      "NL_pred of 2th iteration [[3, 2, 1], [4, 3, 0], [3, 1, 2], [3, 2, 1], [3, 2, 1], [4, 3, 0], [3, 2, 1], [4, 3, 0], [3, 2], [3, 2, 1], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 2, 1], [3, 2], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 3, 0], [3, 2, 1], [4, 3, 0], [3, 4, 0], [3, 1, 2], [3, 1, 2], [4, 3, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 1, 2], [4, 3, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 3, 0], [3, 2, 1], [4, 0, 2], [3, 1, 2], [4, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 2, 1], [3, 2, 1], [3, 2, 1], [3, 2, 1], [4, 3, 0], [4, 3, 0], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 0], [3, 2, 1], [3, 2, 0], [3, 2, 1], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 2, 0], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 1, 2], [3, 1, 2], [3, 2, 1], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 2, 1], [4, 3, 0], [3, 2, 1], [4, 3, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 1, 2], [0, 4, 2], [3, 2, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01458609781482003  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.014585844495079735  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.014585363593968477  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014584676785902544  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.014583809809251265  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.014582776210524818  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.014581594954837452  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.014580280943350359  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.014578845013271679  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.014577302065762606  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.0145756642926823  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.014573937112634832  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.014572134072130377  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.01457026329907504  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.014568330212072893  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.014566340229728005  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.014564300125295465  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.014562216672030363  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.014560095288536766  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.01455793868411671  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.1998511105775833 tensor([0.2130, 0.2047, 0.1999, 0.1708, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19873392581939697 tensor([0.2067, 0.2020, 0.1987, 0.1878, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20487046241760254 tensor([0.1972, 0.2080, 0.2073, 0.2049, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20366604626178741 tensor([0.1770, 0.2037, 0.2064, 0.2476, 0.1654], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22061944007873535 tensor([0.2206, 0.1950, 0.1893, 0.1576, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20126935839653015 tensor([0.2240, 0.1922, 0.2013, 0.1664, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2047484964132309 tensor([0.1889, 0.2154, 0.2047, 0.2109, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20482738316059113 tensor([0.1876, 0.2141, 0.2048, 0.2139, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001620978116989 tensor([0.1992, 0.2050, 0.2002, 0.2004, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21971099078655243 tensor([0.2197, 0.1897, 0.1905, 0.1625, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19927218556404114 tensor([0.2127, 0.1993, 0.2115, 0.1835, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20070020854473114 tensor([0.1866, 0.2255, 0.2007, 0.2091, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681573450565338 tensor([0.1914, 0.2068, 0.2099, 0.2151, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20751585066318512 tensor([0.1795, 0.2075, 0.2124, 0.2376, 0.1630], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21977068483829498 tensor([0.2198, 0.1947, 0.1899, 0.1596, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1985439509153366 tensor([0.2157, 0.1913, 0.1985, 0.1735, 0.2210], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001207023859024 tensor([0.1709, 0.2195, 0.2001, 0.2472, 0.1623], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20424197614192963 tensor([0.1864, 0.2042, 0.2099, 0.2277, 0.1718], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20133599638938904 tensor([0.1797, 0.2046, 0.2013, 0.2412, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21865716576576233 tensor([0.2187, 0.1920, 0.1892, 0.1612, 0.2389], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994735151529312 tensor([0.2047, 0.2003, 0.1995, 0.1936, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2013138383626938 tensor([0.2098, 0.2013, 0.1972, 0.1796, 0.2121], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20505356788635254 tensor([0.1993, 0.2051, 0.2082, 0.2008, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20279550552368164 tensor([0.1761, 0.2230, 0.2028, 0.2299, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19976705312728882 tensor([0.2157, 0.1998, 0.1892, 0.1607, 0.2346], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21847790479660034 tensor([0.2185, 0.1953, 0.1921, 0.1694, 0.2247], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20515640079975128 tensor([0.1964, 0.2063, 0.2052, 0.2065, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20438942313194275 tensor([0.1981, 0.2097, 0.2044, 0.2004, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1972866803407669 tensor([0.2152, 0.1973, 0.1973, 0.1731, 0.2172], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22363154590129852 tensor([0.2236, 0.1833, 0.1777, 0.1448, 0.2705], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19881825149059296 tensor([0.2149, 0.1941, 0.1988, 0.1756, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2195788025856018 tensor([0.1809, 0.2254, 0.1954, 0.2196, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2060733139514923 tensor([0.1834, 0.2128, 0.2061, 0.2248, 0.1729], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20266780257225037 tensor([0.1667, 0.2068, 0.2027, 0.2664, 0.1575], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2206605076789856 tensor([0.2207, 0.1888, 0.1808, 0.1497, 0.2600], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22204530239105225 tensor([0.2220, 0.1923, 0.1920, 0.1645, 0.2292], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19973178207874298 tensor([0.2175, 0.1936, 0.1997, 0.1714, 0.2178], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20275604724884033 tensor([0.2028, 0.2039, 0.2080, 0.1945, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20001721382141113 tensor([0.1758, 0.2135, 0.2000, 0.2400, 0.1707], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19851048290729523 tensor([0.2111, 0.1985, 0.1886, 0.1715, 0.2302], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21904614567756653 tensor([0.2190, 0.1931, 0.1957, 0.1701, 0.2221], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22136810421943665 tensor([0.1749, 0.2214, 0.1974, 0.2343, 0.1720], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20379489660263062 tensor([0.1949, 0.2069, 0.2038, 0.2076, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20039153099060059 tensor([0.1757, 0.2004, 0.2054, 0.2527, 0.1658], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22243481874465942 tensor([0.2224, 0.1878, 0.1880, 0.1544, 0.2473], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19724564254283905 tensor([0.2195, 0.1972, 0.1990, 0.1652, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045329213142395 tensor([0.2158, 0.2045, 0.1977, 0.1674, 0.2146], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20536582171916962 tensor([0.1901, 0.2054, 0.2092, 0.2184, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2021166831254959 tensor([0.1679, 0.2079, 0.2021, 0.2630, 0.1591], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2248012125492096 tensor([0.2248, 0.1896, 0.1822, 0.1485, 0.2548], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21543388068675995 tensor([0.2154, 0.1919, 0.1948, 0.1746, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20519359409809113 tensor([0.2062, 0.2052, 0.1918, 0.1810, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20540256798267365 tensor([0.1923, 0.2118, 0.2054, 0.2084, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2014346718788147 tensor([0.1850, 0.2081, 0.2014, 0.2254, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19721578061580658 tensor([0.2193, 0.1972, 0.1880, 0.1585, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22541698813438416 tensor([0.2263, 0.1908, 0.1972, 0.1603, 0.2254], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19787463545799255 tensor([0.2161, 0.1979, 0.1945, 0.1685, 0.2230], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1995382308959961 tensor([0.2009, 0.1995, 0.2055, 0.1971, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20160432159900665 tensor([0.1946, 0.2124, 0.2016, 0.1993, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2237708568572998 tensor([0.2238, 0.1921, 0.1891, 0.1539, 0.2411], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20144778490066528 tensor([0.2160, 0.1951, 0.2014, 0.1769, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19812744855880737 tensor([0.1978, 0.2147, 0.1981, 0.1952, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2081872969865799 tensor([0.1873, 0.2143, 0.2082, 0.2128, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19721892476081848 tensor([0.1973, 0.2146, 0.1972, 0.1914, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681941509246826 tensor([0.2074, 0.2068, 0.1946, 0.1753, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19961391389369965 tensor([0.2183, 0.1937, 0.1996, 0.1730, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20564673840999603 tensor([0.2078, 0.2056, 0.1956, 0.1777, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20652902126312256 tensor([0.1824, 0.2161, 0.2065, 0.2267, 0.1683], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20088325440883636 tensor([0.1738, 0.2081, 0.2009, 0.2501, 0.1670], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1986161768436432 tensor([0.2105, 0.1986, 0.1968, 0.1791, 0.2149], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21797914803028107 tensor([0.2228, 0.1961, 0.1982, 0.1649, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22563323378562927 tensor([0.2256, 0.1901, 0.1935, 0.1548, 0.2361], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20434686541557312 tensor([0.1983, 0.2097, 0.2043, 0.1967, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2025662660598755 tensor([0.1753, 0.2026, 0.2078, 0.2512, 0.1631], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21829897165298462 tensor([0.2183, 0.1903, 0.1861, 0.1619, 0.2435], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19837526977062225 tensor([0.2200, 0.1963, 0.1984, 0.1677, 0.2177], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20376084744930267 tensor([0.2099, 0.2038, 0.1933, 0.1744, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20522448420524597 tensor([0.1829, 0.2090, 0.2052, 0.2262, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2078424096107483 tensor([0.1752, 0.2131, 0.2078, 0.2417, 0.1622], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22323870658874512 tensor([0.2232, 0.1912, 0.1916, 0.1570, 0.2370], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20090582966804504 tensor([0.2030, 0.2009, 0.2047, 0.1954, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21694140136241913 tensor([0.1819, 0.2252, 0.1948, 0.2169, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20250381529331207 tensor([0.1954, 0.2038, 0.2025, 0.2069, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19888001680374146 tensor([0.2061, 0.2021, 0.1989, 0.1858, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22830066084861755 tensor([0.2283, 0.1854, 0.1880, 0.1486, 0.2497], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19738176465034485 tensor([0.2151, 0.1974, 0.2058, 0.1773, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19953881204128265 tensor([0.2081, 0.2028, 0.1995, 0.1802, 0.2093], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20840086042881012 tensor([0.1991, 0.2106, 0.2084, 0.1944, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20252625644207 tensor([0.1710, 0.2101, 0.2025, 0.2547, 0.1617], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2202637791633606 tensor([0.2203, 0.1915, 0.1851, 0.1581, 0.2450], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19960452616214752 tensor([0.2158, 0.1996, 0.1911, 0.1670, 0.2265], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040564864873886 tensor([0.1905, 0.2111, 0.2041, 0.2138, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20478297770023346 tensor([0.1940, 0.2131, 0.2067, 0.2048, 0.1815], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040480673313141 tensor([0.1767, 0.2150, 0.2040, 0.2365, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21274691820144653 tensor([0.2127, 0.1930, 0.1839, 0.1667, 0.2437], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2147538959980011 tensor([0.2148, 0.1925, 0.1971, 0.1732, 0.2224], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19936096668243408 tensor([0.2203, 0.1994, 0.1939, 0.1607, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040855884552002 tensor([0.1977, 0.2041, 0.2062, 0.2005, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19759707152843475 tensor([0.2069, 0.1976, 0.2024, 0.1938, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2203785479068756 tensor([0.2204, 0.1926, 0.1850, 0.1536, 0.2484], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20131072402000427 tensor([0.2082, 0.2013, 0.2039, 0.1862, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2247297316789627 tensor([0.1745, 0.2247, 0.1982, 0.2362, 0.1664], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20294129848480225 tensor([0.1838, 0.2055, 0.2029, 0.2284, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20391368865966797 tensor([0.1735, 0.2050, 0.2039, 0.2538, 0.1638], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1972821056842804 tensor([0.2197, 0.1973, 0.1888, 0.1579, 0.2363], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22489586472511292 tensor([0.2249, 0.1912, 0.1966, 0.1618, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20407447218894958 tensor([0.1934, 0.2129, 0.2051, 0.2041, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1998339593410492 tensor([0.1990, 0.1998, 0.2073, 0.2056, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20010557770729065 tensor([0.1730, 0.2130, 0.2001, 0.2471, 0.1668], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21864359080791473 tensor([0.2186, 0.1952, 0.1870, 0.1577, 0.2415], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2008243203163147 tensor([0.2008, 0.2061, 0.1949, 0.1916, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19967062771320343 tensor([0.1776, 0.2203, 0.1997, 0.2301, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20477093756198883 tensor([0.1848, 0.2224, 0.2048, 0.2112, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2030434012413025 tensor([0.1692, 0.2030, 0.2040, 0.2644, 0.1593], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22544428706169128 tensor([0.2254, 0.1849, 0.1801, 0.1453, 0.2643], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22350391745567322 tensor([0.2235, 0.1894, 0.1957, 0.1643, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2006112039089203 tensor([0.1924, 0.2167, 0.2006, 0.2051, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20892424881458282 tensor([0.1921, 0.2099, 0.2089, 0.2119, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2034066915512085 tensor([0.1946, 0.2053, 0.2034, 0.2056, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21337063610553741 tensor([0.2134, 0.1940, 0.1882, 0.1679, 0.2366], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22583629190921783 tensor([0.2258, 0.1910, 0.1892, 0.1570, 0.2370], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2026425302028656 tensor([0.2065, 0.2030, 0.2026, 0.1867, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20081016421318054 tensor([0.2008, 0.2011, 0.2057, 0.1995, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20391879975795746 tensor([0.1963, 0.2071, 0.2039, 0.2041, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22396540641784668 tensor([0.2240, 0.1844, 0.1813, 0.1517, 0.2587], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2005506008863449 tensor([0.2188, 0.1936, 0.2006, 0.1759, 0.2112], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19904905557632446 tensor([0.1914, 0.2158, 0.1990, 0.2078, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20132090151309967 tensor([0.2026, 0.2078, 0.2013, 0.1872, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20311133563518524 tensor([0.1977, 0.2040, 0.2078, 0.2031, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22849352657794952 tensor([0.2285, 0.1821, 0.1765, 0.1379, 0.2751], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2036389857530594 tensor([0.2089, 0.2036, 0.2063, 0.1909, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1986909806728363 tensor([0.1739, 0.2276, 0.1987, 0.2339, 0.1660], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20314911007881165 tensor([0.1935, 0.2031, 0.2083, 0.2122, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20175115764141083 tensor([0.1680, 0.2158, 0.2018, 0.2556, 0.1589], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2205265760421753 tensor([0.2205, 0.1906, 0.1867, 0.1577, 0.2444], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20076121389865875 tensor([0.1994, 0.2070, 0.2008, 0.1997, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19948537647724152 tensor([0.1728, 0.2205, 0.1995, 0.2402, 0.1670], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20457828044891357 tensor([0.1998, 0.2096, 0.2046, 0.1979, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20066536962985992 tensor([0.2048, 0.2053, 0.2007, 0.1882, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21484878659248352 tensor([0.2148, 0.1952, 0.1903, 0.1686, 0.2311], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22630512714385986 tensor([0.2263, 0.1904, 0.1960, 0.1600, 0.2272], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2005629539489746 tensor([0.2034, 0.2082, 0.2006, 0.1895, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20773018896579742 tensor([0.1876, 0.2077, 0.2103, 0.2213, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20415040850639343 tensor([0.1759, 0.2042, 0.2076, 0.2494, 0.1629], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21835091710090637 tensor([0.2184, 0.1918, 0.1876, 0.1593, 0.2429], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19892795383930206 tensor([0.2166, 0.1989, 0.1972, 0.1683, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19883210957050323 tensor([0.1948, 0.2117, 0.1988, 0.2013, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19953951239585876 tensor([0.2004, 0.1995, 0.2080, 0.2001, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20664887130260468 tensor([0.1706, 0.2096, 0.2066, 0.2558, 0.1574], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22360675036907196 tensor([0.2236, 0.1912, 0.1795, 0.1448, 0.2609], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20367799699306488 tensor([0.2143, 0.2037, 0.1949, 0.1696, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20761215686798096 tensor([0.2004, 0.2126, 0.1956, 0.1838, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20202748477458954 tensor([0.2020, 0.2078, 0.2103, 0.1904, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20697778463363647 tensor([0.1863, 0.2097, 0.2070, 0.2225, 0.1745], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22690130770206451 tensor([0.2269, 0.1880, 0.1814, 0.1442, 0.2595], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2185111939907074 tensor([0.2185, 0.1951, 0.1938, 0.1691, 0.2235], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20359432697296143 tensor([0.1809, 0.2167, 0.2036, 0.2274, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21026627719402313 tensor([0.1854, 0.2103, 0.2113, 0.2223, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19935698807239532 tensor([0.1718, 0.2137, 0.1994, 0.2477, 0.1674], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2197454869747162 tensor([0.2197, 0.1905, 0.1864, 0.1571, 0.2462], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22473816573619843 tensor([0.2247, 0.1912, 0.1939, 0.1592, 0.2310], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2010500580072403 tensor([0.2011, 0.2018, 0.2074, 0.2025, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19955694675445557 tensor([0.2159, 0.2043, 0.1996, 0.1655, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20178763568401337 tensor([0.1717, 0.2054, 0.2018, 0.2571, 0.1640], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2239682674407959 tensor([0.2240, 0.1924, 0.1814, 0.1456, 0.2566], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19969457387924194 tensor([0.2073, 0.2052, 0.1997, 0.1821, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2102668285369873 tensor([0.1983, 0.2103, 0.1919, 0.1884, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20676249265670776 tensor([0.2090, 0.2068, 0.1944, 0.1740, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994166374206543 tensor([0.1739, 0.2097, 0.1994, 0.2453, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2175549417734146 tensor([0.2176, 0.1932, 0.1927, 0.1668, 0.2298], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21873006224632263 tensor([0.2192, 0.1933, 0.1970, 0.1718, 0.2187], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988457590341568 tensor([0.2070, 0.1999, 0.1988, 0.1865, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20766884088516235 tensor([0.1856, 0.2119, 0.2077, 0.2202, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20471739768981934 tensor([0.1644, 0.2095, 0.2047, 0.2689, 0.1525], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22024595737457275 tensor([0.2202, 0.1904, 0.1846, 0.1585, 0.2463], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2030433565378189 tensor([0.2156, 0.1955, 0.2030, 0.1786, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20063677430152893 tensor([0.2147, 0.2006, 0.1937, 0.1699, 0.2210], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2031339854001999 tensor([0.2031, 0.2037, 0.2037, 0.1916, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20033575594425201 tensor([0.1959, 0.2041, 0.2003, 0.2048, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22499534487724304 tensor([0.2250, 0.1912, 0.1821, 0.1446, 0.2571], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22921782732009888 tensor([0.2292, 0.1880, 0.1941, 0.1556, 0.2332], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20393671095371246 tensor([0.1843, 0.2177, 0.2039, 0.2212, 0.1728], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20198893547058105 tensor([0.1978, 0.2020, 0.2072, 0.2034, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200293630361557 tensor([0.1748, 0.2158, 0.2003, 0.2404, 0.1688], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2301415205001831 tensor([0.2301, 0.1847, 0.1877, 0.1470, 0.2504], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20120251178741455 tensor([0.2135, 0.2012, 0.2012, 0.1775, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1998707801103592 tensor([0.2092, 0.1999, 0.1953, 0.1790, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19868193566799164 tensor([0.2080, 0.2049, 0.1987, 0.1773, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20527392625808716 tensor([0.1880, 0.2053, 0.2093, 0.2224, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1961224228143692 tensor([0.2151, 0.1961, 0.1822, 0.1580, 0.2486], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20098410546779633 tensor([0.2143, 0.1946, 0.2010, 0.1825, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20003820955753326 tensor([0.2137, 0.1964, 0.2000, 0.1792, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20918798446655273 tensor([0.1787, 0.2092, 0.2125, 0.2386, 0.1610], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2027430683374405 tensor([0.1863, 0.2027, 0.2086, 0.2309, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1985151767730713 tensor([0.2141, 0.1985, 0.1917, 0.1694, 0.2263], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997278481721878 tensor([0.2051, 0.2004, 0.1997, 0.1916, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20519883930683136 tensor([0.2052, 0.2091, 0.1939, 0.1794, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20717592537403107 tensor([0.1879, 0.2072, 0.2089, 0.2206, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2011873573064804 tensor([0.1687, 0.2098, 0.2012, 0.2590, 0.1614], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22161151468753815 tensor([0.2216, 0.1874, 0.1890, 0.1596, 0.2423], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19841724634170532 tensor([0.2183, 0.1989, 0.1984, 0.1692, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2034696787595749 tensor([0.1784, 0.2116, 0.2035, 0.2360, 0.1704], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20842938125133514 tensor([0.1869, 0.2113, 0.2084, 0.2197, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20172971487045288 tensor([0.1960, 0.2051, 0.2017, 0.2055, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22504137456417084 tensor([0.2250, 0.1883, 0.1840, 0.1483, 0.2543], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19874216616153717 tensor([0.2211, 0.1912, 0.1987, 0.1656, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22017940878868103 tensor([0.2202, 0.1907, 0.1884, 0.1581, 0.2427], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2031564861536026 tensor([0.1956, 0.2032, 0.2082, 0.2077, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20098602771759033 tensor([0.2010, 0.2078, 0.2049, 0.1931, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2144799828529358 tensor([0.2145, 0.1919, 0.1901, 0.1682, 0.2352], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21237602829933167 tensor([0.2195, 0.1955, 0.1980, 0.1746, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19893231987953186 tensor([0.2071, 0.2006, 0.1989, 0.1852, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20598220825195312 tensor([0.1913, 0.2090, 0.2060, 0.2114, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20277923345565796 tensor([0.1706, 0.2200, 0.2028, 0.2474, 0.1592], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22512277960777283 tensor([0.2251, 0.1903, 0.1878, 0.1518, 0.2449], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19903646409511566 tensor([0.2245, 0.1906, 0.1990, 0.1644, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2125190794467926 tensor([0.1873, 0.2125, 0.1959, 0.2170, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.201875239610672 tensor([0.1913, 0.2118, 0.2019, 0.2098, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20778478682041168 tensor([0.1870, 0.2085, 0.2078, 0.2206, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21918022632598877 tensor([0.2192, 0.1925, 0.1865, 0.1575, 0.2444], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21851159632205963 tensor([0.2185, 0.1931, 0.1932, 0.1699, 0.2253], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007448822259903 tensor([0.1870, 0.2163, 0.2007, 0.2143, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20820169150829315 tensor([0.1913, 0.2104, 0.2082, 0.2135, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20640765130519867 tensor([0.1867, 0.2064, 0.2077, 0.2261, 0.1731], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21695126593112946 tensor([0.2170, 0.1939, 0.1879, 0.1621, 0.2391], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2023567408323288 tensor([0.2038, 0.2024, 0.2042, 0.1945, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20168571174144745 tensor([0.1874, 0.2207, 0.2017, 0.2105, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20389361679553986 tensor([0.1973, 0.2135, 0.2039, 0.1969, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20104938745498657 tensor([0.1923, 0.2027, 0.2010, 0.2146, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2185780256986618 tensor([0.2186, 0.1916, 0.1846, 0.1575, 0.2476], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19995437562465668 tensor([0.2158, 0.2000, 0.1968, 0.1704, 0.2171], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20507638156414032 tensor([0.1904, 0.2156, 0.2051, 0.2078, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20752565562725067 tensor([0.1992, 0.2084, 0.2075, 0.1984, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2025000900030136 tensor([0.1652, 0.2025, 0.2025, 0.2742, 0.1556], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21992988884449005 tensor([0.2199, 0.1933, 0.1837, 0.1534, 0.2497], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20055986940860748 tensor([0.2169, 0.1935, 0.2006, 0.1761, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2037997543811798 tensor([0.1773, 0.2193, 0.2038, 0.2331, 0.1665], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2027050107717514 tensor([0.2027, 0.2057, 0.2053, 0.1887, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2037598192691803 tensor([0.1626, 0.2065, 0.2038, 0.2774, 0.1497], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22254537045955658 tensor([0.2225, 0.1869, 0.1805, 0.1464, 0.2637], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22943606972694397 tensor([0.2294, 0.1889, 0.1937, 0.1564, 0.2316], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19993077218532562 tensor([0.1756, 0.2273, 0.1999, 0.2280, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20442388951778412 tensor([0.1942, 0.2128, 0.2075, 0.2044, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20141850411891937 tensor([0.2150, 0.2014, 0.1933, 0.1647, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21973957121372223 tensor([0.2197, 0.1954, 0.1882, 0.1606, 0.2361], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2231646627187729 tensor([0.2232, 0.1925, 0.1967, 0.1628, 0.2248], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19915664196014404 tensor([0.2109, 0.2006, 0.1992, 0.1794, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20227187871932983 tensor([0.2075, 0.2056, 0.2023, 0.1819, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2146468162536621 tensor([0.1857, 0.2146, 0.1968, 0.2151, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22173035144805908 tensor([0.2217, 0.1880, 0.1794, 0.1514, 0.2594], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 2], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1], [4, 0], [4, 0], [4, 3, 0], [3, 1, 2], [3, 4, 1], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 4, 2], [3, 2], [4, 3, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 3, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 3], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 1], [4, 3, 0], [3, 2, 1], [3, 1], [4, 3, 0, 2], [4, 0], [3, 4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 3, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 1, 2], [4, 3], [4, 0, 2], [4, 0], [3, 2], [3, 1, 2], [3, 4, 1], [3, 2], [4, 3, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 0], [3, 4, 1], [3, 2, 1], [3, 4], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0, 1], [4, 0], [3, 2, 1], [3, 2], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [3, 4], [4, 3], [4, 0], [3, 2, 1], [3, 1], [4, 0, 2], [3, 4], [4, 0], [3, 2, 1], [4, 3], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [4, 3, 0], [4, 0, 2], [4, 3, 0], [3, 4], [3, 2, 1], [3, 1, 2], [3, 4], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 1], [4, 0], [3, 2, 1], [3, 2], [3, 2, 0], [4, 3], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3], [3, 2], [4, 0], [3, 2, 1], [3, 2], [3, 2, 0], [3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1], [3, 2], [3, 4], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [4, 0], [3, 1, 2], [3], [3, 2, 1], [3, 2], [4, 0], [3, 2, 1], [3, 1], [3, 1], [4, 0], [4, 0], [3, 2, 1], [3, 4, 2], [3, 2], [4, 0], [4, 0], [3, 1, 2], [3, 2], [4, 0], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0], [4, 3], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 0], [4, 0], [3, 2, 1], [4, 3], [4, 0], [4, 3, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0], [4, 3, 0], [4, 0], [3, 2, 1], [3, 1], [4, 0], [3, 4], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [3, 2], [3, 2, 1], [3, 1, 2], [3, 2], [3, 4], [0, 4, 2], [3, 2, 1]]\n",
      "[[0, 1, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [1, 2], [0, 4], [0, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1], [0, 1, 4], [1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2], [0, 4], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [0, 2], [1, 2], [0, 4], [0, 2, 4], [1], [1, 2, 3], [1], [0, 1, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [1, 2], [1, 2, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2], [1, 3], [1, 2, 3], [0, 1, 4], [0, 4], [0, 2], [0, 1, 4], [1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [1, 2], [0, 2], [0, 4], [0, 1, 2], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [2, 3], [1, 2, 3], [0, 4], [0, 1, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 1, 2], [0, 1, 2], [1, 2, 3], [0, 4], [0, 2, 4], [1, 3], [0, 1, 2], [1, 2, 3], [0, 4], [0, 1, 2], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [1, 2], [1, 3], [1, 2], [0, 1, 2], [0, 4], [0, 4], [0, 1, 2], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [2], [1, 2, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 1, 2], [0, 1, 4], [1, 2, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 4], [1, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2, 4], [0, 4], [0, 1, 4], [1, 2, 3], [0, 4], [0, 2, 4], [0, 2, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [1, 2, 3], [0, 1, 2], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 1, 2], [1, 2, 3], [1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2], [1, 2, 3], [0, 4], [0, 2, 4], [1, 2, 3], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [0, 1, 4], [0, 4], [0, 4], [0, 1, 4], [0, 1, 2], [1, 3], [0, 4]]\n",
      "NL_pred of 3th iteration [[3, 2], [3, 2], [3, 4, 1], [3, 1, 2], [3, 4, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 1, 2], [3, 2, 1], [3, 1], [3, 2, 1], [3, 2, 1], [4, 3, 1], [4, 3, 0, 2], [3, 4, 0, 2], [3, 1, 2], [3, 2, 1], [3, 1, 2], [3, 2], [3, 4, 1], [3, 2], [3, 2, 1], [3, 2, 1], [3, 4, 1], [3, 2, 1], [4, 0, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 0, 2], [4, 3, 0, 1], [4, 0, 2], [3, 2], [3, 2], [4, 0, 2], [3, 2], [3, 2, 1], [3, 2], [3, 2, 1], [3, 2, 1], [3, 4, 2], [3, 2], [3, 1, 2], [3, 2], [3, 1, 2], [3, 2, 1], [4, 0, 2], [3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.02522419013229071  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.025223470201679303  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.025222091113819796  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.025220134679008934  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.025217645308550668  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.025214681438371248  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.025211296829522823  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.02520753121843525  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.025203419666664274  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.025198999573202693  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.025194298987295114  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.02518935764537138  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.025184189572053796  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.025178820479149912  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.025173280753341375  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.025167575069502287  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0251617291394402  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.02515575698777741  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.025149667964262122  Accuracy on Support set:0.0\n",
      "torch.Size([51, 2048]) torch.Size([51])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.02514348076839073  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20124979317188263 tensor([0.2160, 0.2012, 0.1953, 0.1705, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19870130717754364 tensor([0.2096, 0.1987, 0.1943, 0.1874, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20291925966739655 tensor([0.2003, 0.2048, 0.2029, 0.2047, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20080457627773285 tensor([0.1797, 0.2008, 0.2022, 0.2476, 0.1697], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2235037386417389 tensor([0.2235, 0.1914, 0.1848, 0.1571, 0.2431], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1966758817434311 tensor([0.2271, 0.1889, 0.1967, 0.1659, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20045912265777588 tensor([0.1918, 0.2122, 0.2005, 0.2108, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20060589909553528 tensor([0.1905, 0.2109, 0.2006, 0.2139, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19581523537635803 tensor([0.2021, 0.2017, 0.1958, 0.2001, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22252877056598663 tensor([0.2225, 0.1863, 0.1861, 0.1620, 0.2431], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20699231326580048 tensor([0.2159, 0.1961, 0.2070, 0.1833, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19654880464076996 tensor([0.1895, 0.2223, 0.1965, 0.2091, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20375986397266388 tensor([0.1943, 0.2038, 0.2056, 0.2150, 0.1814], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20461200177669525 tensor([0.1823, 0.2046, 0.2082, 0.2377, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22265994548797607 tensor([0.2227, 0.1911, 0.1854, 0.1591, 0.2417], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21863901615142822 tensor([0.2186, 0.1880, 0.1940, 0.1730, 0.2264], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.196085587143898 tensor([0.1735, 0.2166, 0.1961, 0.2475, 0.1664], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20127488672733307 tensor([0.1893, 0.2013, 0.2056, 0.2276, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972164362668991 tensor([0.1825, 0.2017, 0.1972, 0.2410, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22147510945796967 tensor([0.2215, 0.1885, 0.1848, 0.1607, 0.2445], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1970076858997345 tensor([0.2076, 0.1970, 0.1951, 0.1933, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1979864239692688 tensor([0.2127, 0.1980, 0.1928, 0.1792, 0.2172], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20189210772514343 tensor([0.2023, 0.2019, 0.2038, 0.2006, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19869859516620636 tensor([0.1788, 0.2200, 0.1987, 0.2300, 0.1726], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21857662498950958 tensor([0.2186, 0.1961, 0.1848, 0.1602, 0.2403], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22133639454841614 tensor([0.2213, 0.1920, 0.1877, 0.1690, 0.2300], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20077961683273315 tensor([0.1994, 0.2031, 0.2008, 0.2063, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20004738867282867 tensor([0.2011, 0.2064, 0.2000, 0.2003, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2181292027235031 tensor([0.2181, 0.1939, 0.1928, 0.1727, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22602951526641846 tensor([0.2260, 0.1803, 0.1739, 0.1445, 0.2752], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.217864528298378 tensor([0.2179, 0.1908, 0.1943, 0.1751, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21948257088661194 tensor([0.1837, 0.2221, 0.1913, 0.2195, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2018604427576065 tensor([0.1862, 0.2097, 0.2019, 0.2248, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19864387810230255 tensor([0.1694, 0.2039, 0.1986, 0.2663, 0.1617], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22331956028938293 tensor([0.2233, 0.1853, 0.1765, 0.1492, 0.2657], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22499604523181915 tensor([0.2250, 0.1888, 0.1874, 0.1640, 0.2348], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22046880424022675 tensor([0.2205, 0.1903, 0.1952, 0.1710, 0.2230], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.200687438249588 tensor([0.2058, 0.2007, 0.2035, 0.1942, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19591419398784637 tensor([0.1785, 0.2104, 0.1959, 0.2400, 0.1752], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21395078301429749 tensor([0.2140, 0.1950, 0.1843, 0.1710, 0.2358], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22196801006793976 tensor([0.2220, 0.1897, 0.1912, 0.1697, 0.2274], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21818022429943085 tensor([0.1777, 0.2182, 0.1934, 0.2342, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19948013126850128 tensor([0.1978, 0.2037, 0.1995, 0.2074, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19756807386875153 tensor([0.1784, 0.1976, 0.2013, 0.2527, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22519612312316895 tensor([0.2252, 0.1843, 0.1836, 0.1539, 0.2530], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19449129700660706 tensor([0.2225, 0.1938, 0.1945, 0.1648, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20109893381595612 tensor([0.2188, 0.2011, 0.1932, 0.1671, 0.2199], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022605985403061 tensor([0.1931, 0.2023, 0.2048, 0.2182, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1981133222579956 tensor([0.1706, 0.2050, 0.1981, 0.2629, 0.1633], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22750157117843628 tensor([0.2275, 0.1860, 0.1778, 0.1480, 0.2606], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21829169988632202 tensor([0.2183, 0.1886, 0.1903, 0.1742, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20171381533145905 tensor([0.2091, 0.2017, 0.1875, 0.1806, 0.2211], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20111149549484253 tensor([0.1952, 0.2086, 0.2011, 0.2083, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19721093773841858 tensor([0.1879, 0.2050, 0.1972, 0.2252, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22223582863807678 tensor([0.2222, 0.1936, 0.1836, 0.1580, 0.2426], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22927480936050415 tensor([0.2293, 0.1874, 0.1926, 0.1598, 0.2309], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21905680000782013 tensor([0.2191, 0.1944, 0.1901, 0.1681, 0.2284], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20105628669261932 tensor([0.2038, 0.1964, 0.2011, 0.1968, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972847729921341 tensor([0.1975, 0.2091, 0.1973, 0.1991, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2266249656677246 tensor([0.2266, 0.1886, 0.1847, 0.1534, 0.2467], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969299167394638 tensor([0.2190, 0.1918, 0.1969, 0.1765, 0.2157], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20392508804798126 tensor([0.1901, 0.2112, 0.2039, 0.2128, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20333360135555267 tensor([0.2104, 0.2033, 0.1903, 0.1749, 0.2211], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22065582871437073 tensor([0.2214, 0.1904, 0.1950, 0.1725, 0.2207], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20220789313316345 tensor([0.2106, 0.2022, 0.1912, 0.1774, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.202310249209404 tensor([0.1853, 0.2130, 0.2023, 0.2267, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19677600264549255 tensor([0.1766, 0.2052, 0.1968, 0.2500, 0.1715], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21346743404865265 tensor([0.2135, 0.1952, 0.1924, 0.1787, 0.2202], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2232193648815155 tensor([0.2258, 0.1927, 0.1937, 0.1646, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2285439372062683 tensor([0.2285, 0.1866, 0.1889, 0.1543, 0.2416], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19997286796569824 tensor([0.2013, 0.2065, 0.2000, 0.1965, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1997324377298355 tensor([0.1780, 0.1997, 0.2036, 0.2512, 0.1674], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22100546956062317 tensor([0.2210, 0.1868, 0.1817, 0.1613, 0.2492], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22297930717468262 tensor([0.2230, 0.1929, 0.1938, 0.1673, 0.2230], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20024150609970093 tensor([0.2128, 0.2002, 0.1889, 0.1740, 0.2241], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20099055767059326 tensor([0.1857, 0.2059, 0.2010, 0.2261, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2037273347377777 tensor([0.1778, 0.2102, 0.2037, 0.2419, 0.1663], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22612999379634857 tensor([0.2261, 0.1877, 0.1871, 0.1564, 0.2426], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1976989358663559 tensor([0.2060, 0.1977, 0.2003, 0.1951, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.216802716255188 tensor([0.1847, 0.2219, 0.1908, 0.2168, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19817520678043365 tensor([0.1983, 0.2006, 0.1982, 0.2067, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19872932136058807 tensor([0.2091, 0.1987, 0.1945, 0.1854, 0.2123], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23106572031974792 tensor([0.2311, 0.1820, 0.1835, 0.1481, 0.2554], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20119018852710724 tensor([0.2182, 0.1941, 0.2012, 0.1769, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19940026104450226 tensor([0.2111, 0.1994, 0.1951, 0.1799, 0.2145], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20403927564620972 tensor([0.2020, 0.2074, 0.2040, 0.1943, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19845278561115265 tensor([0.1738, 0.2071, 0.1985, 0.2546, 0.1660], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22307837009429932 tensor([0.2231, 0.1879, 0.1807, 0.1575, 0.2507], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21863621473312378 tensor([0.2186, 0.1961, 0.1868, 0.1666, 0.2318], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997925490140915 tensor([0.1934, 0.2079, 0.1998, 0.2137, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2023124247789383 tensor([0.1970, 0.2099, 0.2023, 0.2047, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19991326332092285 tensor([0.1795, 0.2120, 0.1999, 0.2365, 0.1721], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21546362340450287 tensor([0.2155, 0.1895, 0.1796, 0.1662, 0.2493], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21765069663524628 tensor([0.2177, 0.1891, 0.1926, 0.1728, 0.2278], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22324463725090027 tensor([0.2232, 0.1958, 0.1895, 0.1603, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20091408491134644 tensor([0.2006, 0.2009, 0.2017, 0.2003, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19797776639461517 tensor([0.2099, 0.1944, 0.1980, 0.1935, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2231132984161377 tensor([0.2231, 0.1890, 0.1806, 0.1531, 0.2541], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.198034405708313 tensor([0.2112, 0.1980, 0.1994, 0.1859, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22166258096694946 tensor([0.1772, 0.2217, 0.1941, 0.2363, 0.1707], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19871284067630768 tensor([0.1866, 0.2025, 0.1987, 0.2282, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19976267218589783 tensor([0.1763, 0.2021, 0.1998, 0.2537, 0.1681], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2225494086742401 tensor([0.2225, 0.1937, 0.1844, 0.1575, 0.2418], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22787724435329437 tensor([0.2279, 0.1878, 0.1920, 0.1613, 0.2310], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007911652326584 tensor([0.1964, 0.2097, 0.2008, 0.2039, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20288275182247162 tensor([0.2020, 0.1967, 0.2029, 0.2053, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.196035698056221 tensor([0.1757, 0.2100, 0.1960, 0.2470, 0.1712], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22144922614097595 tensor([0.2214, 0.1916, 0.1826, 0.1572, 0.2472], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2027272880077362 tensor([0.2037, 0.2027, 0.1906, 0.1913, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2172049582004547 tensor([0.1803, 0.2172, 0.1956, 0.2301, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20056526362895966 tensor([0.1877, 0.2192, 0.2006, 0.2111, 0.1814], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19996224343776703 tensor([0.1719, 0.2003, 0.2000, 0.2643, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22799716889858246 tensor([0.2280, 0.1815, 0.1757, 0.1448, 0.2699], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22637508809566498 tensor([0.2264, 0.1861, 0.1911, 0.1638, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19636239111423492 tensor([0.1953, 0.2134, 0.1964, 0.2050, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20462548732757568 tensor([0.1950, 0.2068, 0.2046, 0.2119, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.199057936668396 tensor([0.1975, 0.2021, 0.1991, 0.2054, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21615898609161377 tensor([0.2162, 0.1905, 0.1838, 0.1674, 0.2422], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.228616863489151 tensor([0.2286, 0.1875, 0.1848, 0.1565, 0.2425], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19818419218063354 tensor([0.2095, 0.1997, 0.1982, 0.1864, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1978677660226822 tensor([0.2038, 0.1979, 0.2012, 0.1993, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19958844780921936 tensor([0.1992, 0.2039, 0.1996, 0.2039, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22667816281318665 tensor([0.2267, 0.1811, 0.1770, 0.1510, 0.2642], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19601483643054962 tensor([0.2218, 0.1903, 0.1960, 0.1755, 0.2163], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20763707160949707 tensor([0.1943, 0.2125, 0.1948, 0.2076, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19692277908325195 tensor([0.2055, 0.2044, 0.1969, 0.1869, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2008269727230072 tensor([0.2007, 0.2008, 0.2033, 0.2029, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23052828013896942 tensor([0.2305, 0.1792, 0.1729, 0.1380, 0.2793], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20042169094085693 tensor([0.2120, 0.2004, 0.2019, 0.1907, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2244327813386917 tensor([0.1766, 0.2244, 0.1947, 0.2340, 0.1703], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20001812279224396 tensor([0.1965, 0.2000, 0.2039, 0.2120, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19774016737937927 tensor([0.1706, 0.2129, 0.1977, 0.2558, 0.1630], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22328315675258636 tensor([0.2233, 0.1872, 0.1823, 0.1572, 0.2500], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19641779363155365 tensor([0.2024, 0.2037, 0.1964, 0.1994, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21744763851165771 tensor([0.1755, 0.2174, 0.1954, 0.2402, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20019826292991638 tensor([0.2028, 0.2063, 0.2002, 0.1977, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19631390273571014 tensor([0.2077, 0.2020, 0.1963, 0.1879, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21770265698432922 tensor([0.2177, 0.1917, 0.1859, 0.1681, 0.2366], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2292054444551468 tensor([0.2292, 0.1870, 0.1915, 0.1596, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19616495072841644 tensor([0.2064, 0.2048, 0.1962, 0.1892, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20468005537986755 tensor([0.1906, 0.2047, 0.2060, 0.2212, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20129448175430298 tensor([0.1787, 0.2013, 0.2034, 0.2494, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2211492508649826 tensor([0.2211, 0.1883, 0.1832, 0.1588, 0.2486], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21962320804595947 tensor([0.2196, 0.1955, 0.1927, 0.1679, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2011277973651886 tensor([0.1977, 0.2084, 0.1945, 0.2011, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20253713428974152 tensor([0.1733, 0.2067, 0.2025, 0.2559, 0.1616], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22615790367126465 tensor([0.2262, 0.1876, 0.1753, 0.1445, 0.2665], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20020024478435516 tensor([0.2172, 0.2002, 0.1905, 0.1692, 0.2229], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2091529220342636 tensor([0.2033, 0.2092, 0.1913, 0.1835, 0.2127], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20464198291301727 tensor([0.2050, 0.2046, 0.2059, 0.1903, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20271192491054535 tensor([0.1892, 0.2067, 0.2027, 0.2224, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22960340976715088 tensor([0.2296, 0.1844, 0.1770, 0.1436, 0.2654], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2214515060186386 tensor([0.2215, 0.1916, 0.1894, 0.1686, 0.2289], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.199396550655365 tensor([0.1838, 0.2136, 0.1994, 0.2273, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2070399820804596 tensor([0.1883, 0.2072, 0.2070, 0.2223, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2107057422399521 tensor([0.1745, 0.2107, 0.1954, 0.2477, 0.1717], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22251763939857483 tensor([0.2225, 0.1869, 0.1820, 0.1566, 0.2520], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22771672904491425 tensor([0.2277, 0.1877, 0.1893, 0.1587, 0.2366], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19865907728672028 tensor([0.2040, 0.1987, 0.2030, 0.2023, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20086638629436493 tensor([0.2190, 0.2009, 0.1951, 0.1651, 0.2199], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19776014983654022 tensor([0.1745, 0.2026, 0.1978, 0.2569, 0.1683], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22672097384929657 tensor([0.2267, 0.1887, 0.1770, 0.1450, 0.2625], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20184326171875 tensor([0.2103, 0.2018, 0.1953, 0.1818, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068498730659485 tensor([0.2011, 0.2068, 0.1877, 0.1881, 0.2163], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20328618586063385 tensor([0.2119, 0.2033, 0.1901, 0.1737, 0.2210], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066967636346817 tensor([0.1767, 0.2067, 0.1953, 0.2452, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22045639157295227 tensor([0.2205, 0.1897, 0.1882, 0.1663, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22212296724319458 tensor([0.2221, 0.1899, 0.1925, 0.1713, 0.2240], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.196638822555542 tensor([0.2099, 0.1966, 0.1944, 0.1861, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20338450372219086 tensor([0.1885, 0.2088, 0.2034, 0.2201, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20072583854198456 tensor([0.1670, 0.2067, 0.2007, 0.2691, 0.1565], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22301465272903442 tensor([0.2230, 0.1868, 0.1802, 0.1580, 0.2520], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19851207733154297 tensor([0.2187, 0.1922, 0.1985, 0.1783, 0.2123], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19711549580097198 tensor([0.2176, 0.1971, 0.1893, 0.1695, 0.2265], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1992964893579483 tensor([0.2061, 0.2005, 0.1993, 0.1913, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19603358209133148 tensor([0.1987, 0.2009, 0.1960, 0.2046, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2277175635099411 tensor([0.2277, 0.1875, 0.1777, 0.1441, 0.2629], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2321041226387024 tensor([0.2321, 0.1846, 0.1895, 0.1551, 0.2386], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997358500957489 tensor([0.1872, 0.2146, 0.1997, 0.2212, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19881655275821686 tensor([0.2008, 0.1988, 0.2028, 0.2032, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19623133540153503 tensor([0.1776, 0.2127, 0.1962, 0.2403, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2328510284423828 tensor([0.2329, 0.1814, 0.1833, 0.1466, 0.2559], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1967208832502365 tensor([0.2165, 0.1979, 0.1967, 0.1772, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21209192276000977 tensor([0.2121, 0.1964, 0.1908, 0.1786, 0.2221], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20153766870498657 tensor([0.2109, 0.2015, 0.1942, 0.1769, 0.2163], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20224875211715698 tensor([0.1909, 0.2022, 0.2049, 0.2223, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21778041124343872 tensor([0.2178, 0.1925, 0.1779, 0.1575, 0.2544], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19648410379886627 tensor([0.2173, 0.1914, 0.1965, 0.1821, 0.2127], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19556449353694916 tensor([0.2167, 0.1931, 0.1956, 0.1788, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206288680434227 tensor([0.1815, 0.2063, 0.2083, 0.2387, 0.1652], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19977928698062897 tensor([0.1892, 0.1998, 0.2043, 0.2309, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.216985821723938 tensor([0.2170, 0.1950, 0.1873, 0.1690, 0.2317], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.197144016623497 tensor([0.2081, 0.1971, 0.1953, 0.1913, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2056410163640976 tensor([0.2081, 0.2056, 0.1895, 0.1790, 0.2177], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040979266166687 tensor([0.1908, 0.2041, 0.2046, 0.2205, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19717150926589966 tensor([0.1713, 0.2069, 0.1972, 0.2591, 0.1655], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22443588078022003 tensor([0.2244, 0.1840, 0.1845, 0.1591, 0.2480], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19552546739578247 tensor([0.2213, 0.1955, 0.1939, 0.1689, 0.2204], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19930298626422882 tensor([0.1812, 0.2086, 0.1993, 0.2360, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2041764259338379 tensor([0.1898, 0.2082, 0.2042, 0.2196, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19738484919071198 tensor([0.1989, 0.2019, 0.1974, 0.2052, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22772623598575592 tensor([0.2277, 0.1848, 0.1796, 0.1479, 0.2600], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22411969304084778 tensor([0.2241, 0.1879, 0.1941, 0.1651, 0.2287], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2229943424463272 tensor([0.2230, 0.1872, 0.1839, 0.1575, 0.2484], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20002061128616333 tensor([0.1985, 0.2000, 0.2039, 0.2075, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20054636895656586 tensor([0.2039, 0.2046, 0.2005, 0.1929, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21727246046066284 tensor([0.2173, 0.1885, 0.1857, 0.1678, 0.2407], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21756604313850403 tensor([0.2226, 0.1921, 0.1935, 0.1742, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1972300112247467 tensor([0.2100, 0.1972, 0.1945, 0.1848, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2016526311635971 tensor([0.1943, 0.2058, 0.2017, 0.2112, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.198753222823143 tensor([0.1733, 0.2170, 0.1988, 0.2476, 0.1634], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2279672622680664 tensor([0.2280, 0.1867, 0.1833, 0.1513, 0.2506], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22673486173152924 tensor([0.2275, 0.1873, 0.1945, 0.1639, 0.2267], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2092771828174591 tensor([0.1901, 0.2093, 0.1917, 0.2167, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19762606918811798 tensor([0.1942, 0.2086, 0.1976, 0.2096, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20350535213947296 tensor([0.1898, 0.2055, 0.2035, 0.2205, 0.1807], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22195343673229218 tensor([0.2220, 0.1889, 0.1821, 0.1570, 0.2501], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22135217487812042 tensor([0.2214, 0.1897, 0.1888, 0.1694, 0.2307], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1964946836233139 tensor([0.1899, 0.2131, 0.1965, 0.2142, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2038792222738266 tensor([0.1943, 0.2073, 0.2039, 0.2134, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2034161239862442 tensor([0.1895, 0.2034, 0.2034, 0.2260, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21976414322853088 tensor([0.2198, 0.1904, 0.1835, 0.1615, 0.2448], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19915804266929626 tensor([0.2068, 0.1992, 0.1998, 0.1943, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19744323194026947 tensor([0.1903, 0.2174, 0.1974, 0.2104, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1995638608932495 tensor([0.2002, 0.2102, 0.1996, 0.1968, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19671319425106049 tensor([0.1952, 0.1996, 0.1967, 0.2143, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22132806479930878 tensor([0.2213, 0.1880, 0.1802, 0.1570, 0.2534], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21876706182956696 tensor([0.2188, 0.1965, 0.1924, 0.1701, 0.2223], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007811814546585 tensor([0.1933, 0.2124, 0.2008, 0.2077, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20314085483551025 tensor([0.2022, 0.2052, 0.2031, 0.1982, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19872602820396423 tensor([0.1680, 0.2002, 0.1987, 0.2736, 0.1595], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2226758748292923 tensor([0.2227, 0.1897, 0.1793, 0.1529, 0.2553], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19598908722400665 tensor([0.2199, 0.1902, 0.1960, 0.1757, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19967101514339447 tensor([0.1802, 0.2162, 0.1997, 0.2331, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2008931189775467 tensor([0.2057, 0.2024, 0.2009, 0.1885, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19982203841209412 tensor([0.1652, 0.2038, 0.1998, 0.2774, 0.1537], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22515393793582916 tensor([0.2252, 0.1834, 0.1761, 0.1459, 0.2694], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23242425918579102 tensor([0.2324, 0.1854, 0.1892, 0.1559, 0.2371], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22428439557552338 tensor([0.1783, 0.2243, 0.1958, 0.2281, 0.1734], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20316509902477264 tensor([0.1972, 0.2096, 0.2032, 0.2043, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19789494574069977 tensor([0.2179, 0.1979, 0.1888, 0.1642, 0.2311], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22262033820152283 tensor([0.2226, 0.1918, 0.1837, 0.1601, 0.2417], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2261391133069992 tensor([0.2261, 0.1891, 0.1921, 0.1623, 0.2303], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1972600668668747 tensor([0.2139, 0.1973, 0.1947, 0.1791, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978561133146286 tensor([0.2105, 0.2023, 0.1979, 0.1816, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21133564412593842 tensor([0.1885, 0.2113, 0.1926, 0.2149, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22443720698356628 tensor([0.2244, 0.1845, 0.1752, 0.1507, 0.2652], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 2], [3, 2, 1], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0], [4, 3, 0, 2], [3, 1, 2], [3, 4, 1], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 4, 2, 1], [3, 2, 1], [4, 3, 0], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0], [4, 3, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 3], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 1], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 1], [4, 3, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3, 0, 2], [4, 0], [3, 4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 3, 0, 2], [4, 0, 1], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0], [4, 0], [3, 1, 2], [4, 3, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 4, 1], [3, 2, 1], [4, 3, 0], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 0], [3, 4, 1, 2], [3, 2, 1], [3, 4, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0], [4, 0, 1], [4, 0, 2], [3, 2, 1], [3, 2], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 4, 2], [4, 3, 1], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 4, 2], [4, 0], [3, 2, 1], [4, 3], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [4, 3, 0], [3, 4, 2], [3, 2, 1], [3, 1, 2], [3, 4, 2], [4, 0], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 1], [4, 0], [3, 2, 1], [3, 2], [3, 2, 0], [4, 3], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3, 1], [3, 2], [4, 0, 2], [3, 2, 1], [3, 2], [3, 2, 0], [3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0], [4, 0], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 1], [4, 0, 2], [3, 1, 2], [3, 2], [3, 2, 1], [3, 2], [4, 0], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 0], [4, 0, 1], [3, 2, 1], [3, 4, 2, 1], [3, 2], [4, 0], [4, 0, 2], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0], [4, 3], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0], [4, 0], [3, 2, 1], [4, 3, 1], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0], [4, 3, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 4], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2], [0, 4, 2], [3, 2, 1]]\n",
      "[[0, 1, 4], [0, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [1], [0, 4], [0, 2], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0], [0, 4], [1, 2], [1, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2], [0, 4], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 1, 2], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [2, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 2], [1], [0, 4], [0, 4], [1], [1, 2, 3], [1], [0, 1, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1], [2, 3], [0, 4], [0, 4], [0, 1, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2], [1, 3], [1, 3], [0, 4], [0, 4], [0, 2], [0, 4], [1, 2], [1, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1, 2], [0], [0, 4], [0, 2], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [1, 2, 3], [2, 3], [1, 3], [0, 4], [0, 1, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 1], [0, 2], [1, 3], [0, 4], [0, 4], [1, 3], [0, 1], [1, 2, 3], [0, 4], [0, 1, 2], [1, 3], [1, 2, 3], [1, 3], [0, 4], [1], [1, 3], [1, 2], [0, 1], [0, 4], [0, 4], [0, 1], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [2], [1, 2, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 2], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 2], [0, 1, 4], [1, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 4], [1, 3], [0, 4], [0, 4], [0, 4], [1, 2, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [0, 1], [1, 3], [0, 4], [0, 4], [1, 3], [2, 3], [1, 3], [0, 4], [0, 1, 4], [0, 4], [0, 1, 4], [1, 2, 3], [0, 4], [0, 4], [0, 4], [1, 2, 3], [2, 3], [0, 4], [0], [0, 1, 4], [1, 2, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1, 2, 3], [0, 1, 2], [0, 4], [0, 4], [0, 4], [1, 2, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 2, 3], [0, 4], [0, 2], [1, 3], [1], [1, 3], [0, 4], [0, 4], [1, 2, 3], [1, 2], [1, 3], [0, 4], [0, 4], [1, 3], [0, 1, 2], [1, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [0, 4], [0, 4], [0, 4], [0, 4], [0, 1], [1, 3], [0, 4]]\n",
      "NL_pred of 4th iteration [[3, 2, 1], [3, 1, 2], [4, 3, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 4, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 3, 0, 2], [3, 1, 2], [4, 0, 2], [4, 3, 0, 2], [4, 0, 1], [4, 3, 1], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 4, 1, 2], [3, 4, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 4, 2], [4, 3, 1], [4, 0, 2], [3, 1, 2], [3, 4, 2], [4, 0, 2], [4, 3, 0, 2], [3, 4, 2], [3, 4, 2], [4, 0, 2], [4, 3, 1], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2], [4, 0, 2], [4, 0, 2], [4, 0, 1], [4, 0, 2], [3, 2], [3, 1, 2], [3, 1, 2], [4, 0, 1], [3, 4, 2, 1], [4, 0, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 3, 1], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2], [4, 0, 2], [3, 1, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 4, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01606597602367401  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.016065441071987152  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.016064424812793732  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.016062971949577332  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.01606113016605377  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.016058942675590514  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.016056439280509947  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.016053657233715057  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.01605062484741211  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.016047370433807374  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.01604391485452652  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.01604028046131134  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.016036489605903627  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.01603255569934845  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.016028501093387604  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.016024331748485564  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.016020062565803527  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.016015708446502686  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.016011282801628113  Accuracy on Support set:0.0\n",
      "torch.Size([80, 2048]) torch.Size([80])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.016006788611412047  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.19984477758407593 tensor([0.2183, 0.1998, 0.1896, 0.1705, 0.2218], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2118757665157318 tensor([0.2119, 0.1973, 0.1886, 0.1875, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19703949987888336 tensor([0.2026, 0.2036, 0.1970, 0.2050, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19646833837032318 tensor([0.1819, 0.1998, 0.1965, 0.2480, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2255990356206894 tensor([0.2256, 0.1899, 0.1792, 0.1570, 0.2484], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22632919251918793 tensor([0.2293, 0.1876, 0.1909, 0.1659, 0.2263], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19468329846858978 tensor([0.1941, 0.2110, 0.1947, 0.2111, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948804408311844 tensor([0.1927, 0.2098, 0.1949, 0.2142, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22458705306053162 tensor([0.2246, 0.1848, 0.1805, 0.1619, 0.2482], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20110973715782166 tensor([0.2184, 0.1949, 0.2011, 0.1834, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20937219262123108 tensor([0.1917, 0.2210, 0.1910, 0.2094, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997341811656952 tensor([0.1966, 0.2027, 0.1997, 0.2153, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20238281786441803 tensor([0.1845, 0.2036, 0.2024, 0.2383, 0.1712], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22476261854171753 tensor([0.2248, 0.1895, 0.1798, 0.1590, 0.2469], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2207990288734436 tensor([0.2208, 0.1866, 0.1882, 0.1730, 0.2314], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21553543210029602 tensor([0.1756, 0.2155, 0.1905, 0.2480, 0.1703], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19975189864635468 tensor([0.1915, 0.2002, 0.1998, 0.2280, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20057450234889984 tensor([0.1846, 0.2006, 0.1916, 0.2413, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2235409915447235 tensor([0.2235, 0.1870, 0.1792, 0.1606, 0.2497], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21490514278411865 tensor([0.2149, 0.1966, 0.1872, 0.1793, 0.2220], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19790984690189362 tensor([0.2047, 0.2007, 0.1979, 0.2008, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21889598667621613 tensor([0.1809, 0.2189, 0.1930, 0.2305, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22066394984722137 tensor([0.2207, 0.1946, 0.1792, 0.1601, 0.2455], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22341036796569824 tensor([0.2234, 0.1905, 0.1821, 0.1690, 0.2350], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19495442509651184 tensor([0.2017, 0.2019, 0.1950, 0.2064, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1942652463912964 tensor([0.2034, 0.2052, 0.1943, 0.2005, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22034093737602234 tensor([0.2203, 0.1925, 0.1871, 0.1727, 0.2274], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22749470174312592 tensor([0.2275, 0.1793, 0.1694, 0.1447, 0.2791], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22009232640266418 tensor([0.2201, 0.1894, 0.1886, 0.1751, 0.2269], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2197643667459488 tensor([0.1859, 0.2208, 0.1858, 0.2198, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1961168348789215 tensor([0.1885, 0.2086, 0.1961, 0.2253, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20303629338741302 tensor([0.1716, 0.2030, 0.1931, 0.2666, 0.1657], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22517690062522888 tensor([0.2252, 0.1837, 0.1713, 0.1492, 0.2706], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2271096110343933 tensor([0.2271, 0.1873, 0.1818, 0.1638, 0.2400], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22264701128005981 tensor([0.2226, 0.1890, 0.1895, 0.1710, 0.2278], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19759349524974823 tensor([0.2081, 0.1994, 0.1976, 0.1944, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2092294543981552 tensor([0.1807, 0.2092, 0.1903, 0.2403, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2160165160894394 tensor([0.2160, 0.1935, 0.1787, 0.1709, 0.2408], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22415682673454285 tensor([0.2242, 0.1883, 0.1856, 0.1696, 0.2323], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21696403622627258 tensor([0.1798, 0.2170, 0.1879, 0.2345, 0.1808], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20255780220031738 tensor([0.2001, 0.2026, 0.1937, 0.2076, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.195606529712677 tensor([0.1805, 0.1966, 0.1956, 0.2531, 0.1742], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22720907628536224 tensor([0.2272, 0.1827, 0.1780, 0.1538, 0.2583], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22479639947414398 tensor([0.2248, 0.1924, 0.1887, 0.1648, 0.2293], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1996716558933258 tensor([0.2210, 0.1997, 0.1875, 0.1671, 0.2247], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19898635149002075 tensor([0.1954, 0.2011, 0.1990, 0.2185, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040805071592331 tensor([0.1728, 0.2041, 0.1925, 0.2633, 0.1673], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22931402921676636 tensor([0.2293, 0.1845, 0.1725, 0.1478, 0.2658], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2203807234764099 tensor([0.2204, 0.1872, 0.1847, 0.1741, 0.2336], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2002412974834442 tensor([0.2112, 0.2002, 0.1819, 0.1806, 0.2260], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19534331560134888 tensor([0.1975, 0.2074, 0.1953, 0.2086, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20380528271198273 tensor([0.1901, 0.2038, 0.1915, 0.2255, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22434091567993164 tensor([0.2243, 0.1920, 0.1780, 0.1579, 0.2478], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23151855170726776 tensor([0.2315, 0.1859, 0.1868, 0.1597, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22122539579868317 tensor([0.2212, 0.1929, 0.1844, 0.1681, 0.2334], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19524134695529938 tensor([0.2061, 0.1951, 0.1952, 0.1970, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2286953330039978 tensor([0.2287, 0.1870, 0.1792, 0.1533, 0.2519], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2205582559108734 tensor([0.2213, 0.1905, 0.1911, 0.1766, 0.2206], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19813939929008484 tensor([0.1924, 0.2101, 0.1981, 0.2132, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20188063383102417 tensor([0.2125, 0.2019, 0.1846, 0.1750, 0.2260], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22361446917057037 tensor([0.2236, 0.1890, 0.1892, 0.1725, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20078296959400177 tensor([0.2128, 0.2008, 0.1856, 0.1775, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19653573632240295 tensor([0.1876, 0.2119, 0.1965, 0.2271, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20410408079624176 tensor([0.1787, 0.2041, 0.1911, 0.2504, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2156563401222229 tensor([0.2157, 0.1938, 0.1867, 0.1787, 0.2251], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2280365228652954 tensor([0.2280, 0.1914, 0.1879, 0.1646, 0.2281], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.230690598487854 tensor([0.2307, 0.1851, 0.1833, 0.1542, 0.2468], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19780956208705902 tensor([0.1802, 0.1987, 0.1978, 0.2517, 0.1715], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22294650971889496 tensor([0.2229, 0.1853, 0.1763, 0.1611, 0.2544], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22524870932102203 tensor([0.2252, 0.1915, 0.1881, 0.1673, 0.2280], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19874106347560883 tensor([0.2150, 0.1987, 0.1833, 0.1740, 0.2291], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19521525502204895 tensor([0.1879, 0.2048, 0.1952, 0.2264, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1979808211326599 tensor([0.1800, 0.2093, 0.1980, 0.2424, 0.1703], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22824235260486603 tensor([0.2282, 0.1862, 0.1814, 0.1563, 0.2478], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19445669651031494 tensor([0.2083, 0.1964, 0.1945, 0.1952, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21702171862125397 tensor([0.1868, 0.2205, 0.1853, 0.2170, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19935184717178345 tensor([0.2006, 0.1994, 0.1924, 0.2068, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21127620339393616 tensor([0.2113, 0.1974, 0.1888, 0.1854, 0.2171], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2330770194530487 tensor([0.2331, 0.1804, 0.1779, 0.1479, 0.2607], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1953139305114746 tensor([0.2206, 0.1927, 0.1953, 0.1770, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2132921665906906 tensor([0.2133, 0.1980, 0.1894, 0.1800, 0.2193], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1982429027557373 tensor([0.2043, 0.2063, 0.1982, 0.1945, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2061384618282318 tensor([0.1759, 0.2061, 0.1928, 0.2550, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22511325776576996 tensor([0.2251, 0.1863, 0.1751, 0.1573, 0.2561], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22075161337852478 tensor([0.2208, 0.1946, 0.1813, 0.1666, 0.2367], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20677004754543304 tensor([0.1957, 0.2068, 0.1940, 0.2140, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1964721977710724 tensor([0.1993, 0.2087, 0.1965, 0.2050, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21086540818214417 tensor([0.1817, 0.2109, 0.1942, 0.2370, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21747387945652008 tensor([0.2175, 0.1882, 0.1743, 0.1658, 0.2542], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2198096513748169 tensor([0.2198, 0.1877, 0.1869, 0.1727, 0.2328], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22544080018997192 tensor([0.2254, 0.1943, 0.1838, 0.1602, 0.2363], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1958685964345932 tensor([0.2029, 0.1997, 0.1959, 0.2005, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2250138372182846 tensor([0.2250, 0.1874, 0.1752, 0.1530, 0.2594], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19360898435115814 tensor([0.2135, 0.1967, 0.1936, 0.1861, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22050650417804718 tensor([0.1794, 0.2205, 0.1886, 0.2367, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20130544900894165 tensor([0.1888, 0.2013, 0.1930, 0.2285, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20104560256004333 tensor([0.1784, 0.2010, 0.1940, 0.2542, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22454915940761566 tensor([0.2245, 0.1922, 0.1790, 0.1574, 0.2468], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23001216351985931 tensor([0.2300, 0.1864, 0.1863, 0.1613, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19497616589069366 tensor([0.1987, 0.2084, 0.1950, 0.2041, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1970214545726776 tensor([0.2043, 0.1955, 0.1970, 0.2055, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2089162915945053 tensor([0.1779, 0.2089, 0.1904, 0.2474, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22347964346408844 tensor([0.2235, 0.1901, 0.1770, 0.1570, 0.2524], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20134200155735016 tensor([0.2059, 0.2013, 0.1850, 0.1913, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21604309976100922 tensor([0.1825, 0.2160, 0.1899, 0.2304, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19484549760818481 tensor([0.1900, 0.2180, 0.1948, 0.2114, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19946008920669556 tensor([0.1742, 0.1995, 0.1945, 0.2644, 0.1674], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22973547875881195 tensor([0.2297, 0.1802, 0.1708, 0.1447, 0.2746], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22849330306053162 tensor([0.2285, 0.1847, 0.1854, 0.1637, 0.2376], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2051958441734314 tensor([0.1975, 0.2122, 0.1907, 0.2052, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19883215427398682 tensor([0.1973, 0.2057, 0.1988, 0.2123, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20091229677200317 tensor([0.1997, 0.2009, 0.1933, 0.2056, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21819645166397095 tensor([0.2182, 0.1889, 0.1782, 0.1672, 0.2474], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23062649369239807 tensor([0.2306, 0.1860, 0.1793, 0.1565, 0.2476], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19839660823345184 tensor([0.2117, 0.1984, 0.1924, 0.1865, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19537751376628876 tensor([0.2061, 0.1966, 0.1954, 0.1994, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20268718898296356 tensor([0.2015, 0.2027, 0.1938, 0.2041, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22839507460594177 tensor([0.2284, 0.1799, 0.1721, 0.1508, 0.2687], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22119541466236115 tensor([0.2241, 0.1890, 0.1902, 0.1754, 0.2212], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20782099664211273 tensor([0.1966, 0.2112, 0.1892, 0.2078, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20309028029441833 tensor([0.2078, 0.2031, 0.1912, 0.1870, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19744496047496796 tensor([0.2030, 0.1996, 0.1974, 0.2031, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23185387253761292 tensor([0.2319, 0.1782, 0.1685, 0.1383, 0.2832], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19608727097511292 tensor([0.2144, 0.1992, 0.1961, 0.1909, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22328384220600128 tensor([0.1787, 0.2233, 0.1891, 0.2344, 0.1745], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19805899262428284 tensor([0.1988, 0.1989, 0.1981, 0.2123, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2119089961051941 tensor([0.1727, 0.2119, 0.1922, 0.2563, 0.1669], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22526298463344574 tensor([0.2253, 0.1856, 0.1769, 0.1571, 0.2551], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2163221389055252 tensor([0.1777, 0.2163, 0.1899, 0.2406, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19439482688903809 tensor([0.2051, 0.2051, 0.1944, 0.1979, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20067381858825684 tensor([0.2099, 0.2007, 0.1906, 0.1881, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21977882087230682 tensor([0.2198, 0.1902, 0.1803, 0.1680, 0.2417], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23135176301002502 tensor([0.2314, 0.1856, 0.1858, 0.1595, 0.2377], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2034580558538437 tensor([0.2086, 0.2035, 0.1904, 0.1893, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001112401485443 tensor([0.1929, 0.2036, 0.2001, 0.2216, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976029872894287 tensor([0.1809, 0.2003, 0.1976, 0.2498, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2231919765472412 tensor([0.2232, 0.1868, 0.1776, 0.1586, 0.2538], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22183315455913544 tensor([0.2218, 0.1940, 0.1870, 0.1679, 0.2292], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20130367577075958 tensor([0.1999, 0.2071, 0.1889, 0.2013, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1967904418706894 tensor([0.1755, 0.2057, 0.1968, 0.2564, 0.1656], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22782009840011597 tensor([0.2278, 0.1861, 0.1702, 0.1446, 0.2713], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19873525202274323 tensor([0.2194, 0.1987, 0.1848, 0.1692, 0.2278], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2077498584985733 tensor([0.2054, 0.2077, 0.1857, 0.1836, 0.2175], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20003211498260498 tensor([0.2073, 0.2035, 0.2000, 0.1906, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969183385372162 tensor([0.1914, 0.2056, 0.1969, 0.2228, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23132802546024323 tensor([0.2313, 0.1829, 0.1718, 0.1436, 0.2704], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2236337959766388 tensor([0.2236, 0.1901, 0.1837, 0.1685, 0.2340], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21242755651474 tensor([0.1861, 0.2124, 0.1936, 0.2276, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20119830965995789 tensor([0.1905, 0.2062, 0.2012, 0.2227, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20960286259651184 tensor([0.1767, 0.2096, 0.1899, 0.2480, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22451329231262207 tensor([0.2245, 0.1854, 0.1765, 0.1564, 0.2572], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2298964112997055 tensor([0.2299, 0.1862, 0.1836, 0.1586, 0.2417], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971634030342102 tensor([0.2064, 0.1975, 0.1972, 0.2026, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19945989549160004 tensor([0.2212, 0.1995, 0.1895, 0.1650, 0.2248], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016455978155136 tensor([0.1766, 0.2016, 0.1922, 0.2571, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22866961359977722 tensor([0.2287, 0.1870, 0.1716, 0.1448, 0.2679], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20047366619110107 tensor([0.2126, 0.2005, 0.1896, 0.1818, 0.2155], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20542959868907928 tensor([0.2032, 0.2054, 0.1822, 0.1882, 0.2211], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20184685289859772 tensor([0.2141, 0.2018, 0.1845, 0.1737, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20557454228401184 tensor([0.1788, 0.2056, 0.1897, 0.2455, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2226017564535141 tensor([0.2226, 0.1882, 0.1826, 0.1662, 0.2404], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2242993414402008 tensor([0.2243, 0.1886, 0.1868, 0.1713, 0.2290], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2120818942785263 tensor([0.2121, 0.1953, 0.1888, 0.1862, 0.2177], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1975582242012024 tensor([0.1907, 0.2077, 0.1976, 0.2205, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19504302740097046 tensor([0.1691, 0.2058, 0.1950, 0.2697, 0.1604], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22503282129764557 tensor([0.2250, 0.1853, 0.1748, 0.1577, 0.2572], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21708108484745026 tensor([0.2210, 0.1909, 0.1927, 0.1783, 0.2171], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21977567672729492 tensor([0.2198, 0.1956, 0.1836, 0.1694, 0.2316], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19918237626552582 tensor([0.2084, 0.1992, 0.1935, 0.1915, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1996033787727356 tensor([0.2009, 0.1996, 0.1903, 0.2047, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22960345447063446 tensor([0.2296, 0.1859, 0.1724, 0.1440, 0.2681], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23421557247638702 tensor([0.2342, 0.1831, 0.1838, 0.1550, 0.2438], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21345847845077515 tensor([0.1894, 0.2135, 0.1940, 0.2215, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19690686464309692 tensor([0.2031, 0.1976, 0.1969, 0.2033, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2115401178598404 tensor([0.1797, 0.2115, 0.1907, 0.2407, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23475529253482819 tensor([0.2348, 0.1799, 0.1778, 0.1465, 0.2610], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19650788605213165 tensor([0.2188, 0.1965, 0.1909, 0.1772, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21426740288734436 tensor([0.2143, 0.1950, 0.1851, 0.1785, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20014119148254395 tensor([0.2131, 0.2001, 0.1885, 0.1770, 0.2212], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1991117000579834 tensor([0.1932, 0.2011, 0.1991, 0.2226, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21973314881324768 tensor([0.2197, 0.1910, 0.1726, 0.1572, 0.2595], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21757836639881134 tensor([0.2195, 0.1901, 0.1907, 0.1821, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2189822793006897 tensor([0.2190, 0.1917, 0.1898, 0.1788, 0.2207], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.202486053109169 tensor([0.1837, 0.2053, 0.2025, 0.2393, 0.1692], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19849975407123566 tensor([0.1915, 0.1987, 0.1985, 0.2313, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21901829540729523 tensor([0.2190, 0.1936, 0.1818, 0.1689, 0.2367], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20418529212474823 tensor([0.2103, 0.2042, 0.1839, 0.1790, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988348662853241 tensor([0.1931, 0.2030, 0.1988, 0.2208, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20592713356018066 tensor([0.1734, 0.2059, 0.1915, 0.2596, 0.1696], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22652466595172882 tensor([0.2265, 0.1825, 0.1789, 0.1589, 0.2532], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2235545516014099 tensor([0.2236, 0.1941, 0.1882, 0.1689, 0.2253], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075147032737732 tensor([0.1834, 0.2075, 0.1936, 0.2364, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19837138056755066 tensor([0.1921, 0.2071, 0.1984, 0.2200, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20062771439552307 tensor([0.2012, 0.2006, 0.1916, 0.2054, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2296345829963684 tensor([0.2296, 0.1832, 0.1742, 0.1478, 0.2652], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2263621836900711 tensor([0.2264, 0.1864, 0.1883, 0.1651, 0.2338], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22504495084285736 tensor([0.2250, 0.1856, 0.1783, 0.1574, 0.2536], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19802170991897583 tensor([0.2008, 0.1989, 0.1980, 0.2078, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1947699934244156 tensor([0.2062, 0.2033, 0.1948, 0.1931, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21935182809829712 tensor([0.2194, 0.1871, 0.1802, 0.1676, 0.2458], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2223784327507019 tensor([0.2249, 0.1907, 0.1878, 0.1742, 0.2224], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21222874522209167 tensor([0.2122, 0.1958, 0.1887, 0.1848, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1958351731300354 tensor([0.1965, 0.2046, 0.1958, 0.2115, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.215963676571846 tensor([0.1754, 0.2160, 0.1931, 0.2481, 0.1674], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2300279587507248 tensor([0.2300, 0.1852, 0.1778, 0.1511, 0.2559], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2297983169555664 tensor([0.2298, 0.1859, 0.1887, 0.1639, 0.2317], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20798294246196747 tensor([0.1923, 0.2080, 0.1862, 0.2169, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20735161006450653 tensor([0.1965, 0.2074, 0.1919, 0.2098, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19770601391792297 tensor([0.1921, 0.2043, 0.1977, 0.2210, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22395877540111542 tensor([0.2240, 0.1874, 0.1766, 0.1568, 0.2553], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.223441943526268 tensor([0.2234, 0.1883, 0.1831, 0.1694, 0.2358], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21182113885879517 tensor([0.1921, 0.2118, 0.1908, 0.2144, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19805438816547394 tensor([0.1966, 0.2062, 0.1981, 0.2137, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976214051246643 tensor([0.1918, 0.2023, 0.1976, 0.2264, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22180801630020142 tensor([0.2218, 0.1889, 0.1779, 0.1614, 0.2500], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19404412806034088 tensor([0.2091, 0.1979, 0.1940, 0.1944, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21068553626537323 tensor([0.1925, 0.2162, 0.1917, 0.2107, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19835132360458374 tensor([0.1974, 0.1984, 0.1909, 0.2145, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22330832481384277 tensor([0.2233, 0.1865, 0.1747, 0.1568, 0.2587], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2209620326757431 tensor([0.2210, 0.1952, 0.1867, 0.1701, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19501861929893494 tensor([0.1956, 0.2112, 0.1950, 0.2080, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972988396883011 tensor([0.2045, 0.2040, 0.1973, 0.1985, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19963404536247253 tensor([0.1703, 0.1996, 0.1934, 0.2734, 0.1633], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22465156018733978 tensor([0.2247, 0.1881, 0.1739, 0.1528, 0.2606], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2221699208021164 tensor([0.2222, 0.1888, 0.1902, 0.1757, 0.2231], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21511422097682953 tensor([0.1824, 0.2151, 0.1940, 0.2335, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19507569074630737 tensor([0.2079, 0.2011, 0.1951, 0.1886, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20308877527713776 tensor([0.1675, 0.2031, 0.1944, 0.2776, 0.1575], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2269481122493744 tensor([0.2269, 0.1820, 0.1711, 0.1459, 0.2740], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2345397025346756 tensor([0.2345, 0.1840, 0.1835, 0.1558, 0.2423], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2231500893831253 tensor([0.1804, 0.2232, 0.1902, 0.2286, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19732938706874847 tensor([0.1995, 0.2084, 0.1973, 0.2046, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2200789451599121 tensor([0.2201, 0.1964, 0.1832, 0.1642, 0.2362], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2247161716222763 tensor([0.2247, 0.1903, 0.1781, 0.1599, 0.2469], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2283674031496048 tensor([0.2284, 0.1877, 0.1864, 0.1622, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21617527306079865 tensor([0.2162, 0.1959, 0.1890, 0.1791, 0.2200], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20096150040626526 tensor([0.2128, 0.2010, 0.1921, 0.1817, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2099987268447876 tensor([0.1907, 0.2100, 0.1871, 0.2151, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22631925344467163 tensor([0.2263, 0.1830, 0.1702, 0.1503, 0.2701], grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 3, 0, 2], [3, 1, 2], [3, 4, 1], [4, 0, 2], [4, 0, 2], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 4, 2, 1], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 1, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 1, 2], [4, 3, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3, 0, 2], [4, 0, 2], [3, 4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 3, 0, 2], [4, 0, 1, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 1, 2], [4, 3, 1, 2], [4, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [3, 4, 1, 2], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 0, 2], [3, 4, 1, 2], [3, 2, 1], [3, 4, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 1, 2], [4, 0, 2], [3, 2, 1], [3, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 4, 2, 1], [4, 3, 1, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 4, 2], [4, 0, 2], [3, 2, 1], [4, 3, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [4, 3, 0, 2], [3, 4, 2], [3, 2, 1], [3, 1, 2], [3, 4, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 1], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 2, 0], [4, 3], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3, 1, 2], [3, 2, 1], [4, 0, 2], [3, 2, 1], [3, 2], [3, 2, 0], [3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2, 1], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 1, 2], [4, 0, 2], [3, 1, 2], [3, 2, 1], [3, 2, 1], [3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 0], [4, 0, 1, 2], [3, 2, 1], [3, 4, 2, 1], [3, 2], [4, 0, 2], [4, 0, 2], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 3, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 3, 1, 2], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 4, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2], [0, 4, 2], [3, 2, 1]]\n",
      "[[0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1], [0, 4], [0, 2], [1, 3], [1, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0], [0, 4], [1], [1, 3], [0, 4], [0, 4], [1, 3], [1], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 1], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [3], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 1, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0], [1], [0, 4], [0, 4], [1], [1, 3], [1], [0, 1, 4], [0, 4], [0, 1, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1], [3], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0], [1, 3], [3], [0, 4], [0, 4], [0], [0, 4], [1], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1], [0], [0, 4], [0], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [3], [1, 3], [0, 4], [0, 1, 4], [1, 3], [1, 3], [3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0], [0], [1, 3], [0, 4], [0, 4], [1, 3], [0, 1], [1, 3], [0, 4], [0, 1], [1, 3], [1, 3], [1, 3], [0, 4], [1], [1, 3], [1], [0, 1], [0, 4], [0, 4], [0, 1], [1, 2, 3], [1, 3], [0, 4], [0, 4], [1, 3], [2], [1, 3], [0, 4], [0, 4], [1, 4], [0, 1, 2], [1, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0], [0, 4], [1, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 4], [1, 3], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0], [3], [0, 4], [0, 4], [1, 3], [3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 1, 4], [1, 3], [0, 4], [0, 4], [0, 4], [1, 2, 3], [3], [0, 4], [0], [0, 1, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1, 3], [0, 1], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0], [1, 3], [1], [3], [0, 4], [0, 4], [1, 3], [1], [3], [0, 4], [0, 4], [1, 3], [0, 1], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 4], [0, 1], [1, 3], [0, 4]]\n",
      "NL_pred of 5th iteration [[3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2], [4, 3, 2], [4, 0, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 3, 1, 2], [4, 0, 2, 1], [3, 4, 1, 2], [4, 3, 0, 2], [4, 0, 2], [4, 3, 0, 2], [3, 4, 1, 2], [4, 0, 2], [4, 0, 1, 2], [4, 0, 2], [4, 0, 2, 1], [4, 0, 2], [3, 4, 2, 1], [4, 3, 1, 2], [4, 0, 2], [4, 3, 2], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 0, 2], [4, 3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 4, 2, 1], [4, 0, 2, 1], [4, 0, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 3, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [4, 3, 1, 2], [4, 0, 2, 1], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2, 1], [3, 4, 2], [4, 0, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.018348635946001324  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.018347915581294467  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.018346541268484933  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.01834458453314645  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.018342108385903496  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.01833916051047189  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.01833578859056745  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.018332045418875557  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.01832796164921352  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.018323579856327602  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.01831893239702497  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.01831404311316354  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.018308942658560617  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.018303654875074113  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.01829820019858224  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.018292599064963205  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.01828686509813581  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.01828101873397827  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.01827507700238909  Accuracy on Support set:0.0\n",
      "torch.Size([70, 2048]) torch.Size([70])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.01826904501233782  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 3, 0, 2], [3, 1, 2], [3, 4, 1], [4, 0, 2], [4, 0, 2], [4, 0], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 4, 2, 1], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [4, 3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 1, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 1, 2], [4, 3, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3, 0, 2], [4, 0, 2], [3, 4, 0, 2], [3, 2], [3, 1, 2], [3, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 3, 0, 2], [4, 0, 1, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 1, 2], [4, 3, 1, 2], [4, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [3, 4, 1, 2], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 3, 0, 2], [3, 4, 1, 2], [3, 2, 1], [3, 4, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 1, 2], [4, 0, 2], [3, 2, 1], [3, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 4, 2, 1], [4, 3, 1, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 4, 2], [4, 0, 2], [3, 2, 1], [4, 3, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 3, 0, 2], [4, 0, 2], [4, 3, 0, 2], [3, 4, 2], [3, 2, 1], [3, 1, 2], [3, 4, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 1], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 2, 0], [4, 3], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 3, 1, 2], [3, 2, 1], [4, 0, 2], [3, 2, 1], [3, 2], [3, 2, 0], [3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2, 1], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 1, 2], [4, 0, 2], [3, 1, 2], [3, 2, 1], [3, 2, 1], [3, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 1, 2], [4, 0], [4, 0, 1, 2], [3, 2, 1], [3, 4, 2, 1], [3, 2], [4, 0, 2], [4, 0, 2], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 3, 2], [3, 2, 1], [3, 1, 2], [3, 2, 1], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [4, 3, 1, 2], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 2, 1], [4, 0, 2], [4, 3, 0, 2], [4, 0, 2, 1], [3, 2, 1], [3, 1, 2], [4, 0, 2], [3, 4, 2], [4, 0, 2], [3, 2, 1], [3, 1, 2], [4, 0, 2], [4, 0, 2], [3, 2, 1], [3, 2, 1], [3, 1, 2], [3, 2, 1], [3, 4, 2], [0, 4, 2], [3, 2, 1]]\n",
      "POSITION :  [[0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1], [0, 4], [0, 2], [1, 3], [1, 3], [1, 2, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0], [0, 4], [1], [1, 3], [0, 4], [0, 4], [1, 3], [1], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 1], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [3], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 1, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0], [1], [0, 4], [0, 4], [1], [1, 3], [1], [0, 1, 4], [0, 4], [0, 1, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1], [3], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0], [1, 3], [3], [0, 4], [0, 4], [0], [0, 4], [1], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1], [0], [0, 4], [0], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [3], [1, 3], [0, 4], [0, 1, 4], [1, 3], [1, 3], [3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0], [0], [1, 3], [0, 4], [0, 4], [1, 3], [0, 1], [1, 3], [0, 4], [0, 1], [1, 3], [1, 3], [1, 3], [0, 4], [1], [1, 3], [1], [0, 1], [0, 4], [0, 4], [0, 1], [1, 2, 3], [1, 3], [0, 4], [0, 4], [1, 3], [2], [1, 3], [0, 4], [0, 4], [1, 4], [0, 1, 2], [1, 3], [0, 4], [0, 4], [1, 3], [1, 2, 3], [1, 3], [0, 4], [0, 4], [0], [0, 4], [1, 3], [0, 4], [0, 1, 4], [1, 4], [0, 1, 4], [1, 3], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0], [3], [0, 4], [0, 4], [1, 3], [3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 1, 4], [1, 3], [0, 4], [0, 4], [0, 4], [1, 2, 3], [3], [0, 4], [0], [0, 1, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [1, 3], [0, 1], [0, 4], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [1, 3], [0, 4], [0], [1, 3], [1], [3], [0, 4], [0, 4], [1, 3], [1], [3], [0, 4], [0, 4], [1, 3], [0, 1], [1, 3], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [0, 4], [0, 4], [0, 1], [1, 3], [0, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.708\n",
      "[0, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 2/150 [02:09<2:40:05, 64.90s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m         t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(item)\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m--> 132\u001b[0m         pseudo_label\u001b[39m.\u001b[39mappend(max_(item[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m    133\u001b[0m         index_pl\u001b[39m.\u001b[39mappend(idx)\n\u001b[0;32m    134\u001b[0m pseudo_label \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(pseudo_label)\n",
      "Cell \u001b[1;32mIn[88], line 3\u001b[0m, in \u001b[0;36mmax_\u001b[1;34m(li)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmax_\u001b[39m(li):\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(li)\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mif\u001b[39;00m(li[\u001b[39m0\u001b[39m]\u001b[39m!=\u001b[39mli[\u001b[39m1\u001b[39m] \u001b[39mand\u001b[39;00m li[\u001b[39m1\u001b[39m]\u001b[39m!=\u001b[39mli[\u001b[39m2\u001b[39;49m] \u001b[39mand\u001b[39;00m li[\u001b[39m0\u001b[39m]\u001b[39m!=\u001b[39mli[\u001b[39m2\u001b[39m]):\n\u001b[0;32m      4\u001b[0m         \u001b[39mreturn\u001b[39;00m li[\u001b[39m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39mset\u001b[39m(li), key \u001b[39m=\u001b[39m li\u001b[39m.\u001b[39mcount)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Start the training process \n",
    "\n",
    "# in the below code I am not using query set which should be concated with unlabeled data.\n",
    "\n",
    "for data in tqdm(trainloader):\n",
    "\n",
    "        # create different sets of data from the train loader\n",
    "        data = data.cpu()\n",
    "        targets = torch.arange(args.num_ways).repeat(args.k_shot+args.query+args.unlabel).long()\n",
    "\n",
    "        #print(data,targets)\n",
    "        \n",
    "        support_data = data[:num_support]\n",
    "        query_data = data[num_support:num_support+num_query]\n",
    "        unlabel_data = data[num_support+num_query:]\n",
    "        \n",
    "        support_inputs=[]\n",
    "        for i in range(2):             \n",
    "            support_inputs.append(normalize(get_features(model[i+1], support_data)))  # get feature embeddings for \n",
    "        #support_inputs=np.array(support_inputs)\n",
    "        support_targets = targets[:num_support].cpu().numpy()\n",
    "\n",
    "        #print(support_inputs.shape,support_targets)\n",
    "        query_inputs=[]\n",
    "        for i in range(2):   \n",
    "            query_inputs.append(normalize(get_features(model[i+1], query_data)))\n",
    "        query_targets = targets[num_support:num_support+num_query].cpu().numpy()\n",
    "\n",
    "        #print(query_inputs.shape,query_targets.shape)\n",
    "        unlabel_inputs=[]\n",
    "        for i in range(2):\n",
    "            unlabel_inputs.append(normalize(get_features(model[i+1], unlabel_data)))\n",
    "        unlabel_targets = targets[num_support+num_query:].cpu().numpy()\n",
    "\n",
    "        # The classifier has already been decided as linear classifier with a single dense layer and the output dimension=5\n",
    "        \n",
    "        ori_index = [x for x in range(args.unlabel*args.num_ways)]  # Store the index position of 250 images\n",
    "        _POSITION = [[] for _ in range(args.unlabel*args.num_ways)] # Create a 2D list to store the list of 5 classes in passed along with the image batch.\n",
    "        POSITION = [[0, 1, 2, 3, 4] for _ in range(args.unlabel*args.num_ways)] # [0,1,2,3,4] was chosen for encoding the 5 classes\n",
    "        global_nl_pred=[[] for _ in range(args.unlabel*args.num_ways)] # Store the negative labels of each image after every iteration\n",
    "        temp_nl_pred=[[] for _ in range(args.unlabel*args.num_ways)]\n",
    "        \n",
    "        # Define the loss criterion and the SGD optimizer used here for initial training of model.\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "      \n",
    "        optimizer = torch.optim.SGD(Cls[1].parameters(), lr = 1e-3, momentum=0.9, weight_decay=5e-4)  # weight decay is for L2 regularization.\n",
    "\n",
    "        # Begin initial training\n",
    "\n",
    "        print('\\n****************  Initial training the model on Support set')\n",
    "        for epoch in range(50):\n",
    "              loss,_,acc=train_loop2(Cls[1], None, criterion, optimizer, support_inputs[1], support_targets)\n",
    "              print(f\"Train_Epoch: {epoch}  Train_Loss: {loss}  Accuracy on Support set:{acc}\")\n",
    "        \n",
    "        print(\"Testing after training on support set\")\n",
    "        print(\"Accuracy of testing on Query Set: \",test_loop(Cls[1], query_inputs[1],query_targets))\n",
    "        \n",
    "        # Start of code using complimentary labels\n",
    "        '''\n",
    "        Important : Stop loss propagation in the next step only. Not immediately when label is found\n",
    "        '''\n",
    "        glob_pos=[]\n",
    "        for j in range(2):\n",
    "         i=0\n",
    "         ori_index = [x for x in range(args.unlabel*args.num_ways)]  # Store the index position of 250 images\n",
    "         _POSITION = [[] for _ in range(args.unlabel*args.num_ways)] # Create a 2D list to store the list of 5 classes in passed along with the image batch.\n",
    "         POSITION = [[0, 1, 2, 3, 4] for _ in range(args.unlabel*args.num_ways)] # [0,1,2,3,4] was chosen for encoding the 5 classes\n",
    "         global_nl_pred=[[] for _ in range(args.unlabel*args.num_ways)] # Store the negative labels of each image after every iteration\n",
    "         temp_nl_pred=[[] for _ in range(args.unlabel*args.num_ways)]\n",
    "         while(True and i<6):\n",
    "            print('\\n********************************************  Training with complimentatry labels')\n",
    "            select_idx=[]\n",
    "            nl_pred=[]\n",
    "            unselect_idx=[]\n",
    "            unlabel_out = Cls[j](torch.tensor(unlabel_inputs[j]).to(device))\n",
    "            #print(\"unlabel_out shape\", unlabel_out)\n",
    "            nl_pred, unselect_idx,_POSITION,POSITION = get_preds_position_(unlabel_out, POSITION, _POSITION, args.threshold)  # Changed\n",
    "            #print(unselect_idx)\n",
    "            print(_POSITION)\n",
    "            print(POSITION)\n",
    "            select_idx = [x for x in ori_index if x not in unselect_idx]\n",
    "            _unlabel_embeddings = unlabel_inputs[j][select_idx]\n",
    "            #print(_unlabel_embeddings)\n",
    "            negative_pred=[global_nl_pred[x] for x in ori_index if x in select_idx]# list containing all the negative labels predicted for that class\n",
    "            #nl_pred = [nl_pred[x] for x in ori_index if x in sel\n",
    "            # ect_idx]  # May not be required because nl_pred comes without unselect indexes\n",
    "            print(f\"NL_pred of {i}th iteration\",negative_pred)\n",
    "            if(len(nl_pred)==0  or len(select_idx)==0):   # \n",
    "                  break\n",
    "            optimizer_NL = torch.optim.SGD(Cls[j].parameters(), lr = 1e-3, momentum=0.9, weight_decay=5e-4) #, weight_decay=5e-4\n",
    "            print(\"Start of Epoch\")\n",
    "            for epoch in range(20):\n",
    "                loss,_,acc = train_loop(Cls[j], None, NL_loss, optimizer_NL, _unlabel_embeddings, nl_pred,negative_pred)\n",
    "                print(f\"Train_Epoch_NL: {epoch}  Train_Loss: {loss}  Accuracy on Support set:{acc}\")\n",
    "                #print(\"Classifier Parameters :\",Classifier.parameters())\n",
    "                \n",
    "\n",
    "            # Break condition no negative label found below threshold condition\n",
    "            i=i+1\n",
    "         glob_pos.append(POSITION)\n",
    "\n",
    "\n",
    "        print(\"Start of training with pseudo labels\\n\")    \n",
    "        print(\"Global NL pred list :\",global_nl_pred) # Printing NULL . Check\n",
    "\n",
    "        print(\"POSITION : \",POSITION)\n",
    "        acc=0\n",
    "        c=0\n",
    "        print(unlabel_targets,unlabel_targets.shape)\n",
    "        for i in unlabel_targets:\n",
    "              if i in POSITION[c]:\n",
    "                    acc+=1\n",
    "              c+=1\n",
    "        print(\"Accuracy of Pseudo labels :\",acc/c)\n",
    "\n",
    "        # to be corrected\n",
    "\n",
    "        \n",
    "        class_num = [0 for _ in range(5)]\n",
    "        pseudo_label = []\n",
    "        index_pl = []\n",
    "        for idx in range(len(glob_pos[0])):\n",
    "            item=[]\n",
    "            t=0\n",
    "            for i in glob_pos:\n",
    "                if(len(i[idx])!=1):\n",
    "                    t+1\n",
    "                    continue\n",
    "                item.append(i[t])\n",
    "                t+1\n",
    "            if len(item)!=0:\n",
    "                pseudo_label.append(max_(item[0]))\n",
    "                index_pl.append(idx)\n",
    "        pseudo_label = np.asarray(pseudo_label)\n",
    "        t1_ = unlabel_inputs[0][index_pl]\n",
    "        t2_ = torch.tensor(pseudo_label, dtype=torch.int64)\n",
    "        print(t2_)\n",
    "        if(len(t2_!=0)):\n",
    "          print(\"Start of final training\")\n",
    "          for epoch in range(10):\n",
    "              loss,_,acc=train_loop2(Cls[0], None, criterion, optimizer, t1_, t2_)\n",
    "              print(f\"Epoch: {epoch}  Loss: {acc}\")   \n",
    "        \n",
    "        print(\"Start of testing\")\n",
    "        print(\"Accuracy of testing on Query Set: \",test_loop(Cls[0], query_inputs[0],query_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "793_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
