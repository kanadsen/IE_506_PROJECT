{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Normalize, Resize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n0153282900000005.jpg</td>\n",
       "      <td>n01532829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n0153282900000006.jpg</td>\n",
       "      <td>n01532829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n0153282900000007.jpg</td>\n",
       "      <td>n01532829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n0153282900000010.jpg</td>\n",
       "      <td>n01532829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n0153282900000014.jpg</td>\n",
       "      <td>n01532829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38395</th>\n",
       "      <td>n1313361300001288.jpg</td>\n",
       "      <td>n13133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38396</th>\n",
       "      <td>n1313361300001290.jpg</td>\n",
       "      <td>n13133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38397</th>\n",
       "      <td>n1313361300001296.jpg</td>\n",
       "      <td>n13133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38398</th>\n",
       "      <td>n1313361300001297.jpg</td>\n",
       "      <td>n13133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38399</th>\n",
       "      <td>n1313361300001299.jpg</td>\n",
       "      <td>n13133613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename      label\n",
       "0      n0153282900000005.jpg  n01532829\n",
       "1      n0153282900000006.jpg  n01532829\n",
       "2      n0153282900000007.jpg  n01532829\n",
       "3      n0153282900000010.jpg  n01532829\n",
       "4      n0153282900000014.jpg  n01532829\n",
       "...                      ...        ...\n",
       "38395  n1313361300001288.jpg  n13133613\n",
       "38396  n1313361300001290.jpg  n13133613\n",
       "38397  n1313361300001296.jpg  n13133613\n",
       "38398  n1313361300001297.jpg  n13133613\n",
       "38399  n1313361300001299.jpg  n13133613\n",
       "\n",
       "[38400 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the csv file given for the mini-imagenet dataset\n",
    "dataset = pd.read_csv('datasets/data/mini-imagenet/train.csv', low_memory=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique classes\n",
    "classes=dataset.label.unique()\n",
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n01930112' 'n01981276' 'n02099601' 'n02110063' 'n02110341'] ['n01930112' 'n01981276' 'n02099601' 'n02110063' 'n02110341' 'n02116738'\n",
      " 'n02129165' 'n02219486' 'n02443484' 'n02871525']\n"
     ]
    }
   ],
   "source": [
    "# Create 2 classes list to form a group of 5 classes and 10 classes respectively\n",
    "classes_5=classes[:5]\n",
    "classes_10=classes[:10]\n",
    "print(classes_5,classes_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to datasets\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d60fb2327044dd9cac152a23f4f7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\\cifar-100-python.tar.gz to datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 118, in __getitem__\n    img = self.transform(img)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 94, in __call__\n    img = t(img)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 676, in forward\n    i, j, h, w = self.get_params(img, self.size)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 635, in get_params\n    raise ValueError(f\"Required crop size {(th, tw)} is larger then input image size {(h, w)}\")\nValueError: Required crop size (64, 64) is larger then input image size (40, 40)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     50\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[0;32m     52\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     53\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1375\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1376\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1402\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1403\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 118, in __getitem__\n    img = self.transform(img)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 94, in __call__\n    img = t(img)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 676, in forward\n    i, j, h, w = self.get_params(img, self.size)\n  File \"c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 635, in get_params\n    raise ValueError(f\"Required crop size {(th, tw)} is larger then input image size {(h, w)}\")\nValueError: Required crop size (64, 64) is larger then input image size (40, 40)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define transforms for data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(64, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load MiniImageNet dataset\n",
    "trainset = torchvision.datasets.CIFAR100(root='datasets', train=True,\n",
    "                                             download=True,\n",
    "                                             transform=transform_train)\n",
    "\n",
    "# Define dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Define ResNet-50 model\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet50()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
