{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "#from torcheval.metrics import R2Score # To be implemented\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "\n",
    "#from aux_func import*\n",
    "from datset_process import *\n",
    "from torch.utils.data import DataLoader\n",
    "from statistics import mode\n",
    "# Import the pretrained default model resnet18/resnet50/resnet101\n",
    "from torchvision.models import resnet50,resnet152,resnet101,efficientnet_v2_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entropy_loss function measures the entropy of the predicted probability distribution. \n",
    "# It aims to maximize the entropy of the output probabilities, which can encourage the network to output less confident predictions. \n",
    "# The entropy loss can be used as a regularization technique to prevent overfitting.\n",
    "\n",
    "def entropy_loss(p):\n",
    "    p = F.softmax(p, dim=1)\n",
    "    epsilon = 1e-5\n",
    "    return (-1 * torch.sum(p * torch.log(p + epsilon))) / p.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune for class exclusion with values of Q\n",
    "def NL_loss_2(f,labels, global_negative_pred):\n",
    "    Q_1 = 1 - F.softmax(f, dim=1) # softmax of f\n",
    "    return F.cross_entropy(Q_1, labels)  # ignore_index=global_negative_pred  Ignores all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tune for class exclusion with values of Q\n",
    "def NL_loss(f,labels, global_negative_pred):\n",
    "    '''\n",
    "    def th_delete(tensor, indices):\n",
    "       mask = torch.ones(tensor.numel(), dtype=torch.bool)\n",
    "       mask[indices] = False\n",
    "       print(mask)\n",
    "       return tensor[:, mask]\n",
    "   \n",
    "    f=th_delete(f,global_negative_pred)\n",
    "\n",
    "    This method of masking softmax does not work\n",
    "    '''\n",
    "    \n",
    "    Q_1 = 1 - F.softmax(f, dim=1) # softmax of f\n",
    "   \n",
    "    '''\n",
    "    for i in global_negative_pred:              # Giving NAN values\n",
    "        Q_1[0][i]=-1000\n",
    "    #print(Q_1)\n",
    "    '''\n",
    "    Q = F.softmax(Q_1, dim=1) # for calculating weights\n",
    "    weight = 1 - Q        \n",
    "    # Set the weights of indices in global_negative_pred to zero\n",
    "    #weight[:, global_negative_pred] = 0\n",
    "\n",
    "    out = weight *torch.log(Q) # weight *  Changed here to see difference\n",
    "    '''\n",
    "    for i in global_negative_pred:\n",
    "        out[0][i]=0.001                     # This increases accuracy of pseudo labels\n",
    "    #print(weight,weight.shape)\n",
    "    '''\n",
    "    #out.require_grad = False    # Changed in this code\n",
    "    weight.require_grad = False\n",
    "    return F.nll_loss(out, labels)  # ignore_index=global_negative_pred  Ignores all classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define arg parser\n",
    "seed=200\n",
    "paser = argparse.ArgumentParser() \n",
    "args = paser.parse_args(\"\")\n",
    "np.random.seed(200)\n",
    "torch.manual_seed(seed)\n",
    "device=input(\"Enter cuda or cpu for device type\")\n",
    "device = torch.device(device)\n",
    "#'cuda' if torch.cuda.is_available() else\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take user inputs \n",
    "args.dataset='miniimagenet'\n",
    "args.data_path='datasets/data/mini-imagenet/'\n",
    "args.num_classes=5 # Output dimension\n",
    "args.image_size=84\n",
    "\n",
    "# FSL definitions\n",
    "args.num_ways=5 # Number of classes per batch\n",
    "args.k_shot=5 # number of Images per class\n",
    "args.query=30 # Query set of the FSL\n",
    "args.unlabel=50 # Number of unlabel samples per class \n",
    "args.steps=5 # Select how many unlabeled data for each class in one iteration.\n",
    "args.threshold=0.2  # Since we have 5 classes in each support set. So if all the classes are equally probable then mininmum p=0.2\n",
    "\n",
    "# set in semi-supervised few-shot learning\n",
    "num_support = args.k_shot * args.num_ways\n",
    "num_query = args.query * args.num_ways\n",
    "num_unlabeled = args.unlabel * args.num_ways\n",
    "\n",
    "# Training or testing definitions \n",
    "args.episodes=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sets of unlabeled data\n",
    "num_select = int(args.unlabel / args.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kanad\\Desktop\\Github repos\\IE_506_PROJECT\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (23): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (24): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (25): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (26): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (27): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (28): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (29): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (30): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (31): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (32): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (33): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (34): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (35): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the resnet model and define the model to be used \n",
    "#model=torch.load('Mymodel.pt')\n",
    "#model=resnet12(args.num_classes)\n",
    "model=resnet152(num_classes=1000,pretrained=True)\n",
    "model=model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (23): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (24): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (25): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (26): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (27): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (28): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (29): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (30): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (31): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (32): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (33): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (34): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (35): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Freeze the CNN layers of Resnet\n",
    "'''\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "'''\n",
    "model.fc=nn.Flatten()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ANN classifier\n",
    "Cls=nn.Sequential(nn.Linear(2048,512,bias=True),nn.ReLU(),nn.Linear(512,128,bias=True),nn.ReLU(),nn.Linear(128,5,bias=True))\n",
    "Cls=Cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the features from the resnet model\n",
    "\n",
    "def get_features(model,input):\n",
    "    '''\n",
    "    The function first checks if the input batch size exceeds a desired batch size. If it does, the input batch is split into smaller batches of size 64, and the \n",
    "    ResNet model is called on each smaller batch using the model function with the return_feat=True argument to return the output features in addition to the classification results. \n",
    "    The output features are then detached from the computation graph, transferred to the CPU, and appended to a list embed. \n",
    "    Once all batches have been processed, the list of output features is concatenated using torch.cat to form a single tensor embed.\n",
    "    If the input batch size is less than or equal to the desired batch size, the ResNet model is called once with the input batch using the model function with the return_feat=True argument to return the output features.\n",
    "    Finally, the function checks if the shape of the output features embed matches the shape of the input batch, and returns the output features as a NumPy array using the numpy() method.\n",
    "    '''\n",
    "    batch_size = 64  # Use the desired batch size\n",
    "    input = torch.tensor(input).to(device)\n",
    "    \n",
    "    # Check to prevent the input shape from exceeding the desired batch size\n",
    "    if input.shape[0] > batch_size:\n",
    "        embed = []\n",
    "        i = 0\n",
    "        while i <= input.shape[0]-1:\n",
    "            embed.append(model(input[i:i+batch_size])) # Changed. Check\n",
    "            i += batch_size\n",
    "        embed = torch.cat(embed)\n",
    "    else:\n",
    "        embed = model(input) # Changed. Check. Removed return feat\n",
    "    assert embed.shape[0] == input.shape[0] # Check if input shape = embed shape  as we will be working on input shape.\n",
    "    return embed.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model1, dataset, loss_fn, optimizer, inputs, targets,negative_pred):\n",
    "    acc =0\n",
    "    def forward_fn(data, label,neg_pr):\n",
    "        logits = model1(data)\n",
    "        logits.require_grad = False  # Changed\n",
    "        #logits=F.softmax(logits) # Changed here       \n",
    "        loss = loss_fn(logits, label,neg_pr) #+ entropy_loss(logits) # Combination of two loss functions used here. The entropy_loss act as regularizer\n",
    "        # loss = loss_fn(logits, label)\n",
    "        #print(loss)\n",
    "        return loss, logits\n",
    "    \n",
    "    def train_step(data, label,neg_pre):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = forward_fn(data, label,neg_pre)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item(), logits\n",
    "    \n",
    "    model1.train()\n",
    "    final_loss=0\n",
    "     #print(i)\n",
    "    img = torch.tensor(inputs)\n",
    "    y = torch.tensor(targets).long() # Changed\n",
    "    img=img.to(device)\n",
    "    y=y.to(device)\n",
    "    print(img.shape,y.shape)\n",
    "    loss, logits = train_step(img, y,negative_pred)\n",
    "    final_loss+=loss\n",
    "    final_loss/=len(inputs)\n",
    "    # Check accuracy\n",
    "    for i in range(len(inputs)):\n",
    "        model1.eval()\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        preds = model1(img)\n",
    "        preds = torch.argmax(preds, 1).reshape(-1)\n",
    "        y = y.reshape(-1)\n",
    "        if (preds==y):\n",
    "          acc +=1\n",
    "    acc = acc/len(inputs)*100\n",
    "    return final_loss, logits,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop2(model1, dataset, loss_fn, optimizer, inputs, targets):\n",
    "    acc =0\n",
    "    def forward_fn(data, label):\n",
    "        logits = model1(data)\n",
    "        logits.require_grad = False  # Changed\n",
    "        #logits=F.softmax(logits) # Changed here       \n",
    "        loss = loss_fn(logits, label) #+ entropy_loss(logits) # Combination of two loss functions used here. The entropy_loss act as regularizer\n",
    "        # loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    \n",
    "    def train_step(data, label):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = forward_fn(data, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item(), logits\n",
    "    \n",
    "    model1.train()\n",
    "    final_loss=0\n",
    "    for i in range(len(inputs)):\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        loss, logits = train_step(img, y)\n",
    "        final_loss+=loss\n",
    "    final_loss/=len(inputs)\n",
    "    # Check accuracy\n",
    "    \n",
    "    model1.eval()\n",
    "    # Check accuracy\n",
    "    for i in range(len(inputs)):\n",
    "        model1.eval()\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        preds = model1(img)\n",
    "        preds = torch.argmax(preds, 1).reshape(-1)\n",
    "        y = y.reshape(-1)\n",
    "        if (preds==y):\n",
    "          acc +=1\n",
    "    acc = acc/len(inputs)*100\n",
    "    return final_loss, logits,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model1,inputs, targets):\n",
    "    acc =0\n",
    "    # Check accuracy\n",
    "    for i in range(len(inputs)):\n",
    "        model1.eval()\n",
    "        #print(i)\n",
    "        img = torch.tensor(inputs[i])\n",
    "        y = torch.tensor(targets[i]).long() # Changed\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        y=y.unsqueeze(0).to(device)\n",
    "        preds = model1(img)\n",
    "        preds = torch.argmax(preds, 1).reshape(-1)\n",
    "        y = y.reshape(-1)\n",
    "        if (preds==y):\n",
    "          acc +=1\n",
    "    acc = acc/len(inputs)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(out):\n",
    "    preds = torch.argmin(out, dim=0).item()\n",
    "    return preds, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_preds(out,nl_pred):\n",
    "    def th_delete(tensor, indices):\n",
    "       mask = torch.ones(tensor.numel(), dtype=torch.bool)\n",
    "       mask[indices] = False\n",
    "       #print(mask)\n",
    "       return tensor[mask]\n",
    "    #print(out,out.shape)\n",
    "    new_out=th_delete(out,nl_pred)\n",
    "    index =torch.argmin(new_out).item()\n",
    "    val=new_out.view(-1)[index].item()\n",
    "    original_index=0\n",
    "    for i in range(out.shape[0]):\n",
    "        if out[i]==val:\n",
    "            original_index=i\n",
    "    print(original_index,val,out)\n",
    "    return original_index,val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_preds_position_(unlabel_out, position, _postion, thres=0.001):\\n        length = len(position)\\n        r = []\\n        un_idx = []\\n        for idx in range(length):\\n           pos = position[idx]\\n           _pos = _postion[idx]\\n           _out = unlabel_out[idx][pos]\\n           out = F.softmax(_out,dim=0)  # Check if dim=0 or 1\\n           #print(out,idx)\\n           if len(pos)==1:\\n               un_idx.append(idx)\\n               continue\\n           conf =torch.argmin(out).item()\\n           conf=out.view(-1)[conf].item()\\n           #print(\"conf\",conf)\\n           if conf>thres:\\n               un_idx.append(idx)\\n               if len(_pos)==0:\\n                   r.append(np.array(torch.argmin(out, dim=0).item()))  # check if asnumpy works here or not\\n               else:\\n                   r.append(_pos[-1])\\n               continue\\n           t, _ = get_preds(out)\\n           #print(\"Negative label :\",t)\\n           a = pos[t]\\n           _postion[idx].append(a)\\n           position[idx].remove(a)\\n           r.append(a)\\n        #print(\"New _POSITION :\", _postion)\\n        return np.asarray(r), un_idx,_postion,position\\n        '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_preds_position_(unlabel_out, position, _postion, thres=0.001):\n",
    "        length = len(position)\n",
    "        r = []\n",
    "        un_idx = []\n",
    "        for idx in range(length):\n",
    "           pos = position[idx]\n",
    "           _pos = _postion[idx]\n",
    "           _out = unlabel_out[idx][pos]\n",
    "           out = F.softmax(_out,dim=0)  # Check if dim=0 or 1\n",
    "           #print(out,idx)\n",
    "           if len(pos)==1:\n",
    "               un_idx.append(idx)\n",
    "               continue\n",
    "           conf =torch.argmin(out).item()\n",
    "           conf=out.view(-1)[conf].item()\n",
    "           #print(\"conf\",conf)\n",
    "           if conf>thres:\n",
    "               un_idx.append(idx)\n",
    "               if len(_pos)==0:\n",
    "                   r.append(np.array(torch.argmin(out, dim=0).item()))  # check if asnumpy works here or not\n",
    "               else:\n",
    "                   r.append(_pos[-1])\n",
    "               continue\n",
    "           t, _ = get_preds(out)\n",
    "           #print(\"Negative label :\",t)\n",
    "           a = pos[t]\n",
    "           _postion[idx].append(a)\n",
    "           position[idx].remove(a)\n",
    "           r.append(a)\n",
    "        #print(\"New _POSITION :\", _postion)\n",
    "        return np.asarray(r), un_idx,_postion,position\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preds_position_(unlabel_out, position, _postion, thres=0.001):\n",
    "    length = len(position)\n",
    "    r = []\n",
    "    un_idx = []\n",
    "    for idx in range(length):\n",
    "        pos = position[idx]\n",
    "        _pos = _postion[idx]\n",
    "        _out = unlabel_out[idx]  # [pos] removed\n",
    "        out = F.softmax(_out,dim=0)  # Check if dim=0 or 1 # Correct\n",
    "        out.require_grad = False  # Changed\n",
    "        #print(out)\n",
    "        nl_pred=global_nl_pred[idx] # defined later in the main loop. Taking the list of negative labels already found\n",
    "        \n",
    "        \n",
    "        # The logic is changed here for proper implementation\n",
    "        \n",
    "        if len(pos)==1:\n",
    "           un_idx.append(idx)\n",
    "           continue\n",
    "        t,conf=get_neg_preds(out,nl_pred)\n",
    "        #print(conf)\n",
    "        \n",
    "        if conf>thres:\n",
    "            un_idx.append(idx)\n",
    "            '''\n",
    "            if len(_pos)==0:\n",
    "                r.append(torch.argmin(out,dim=1).item().asnumpy())  # check if asnumpy works here or not\n",
    "            else:\n",
    "                r.append(_pos[-1])\n",
    "            '''\n",
    "            continue\n",
    "        #t, _ = get_preds(out)\n",
    "        #a = pos[t]\n",
    "        _postion[idx].append(t)\n",
    "        position[idx].remove(t)\n",
    "        #print(position)\n",
    "        global_nl_pred[idx].append(t)   # Append the nl_value index\n",
    "        r.append(t)\n",
    "    return np.asarray(r), un_idx,_postion,position  # Changed here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the respective loaders\n",
    "args.train_episodes=15\n",
    "train_dataset = DataSet( args.image_size, 'test',args.data_path)\n",
    "train_sampler = EpisodeSampler(train_dataset.label, args.train_episodes,args.num_ways, args.k_shot, args.query, args.unlabel)\n",
    "trainloader = DataLoader(train_dataset, batch_sampler=train_sampler,shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(li):\n",
    "    print(li)\n",
    "    if(li[0]!=li[1] and li[1]!=li[2] and li[0]!=li[2]):\n",
    "        return li[0]\n",
    "    return max(set(li), key = li.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 1.6105047416687013  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 1.6098938179016113  Accuracy on Support set:20.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6094286918640137  Accuracy on Support set:20.0\n",
      "Train_Epoch: 3  Train_Loss: 1.6089944171905517  Accuracy on Support set:20.0\n",
      "Train_Epoch: 4  Train_Loss: 1.608568515777588  Accuracy on Support set:20.0\n",
      "Train_Epoch: 5  Train_Loss: 1.6081661796569824  Accuracy on Support set:20.0\n",
      "Train_Epoch: 6  Train_Loss: 1.6077784156799317  Accuracy on Support set:20.0\n",
      "Train_Epoch: 7  Train_Loss: 1.6074002695083618  Accuracy on Support set:20.0\n",
      "Train_Epoch: 8  Train_Loss: 1.6070206451416016  Accuracy on Support set:20.0\n",
      "Train_Epoch: 9  Train_Loss: 1.6066393804550172  Accuracy on Support set:20.0\n",
      "Train_Epoch: 10  Train_Loss: 1.6062870264053344  Accuracy on Support set:20.0\n",
      "Train_Epoch: 11  Train_Loss: 1.6059306049346924  Accuracy on Support set:20.0\n",
      "Train_Epoch: 12  Train_Loss: 1.6055778980255127  Accuracy on Support set:20.0\n",
      "Train_Epoch: 13  Train_Loss: 1.6052324676513672  Accuracy on Support set:20.0\n",
      "Train_Epoch: 14  Train_Loss: 1.6048806667327882  Accuracy on Support set:20.0\n",
      "Train_Epoch: 15  Train_Loss: 1.604528775215149  Accuracy on Support set:20.0\n",
      "Train_Epoch: 16  Train_Loss: 1.6041768884658814  Accuracy on Support set:20.0\n",
      "Train_Epoch: 17  Train_Loss: 1.6038235330581665  Accuracy on Support set:20.0\n",
      "Train_Epoch: 18  Train_Loss: 1.6034714889526367  Accuracy on Support set:20.0\n",
      "Train_Epoch: 19  Train_Loss: 1.6031104040145874  Accuracy on Support set:20.0\n",
      "Train_Epoch: 20  Train_Loss: 1.6027536296844482  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 21  Train_Loss: 1.6023876094818115  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 22  Train_Loss: 1.6020197057724  Accuracy on Support set:32.0\n",
      "Train_Epoch: 23  Train_Loss: 1.6016509675979613  Accuracy on Support set:32.0\n",
      "Train_Epoch: 24  Train_Loss: 1.601276307106018  Accuracy on Support set:44.0\n",
      "Train_Epoch: 25  Train_Loss: 1.6009021091461182  Accuracy on Support set:44.0\n",
      "Train_Epoch: 26  Train_Loss: 1.600517554283142  Accuracy on Support set:52.0\n",
      "Train_Epoch: 27  Train_Loss: 1.600127282142639  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 28  Train_Loss: 1.5997155427932739  Accuracy on Support set:64.0\n",
      "Train_Epoch: 29  Train_Loss: 1.5992998027801513  Accuracy on Support set:68.0\n",
      "Train_Epoch: 30  Train_Loss: 1.5988720655441284  Accuracy on Support set:80.0\n",
      "Train_Epoch: 31  Train_Loss: 1.5984129190444947  Accuracy on Support set:84.0\n",
      "Train_Epoch: 32  Train_Loss: 1.597949047088623  Accuracy on Support set:92.0\n",
      "Train_Epoch: 33  Train_Loss: 1.5974562168121338  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 1.5969600772857666  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 1.5964499664306642  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 1.595932250022888  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 1.5954106664657592  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 1.5948778247833253  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 1.5943306827545165  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 1.5937667608261108  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 1.5931894493103027  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 1.5926007080078124  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 1.5919858455657958  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 1.5913550138473511  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 1.590704026222229  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 1.5900400638580323  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 1.5893411922454834  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 1.5886207246780395  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 1.5878752946853638  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  80.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.19907814264297485 tensor([0.2000, 0.2004, 0.1995, 0.2010, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19822250306606293 tensor([0.1992, 0.2011, 0.2007, 0.2007, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983024626970291 tensor([0.1983, 0.2015, 0.2013, 0.1991, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19804546236991882 tensor([0.2008, 0.2006, 0.1980, 0.2017, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19889338314533234 tensor([0.2004, 0.1989, 0.1990, 0.2000, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19800671935081482 tensor([0.2021, 0.2010, 0.1980, 0.2003, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19918285310268402 tensor([0.1994, 0.2014, 0.2001, 0.1999, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1992569863796234 tensor([0.1993, 0.2000, 0.2018, 0.1996, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978754997253418 tensor([0.2005, 0.1997, 0.1979, 0.2034, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19886505603790283 tensor([0.1989, 0.2002, 0.1997, 0.2003, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197408989071846 tensor([0.2035, 0.1999, 0.1974, 0.2013, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19870927929878235 tensor([0.1987, 0.2009, 0.2003, 0.2006, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1977042406797409 tensor([0.1977, 0.2005, 0.2043, 0.1990, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19873633980751038 tensor([0.1996, 0.2006, 0.2006, 0.2005, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1985950469970703 tensor([0.1986, 0.1990, 0.1999, 0.1996, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969410926103592 tensor([0.2037, 0.1986, 0.1969, 0.2015, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19775649905204773 tensor([0.1988, 0.2036, 0.1996, 0.2002, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19885455071926117 tensor([0.1989, 0.1998, 0.2022, 0.1998, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19822178781032562 tensor([0.2000, 0.2010, 0.1999, 0.2008, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988348662853241 tensor([0.1995, 0.2003, 0.1988, 0.1995, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19629451632499695 tensor([0.2041, 0.1994, 0.1963, 0.2020, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19938842952251434 tensor([0.2001, 0.2002, 0.1994, 0.2001, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199178546667099 tensor([0.1993, 0.2009, 0.2010, 0.1996, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19775143265724182 tensor([0.1999, 0.2001, 0.2003, 0.2020, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1994280219078064 tensor([0.1994, 0.2005, 0.1998, 0.1997, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19829146564006805 tensor([0.2019, 0.1998, 0.1983, 0.2003, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19909988343715668 tensor([0.1994, 0.2025, 0.1994, 0.1995, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989307850599289 tensor([0.1989, 0.2003, 0.2020, 0.1992, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1988636553287506 tensor([0.1989, 0.2001, 0.2001, 0.2018, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19839566946029663 tensor([0.2007, 0.1991, 0.1984, 0.2002, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19762827455997467 tensor([0.2016, 0.2004, 0.1976, 0.2010, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19824181497097015 tensor([0.2016, 0.2005, 0.1982, 0.2009, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19907575845718384 tensor([0.1995, 0.2002, 0.2014, 0.1991, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19896440207958221 tensor([0.2006, 0.2000, 0.1990, 0.2015, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19915108382701874 tensor([0.1992, 0.1992, 0.1993, 0.1996, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19757002592086792 tensor([0.2028, 0.1984, 0.1976, 0.2013, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19813227653503418 tensor([0.1992, 0.2034, 0.1992, 0.2000, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19901704788208008 tensor([0.1996, 0.1994, 0.2021, 0.2000, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19912943243980408 tensor([0.1991, 0.2006, 0.2007, 0.2001, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1986575424671173 tensor([0.2004, 0.1996, 0.1987, 0.1995, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971350610256195 tensor([0.2033, 0.1994, 0.1971, 0.2015, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19870734214782715 tensor([0.1987, 0.2032, 0.1999, 0.1993, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19756343960762024 tensor([0.1976, 0.2007, 0.2035, 0.1987, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19854174554347992 tensor([0.1986, 0.2002, 0.1994, 0.2032, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19861847162246704 tensor([0.2022, 0.1995, 0.1986, 0.2006, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19854195415973663 tensor([0.2020, 0.1992, 0.1985, 0.2008, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19830934703350067 tensor([0.1983, 0.2013, 0.2021, 0.1993, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19902001321315765 tensor([0.1990, 0.1995, 0.2020, 0.1998, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19852912425994873 tensor([0.1992, 0.1992, 0.1985, 0.2032, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19944941997528076 tensor([0.2005, 0.1994, 0.1997, 0.2002, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19892656803131104 tensor([0.2004, 0.1999, 0.1992, 0.2015, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19876252114772797 tensor([0.1988, 0.2020, 0.1998, 0.1998, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19817309081554413 tensor([0.1986, 0.2016, 0.2025, 0.1991, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19909566640853882 tensor([0.2014, 0.1998, 0.1991, 0.2005, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1979699432849884 tensor([0.2016, 0.1997, 0.1980, 0.2008, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19811244308948517 tensor([0.2020, 0.1991, 0.1981, 0.2014, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19815854728221893 tensor([0.1999, 0.2032, 0.1988, 0.2000, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19763001799583435 tensor([0.1976, 0.2018, 0.2037, 0.1978, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19892707467079163 tensor([0.1989, 0.1996, 0.1995, 0.2028, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19924388825893402 tensor([0.2000, 0.2002, 0.1992, 0.2003, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19700656831264496 tensor([0.2035, 0.2001, 0.1970, 0.2009, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19848036766052246 tensor([0.1985, 0.2028, 0.2003, 0.1996, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1981705129146576 tensor([0.1982, 0.2008, 0.2027, 0.1997, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983993649482727 tensor([0.1984, 0.2000, 0.2019, 0.2009, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19855405390262604 tensor([0.2008, 0.1986, 0.1987, 0.1996, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19704148173332214 tensor([0.2036, 0.1994, 0.1970, 0.2017, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976790726184845 tensor([0.1992, 0.2032, 0.2007, 0.1993, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1972319632768631 tensor([0.1972, 0.2000, 0.2040, 0.1990, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197824165225029 tensor([0.2023, 0.1998, 0.1978, 0.2008, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19946779310703278 tensor([0.1995, 0.1995, 0.1996, 0.1999, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19708813726902008 tensor([0.2032, 0.1995, 0.1971, 0.2015, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980854570865631 tensor([0.1993, 0.2035, 0.1991, 0.2000, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19830811023712158 tensor([0.1985, 0.2005, 0.2035, 0.1983, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19750303030014038 tensor([0.1987, 0.2002, 0.2008, 0.2027, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19863928854465485 tensor([0.2008, 0.1990, 0.1986, 0.1992, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19807370007038116 tensor([0.2019, 0.1997, 0.1981, 0.2004, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19795072078704834 tensor([0.1980, 0.2006, 0.2018, 0.2000, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19806493818759918 tensor([0.1981, 0.2008, 0.2019, 0.2001, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19952668249607086 tensor([0.1995, 0.2006, 0.2001, 0.1996, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19702021777629852 tensor([0.2022, 0.1996, 0.1970, 0.2007, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19773004949092865 tensor([0.2025, 0.1996, 0.1977, 0.2012, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19842223823070526 tensor([0.1984, 0.2027, 0.2008, 0.1995, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19693247973918915 tensor([0.1969, 0.2018, 0.2037, 0.1994, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19892336428165436 tensor([0.1989, 0.2000, 0.2004, 0.2005, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19834069907665253 tensor([0.2000, 0.1987, 0.1983, 0.2011, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19818343222141266 tensor([0.2019, 0.1999, 0.1982, 0.2013, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19908146560192108 tensor([0.1995, 0.2011, 0.1998, 0.2005, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19796118140220642 tensor([0.1980, 0.2011, 0.2024, 0.1991, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983989179134369 tensor([0.1989, 0.2008, 0.2024, 0.1995, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19904828071594238 tensor([0.2012, 0.1990, 0.1991, 0.1996, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19648434221744537 tensor([0.2044, 0.2002, 0.1965, 0.2015, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19828973710536957 tensor([0.1994, 0.2007, 0.2015, 0.2002, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988486349582672 tensor([0.1991, 0.1999, 0.2021, 0.2000, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19869200885295868 tensor([0.1987, 0.1999, 0.1998, 0.1997, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1982724964618683 tensor([0.1983, 0.1996, 0.2009, 0.2003, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19916769862174988 tensor([0.2004, 0.1992, 0.2008, 0.1999, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19904358685016632 tensor([0.1995, 0.2013, 0.2008, 0.1994, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19695495069026947 tensor([0.1970, 0.2006, 0.2047, 0.1982, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1974238008260727 tensor([0.1974, 0.2004, 0.2009, 0.2014, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19779615104198456 tensor([0.1990, 0.1978, 0.1996, 0.1999, 0.2036], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19884230196475983 tensor([0.2009, 0.2000, 0.1988, 0.2009, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19887834787368774 tensor([0.1989, 0.2014, 0.2007, 0.2001, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983674168586731 tensor([0.1984, 0.2000, 0.2024, 0.1999, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19802622497081757 tensor([0.1980, 0.2004, 0.2003, 0.2029, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19910570979118347 tensor([0.1992, 0.2002, 0.2003, 0.1991, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19756002724170685 tensor([0.2029, 0.2002, 0.1987, 0.2006, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19929681718349457 tensor([0.1997, 0.2011, 0.2001, 0.1998, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19910362362861633 tensor([0.1998, 0.2000, 0.2005, 0.2006, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.197318896651268 tensor([0.1973, 0.2011, 0.2022, 0.2006, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987466812133789 tensor([0.1989, 0.1990, 0.1987, 0.2010, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971791684627533 tensor([0.2026, 0.1996, 0.1972, 0.2015, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977720707654953 tensor([0.1996, 0.2028, 0.1996, 0.2001, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19699013233184814 tensor([0.1970, 0.2009, 0.2036, 0.1996, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19838111102581024 tensor([0.1998, 0.2002, 0.1984, 0.2031, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19691525399684906 tensor([0.2022, 0.1983, 0.1969, 0.2007, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.196511909365654 tensor([0.2042, 0.1998, 0.1965, 0.2009, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988268345594406 tensor([0.1998, 0.2020, 0.1988, 0.2003, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19834575057029724 tensor([0.2001, 0.2006, 0.2009, 0.2001, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19855883717536926 tensor([0.1998, 0.1997, 0.1986, 0.2033, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19907495379447937 tensor([0.1993, 0.1997, 0.1997, 0.1991, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19741149246692657 tensor([0.2021, 0.2002, 0.1974, 0.2013, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19848039746284485 tensor([0.1989, 0.2023, 0.2004, 0.1999, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19760510325431824 tensor([0.1976, 0.2007, 0.2027, 0.1995, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1981348693370819 tensor([0.1981, 0.2005, 0.2014, 0.2015, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19928930699825287 tensor([0.2005, 0.1997, 0.1993, 0.2004, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19716547429561615 tensor([0.2031, 0.1998, 0.1972, 0.2013, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979045271873474 tensor([0.1988, 0.2039, 0.1997, 0.1997, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19804152846336365 tensor([0.2006, 0.2002, 0.2010, 0.2002, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19790488481521606 tensor([0.2012, 0.2001, 0.1987, 0.2021, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1986120343208313 tensor([0.2017, 0.1989, 0.1986, 0.1999, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972717046737671 tensor([0.2034, 0.1996, 0.1973, 0.2011, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1988377869129181 tensor([0.1988, 0.2015, 0.2011, 0.1990, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19839274883270264 tensor([0.1984, 0.2003, 0.2030, 0.1986, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19892792403697968 tensor([0.2002, 0.2008, 0.2001, 0.2000, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1985873281955719 tensor([0.1986, 0.1999, 0.1990, 0.2000, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19843453168869019 tensor([0.2021, 0.2000, 0.1984, 0.2006, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19837647676467896 tensor([0.1991, 0.2013, 0.2013, 0.1999, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1971045583486557 tensor([0.1971, 0.2012, 0.2033, 0.1989, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19854620099067688 tensor([0.2003, 0.2011, 0.1985, 0.2003, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19857577979564667 tensor([0.1996, 0.1986, 0.1995, 0.1999, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19617894291877747 tensor([0.2049, 0.1994, 0.1962, 0.2015, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1990164816379547 tensor([0.1996, 0.2015, 0.1990, 0.2007, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19754189252853394 tensor([0.1975, 0.2009, 0.2040, 0.1993, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19897226989269257 tensor([0.1990, 0.1997, 0.1997, 0.2024, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19902746379375458 tensor([0.1995, 0.1990, 0.2000, 0.1999, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1965717077255249 tensor([0.2036, 0.1997, 0.1966, 0.2011, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19904078543186188 tensor([0.1995, 0.2015, 0.2006, 0.1994, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19731177389621735 tensor([0.1973, 0.2011, 0.2032, 0.1997, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19886235892772675 tensor([0.1989, 0.2000, 0.2004, 0.2017, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19874995946884155 tensor([0.2007, 0.1989, 0.1987, 0.2014, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19683364033699036 tensor([0.2033, 0.1996, 0.1968, 0.2008, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19841191172599792 tensor([0.1984, 0.2001, 0.2013, 0.1998, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19712428748607635 tensor([0.1971, 0.2009, 0.2035, 0.1995, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980431228876114 tensor([0.2004, 0.1999, 0.1992, 0.2025, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19849945604801178 tensor([0.2003, 0.1990, 0.1985, 0.1993, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19830851256847382 tensor([0.2017, 0.1997, 0.1983, 0.2008, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983870267868042 tensor([0.1989, 0.2032, 0.1996, 0.1999, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19814668595790863 tensor([0.1981, 0.2004, 0.2011, 0.2003, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19908715784549713 tensor([0.1993, 0.2001, 0.2006, 0.2008, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19816333055496216 tensor([0.1982, 0.2008, 0.2014, 0.1998, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19705982506275177 tensor([0.2037, 0.1997, 0.1971, 0.2014, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19828858971595764 tensor([0.1988, 0.2038, 0.1999, 0.1992, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19792908430099487 tensor([0.1979, 0.2012, 0.2030, 0.1995, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1986619234085083 tensor([0.1992, 0.2006, 0.1987, 0.2029, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19855020940303802 tensor([0.2008, 0.2001, 0.1986, 0.1999, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19738732278347015 tensor([0.2022, 0.2002, 0.1974, 0.2012, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19851432740688324 tensor([0.1998, 0.2016, 0.1994, 0.2007, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19827282428741455 tensor([0.1983, 0.1998, 0.2017, 0.2000, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19902639091014862 tensor([0.1998, 0.1991, 0.2003, 0.2017, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.198417529463768 tensor([0.2003, 0.1998, 0.1984, 0.2000, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19672200083732605 tensor([0.2051, 0.1992, 0.1967, 0.2015, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19747930765151978 tensor([0.1997, 0.2034, 0.1994, 0.2000, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19682705402374268 tensor([0.1968, 0.2018, 0.2038, 0.1989, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.198186993598938 tensor([0.1995, 0.2018, 0.1994, 0.2011, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19814975559711456 tensor([0.2008, 0.1998, 0.1981, 0.2000, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19753219187259674 tensor([0.2029, 0.2000, 0.1975, 0.2009, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19927166402339935 tensor([0.1996, 0.2004, 0.2012, 0.1996, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19828678667545319 tensor([0.1983, 0.2002, 0.2024, 0.1997, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19875985383987427 tensor([0.1995, 0.2003, 0.2001, 0.2014, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19824452698230743 tensor([0.2007, 0.1993, 0.1982, 0.2004, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19616791605949402 tensor([0.2040, 0.1992, 0.1962, 0.2018, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1990320086479187 tensor([0.1994, 0.2024, 0.1996, 0.1996, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19745802879333496 tensor([0.1975, 0.2002, 0.2029, 0.1990, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19765649735927582 tensor([0.1977, 0.1993, 0.2020, 0.2010, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1987275928258896 tensor([0.2003, 0.1987, 0.1991, 0.1996, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19761593639850616 tensor([0.2025, 0.1993, 0.1976, 0.1999, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19850359857082367 tensor([0.1996, 0.2022, 0.1992, 0.2006, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19836680591106415 tensor([0.1992, 0.2002, 0.2028, 0.1993, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19852885603904724 tensor([0.1986, 0.2005, 0.1994, 0.2030, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19827596843242645 tensor([0.2006, 0.2000, 0.1983, 0.1996, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1989433318376541 tensor([0.2001, 0.2011, 0.2004, 0.1994, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1988673210144043 tensor([0.1989, 0.2001, 0.2010, 0.2010, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19837775826454163 tensor([0.1984, 0.2005, 0.2017, 0.2006, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19908493757247925 tensor([0.2004, 0.1991, 0.1996, 0.2015, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19931560754776 tensor([0.1998, 0.1993, 0.1994, 0.2003, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19684812426567078 tensor([0.2033, 0.1997, 0.1968, 0.2012, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19879819452762604 tensor([0.1999, 0.2017, 0.1989, 0.2007, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19624178111553192 tensor([0.1962, 0.2011, 0.2038, 0.1993, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19749583303928375 tensor([0.2003, 0.1997, 0.1985, 0.2040, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19806505739688873 tensor([0.2019, 0.1988, 0.1981, 0.2016, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1973033845424652 tensor([0.2024, 0.2004, 0.1973, 0.2009, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19860415160655975 tensor([0.2004, 0.2021, 0.1988, 0.2001, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19734787940979004 tensor([0.1973, 0.2013, 0.2041, 0.1992, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985238641500473 tensor([0.1988, 0.2014, 0.2004, 0.2009, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19921712577342987 tensor([0.1995, 0.1998, 0.1993, 0.1992, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19883683323860168 tensor([0.2014, 0.1999, 0.1988, 0.2007, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980842649936676 tensor([0.2002, 0.2020, 0.1991, 0.2007, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19669322669506073 tensor([0.1967, 0.2009, 0.2041, 0.1995, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19873124361038208 tensor([0.1988, 0.2008, 0.2019, 0.1998, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19889532029628754 tensor([0.2005, 0.1993, 0.1989, 0.2001, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971178948879242 tensor([0.2032, 0.1996, 0.1971, 0.2011, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1985471546649933 tensor([0.1985, 0.2013, 0.2010, 0.1997, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19819843769073486 tensor([0.1982, 0.1999, 0.2015, 0.2007, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983608454465866 tensor([0.1996, 0.2011, 0.2001, 0.2008, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19912485778331757 tensor([0.1991, 0.1998, 0.1997, 0.2014, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19684699177742004 tensor([0.2037, 0.1985, 0.1968, 0.2014, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19723628461360931 tensor([0.1996, 0.2030, 0.1997, 0.2004, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19879482686519623 tensor([0.1988, 0.2006, 0.2009, 0.1996, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19952504336833954 tensor([0.1998, 0.2001, 0.1995, 0.2009, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19916148483753204 tensor([0.2006, 0.1992, 0.1995, 0.1992, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1961563676595688 tensor([0.2032, 0.1997, 0.1962, 0.2009, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19904972612857819 tensor([0.1996, 0.2020, 0.1998, 0.1996, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1988467425107956 tensor([0.1988, 0.2009, 0.2015, 0.1992, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987374871969223 tensor([0.1995, 0.2004, 0.2004, 0.2009, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19767265021800995 tensor([0.2004, 0.1977, 0.1985, 0.2007, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19936202466487885 tensor([0.2005, 0.1998, 0.2002, 0.2002, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19902797043323517 tensor([0.1991, 0.2014, 0.2011, 0.1990, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19870072603225708 tensor([0.1995, 0.2014, 0.2007, 0.1996, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19929860532283783 tensor([0.1996, 0.2005, 0.1999, 0.2006, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19878363609313965 tensor([0.2006, 0.2002, 0.1988, 0.1997, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983110010623932 tensor([0.2024, 0.1997, 0.1986, 0.2009, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982887089252472 tensor([0.1985, 0.2030, 0.2003, 0.1999, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983744502067566 tensor([0.1984, 0.2010, 0.2022, 0.1990, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985931247472763 tensor([0.1997, 0.1997, 0.1987, 0.2034, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19874891638755798 tensor([0.1998, 0.1995, 0.1987, 0.1999, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19802157580852509 tensor([0.2030, 0.1998, 0.1980, 0.2008, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19928817451000214 tensor([0.2007, 0.1997, 0.1993, 0.2009, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19910834729671478 tensor([0.2019, 0.1999, 0.1994, 0.1996, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19890430569648743 tensor([0.1991, 0.2004, 0.2010, 0.2006, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19893349707126617 tensor([0.1999, 0.1999, 0.1989, 0.1998, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19934406876564026 tensor([0.2014, 0.1996, 0.1995, 0.2001, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19898733496665955 tensor([0.1990, 0.2001, 0.2005, 0.2002, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985846310853958 tensor([0.1987, 0.2011, 0.2029, 0.1987, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19882209599018097 tensor([0.1993, 0.1999, 0.2002, 0.2017, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19875752925872803 tensor([0.2005, 0.1996, 0.1988, 0.1998, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1979914754629135 tensor([0.2031, 0.1992, 0.1980, 0.2001, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979149729013443 tensor([0.1990, 0.2039, 0.2004, 0.1988, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19888083636760712 tensor([0.1989, 0.2002, 0.1998, 0.2015, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987016499042511 tensor([0.2011, 0.2006, 0.1987, 0.2009, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19895628094673157 tensor([0.1994, 0.1991, 0.2005, 0.1990, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "[[4], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [0], [0], [4], [0], [2], [4], [0], [4], [2], [2], [2], [4], [4], [0], [2], [4], [0], [0], [2], [2], [2], [3], [2], [0], [2], [4], [4], [0], [2], [2], [0], [0], [4], [2], [2], [0], [0], [2], [1], [4], [0], [4], [2], [2], [2], [4], [0], [0], [2], [2], [0], [0], [0], [1], [2], [4], [0], [2], [0], [2], [4], [3], [4], [2], [2], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [4], [0], [4], [1], [2], [4], [4], [0], [0], [1], [4], [0], [0], [1], [2], [4], [0], [0], [3], [4], [4], [4], [0], [2], [2], [4], [0], [2], [2], [2], [2], [4], [2], [3], [2], [4], [0], [0], [2], [2], [4], [4], [4], [2], [2], [0], [0], [4], [0], [2], [4], [0], [2], [1], [2], [2], [0], [0], [1], [2], [4], [0], [0], [2], [2], [0], [0], [4], [2], [2], [4], [0], [4], [0], [2], [4], [0], [4], [2], [2], [4], [0], [4], [2], [2], [4], [0], [4], [2], [2], [4], [0], [4], [2], [2], [4], [0], [0], [1], [2], [4], [4], [4], [2], [4], [0], [0], [1], [1], [2], [4], [0], [4], [2], [2], [4], [0], [4], [3], [2], [4], [0], [4], [2], [2], [0], [0], [4], [0], [2], [4], [0], [2], [3], [2], [4], [0], [4], [1], [4], [3], [4], [4], [2], [4], [4], [0], [4], [2], [2], [4], [4], [4], [2], [4], [0], [4], [4], [2], [2], [4], [0], [2], [3]]\n",
      "[[0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4]]\n",
      "NL_pred of 0th iteration [[4], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [0], [0], [4], [0], [2], [4], [0], [4], [2], [2], [2], [4], [4], [0], [2], [4], [0], [0], [2], [2], [2], [3], [2], [0], [2], [4], [4], [0], [2], [2], [0], [0], [4], [2], [2], [0], [0], [2], [1], [4], [0], [4], [2], [2], [2], [4], [0], [0], [2], [2], [0], [0], [0], [1], [2], [4], [0], [2], [0], [2], [4], [3], [4], [2], [2], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [4], [0], [4], [1], [2], [4], [4], [0], [0], [1], [4], [0], [0], [1], [2], [4], [0], [0], [3], [4], [4], [4], [0], [2], [2], [4], [0], [2], [2], [2], [2], [4], [2], [3], [2], [4], [0], [0], [2], [2], [4], [4], [4], [2], [2], [0], [0], [4], [0], [2], [4], [0], [2], [1], [2], [2], [0], [0], [1], [2], [4], [0], [0], [2], [2], [0], [0], [4], [2], [2], [4], [0], [4], [0], [2], [4], [0], [4], [2], [2], [4], [0], [4], [2], [2], [4], [0], [4], [2], [2], [4], [0], [4], [2], [2], [4], [0], [0], [1], [2], [4], [4], [4], [2], [4], [0], [0], [1], [1], [2], [4], [0], [4], [2], [2], [4], [0], [4], [3], [2], [4], [0], [4], [2], [2], [0], [0], [4], [0], [2], [4], [0], [2], [3], [2], [4], [0], [4], [1], [4], [3], [4], [4], [2], [4], [4], [0], [4], [2], [2], [4], [4], [4], [2], [4], [0], [4], [4], [2], [2], [4], [0], [2], [3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005142489910125733  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005142467498779297  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.00514242172241211  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005142357349395752  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0051422762870788575  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005142178535461426  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005142067909240723  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.00514194393157959  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005141809463500977  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005141663551330567  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005141510486602783  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005141346931457519  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005141176700592041  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005141000270843506  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00514081859588623  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005140629768371582  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0051404361724853515  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005140239238739013  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005140037536621094  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005139832973480225  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.1984669715166092 tensor([0.1995, 0.2015, 0.1985, 0.2022, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19860884547233582 tensor([0.1986, 0.2023, 0.1997, 0.2020, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19905520975589752 tensor([0.1977, 0.2026, 0.2003, 0.2003, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980915665626526 tensor([0.2003, 0.2017, 0.1970, 0.2029, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19801247119903564 tensor([0.1998, 0.2000, 0.1980, 0.2013, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978331357240677 tensor([0.2015, 0.2022, 0.1970, 0.2015, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19884252548217773 tensor([0.1988, 0.2026, 0.1991, 0.2011, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19863735139369965 tensor([0.1987, 0.2011, 0.2007, 0.2008, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19768546521663666 tensor([0.1999, 0.2009, 0.1968, 0.2047, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19862952828407288 tensor([0.1983, 0.2013, 0.1986, 0.2016, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19711200892925262 tensor([0.2029, 0.2010, 0.1964, 0.2025, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19868169724941254 tensor([0.1982, 0.2021, 0.1993, 0.2018, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19774779677391052 tensor([0.1971, 0.2016, 0.2033, 0.2002, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19905532896518707 tensor([0.1991, 0.2017, 0.1995, 0.2017, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19890379905700684 tensor([0.1980, 0.2001, 0.1989, 0.2009, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19846895337104797 tensor([0.2032, 0.1997, 0.1959, 0.2027, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1982911378145218 tensor([0.1983, 0.2047, 0.1986, 0.2014, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985633820295334 tensor([0.1983, 0.2009, 0.2012, 0.2010, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1989039182662964 tensor([0.1994, 0.2022, 0.1989, 0.2021, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989675611257553 tensor([0.1990, 0.2014, 0.1978, 0.2007, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974385529756546 tensor([0.2035, 0.2006, 0.1953, 0.2032, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19935400784015656 tensor([0.1996, 0.2014, 0.1984, 0.2013, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19876728951931 tensor([0.1988, 0.2020, 0.2000, 0.2008, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19927671551704407 tensor([0.1993, 0.2012, 0.1993, 0.2032, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19875144958496094 tensor([0.1989, 0.2016, 0.1988, 0.2009, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19888482987880707 tensor([0.2014, 0.2009, 0.1973, 0.2015, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19841057062149048 tensor([0.1989, 0.2037, 0.1984, 0.2007, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19871602952480316 tensor([0.1984, 0.2014, 0.2010, 0.2005, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19833742082118988 tensor([0.1983, 0.2013, 0.1991, 0.2030, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20010416209697723 tensor([0.2001, 0.2002, 0.1974, 0.2014, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19860807061195374 tensor([0.2010, 0.2015, 0.1966, 0.2023, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979718804359436 tensor([0.2011, 0.2016, 0.1972, 0.2022, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19890764355659485 tensor([0.1989, 0.2014, 0.2004, 0.2003, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19819515943527222 tensor([0.2000, 0.2011, 0.1979, 0.2027, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1982446312904358 tensor([0.1986, 0.2004, 0.1982, 0.2008, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19917717576026917 tensor([0.2022, 0.1995, 0.1966, 0.2025, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1982286125421524 tensor([0.1986, 0.2045, 0.1982, 0.2012, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19902825355529785 tensor([0.1990, 0.2005, 0.2010, 0.2012, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19866476953029633 tensor([0.1986, 0.2017, 0.1997, 0.2013, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19984468817710876 tensor([0.1998, 0.2007, 0.1976, 0.2007, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978485882282257 tensor([0.2027, 0.2006, 0.1961, 0.2027, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981029361486435 tensor([0.1982, 0.2043, 0.1989, 0.2005, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19880230724811554 tensor([0.1970, 0.2018, 0.2025, 0.1999, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19809097051620483 tensor([0.1981, 0.2013, 0.1984, 0.2045, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19833484292030334 tensor([0.2017, 0.2006, 0.1976, 0.2018, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19870628416538239 tensor([0.2014, 0.2003, 0.1975, 0.2021, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19830313324928284 tensor([0.1978, 0.2024, 0.2011, 0.2005, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19891126453876495 tensor([0.1985, 0.2006, 0.2010, 0.2010, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19866225123405457 tensor([0.1987, 0.2004, 0.1975, 0.2044, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19872991740703583 tensor([0.1999, 0.2006, 0.1987, 0.2014, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19822192192077637 tensor([0.1998, 0.2011, 0.1982, 0.2027, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19879859685897827 tensor([0.1982, 0.2031, 0.1988, 0.2010, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19806428253650665 tensor([0.1981, 0.2027, 0.2015, 0.2003, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19836078584194183 tensor([0.2009, 0.2010, 0.1981, 0.2017, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19907787442207336 tensor([0.2010, 0.2009, 0.1969, 0.2021, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19857674837112427 tensor([0.2014, 0.2002, 0.1971, 0.2027, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978011578321457 tensor([0.1993, 0.2043, 0.1978, 0.2012, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19826216995716095 tensor([0.1971, 0.2029, 0.2027, 0.1990, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19836659729480743 tensor([0.1984, 0.2007, 0.1985, 0.2041, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19945402443408966 tensor([0.1995, 0.2013, 0.1982, 0.2015, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977270543575287 tensor([0.2029, 0.2012, 0.1960, 0.2021, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979983150959015 tensor([0.1979, 0.2039, 0.1993, 0.2009, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19787855446338654 tensor([0.1976, 0.2019, 0.2017, 0.2009, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19805829226970673 tensor([0.1978, 0.2011, 0.2009, 0.2021, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976865828037262 tensor([0.2002, 0.1997, 0.1977, 0.2008, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19753903150558472 tensor([0.2030, 0.2005, 0.1960, 0.2029, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19860443472862244 tensor([0.1986, 0.2044, 0.1997, 0.2005, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19905589520931244 tensor([0.1967, 0.2011, 0.2030, 0.2002, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19840537011623383 tensor([0.2017, 0.2010, 0.1968, 0.2021, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19859503209590912 tensor([0.1989, 0.2006, 0.1986, 0.2011, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19789335131645203 tensor([0.2027, 0.2007, 0.1961, 0.2027, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19808702170848846 tensor([0.1988, 0.2046, 0.1981, 0.2013, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.197921022772789 tensor([0.1979, 0.2016, 0.2025, 0.1995, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1981280893087387 tensor([0.1981, 0.2014, 0.1998, 0.2040, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20010490715503693 tensor([0.2002, 0.2001, 0.1976, 0.2004, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1991296261548996 tensor([0.2013, 0.2009, 0.1971, 0.2016, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19885429739952087 tensor([0.1974, 0.2017, 0.2008, 0.2013, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19828557968139648 tensor([0.1975, 0.2020, 0.2009, 0.2013, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19906006753444672 tensor([0.1990, 0.2017, 0.1991, 0.2008, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199728861451149 tensor([0.2016, 0.2007, 0.1960, 0.2019, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19815312325954437 tensor([0.2019, 0.2007, 0.1967, 0.2025, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977909356355667 tensor([0.1979, 0.2038, 0.1997, 0.2008, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19745144248008728 tensor([0.1964, 0.2029, 0.2026, 0.2007, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993311643600464 tensor([0.1984, 0.2011, 0.1993, 0.2017, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19949038326740265 tensor([0.1995, 0.1998, 0.1973, 0.2023, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19797390699386597 tensor([0.2014, 0.2010, 0.1972, 0.2025, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987968385219574 tensor([0.1989, 0.2022, 0.1988, 0.2018, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19862507283687592 tensor([0.1974, 0.2022, 0.2014, 0.2003, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19831545650959015 tensor([0.1983, 0.2020, 0.2014, 0.2007, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19807402789592743 tensor([0.2006, 0.2002, 0.1981, 0.2009, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19669954478740692 tensor([0.2038, 0.2013, 0.1955, 0.2027, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19880899786949158 tensor([0.1988, 0.2018, 0.2004, 0.2014, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19854658842086792 tensor([0.1985, 0.2010, 0.2011, 0.2012, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987774819135666 tensor([0.1981, 0.2010, 0.1988, 0.2009, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1999104917049408 tensor([0.1977, 0.2007, 0.1999, 0.2015, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19903089106082916 tensor([0.1998, 0.2003, 0.1998, 0.2011, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989072859287262 tensor([0.1989, 0.2024, 0.1998, 0.2006, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988108903169632 tensor([0.1964, 0.2017, 0.2037, 0.1994, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199192076921463 tensor([0.1969, 0.2015, 0.1998, 0.2026, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19849401712417603 tensor([0.1985, 0.1989, 0.1986, 0.2011, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19856615364551544 tensor([0.2003, 0.2011, 0.1978, 0.2022, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19837453961372375 tensor([0.1984, 0.2025, 0.1997, 0.2013, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19850300252437592 tensor([0.1978, 0.2012, 0.2014, 0.2011, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19756482541561127 tensor([0.1975, 0.2015, 0.1993, 0.2041, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19865377247333527 tensor([0.1987, 0.2013, 0.1993, 0.2003, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19765345752239227 tensor([0.2023, 0.2014, 0.1977, 0.2019, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1990707814693451 tensor([0.1992, 0.2022, 0.1991, 0.2010, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1992032527923584 tensor([0.1992, 0.2012, 0.1995, 0.2019, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19805236160755157 tensor([0.1968, 0.2022, 0.2012, 0.2018, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983305960893631 tensor([0.1983, 0.2002, 0.1977, 0.2022, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19830946624279022 tensor([0.2020, 0.2008, 0.1962, 0.2027, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1986071765422821 tensor([0.1991, 0.2040, 0.1986, 0.2014, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981971710920334 tensor([0.1964, 0.2020, 0.2025, 0.2008, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19779331982135773 tensor([0.1992, 0.2013, 0.1974, 0.2044, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19942621886730194 tensor([0.2017, 0.1994, 0.1959, 0.2019, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19786444306373596 tensor([0.2036, 0.2009, 0.1955, 0.2021, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19828975200653076 tensor([0.1992, 0.2032, 0.1978, 0.2015, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19956927001476288 tensor([0.1996, 0.2017, 0.1999, 0.2013, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.197830930352211 tensor([0.1993, 0.2008, 0.1975, 0.2045, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19865891337394714 tensor([0.1988, 0.2009, 0.1987, 0.2003, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19817188382148743 tensor([0.2016, 0.2014, 0.1964, 0.2025, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1983456164598465 tensor([0.1983, 0.2034, 0.1994, 0.2011, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987222284078598 tensor([0.1971, 0.2018, 0.2017, 0.2007, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19761818647384644 tensor([0.1976, 0.2017, 0.2004, 0.2028, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19931960105895996 tensor([0.2000, 0.2009, 0.1983, 0.2016, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19783759117126465 tensor([0.2026, 0.2009, 0.1961, 0.2025, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19820663332939148 tensor([0.1982, 0.2051, 0.1987, 0.2009, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20002056658267975 tensor([0.2000, 0.2013, 0.2000, 0.2014, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976652890443802 tensor([0.2006, 0.2012, 0.1977, 0.2033, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20002208650112152 tensor([0.2012, 0.2000, 0.1976, 0.2011, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978192925453186 tensor([0.2029, 0.2007, 0.1962, 0.2023, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987418383359909 tensor([0.1983, 0.2027, 0.2000, 0.2002, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19895493984222412 tensor([0.1978, 0.2014, 0.2020, 0.1999, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19908124208450317 tensor([0.1997, 0.2019, 0.1991, 0.2012, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1979861706495285 tensor([0.1980, 0.2010, 0.1980, 0.2012, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19804388284683228 tensor([0.2015, 0.2012, 0.1974, 0.2018, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19852608442306519 tensor([0.1985, 0.2024, 0.2003, 0.2012, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19868212938308716 tensor([0.1966, 0.2023, 0.2023, 0.2002, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1989598274230957 tensor([0.1997, 0.2022, 0.1975, 0.2015, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19848652184009552 tensor([0.1990, 0.1997, 0.1985, 0.2011, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19723591208457947 tensor([0.2043, 0.2005, 0.1952, 0.2027, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1984184831380844 tensor([0.1990, 0.2027, 0.1980, 0.2019, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19749927520751953 tensor([0.1970, 0.2020, 0.2030, 0.2005, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19845926761627197 tensor([0.1984, 0.2008, 0.1986, 0.2037, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19895674288272858 tensor([0.1990, 0.2002, 0.1990, 0.2011, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19823940098285675 tensor([0.2030, 0.2008, 0.1956, 0.2024, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19890278577804565 tensor([0.1989, 0.2026, 0.1996, 0.2006, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19792214035987854 tensor([0.1968, 0.2023, 0.2022, 0.2009, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1983136385679245 tensor([0.1983, 0.2011, 0.1994, 0.2029, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1994854062795639 tensor([0.2002, 0.2000, 0.1977, 0.2026, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.198650062084198 tensor([0.2028, 0.2008, 0.1958, 0.2020, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19971495866775513 tensor([0.1979, 0.2012, 0.2002, 0.2010, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980990320444107 tensor([0.1966, 0.2021, 0.2025, 0.2007, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19816452264785767 tensor([0.1999, 0.2010, 0.1982, 0.2037, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1997549682855606 tensor([0.1998, 0.2002, 0.1975, 0.2005, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19861170649528503 tensor([0.2012, 0.2009, 0.1973, 0.2021, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19837261736392975 tensor([0.1984, 0.2043, 0.1986, 0.2011, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1992652863264084 tensor([0.1976, 0.2015, 0.2001, 0.2016, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19877609610557556 tensor([0.1988, 0.2013, 0.1996, 0.2021, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19905461370944977 tensor([0.1976, 0.2019, 0.2004, 0.2010, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19728820025920868 tensor([0.2032, 0.2009, 0.1960, 0.2026, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19826067984104156 tensor([0.1983, 0.2050, 0.1989, 0.2004, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976110190153122 tensor([0.1974, 0.2024, 0.2019, 0.2007, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976437121629715 tensor([0.1987, 0.2017, 0.1976, 0.2041, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19978690147399902 tensor([0.2002, 0.2013, 0.1975, 0.2012, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19820116460323334 tensor([0.2017, 0.2013, 0.1963, 0.2024, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19837500154972076 tensor([0.1993, 0.2027, 0.1984, 0.2019, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1994512528181076 tensor([0.1977, 0.2009, 0.2007, 0.2012, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1992667019367218 tensor([0.1993, 0.2003, 0.1993, 0.2029, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1997058242559433 tensor([0.1997, 0.2009, 0.1974, 0.2012, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19676260650157928 tensor([0.2045, 0.2003, 0.1957, 0.2027, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19839543104171753 tensor([0.1992, 0.2045, 0.1984, 0.2012, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19790397584438324 tensor([0.1963, 0.2029, 0.2028, 0.2001, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1984032392501831 tensor([0.1990, 0.2029, 0.1984, 0.2023, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20020176470279694 tensor([0.2002, 0.2010, 0.1971, 0.2012, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19787785410881042 tensor([0.2024, 0.2011, 0.1965, 0.2021, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19907979667186737 tensor([0.1991, 0.2015, 0.2001, 0.2008, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19860388338565826 tensor([0.1977, 0.2014, 0.2013, 0.2010, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19890464842319489 tensor([0.1989, 0.2014, 0.1991, 0.2026, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2001519650220871 tensor([0.2002, 0.2004, 0.1972, 0.2016, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19806991517543793 tensor([0.2035, 0.2003, 0.1951, 0.2030, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19861732423305511 tensor([0.1988, 0.2035, 0.1986, 0.2008, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19957974553108215 tensor([0.1969, 0.2014, 0.2019, 0.2002, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1993105411529541 tensor([0.1971, 0.2004, 0.2009, 0.2022, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1980508714914322 tensor([0.1997, 0.1998, 0.1981, 0.2008, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20001943409442902 tensor([0.2019, 0.2004, 0.1966, 0.2011, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19817836582660675 tensor([0.1990, 0.2033, 0.1982, 0.2018, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1986846774816513 tensor([0.1987, 0.2014, 0.2018, 0.2005, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19804547727108002 tensor([0.1980, 0.2016, 0.1984, 0.2042, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20008403062820435 tensor([0.2001, 0.2011, 0.1973, 0.2009, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993958055973053 tensor([0.1996, 0.2022, 0.1994, 0.2006, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19821928441524506 tensor([0.1983, 0.2013, 0.1999, 0.2023, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19809399545192719 tensor([0.1978, 0.2016, 0.2007, 0.2018, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985289305448532 tensor([0.1999, 0.2002, 0.1986, 0.2028, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1983700692653656 tensor([0.1992, 0.2004, 0.1984, 0.2016, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982024759054184 tensor([0.2027, 0.2008, 0.1958, 0.2024, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978885978460312 tensor([0.1994, 0.2028, 0.1979, 0.2019, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988195925951004 tensor([0.1957, 0.2022, 0.2027, 0.2005, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19742253422737122 tensor([0.1997, 0.2009, 0.1974, 0.2053, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19882424175739288 tensor([0.2014, 0.1999, 0.1970, 0.2029, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19821538031101227 tensor([0.2018, 0.2015, 0.1963, 0.2022, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19774462282657623 tensor([0.1999, 0.2033, 0.1977, 0.2013, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972873955965042 tensor([0.1968, 0.2024, 0.2031, 0.2004, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19819852709770203 tensor([0.1982, 0.2026, 0.1994, 0.2021, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19823779165744781 tensor([0.1990, 0.2009, 0.1982, 0.2004, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1984170526266098 tensor([0.2008, 0.2010, 0.1978, 0.2019, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1981053650379181 tensor([0.1996, 0.2031, 0.1981, 0.2019, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980104297399521 tensor([0.1961, 0.2020, 0.2031, 0.2007, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19821621477603912 tensor([0.1982, 0.2020, 0.2009, 0.2010, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19996102154254913 tensor([0.2000, 0.2004, 0.1979, 0.2013, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19822195172309875 tensor([0.2026, 0.2008, 0.1961, 0.2023, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1986706554889679 tensor([0.1980, 0.2025, 0.1999, 0.2009, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19895091652870178 tensor([0.1976, 0.2010, 0.2004, 0.2019, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19906499981880188 tensor([0.1991, 0.2022, 0.1991, 0.2021, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19864800572395325 tensor([0.1986, 0.2010, 0.1986, 0.2026, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19880051910877228 tensor([0.2031, 0.1997, 0.1958, 0.2026, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19864468276500702 tensor([0.1991, 0.2042, 0.1986, 0.2016, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19924665987491608 tensor([0.1982, 0.2018, 0.1999, 0.2008, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19883006811141968 tensor([0.1993, 0.2012, 0.1985, 0.2022, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19847381114959717 tensor([0.2001, 0.2003, 0.1985, 0.2004, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19925040006637573 tensor([0.2027, 0.2008, 0.1951, 0.2022, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19874732196331024 tensor([0.1991, 0.2031, 0.1987, 0.2008, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19883166253566742 tensor([0.1983, 0.2020, 0.2004, 0.2004, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19899091124534607 tensor([0.1990, 0.2016, 0.1994, 0.2021, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1975153088569641 tensor([0.1998, 0.1988, 0.1975, 0.2019, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19921724498271942 tensor([0.1999, 0.2009, 0.1992, 0.2014, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19855685532093048 tensor([0.1986, 0.2025, 0.2001, 0.2002, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989668607711792 tensor([0.1990, 0.2025, 0.1997, 0.2009, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19885212182998657 tensor([0.1991, 0.2017, 0.1989, 0.2018, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20000874996185303 tensor([0.2000, 0.2013, 0.1978, 0.2009, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976020634174347 tensor([0.2019, 0.2009, 0.1976, 0.2021, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19794102013111115 tensor([0.1979, 0.2041, 0.1993, 0.2011, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19864509999752045 tensor([0.1978, 0.2021, 0.2012, 0.2002, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197673037648201 tensor([0.1991, 0.2008, 0.1977, 0.2046, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19925029575824738 tensor([0.1993, 0.2007, 0.1977, 0.2012, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977037489414215 tensor([0.2024, 0.2009, 0.1970, 0.2020, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19832049310207367 tensor([0.2002, 0.2009, 0.1983, 0.2021, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19843311607837677 tensor([0.2013, 0.2011, 0.1984, 0.2008, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19849887490272522 tensor([0.1985, 0.2016, 0.2000, 0.2018, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19937734305858612 tensor([0.1994, 0.2010, 0.1979, 0.2010, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19847364723682404 tensor([0.2009, 0.2008, 0.1985, 0.2013, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19944067299365997 tensor([0.1984, 0.2012, 0.1994, 0.2015, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1981755793094635 tensor([0.1982, 0.2022, 0.2019, 0.1999, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1987733691930771 tensor([0.1988, 0.2011, 0.1992, 0.2029, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19992657005786896 tensor([0.1999, 0.2007, 0.1977, 0.2010, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19879846274852753 tensor([0.2026, 0.2003, 0.1970, 0.2014, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19842840731143951 tensor([0.1984, 0.2051, 0.1994, 0.2000, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988040655851364 tensor([0.1983, 0.2014, 0.1988, 0.2027, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979539543390274 tensor([0.2005, 0.2017, 0.1977, 0.2021, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19880856573581696 tensor([0.1988, 0.2003, 0.1995, 0.2002, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 2], [4, 0], [0, 4], [2, 4], [1, 2], [2, 4], [4, 0], [0, 4], [2, 4], [0, 2], [2, 4], [0, 4], [0, 4], [4, 0], [0, 2], [2, 4], [4, 0], [0, 4], [4, 2], [2, 0], [2, 4], [2, 4], [4, 0], [4, 2], [0, 2], [2, 4], [4, 2], [0, 4], [0, 4], [2], [2, 4], [2, 4], [3, 0], [2, 4], [0, 2], [2, 4], [4, 2], [4, 0], [0, 4], [2, 0], [2, 4], [0, 4], [0, 4], [4, 0], [2, 4], [2, 4], [0, 4], [0, 4], [2, 0], [1, 2], [4, 2], [0, 2], [4, 0], [2, 4], [2, 4], [2, 4], [4, 2], [0, 4], [0, 4], [2, 4], [2, 4], [0, 4], [0, 4], [0, 4], [1, 2], [2, 4], [4, 0], [0, 4], [2, 4], [0, 2], [2, 4], [4, 2], [3, 0], [4, 0], [2], [2, 4], [0, 4], [0, 4], [0, 2], [2, 4], [2, 4], [0, 4], [0, 4], [0, 2], [2, 0], [2, 4], [4, 2], [0, 4], [4, 0], [1, 2], [2, 4], [4, 0], [4, 0], [0, 2], [0, 2], [1, 4], [4, 0], [0, 4], [0, 4], [1, 0], [2, 4], [4, 0], [0, 4], [0, 4], [3, 0], [4, 2], [4, 2], [4, 0], [0, 4], [2, 0], [2, 4], [4, 2], [0, 4], [2, 4], [2, 1], [2, 4], [2, 4], [4, 0], [2, 4], [3, 2], [2, 4], [4, 0], [0, 4], [0, 4], [2, 4], [2, 4], [4, 0], [4], [4, 2], [2], [2, 4], [0, 4], [0, 4], [4, 2], [0, 2], [2, 4], [4, 0], [0, 4], [2, 4], [1, 2], [2, 4], [2, 4], [0, 4], [0, 4], [1, 2], [2, 4], [4, 0], [0, 4], [0, 4], [2, 4], [2, 4], [0, 4], [0, 4], [4, 2], [2, 0], [2, 4], [4, 0], [0, 4], [4, 0], [0, 4], [2, 4], [4, 0], [0, 4], [4, 2], [2, 4], [2, 4], [4, 2], [0, 4], [4, 2], [2, 0], [2, 4], [4, 2], [0, 4], [4, 2], [2], [2, 4], [4, 0], [0, 4], [4, 0], [2], [2, 4], [4, 2], [0, 4], [0, 4], [1, 2], [2], [4, 2], [4, 0], [4, 0], [2], [4, 2], [0, 4], [0, 4], [1, 4], [1, 2], [2, 4], [4, 2], [0, 4], [4, 2], [2, 4], [2, 4], [4, 2], [0, 4], [4, 0], [3, 2], [2, 4], [4, 2], [0, 4], [4, 0], [2, 0], [2, 4], [0, 4], [0, 4], [4, 2], [0, 2], [2, 4], [4, 2], [0, 4], [2, 4], [3, 2], [2, 4], [4, 2], [0, 4], [4, 0], [1, 2], [4, 2], [3, 0], [4, 0], [4, 2], [2], [4, 2], [4, 0], [0, 4], [4, 2], [2, 0], [2, 4], [4, 2], [4, 2], [4, 0], [2, 0], [4, 2], [0, 2], [4, 0], [4, 0], [2, 0], [2, 4], [4, 0], [0, 4], [2, 4], [3, 0]]\n",
      "[[0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 3], [0, 1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 2, 3], [0, 1, 3], [0, 1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 3, 4], [0, 1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 2, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 3], [0, 1, 3], [0, 1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [1, 2, 4]]\n",
      "NL_pred of 1th iteration [[4, 2], [4, 0], [0, 4], [2, 4], [1, 2], [2, 4], [4, 0], [0, 4], [2, 4], [0, 2], [2, 4], [0, 4], [0, 4], [4, 0], [0, 2], [2, 4], [4, 0], [0, 4], [4, 2], [2, 0], [2, 4], [2, 4], [4, 0], [4, 2], [0, 2], [2, 4], [4, 2], [0, 4], [0, 4], [2, 4], [2, 4], [3, 0], [2, 4], [0, 2], [2, 4], [4, 2], [4, 0], [0, 4], [2, 0], [2, 4], [0, 4], [0, 4], [4, 0], [2, 4], [2, 4], [0, 4], [0, 4], [2, 0], [1, 2], [4, 2], [0, 2], [4, 0], [2, 4], [2, 4], [2, 4], [4, 2], [0, 4], [0, 4], [2, 4], [2, 4], [0, 4], [0, 4], [0, 4], [1, 2], [2, 4], [4, 0], [0, 4], [2, 4], [0, 2], [2, 4], [4, 2], [3, 0], [4, 0], [2, 4], [0, 4], [0, 4], [0, 2], [2, 4], [2, 4], [0, 4], [0, 4], [0, 2], [2, 0], [2, 4], [4, 2], [0, 4], [4, 0], [1, 2], [2, 4], [4, 0], [4, 0], [0, 2], [0, 2], [1, 4], [4, 0], [0, 4], [0, 4], [1, 0], [2, 4], [4, 0], [0, 4], [0, 4], [3, 0], [4, 2], [4, 2], [4, 0], [0, 4], [2, 0], [2, 4], [4, 2], [0, 4], [2, 4], [2, 1], [2, 4], [2, 4], [4, 0], [2, 4], [3, 2], [2, 4], [4, 0], [0, 4], [0, 4], [2, 4], [2, 4], [4, 0], [4, 2], [2, 4], [0, 4], [0, 4], [4, 2], [0, 2], [2, 4], [4, 0], [0, 4], [2, 4], [1, 2], [2, 4], [2, 4], [0, 4], [0, 4], [1, 2], [2, 4], [4, 0], [0, 4], [0, 4], [2, 4], [2, 4], [0, 4], [0, 4], [4, 2], [2, 0], [2, 4], [4, 0], [0, 4], [4, 0], [0, 4], [2, 4], [4, 0], [0, 4], [4, 2], [2, 4], [2, 4], [4, 2], [0, 4], [4, 2], [2, 0], [2, 4], [4, 2], [0, 4], [4, 2], [2, 4], [4, 0], [0, 4], [4, 0], [2, 4], [4, 2], [0, 4], [0, 4], [1, 2], [4, 2], [4, 0], [4, 0], [4, 2], [0, 4], [0, 4], [1, 4], [1, 2], [2, 4], [4, 2], [0, 4], [4, 2], [2, 4], [2, 4], [4, 2], [0, 4], [4, 0], [3, 2], [2, 4], [4, 2], [0, 4], [4, 0], [2, 0], [2, 4], [0, 4], [0, 4], [4, 2], [0, 2], [2, 4], [4, 2], [0, 4], [2, 4], [3, 2], [2, 4], [4, 2], [0, 4], [4, 0], [1, 2], [4, 2], [3, 0], [4, 0], [4, 2], [4, 2], [4, 0], [0, 4], [4, 2], [2, 0], [2, 4], [4, 2], [4, 2], [4, 0], [2, 0], [4, 2], [0, 2], [4, 0], [4, 0], [2, 0], [2, 4], [4, 0], [0, 4], [2, 4], [3, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005335548606650958  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00533549617434933  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005335396750833978  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005335256271837163  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005335077210580659  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005334863029574952  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005334617191330526  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005334345631579641  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005334048350322296  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005333728810068977  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005333389978685815  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005333032845461517  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00533265889432915  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005332271587799199  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005331870431227308  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005331457403190898  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005331033492978678  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005330600184523713  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005330158961759069  Accuracy on Support set:0.0\n",
      "torch.Size([241, 2048]) torch.Size([241])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005329710319329099  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.1993921399116516 tensor([0.1994, 0.2030, 0.1979, 0.2037, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19914869964122772 tensor([0.1985, 0.2038, 0.1991, 0.2035, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.199746236205101 tensor([0.1977, 0.2041, 0.1997, 0.2018, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20017534494400024 tensor([0.2002, 0.2032, 0.1965, 0.2044, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19853796064853668 tensor([0.1997, 0.2015, 0.1975, 0.2028, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2014315277338028 tensor([0.2014, 0.2036, 0.1965, 0.2030, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19853471219539642 tensor([0.1988, 0.2040, 0.1985, 0.2026, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20018941164016724 tensor([0.1986, 0.2026, 0.2002, 0.2023, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19980381429195404 tensor([0.1998, 0.2024, 0.1963, 0.2062, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19784535467624664 tensor([0.1982, 0.2028, 0.1981, 0.2031, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20251284539699554 tensor([0.2028, 0.2025, 0.1958, 0.2040, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987573653459549 tensor([0.1981, 0.2035, 0.1988, 0.2033, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20167338848114014 tensor([0.1971, 0.2031, 0.2027, 0.2017, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19899074733257294 tensor([0.1990, 0.2032, 0.1990, 0.2032, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19970403611660004 tensor([0.1980, 0.2016, 0.1984, 0.2023, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20115794241428375 tensor([0.2031, 0.2012, 0.1954, 0.2042, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19803151488304138 tensor([0.1982, 0.2062, 0.1980, 0.2029, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20066522061824799 tensor([0.1982, 0.2024, 0.2007, 0.2025, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1993328183889389 tensor([0.1993, 0.2036, 0.1984, 0.2036, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987108737230301 tensor([0.1989, 0.2029, 0.1973, 0.2022, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20205670595169067 tensor([0.2034, 0.2021, 0.1947, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19947999715805054 tensor([0.1995, 0.2028, 0.1978, 0.2028, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994600147008896 tensor([0.1987, 0.2035, 0.1995, 0.2023, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19926193356513977 tensor([0.1993, 0.2027, 0.1987, 0.2047, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19754797220230103 tensor([0.1988, 0.2031, 0.1982, 0.2024, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20129609107971191 tensor([0.2013, 0.2024, 0.1967, 0.2030, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19876135885715485 tensor([0.1988, 0.2052, 0.1979, 0.2022, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200453519821167 tensor([0.1983, 0.2029, 0.2005, 0.2019, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19853535294532776 tensor([0.1982, 0.2027, 0.1985, 0.2045, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19851386547088623 tensor([0.2000, 0.2017, 0.1968, 0.2029, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2009584754705429 tensor([0.2010, 0.2030, 0.1961, 0.2037, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20097440481185913 tensor([0.2010, 0.2031, 0.1967, 0.2036, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668877124786377 tensor([0.1988, 0.2028, 0.1999, 0.2018, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19990511238574982 tensor([0.1999, 0.2026, 0.1974, 0.2042, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19961409270763397 tensor([0.1985, 0.2018, 0.1977, 0.2023, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20097368955612183 tensor([0.2022, 0.2010, 0.1960, 0.2040, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19854070246219635 tensor([0.1985, 0.2060, 0.1977, 0.2027, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2004867047071457 tensor([0.1989, 0.2020, 0.2005, 0.2027, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.199147567152977 tensor([0.1985, 0.2032, 0.1991, 0.2028, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19868211448192596 tensor([0.1998, 0.2022, 0.1971, 0.2022, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20204533636569977 tensor([0.2026, 0.2020, 0.1956, 0.2042, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.198350727558136 tensor([0.1981, 0.2058, 0.1984, 0.2020, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20141731202602386 tensor([0.1969, 0.2032, 0.2019, 0.2014, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978541761636734 tensor([0.1980, 0.2028, 0.1979, 0.2060, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20161327719688416 tensor([0.2016, 0.2021, 0.1971, 0.2033, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20132160186767578 tensor([0.2013, 0.2018, 0.1970, 0.2036, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2005043625831604 tensor([0.1977, 0.2039, 0.2005, 0.2020, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2004566490650177 tensor([0.1984, 0.2021, 0.2005, 0.2025, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967378407716751 tensor([0.1986, 0.2018, 0.1970, 0.2059, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1970602571964264 tensor([0.1998, 0.2021, 0.1982, 0.2029, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19972147047519684 tensor([0.1997, 0.2026, 0.1977, 0.2042, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19650427997112274 tensor([0.1981, 0.2046, 0.1983, 0.2025, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2009793072938919 tensor([0.1980, 0.2042, 0.2010, 0.2018, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20075885951519012 tensor([0.2008, 0.2024, 0.1975, 0.2033, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20095530152320862 tensor([0.2010, 0.2023, 0.1964, 0.2036, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.201324462890625 tensor([0.2013, 0.2017, 0.1966, 0.2042, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1992463916540146 tensor([0.1992, 0.2058, 0.1973, 0.2026, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20051583647727966 tensor([0.1970, 0.2044, 0.2022, 0.2005, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19797295331954956 tensor([0.1983, 0.2022, 0.1980, 0.2056, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19940710067749023 tensor([0.1994, 0.2028, 0.1977, 0.2030, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20272107422351837 tensor([0.2028, 0.2027, 0.1954, 0.2036, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19874438643455505 tensor([0.1978, 0.2054, 0.1987, 0.2023, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2011634111404419 tensor([0.1975, 0.2034, 0.2012, 0.2024, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20034350454807281 tensor([0.1977, 0.2026, 0.2003, 0.2036, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199227437376976 tensor([0.2002, 0.2011, 0.1972, 0.2023, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20198965072631836 tensor([0.2029, 0.2020, 0.1955, 0.2044, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19910657405853271 tensor([0.1985, 0.2058, 0.1991, 0.2020, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20165172219276428 tensor([0.1966, 0.2026, 0.2024, 0.2017, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2016519457101822 tensor([0.2017, 0.2025, 0.1963, 0.2036, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19838494062423706 tensor([0.1988, 0.2021, 0.1981, 0.2026, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20212805271148682 tensor([0.2026, 0.2021, 0.1955, 0.2042, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19865939021110535 tensor([0.1987, 0.2061, 0.1975, 0.2027, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19611412286758423 tensor([0.1978, 0.2031, 0.2020, 0.2010, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19923947751522064 tensor([0.1980, 0.2028, 0.1992, 0.2055, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19930265843868256 tensor([0.2002, 0.2016, 0.1971, 0.2019, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20123717188835144 tensor([0.2012, 0.2023, 0.1965, 0.2031, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2002311497926712 tensor([0.1973, 0.2032, 0.2002, 0.2027, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20036198198795319 tensor([0.1974, 0.2034, 0.2004, 0.2028, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19705887138843536 tensor([0.1989, 0.2032, 0.1985, 0.2023, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20153185725212097 tensor([0.2015, 0.2022, 0.1955, 0.2034, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2018420547246933 tensor([0.2018, 0.2022, 0.1962, 0.2040, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19917601346969604 tensor([0.1978, 0.2053, 0.1992, 0.2023, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20209677517414093 tensor([0.1963, 0.2043, 0.2021, 0.2021, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19715189933776855 tensor([0.1983, 0.2026, 0.1988, 0.2032, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19870468974113464 tensor([0.1994, 0.2013, 0.1968, 0.2038, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2012675702571869 tensor([0.2013, 0.2025, 0.1966, 0.2040, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19884926080703735 tensor([0.1988, 0.2037, 0.1983, 0.2032, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20089119672775269 tensor([0.1973, 0.2037, 0.2009, 0.2018, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2008410096168518 tensor([0.1982, 0.2034, 0.2008, 0.2022, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979466676712036 tensor([0.2005, 0.2016, 0.1975, 0.2023, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20277343690395355 tensor([0.2037, 0.2028, 0.1949, 0.2042, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19989407062530518 tensor([0.1987, 0.2033, 0.1999, 0.2029, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20058108866214752 tensor([0.1985, 0.2025, 0.2006, 0.2027, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988564282655716 tensor([0.1981, 0.2025, 0.1982, 0.2024, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19788332283496857 tensor([0.1976, 0.2021, 0.1994, 0.2030, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1992213875055313 tensor([0.1997, 0.2018, 0.1992, 0.2026, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19923129677772522 tensor([0.1988, 0.2039, 0.1992, 0.2021, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20087279379367828 tensor([0.1963, 0.2031, 0.2032, 0.2009, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19929222762584686 tensor([0.1968, 0.2030, 0.1993, 0.2041, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1980660855770111 tensor([0.1984, 0.2004, 0.1981, 0.2026, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2002461701631546 tensor([0.2002, 0.2026, 0.1973, 0.2037, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19918976724147797 tensor([0.1983, 0.2039, 0.1992, 0.2028, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.200845867395401 tensor([0.1977, 0.2026, 0.2008, 0.2026, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987522840499878 tensor([0.1974, 0.2030, 0.1988, 0.2056, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19806039333343506 tensor([0.1986, 0.2028, 0.1988, 0.2018, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2022208571434021 tensor([0.2022, 0.2029, 0.1971, 0.2034, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1990838646888733 tensor([0.1991, 0.2037, 0.1985, 0.2025, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1989099532365799 tensor([0.1991, 0.2026, 0.1989, 0.2034, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20060831308364868 tensor([0.1967, 0.2037, 0.2006, 0.2033, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19920006394386292 tensor([0.1982, 0.2016, 0.1972, 0.2037, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20193377137184143 tensor([0.2019, 0.2023, 0.1956, 0.2042, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989898383617401 tensor([0.1990, 0.2054, 0.1981, 0.2028, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2019893079996109 tensor([0.1964, 0.2035, 0.2020, 0.2023, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19910958409309387 tensor([0.1991, 0.2028, 0.1968, 0.2059, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19868776202201843 tensor([0.2016, 0.2009, 0.1954, 0.2034, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202382892370224 tensor([0.2035, 0.2024, 0.1949, 0.2036, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19912172853946686 tensor([0.1991, 0.2047, 0.1973, 0.2030, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993418037891388 tensor([0.1995, 0.2032, 0.1993, 0.2028, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19919222593307495 tensor([0.1992, 0.2023, 0.1970, 0.2060, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19868750870227814 tensor([0.1987, 0.2023, 0.1981, 0.2018, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20146998763084412 tensor([0.2015, 0.2028, 0.1958, 0.2040, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19880688190460205 tensor([0.1983, 0.2049, 0.1988, 0.2026, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20115230977535248 tensor([0.1970, 0.2033, 0.2012, 0.2022, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19982048869132996 tensor([0.1975, 0.2031, 0.1998, 0.2043, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1998891830444336 tensor([0.1999, 0.2023, 0.1977, 0.2031, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20239481329917908 tensor([0.2025, 0.2024, 0.1956, 0.2040, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19812481105327606 tensor([0.1981, 0.2066, 0.1981, 0.2024, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19948293268680573 tensor([0.1999, 0.2028, 0.1995, 0.2029, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20050722360610962 tensor([0.2005, 0.2027, 0.1971, 0.2048, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977473646402359 tensor([0.2011, 0.2015, 0.1971, 0.2026, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20221875607967377 tensor([0.2028, 0.2022, 0.1957, 0.2038, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19950275123119354 tensor([0.1982, 0.2042, 0.1995, 0.2017, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20135101675987244 tensor([0.1978, 0.2029, 0.2014, 0.2014, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1995730698108673 tensor([0.1996, 0.2034, 0.1985, 0.2027, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1993996798992157 tensor([0.1980, 0.2025, 0.1975, 0.2027, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2014497071504593 tensor([0.2014, 0.2026, 0.1969, 0.2033, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19977183640003204 tensor([0.1984, 0.2039, 0.1998, 0.2026, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20162592828273773 tensor([0.1965, 0.2038, 0.2018, 0.2016, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1996545046567917 tensor([0.1997, 0.2037, 0.1970, 0.2030, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989348977804184 tensor([0.1989, 0.2011, 0.1979, 0.2026, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200127363204956 tensor([0.2042, 0.2020, 0.1946, 0.2042, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19891875982284546 tensor([0.1989, 0.2042, 0.1975, 0.2034, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20202215015888214 tensor([0.1969, 0.2035, 0.2024, 0.2020, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1980857104063034 tensor([0.1983, 0.2023, 0.1981, 0.2052, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19846603274345398 tensor([0.1989, 0.2016, 0.1984, 0.2026, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022743821144104 tensor([0.2030, 0.2023, 0.1950, 0.2039, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19906304776668549 tensor([0.1988, 0.2041, 0.1991, 0.2021, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20161251723766327 tensor([0.1967, 0.2037, 0.2016, 0.2024, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19884654879570007 tensor([0.1982, 0.2026, 0.1988, 0.2044, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20007163286209106 tensor([0.2001, 0.2015, 0.1972, 0.2041, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20222720503807068 tensor([0.2027, 0.2022, 0.1953, 0.2035, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19970980286598206 tensor([0.1978, 0.2027, 0.1997, 0.2025, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20197725296020508 tensor([0.1965, 0.2036, 0.2020, 0.2022, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19975997507572174 tensor([0.1998, 0.2025, 0.1976, 0.2052, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19976463913917542 tensor([0.1997, 0.2016, 0.1970, 0.2020, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2010926753282547 tensor([0.2011, 0.2023, 0.1968, 0.2035, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19802804291248322 tensor([0.1983, 0.2058, 0.1980, 0.2026, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19952228665351868 tensor([0.1975, 0.2030, 0.1995, 0.2031, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19901303946971893 tensor([0.1987, 0.2028, 0.1990, 0.2036, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19988958537578583 tensor([0.1975, 0.2034, 0.1999, 0.2024, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20233473181724548 tensor([0.2031, 0.2023, 0.1955, 0.2041, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19836850464344025 tensor([0.1982, 0.2064, 0.1984, 0.2018, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2013912796974182 tensor([0.1973, 0.2038, 0.2014, 0.2022, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19857454299926758 tensor([0.1986, 0.2032, 0.1971, 0.2056, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20015893876552582 tensor([0.2002, 0.2028, 0.1970, 0.2027, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20160537958145142 tensor([0.2016, 0.2028, 0.1958, 0.2039, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19918182492256165 tensor([0.1992, 0.2042, 0.1978, 0.2034, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20013593137264252 tensor([0.1976, 0.2024, 0.2001, 0.2027, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19917865097522736 tensor([0.1992, 0.2017, 0.1987, 0.2045, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19837148487567902 tensor([0.1996, 0.2024, 0.1969, 0.2027, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20177075266838074 tensor([0.2044, 0.2018, 0.1952, 0.2042, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19906800985336304 tensor([0.1991, 0.2060, 0.1978, 0.2027, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20160216093063354 tensor([0.1962, 0.2044, 0.2022, 0.2016, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19887767732143402 tensor([0.1989, 0.2044, 0.1979, 0.2038, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19813765585422516 tensor([0.2001, 0.2024, 0.1966, 0.2027, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20228321850299835 tensor([0.2023, 0.2026, 0.1960, 0.2036, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19960427284240723 tensor([0.1990, 0.2030, 0.1996, 0.2023, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007901668548584 tensor([0.1976, 0.2028, 0.2008, 0.2024, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1985444873571396 tensor([0.1988, 0.2029, 0.1985, 0.2041, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982518881559372 tensor([0.2001, 0.2019, 0.1967, 0.2031, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017795592546463 tensor([0.2034, 0.2018, 0.1946, 0.2045, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19870345294475555 tensor([0.1987, 0.2050, 0.1981, 0.2023, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2013709992170334 tensor([0.1968, 0.2028, 0.2014, 0.2017, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20038694143295288 tensor([0.1970, 0.2019, 0.2004, 0.2037, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19921232759952545 tensor([0.1996, 0.2013, 0.1975, 0.2023, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19767431914806366 tensor([0.2018, 0.2019, 0.1961, 0.2026, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989167481660843 tensor([0.1989, 0.2048, 0.1976, 0.2033, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20125550031661987 tensor([0.1986, 0.2028, 0.2013, 0.2020, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19783326983451843 tensor([0.1979, 0.2031, 0.1978, 0.2057, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19836674630641937 tensor([0.2000, 0.2026, 0.1967, 0.2023, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19949562847614288 tensor([0.1995, 0.2037, 0.1988, 0.2021, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19938163459300995 tensor([0.1982, 0.2028, 0.1994, 0.2038, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001393586397171 tensor([0.1977, 0.2031, 0.2001, 0.2033, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1980864256620407 tensor([0.1998, 0.2017, 0.1981, 0.2043, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980437934398651 tensor([0.1991, 0.2019, 0.1978, 0.2031, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20230013132095337 tensor([0.2026, 0.2023, 0.1953, 0.2040, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1992829591035843 tensor([0.1993, 0.2043, 0.1974, 0.2034, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20197910070419312 tensor([0.1956, 0.2037, 0.2022, 0.2020, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19961506128311157 tensor([0.1996, 0.2023, 0.1969, 0.2068, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2012975513935089 tensor([0.2013, 0.2013, 0.1965, 0.2044, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20175835490226746 tensor([0.2018, 0.2030, 0.1957, 0.2037, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1997637152671814 tensor([0.1998, 0.2047, 0.1972, 0.2028, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2018873691558838 tensor([0.1967, 0.2039, 0.2025, 0.2019, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988111287355423 tensor([0.1981, 0.2040, 0.1988, 0.2036, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19889302551746368 tensor([0.1989, 0.2024, 0.1977, 0.2019, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20075495541095734 tensor([0.2008, 0.2025, 0.1973, 0.2034, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19950877130031586 tensor([0.1995, 0.2046, 0.1976, 0.2034, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20221805572509766 tensor([0.1961, 0.2035, 0.2025, 0.2022, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20031429827213287 tensor([0.1981, 0.2035, 0.2003, 0.2025, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19805456697940826 tensor([0.1999, 0.2019, 0.1973, 0.2028, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20225153863430023 tensor([0.2025, 0.2023, 0.1955, 0.2038, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993992030620575 tensor([0.1979, 0.2040, 0.1994, 0.2024, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19989021122455597 tensor([0.1976, 0.2025, 0.1999, 0.2034, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989828199148178 tensor([0.1990, 0.2037, 0.1985, 0.2036, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19683828949928284 tensor([0.1985, 0.2024, 0.1981, 0.2041, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20114286243915558 tensor([0.2030, 0.2011, 0.1953, 0.2041, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989721953868866 tensor([0.1990, 0.2057, 0.1981, 0.2031, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19937318563461304 tensor([0.1982, 0.2032, 0.1994, 0.2023, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19918031990528107 tensor([0.1992, 0.2027, 0.1980, 0.2037, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19841992855072021 tensor([0.2000, 0.2018, 0.1979, 0.2018, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022809535264969 tensor([0.2026, 0.2023, 0.1946, 0.2037, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1989770084619522 tensor([0.1990, 0.2046, 0.1982, 0.2023, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19987957179546356 tensor([0.1982, 0.2035, 0.1999, 0.2019, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988053172826767 tensor([0.1989, 0.2030, 0.1988, 0.2036, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19961793720722198 tensor([0.1997, 0.2003, 0.1970, 0.2034, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19981786608695984 tensor([0.1998, 0.2024, 0.1987, 0.2029, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962307095527649 tensor([0.1985, 0.2040, 0.1996, 0.2017, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19915641844272614 tensor([0.1989, 0.2040, 0.1992, 0.2023, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19901715219020844 tensor([0.1990, 0.2032, 0.1983, 0.2033, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976584643125534 tensor([0.1999, 0.2028, 0.1972, 0.2024, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2017991691827774 tensor([0.2018, 0.2023, 0.1971, 0.2036, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987888067960739 tensor([0.1978, 0.2056, 0.1988, 0.2026, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20061956346035004 tensor([0.1977, 0.2036, 0.2006, 0.2017, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19900964200496674 tensor([0.1990, 0.2023, 0.1971, 0.2061, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19885297119617462 tensor([0.1992, 0.2021, 0.1972, 0.2026, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20234039425849915 tensor([0.2023, 0.2024, 0.1965, 0.2035, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2000844031572342 tensor([0.2001, 0.2023, 0.1978, 0.2036, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2012287676334381 tensor([0.2012, 0.2025, 0.1979, 0.2023, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19944734871387482 tensor([0.1984, 0.2030, 0.1994, 0.2033, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19836533069610596 tensor([0.1993, 0.2025, 0.1974, 0.2025, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20076370239257812 tensor([0.2008, 0.2023, 0.1979, 0.2028, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971406638622284 tensor([0.1983, 0.2027, 0.1989, 0.2030, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2013557255268097 tensor([0.1981, 0.2036, 0.2014, 0.2014, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19866058230400085 tensor([0.1987, 0.2025, 0.1987, 0.2044, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19829899072647095 tensor([0.1998, 0.2022, 0.1972, 0.2025, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20180469751358032 tensor([0.2025, 0.2018, 0.1964, 0.2028, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19885189831256866 tensor([0.1983, 0.2066, 0.1989, 0.2014, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19825726747512817 tensor([0.1982, 0.2028, 0.1983, 0.2042, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20043358206748962 tensor([0.2004, 0.2032, 0.1971, 0.2036, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19891364872455597 tensor([0.1987, 0.2017, 0.1990, 0.2017, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 4], [1, 2, 4], [2, 4], [4, 0, 2], [0, 4], [2, 4, 0], [0, 2, 4], [2, 4], [0, 4, 2], [0, 4], [4, 0, 2], [0, 2, 4], [2, 4], [4, 0, 2], [0, 4], [4, 2, 0], [2, 0, 4], [2, 4], [2, 4, 0], [4, 0, 2], [4, 2, 0], [0, 2, 4], [2, 4], [4, 2, 0], [0, 4], [0, 4, 2], [2, 4], [2, 4], [2, 4], [3, 0, 4], [2, 4, 0], [0, 2, 4], [2, 4], [4, 2, 0], [4, 0], [0, 4, 2], [2, 0, 4], [2, 4], [0, 4, 2], [0, 4], [4, 0, 2], [2, 4], [2, 4], [0, 4], [0, 4], [2, 0, 4], [1, 2, 4], [4, 2, 0], [0, 2, 4], [4, 0], [2, 4], [2, 4], [2, 4], [4, 2, 0], [0, 4], [0, 4, 2], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4], [0, 4], [1, 2, 4], [2, 4], [4, 0, 2], [0, 4], [2, 4], [0, 2, 4], [2, 4], [4, 2, 0], [3, 0, 4], [4, 0, 2], [2, 4], [2, 4], [0, 4], [0, 4], [0, 2, 4], [2, 4], [2, 4], [0, 4, 2], [0, 4], [0, 2, 4], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4], [4, 0], [1, 2, 4], [2, 4], [4, 0, 2], [4, 0], [0, 2, 4], [0, 2, 4], [1, 4, 2], [4, 0, 2], [0, 4], [0, 4, 2], [1, 0, 2], [2, 4], [4, 0, 2], [0, 4], [0, 4, 2], [3, 0, 4], [4, 2], [4, 2, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4], [2, 4, 0], [2, 1, 4], [2, 4], [2, 4, 0], [4, 0, 2], [2, 4, 0], [3, 2, 0], [2, 4], [4, 0, 2], [0, 4], [0, 4, 2], [2, 4, 0], [2, 4], [4, 0, 2], [4, 2], [4, 2], [2, 4], [2, 4], [0, 4, 2], [0, 4], [4, 2, 0], [0, 2, 4], [2, 4], [4, 0, 2], [0, 4], [2, 4, 0], [1, 2, 0], [2, 4], [2, 4, 0], [0, 4], [0, 4, 2], [1, 2, 4], [2, 4], [4, 0, 2], [0, 4], [0, 4, 2], [2, 4], [2, 4], [0, 4, 2], [0, 4], [4, 2, 0], [2, 0, 4], [2, 4], [4, 0, 2], [0, 4, 2], [4, 0, 2], [0, 4, 2], [2, 4], [4, 0, 2], [0, 4], [4, 2, 0], [2, 4], [2, 4], [4, 2, 0], [0, 4], [4, 2, 0], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4], [4, 2, 0], [2, 4], [2, 4], [4, 0, 2], [0, 4], [4, 0, 2], [2, 4], [2, 4], [4, 2, 0], [0, 4], [0, 4], [1, 2, 4], [2, 4], [4, 2, 0], [4, 0], [4, 0, 2], [2, 4], [4, 2, 0], [0, 4, 2], [0, 4], [1, 4, 2], [1, 2, 4], [2, 4], [4, 2, 0], [0, 4], [4, 2, 0], [2, 4], [2, 4], [4, 2, 0], [0, 4], [4, 0, 2], [3, 2, 0], [2, 4], [4, 2, 0], [0, 4], [4, 0], [2, 0, 4], [2, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4], [4, 2, 0], [0, 4, 2], [2, 4, 0], [3, 2, 4], [2, 4], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4], [4, 2, 0], [3, 0, 4], [4, 0, 2], [4, 2, 0], [2, 4], [4, 2], [4, 0, 2], [0, 4], [4, 2, 0], [2, 0, 4], [2, 4], [4, 2], [4, 2], [4, 0, 2], [2, 0, 4], [4, 2], [0, 2, 4], [4, 0], [4, 0, 2], [2, 0, 4], [2, 4], [4, 0, 2], [0, 4, 2], [2, 4], [3, 0, 4]]\n",
      "[[1, 3], [1, 3], [1, 3], [0, 1, 3], [0, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3], [0, 3], [1, 3], [1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 2, 3], [0, 3], [0, 1, 3], [1, 3], [1, 2, 3], [0, 1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2], [1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 2, 3], [0, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 3], [1, 3], [1, 2, 3], [1, 3], [3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 2], [0, 1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 4], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 2, 3], [0, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 2, 3], [0, 3], [0, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 4], [0, 1, 3], [1, 3], [1, 2, 3], [1, 2, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [0, 1], [0, 1, 3], [1, 3], [1, 3], [1, 3], [0, 3], [1, 3], [1, 2], [1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 2]]\n",
      "NL_pred of 2th iteration [[4, 2, 0], [4, 0, 2], [0, 4, 2], [1, 2, 4], [4, 0, 2], [2, 4, 0], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [4, 0, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 0, 2], [4, 2, 0], [0, 2, 4], [4, 2, 0], [0, 4, 2], [2, 4], [3, 0, 4], [2, 4, 0], [0, 2, 4], [4, 2, 0], [0, 4, 2], [2, 0, 4], [0, 4, 2], [4, 0, 2], [2, 0, 4], [1, 2, 4], [4, 2, 0], [0, 2, 4], [4, 2, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [1, 2, 4], [4, 0, 2], [0, 2, 4], [4, 2, 0], [3, 0, 4], [4, 0, 2], [2, 4], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [4, 2, 0], [1, 2, 4], [4, 0, 2], [0, 2, 4], [0, 2, 4], [1, 4, 2], [4, 0, 2], [0, 4, 2], [1, 0, 2], [4, 0, 2], [0, 4, 2], [3, 0, 4], [4, 2, 0], [4, 0, 2], [2, 0, 4], [4, 2, 0], [2, 4, 0], [2, 1, 4], [2, 4, 0], [4, 0, 2], [2, 4, 0], [3, 2, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [4, 0, 2], [4, 2], [2, 4], [0, 4, 2], [4, 2, 0], [0, 2, 4], [4, 0, 2], [2, 4, 0], [1, 2, 0], [2, 4, 0], [0, 4, 2], [1, 2, 4], [4, 0, 2], [0, 4, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [4, 0, 2], [0, 4, 2], [4, 0, 2], [0, 4, 2], [4, 0, 2], [4, 2, 0], [4, 2, 0], [4, 2, 0], [2, 0, 4], [4, 2, 0], [4, 2, 0], [2, 4], [4, 0, 2], [4, 0, 2], [2, 4], [4, 2, 0], [1, 2, 4], [2, 4], [4, 2, 0], [4, 0, 2], [2, 4], [4, 2, 0], [0, 4, 2], [1, 4, 2], [1, 2, 4], [4, 2, 0], [4, 2, 0], [4, 2, 0], [4, 0, 2], [3, 2, 0], [4, 2, 0], [2, 0, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [4, 2, 0], [0, 4, 2], [2, 4, 0], [3, 2, 4], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4], [4, 2, 0], [3, 0, 4], [4, 0, 2], [4, 2, 0], [2, 4], [4, 0, 2], [4, 2, 0], [2, 0, 4], [4, 0, 2], [2, 0, 4], [0, 2, 4], [4, 0, 2], [2, 0, 4], [4, 0, 2], [0, 4, 2], [3, 0, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.008632231878754277  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.008632166273641906  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.008632043064040626  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.008631868650449202  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.008631645433054674  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.008631380612418155  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.00863107738878903  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.008630739762479026  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.008630371733799876  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.008629975702938617  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.00862955327001994  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.008629109235417922  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.008628645199257256  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.00862816276166263  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.008627662722696394  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00862714828260793  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.008626621841584275  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.00862608179950074  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.008625532156669053  Accuracy on Support set:0.0\n",
      "torch.Size([149, 2048]) torch.Size([149])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.008624972113026869  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20451690256595612 tensor([0.1987, 0.2045, 0.1965, 0.2052, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20495393872261047 tensor([0.1979, 0.2053, 0.1977, 0.2050, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2032303810119629 tensor([0.1970, 0.2056, 0.1983, 0.2032, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1995159536600113 tensor([0.1995, 0.2047, 0.1951, 0.2059, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1990809142589569 tensor([0.1991, 0.2030, 0.1961, 0.2042, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20075416564941406 tensor([0.2008, 0.2052, 0.1950, 0.2045, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20409420132637024 tensor([0.1981, 0.2055, 0.1971, 0.2041, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19874635338783264 tensor([0.1980, 0.2041, 0.1987, 0.2038, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038721740245819 tensor([0.1991, 0.2039, 0.1949, 0.2077, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20429082214832306 tensor([0.1976, 0.2043, 0.1967, 0.2045, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20214694738388062 tensor([0.2021, 0.2040, 0.1944, 0.2055, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20478291809558868 tensor([0.1974, 0.2050, 0.1973, 0.2048, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20127668976783752 tensor([0.1964, 0.2046, 0.2013, 0.2032, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047116607427597 tensor([0.1983, 0.2047, 0.1976, 0.2047, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20311789214611053 tensor([0.1973, 0.2031, 0.1970, 0.2038, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2024250477552414 tensor([0.2024, 0.2027, 0.1940, 0.2057, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2044101059436798 tensor([0.1975, 0.2077, 0.1966, 0.2044, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1992287039756775 tensor([0.1976, 0.2039, 0.1992, 0.2040, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20504462718963623 tensor([0.1987, 0.2052, 0.1969, 0.2050, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20371851325035095 tensor([0.1982, 0.2044, 0.1959, 0.2037, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2027093917131424 tensor([0.2027, 0.2036, 0.1933, 0.2062, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20427949726581573 tensor([0.1988, 0.2044, 0.1964, 0.2043, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20374417304992676 tensor([0.1980, 0.2050, 0.1980, 0.2037, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20418234169483185 tensor([0.1986, 0.2042, 0.1973, 0.2062, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20384851098060608 tensor([0.1981, 0.2046, 0.1968, 0.2038, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20063091814517975 tensor([0.2006, 0.2039, 0.1953, 0.2045, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20367208123207092 tensor([0.1981, 0.2067, 0.1965, 0.2037, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19902560114860535 tensor([0.1977, 0.2044, 0.1990, 0.2034, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20424723625183105 tensor([0.1975, 0.2042, 0.1971, 0.2060, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19936273992061615 tensor([0.1994, 0.2032, 0.1954, 0.2044, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2002965658903122 tensor([0.2003, 0.2045, 0.1946, 0.2052, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20030592381954193 tensor([0.2003, 0.2046, 0.1953, 0.2051, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19842438399791718 tensor([0.1982, 0.2044, 0.1984, 0.2033, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20413443446159363 tensor([0.1992, 0.2041, 0.1960, 0.2057, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20333269238471985 tensor([0.1979, 0.2033, 0.1963, 0.2038, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20149226486682892 tensor([0.2015, 0.2025, 0.1946, 0.2055, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042004019021988 tensor([0.1979, 0.2076, 0.1962, 0.2042, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19904091954231262 tensor([0.1983, 0.2035, 0.1990, 0.2042, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2043033093214035 tensor([0.1979, 0.2047, 0.1977, 0.2043, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2037040740251541 tensor([0.1991, 0.2037, 0.1957, 0.2037, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20196384191513062 tensor([0.2020, 0.2036, 0.1942, 0.2057, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20347493886947632 tensor([0.1974, 0.2073, 0.1969, 0.2035, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20047414302825928 tensor([0.1963, 0.2047, 0.2005, 0.2029, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20426282286643982 tensor([0.1973, 0.2043, 0.1964, 0.2075, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2009502649307251 tensor([0.2010, 0.2036, 0.1956, 0.2048, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20065386593341827 tensor([0.2007, 0.2033, 0.1955, 0.2051, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19905412197113037 tensor([0.1970, 0.2054, 0.1991, 0.2035, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19900985062122345 tensor([0.1977, 0.2036, 0.1990, 0.2040, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.203345388174057 tensor([0.1979, 0.2033, 0.1955, 0.2074, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19914864003658295 tensor([0.1991, 0.2036, 0.1968, 0.2044, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040715217590332 tensor([0.1991, 0.2041, 0.1963, 0.2057, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20397838950157166 tensor([0.1975, 0.2061, 0.1968, 0.2040, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19953402876853943 tensor([0.1973, 0.2057, 0.1995, 0.2033, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2000916600227356 tensor([0.2001, 0.2039, 0.1961, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20028865337371826 tensor([0.2003, 0.2038, 0.1950, 0.2051, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20065581798553467 tensor([0.2007, 0.2032, 0.1951, 0.2057, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20411352813243866 tensor([0.1986, 0.2073, 0.1958, 0.2041, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2007211446762085 tensor([0.1964, 0.2059, 0.2007, 0.2020, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20368771255016327 tensor([0.1976, 0.2037, 0.1965, 0.2071, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20432701706886292 tensor([0.1987, 0.2043, 0.1962, 0.2045, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20215673744678497 tensor([0.2022, 0.2042, 0.1940, 0.2051, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20381374657154083 tensor([0.1972, 0.2070, 0.1973, 0.2038, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19970296323299408 tensor([0.1969, 0.2049, 0.1997, 0.2039, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19889825582504272 tensor([0.1971, 0.2041, 0.1989, 0.2051, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19950662553310394 tensor([0.1995, 0.2027, 0.1958, 0.2038, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20224922895431519 tensor([0.2022, 0.2035, 0.1941, 0.2059, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20343205332756042 tensor([0.1978, 0.2074, 0.1977, 0.2034, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20097947120666504 tensor([0.1960, 0.2041, 0.2010, 0.2031, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2009790688753128 tensor([0.2010, 0.2040, 0.1948, 0.2051, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2036319226026535 tensor([0.1982, 0.2036, 0.1966, 0.2041, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20188502967357635 tensor([0.2019, 0.2036, 0.1941, 0.2057, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042156457901001 tensor([0.1980, 0.2076, 0.1961, 0.2042, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2005481719970703 tensor([0.1972, 0.2046, 0.2005, 0.2025, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20435091853141785 tensor([0.1974, 0.2044, 0.1978, 0.2070, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19951273500919342 tensor([0.1995, 0.2031, 0.1957, 0.2033, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.200571671128273 tensor([0.2006, 0.2039, 0.1951, 0.2046, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987912803888321 tensor([0.1967, 0.2047, 0.1988, 0.2042, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1989153027534485 tensor([0.1968, 0.2050, 0.1989, 0.2043, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20381012558937073 tensor([0.1982, 0.2047, 0.1971, 0.2038, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2008698731660843 tensor([0.2009, 0.2037, 0.1941, 0.2049, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2011694461107254 tensor([0.2012, 0.2037, 0.1948, 0.2055, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20373129844665527 tensor([0.1971, 0.2069, 0.1977, 0.2037, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2006468027830124 tensor([0.1957, 0.2058, 0.2006, 0.2036, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040879726409912 tensor([0.1976, 0.2041, 0.1973, 0.2047, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20280452072620392 tensor([0.1987, 0.2028, 0.1954, 0.2053, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20059798657894135 tensor([0.2006, 0.2040, 0.1952, 0.2055, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20473453402519226 tensor([0.1982, 0.2052, 0.1968, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19945618510246277 tensor([0.1967, 0.2052, 0.1995, 0.2033, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993994265794754 tensor([0.1976, 0.2049, 0.1994, 0.2037, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1998743712902069 tensor([0.1999, 0.2032, 0.1961, 0.2038, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20305421948432922 tensor([0.2031, 0.2043, 0.1935, 0.2057, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2043701559305191 tensor([0.1981, 0.2048, 0.1985, 0.2044, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19914333522319794 tensor([0.1978, 0.2040, 0.1991, 0.2042, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20385347306728363 tensor([0.1974, 0.2040, 0.1968, 0.2039, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20364809036254883 tensor([0.1970, 0.2036, 0.1979, 0.2045, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19909535348415375 tensor([0.1991, 0.2033, 0.1978, 0.2041, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20355407893657684 tensor([0.1982, 0.2054, 0.1978, 0.2036, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20173515379428864 tensor([0.1957, 0.2046, 0.2017, 0.2024, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20451435446739197 tensor([0.1961, 0.2045, 0.1979, 0.2056, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19954583048820496 tensor([0.1978, 0.2019, 0.1966, 0.2041, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19957634806632996 tensor([0.1996, 0.2041, 0.1959, 0.2052, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042984962463379 tensor([0.1976, 0.2054, 0.1978, 0.2043, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19940312206745148 tensor([0.1971, 0.2042, 0.1994, 0.2041, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20450501143932343 tensor([0.1967, 0.2045, 0.1973, 0.2071, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19736628234386444 tensor([0.1979, 0.2043, 0.1974, 0.2033, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2015550285577774 tensor([0.2016, 0.2044, 0.1957, 0.2048, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2040039300918579 tensor([0.1984, 0.2052, 0.1971, 0.2040, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20416030287742615 tensor([0.1985, 0.2042, 0.1975, 0.2049, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1991523653268814 tensor([0.1960, 0.2052, 0.1992, 0.2048, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20311994850635529 tensor([0.1976, 0.2031, 0.1958, 0.2052, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2012622058391571 tensor([0.2013, 0.2038, 0.1942, 0.2057, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2043362408876419 tensor([0.1983, 0.2069, 0.1966, 0.2043, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20053301751613617 tensor([0.1957, 0.2050, 0.2005, 0.2037, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042785882949829 tensor([0.1984, 0.2043, 0.1954, 0.2074, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20092453062534332 tensor([0.2009, 0.2024, 0.1940, 0.2049, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.202856183052063 tensor([0.2029, 0.2039, 0.1935, 0.2051, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20448197424411774 tensor([0.1985, 0.2062, 0.1958, 0.2045, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042437642812729 tensor([0.1988, 0.2047, 0.1979, 0.2042, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20380978286266327 tensor([0.1985, 0.2038, 0.1955, 0.2075, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981509029865265 tensor([0.1980, 0.2039, 0.1967, 0.2033, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20079927146434784 tensor([0.2008, 0.2043, 0.1944, 0.2055, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20411303639411926 tensor([0.1976, 0.2064, 0.1974, 0.2041, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19970005750656128 tensor([0.1963, 0.2048, 0.1997, 0.2037, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20466354489326477 tensor([0.1968, 0.2047, 0.1984, 0.2058, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20384712517261505 tensor([0.1992, 0.2038, 0.1963, 0.2046, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20181280374526978 tensor([0.2018, 0.2039, 0.1942, 0.2055, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20387305319309235 tensor([0.1975, 0.2081, 0.1967, 0.2039, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19926296174526215 tensor([0.1993, 0.2043, 0.1980, 0.2043, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1998322308063507 tensor([0.1998, 0.2042, 0.1957, 0.2063, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20045441389083862 tensor([0.2005, 0.2030, 0.1956, 0.2041, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20210838317871094 tensor([0.2021, 0.2037, 0.1943, 0.2053, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20320546627044678 tensor([0.1975, 0.2057, 0.1981, 0.2032, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19997136294841766 tensor([0.1971, 0.2044, 0.2000, 0.2028, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20414529740810394 tensor([0.1989, 0.2049, 0.1971, 0.2041, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20398563146591187 tensor([0.1973, 0.2040, 0.1960, 0.2042, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20078444480895996 tensor([0.2008, 0.2041, 0.1955, 0.2048, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20412178337574005 tensor([0.1978, 0.2054, 0.1983, 0.2041, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2003352791070938 tensor([0.1958, 0.2053, 0.2003, 0.2031, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20452475547790527 tensor([0.1990, 0.2052, 0.1956, 0.2045, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19845972955226898 tensor([0.1983, 0.2027, 0.1965, 0.2041, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2035038322210312 tensor([0.2036, 0.2035, 0.1932, 0.2057, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20487327873706818 tensor([0.1983, 0.2057, 0.1960, 0.2049, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20099519193172455 tensor([0.1962, 0.2050, 0.2010, 0.2035, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20379573106765747 tensor([0.1977, 0.2038, 0.1966, 0.2067, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1982530951499939 tensor([0.1983, 0.2032, 0.1970, 0.2041, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20226888358592987 tensor([0.2023, 0.2038, 0.1936, 0.2054, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20354610681533813 tensor([0.1982, 0.2057, 0.1976, 0.2035, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20016512274742126 tensor([0.1960, 0.2053, 0.2002, 0.2038, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20407776534557343 tensor([0.1976, 0.2041, 0.1974, 0.2059, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19940508902072906 tensor([0.1994, 0.2030, 0.1958, 0.2056, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20201808214187622 tensor([0.2020, 0.2037, 0.1939, 0.2050, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20394010841846466 tensor([0.1971, 0.2042, 0.1983, 0.2039, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2005256712436676 tensor([0.1959, 0.2051, 0.2005, 0.2037, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20398588478565216 tensor([0.1991, 0.2040, 0.1962, 0.2067, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20311979949474335 tensor([0.1990, 0.2031, 0.1956, 0.2034, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20042313635349274 tensor([0.2004, 0.2038, 0.1954, 0.2050, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20406284928321838 tensor([0.1976, 0.2074, 0.1966, 0.2041, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20451612770557404 tensor([0.1969, 0.2045, 0.1981, 0.2045, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20428849756717682 tensor([0.1980, 0.2043, 0.1976, 0.2051, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20392392575740814 tensor([0.1969, 0.2049, 0.1985, 0.2039, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2024310678243637 tensor([0.2024, 0.2038, 0.1941, 0.2056, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20330750942230225 tensor([0.1975, 0.2080, 0.1969, 0.2033, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19992993772029877 tensor([0.1966, 0.2054, 0.1999, 0.2037, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20470894873142242 tensor([0.1979, 0.2047, 0.1957, 0.2071, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19949635863304138 tensor([0.1995, 0.2043, 0.1956, 0.2042, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20093289017677307 tensor([0.2009, 0.2043, 0.1944, 0.2054, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2048674076795578 tensor([0.1985, 0.2058, 0.1964, 0.2049, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19869473576545715 tensor([0.1970, 0.2039, 0.1987, 0.2042, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20325501263141632 tensor([0.1985, 0.2033, 0.1973, 0.2060, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2039058655500412 tensor([0.1990, 0.2039, 0.1955, 0.2042, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20327959954738617 tensor([0.2037, 0.2033, 0.1938, 0.2057, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20417486131191254 tensor([0.1984, 0.2075, 0.1964, 0.2042, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20079432427883148 tensor([0.1956, 0.2059, 0.2008, 0.2031, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20530258119106293 tensor([0.1982, 0.2059, 0.1964, 0.2053, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19945424795150757 tensor([0.1995, 0.2040, 0.1952, 0.2042, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.201610267162323 tensor([0.2016, 0.2041, 0.1946, 0.2051, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20376056432724 tensor([0.1984, 0.2045, 0.1982, 0.2038, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19934815168380737 tensor([0.1970, 0.2044, 0.1993, 0.2039, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20439964532852173 tensor([0.1981, 0.2044, 0.1971, 0.2056, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1993987113237381 tensor([0.1994, 0.2034, 0.1953, 0.2046, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2026725858449936 tensor([0.2027, 0.2033, 0.1932, 0.2060, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2037651687860489 tensor([0.1980, 0.2065, 0.1966, 0.2038, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19992417097091675 tensor([0.1962, 0.2043, 0.1999, 0.2032, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1989404559135437 tensor([0.1964, 0.2034, 0.1989, 0.2052, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19898836314678192 tensor([0.1990, 0.2028, 0.1961, 0.2038, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2011719048023224 tensor([0.2012, 0.2034, 0.1947, 0.2040, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20472514629364014 tensor([0.1983, 0.2063, 0.1962, 0.2047, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1998032182455063 tensor([0.1979, 0.2043, 0.1998, 0.2035, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204623743891716 tensor([0.1973, 0.2046, 0.1964, 0.2072, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19933877885341644 tensor([0.1993, 0.2041, 0.1953, 0.2038, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20359580218791962 tensor([0.1989, 0.2052, 0.1974, 0.2036, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042805552482605 tensor([0.1976, 0.2043, 0.1979, 0.2053, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19869329035282135 tensor([0.1971, 0.2046, 0.1987, 0.2048, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19912365078926086 tensor([0.1991, 0.2032, 0.1967, 0.2058, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1984778493642807 tensor([0.1985, 0.2035, 0.1964, 0.2046, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20194649696350098 tensor([0.2019, 0.2038, 0.1939, 0.2054, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20490829646587372 tensor([0.1986, 0.2058, 0.1959, 0.2049, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20074081420898438 tensor([0.1950, 0.2052, 0.2007, 0.2035, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038470059633255 tensor([0.1989, 0.2038, 0.1954, 0.2083, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20063002407550812 tensor([0.2006, 0.2029, 0.1951, 0.2059, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20108431577682495 tensor([0.2011, 0.2045, 0.1943, 0.2052, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2042931765317917 tensor([0.1991, 0.2063, 0.1958, 0.2043, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20108817517757416 tensor([0.1961, 0.2054, 0.2011, 0.2034, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2050999402999878 tensor([0.1974, 0.2056, 0.1974, 0.2051, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19814433157444 tensor([0.1982, 0.2040, 0.1963, 0.2034, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20008932054042816 tensor([0.2001, 0.2040, 0.1959, 0.2049, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20485346019268036 tensor([0.1988, 0.2061, 0.1961, 0.2049, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20108304917812347 tensor([0.1954, 0.2050, 0.2011, 0.2037, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19887512922286987 tensor([0.1975, 0.2050, 0.1989, 0.2039, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2034292221069336 tensor([0.1992, 0.2034, 0.1959, 0.2043, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20185504853725433 tensor([0.2019, 0.2038, 0.1941, 0.2053, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20387157797813416 tensor([0.1973, 0.2055, 0.1980, 0.2039, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040252536535263 tensor([0.1969, 0.2040, 0.1984, 0.2049, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20505736768245697 tensor([0.1983, 0.2052, 0.1971, 0.2051, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2039535492658615 tensor([0.1978, 0.2040, 0.1967, 0.2056, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20234911143779755 tensor([0.2023, 0.2027, 0.1939, 0.2056, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20460760593414307 tensor([0.1983, 0.2072, 0.1967, 0.2046, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2038060873746872 tensor([0.1975, 0.2048, 0.1979, 0.2038, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20423033833503723 tensor([0.1985, 0.2042, 0.1965, 0.2052, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19933438301086426 tensor([0.1993, 0.2033, 0.1965, 0.2033, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20189163088798523 tensor([0.2019, 0.2038, 0.1932, 0.2051, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20378436148166656 tensor([0.1983, 0.2061, 0.1968, 0.2038, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20341020822525024 tensor([0.1976, 0.2050, 0.1984, 0.2034, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2045552283525467 tensor([0.1983, 0.2046, 0.1974, 0.2051, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19908159971237183 tensor([0.1991, 0.2018, 0.1956, 0.2049, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038920819759369 tensor([0.1992, 0.2039, 0.1972, 0.2044, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19812680780887604 tensor([0.1978, 0.2055, 0.1981, 0.2032, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2038189023733139 tensor([0.1982, 0.2056, 0.1977, 0.2038, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046739161014557 tensor([0.1984, 0.2047, 0.1969, 0.2048, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19929486513137817 tensor([0.1993, 0.2043, 0.1958, 0.2039, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20114058256149292 tensor([0.2011, 0.2038, 0.1957, 0.2051, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20405539870262146 tensor([0.1972, 0.2071, 0.1974, 0.2041, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19918107986450195 tensor([0.1971, 0.2051, 0.1992, 0.2032, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20377826690673828 tensor([0.1983, 0.2038, 0.1957, 0.2076, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2036300003528595 tensor([0.1985, 0.2036, 0.1958, 0.2041, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20167745649814606 tensor([0.2017, 0.2039, 0.1951, 0.2049, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19941993057727814 tensor([0.1994, 0.2039, 0.1963, 0.2051, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20057250559329987 tensor([0.2006, 0.2041, 0.1965, 0.2038, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20452868938446045 tensor([0.1978, 0.2045, 0.1980, 0.2048, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20395205914974213 tensor([0.1986, 0.2040, 0.1960, 0.2040, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20010054111480713 tensor([0.2001, 0.2038, 0.1965, 0.2043, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20417512953281403 tensor([0.1977, 0.2042, 0.1975, 0.2044, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1999107152223587 tensor([0.1975, 0.2051, 0.1999, 0.2029, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20405474305152893 tensor([0.1980, 0.2041, 0.1972, 0.2059, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20368880033493042 tensor([0.1992, 0.2037, 0.1958, 0.2040, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20180045068264008 tensor([0.2018, 0.2033, 0.1950, 0.2043, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2029201239347458 tensor([0.1977, 0.2081, 0.1974, 0.2029, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043674886226654 tensor([0.1976, 0.2044, 0.1968, 0.2057, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19976279139518738 tensor([0.1998, 0.2047, 0.1957, 0.2051, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19754375517368317 tensor([0.1981, 0.2033, 0.1975, 0.2031, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 4, 0], [2, 4], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4], [0, 4, 2], [0, 4], [4, 0, 2], [0, 2, 4], [2, 4], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4], [2, 4, 0], [4, 0, 2], [4, 2, 0], [0, 2, 4], [2, 4], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4], [2, 4], [3, 0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4], [0, 4, 2], [0, 4], [4, 0, 2], [2, 4], [2, 4], [0, 4, 2], [0, 4, 2], [2, 0, 4], [1, 2, 4, 0], [4, 2, 0], [0, 2, 4], [4, 0, 2], [2, 4], [2, 4], [2, 4], [4, 2, 0], [0, 4], [0, 4, 2], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4], [4, 0, 2], [0, 4], [2, 4], [0, 2, 4], [2, 4], [4, 2, 0], [3, 0, 4], [4, 0, 2], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 4], [2, 4], [0, 4, 2], [0, 4], [0, 2, 4], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [2, 4], [4, 0, 2], [4, 0, 2], [0, 2, 4], [0, 2, 4], [1, 4, 2, 0], [4, 0, 2], [0, 4], [0, 4, 2], [1, 0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [3, 0, 4, 2], [4, 2], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4], [2, 4, 0], [2, 1, 4], [2, 4], [2, 4, 0], [4, 0, 2], [2, 4, 0], [3, 2, 0, 4], [2, 4], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4], [4, 0, 2], [4, 2, 0], [4, 2, 0], [2, 4], [2, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4], [4, 0, 2], [0, 4], [2, 4, 0], [1, 2, 0, 4], [2, 4], [2, 4, 0], [0, 4], [0, 4, 2], [1, 2, 4, 0], [2, 4], [4, 0, 2], [0, 4], [0, 4, 2], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4], [4, 2, 0], [2, 0, 4], [2, 4], [4, 0, 2], [0, 4, 2], [4, 0, 2], [0, 4, 2], [2, 4], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4], [4, 2, 0], [2, 4, 0], [2, 4], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4], [4, 2, 0], [4, 0, 2], [4, 0, 2], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 4, 2, 0], [1, 2, 4, 0], [2, 4], [4, 2, 0], [0, 4], [4, 2, 0], [2, 4], [2, 4], [4, 2, 0], [0, 4], [4, 0, 2], [3, 2, 0, 4], [2, 4], [4, 2, 0], [0, 4], [4, 0, 2], [2, 0, 4], [2, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4], [4, 2, 0], [0, 4, 2], [2, 4, 0], [3, 2, 4, 0], [2, 4], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [4, 2, 0], [2, 4, 0], [4, 2], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4], [4, 2, 0], [4, 2], [4, 0, 2], [2, 0, 4], [4, 2], [0, 2, 4], [4, 0, 2], [4, 0, 2], [2, 0, 4], [2, 4], [4, 0, 2], [0, 4, 2], [2, 4, 0], [3, 0, 4, 2]]\n",
      "[[1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 2, 3], [0, 1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 2, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1], [0, 1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1]]\n",
      "NL_pred of 3th iteration [[2, 4, 0], [1, 2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [3, 0, 4, 2], [4, 0, 2], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [4, 0, 2], [1, 4, 2, 0], [1, 0, 2, 4], [2, 4, 0], [0, 4, 2], [3, 0, 4, 2], [0, 4, 2], [3, 2, 0, 4], [0, 4, 2], [4, 2, 0], [4, 2, 0], [0, 4, 2], [1, 2, 0, 4], [1, 2, 4, 0], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [4, 0, 2], [2, 4, 0], [0, 4, 2], [1, 4, 2, 0], [1, 2, 4, 0], [3, 2, 0, 4], [4, 0, 2], [3, 2, 4, 0], [1, 2, 4, 0], [3, 0, 4, 2], [2, 4, 0], [0, 4, 2], [4, 2, 0], [4, 0, 2], [2, 4, 0], [3, 0, 4, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.021091771907493718  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.021091494403901647  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.021090968710477234  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.02109021827822826  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.021089264603911854  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.021088129184285147  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.02108682960760398  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.021085383462124182  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.021083806381850947  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.02108210813803751  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.021080308273190358  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.02107840874156014  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.02107642517715204  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.021074363442718007  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.021072233309511277  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0210700445487851  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.02106779716053947  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.02106550287027828  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.02106316167800153  Accuracy on Support set:0.0\n",
      "torch.Size([61, 2048]) torch.Size([61])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.02106078140071181  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20603513717651367 tensor([0.1973, 0.2060, 0.1941, 0.2067, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20642854273319244 tensor([0.1964, 0.2068, 0.1953, 0.2064, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.204681396484375 tensor([0.1956, 0.2071, 0.1959, 0.2047, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2062329798936844 tensor([0.1980, 0.2062, 0.1927, 0.2073, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19927825033664703 tensor([0.1993, 0.2067, 0.1927, 0.2059, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20556527376174927 tensor([0.1967, 0.2070, 0.1947, 0.2056, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20525020360946655 tensor([0.1966, 0.2056, 0.1963, 0.2053, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20538020133972168 tensor([0.1977, 0.2054, 0.1925, 0.2092, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2058013528585434 tensor([0.1961, 0.2058, 0.1943, 0.2060, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2006593495607376 tensor([0.2007, 0.2055, 0.1921, 0.2070, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20626164972782135 tensor([0.1960, 0.2066, 0.1949, 0.2063, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19881416857242584 tensor([0.1950, 0.2061, 0.1988, 0.2046, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20620252192020416 tensor([0.1969, 0.2062, 0.1951, 0.2062, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20461706817150116 tensor([0.1959, 0.2046, 0.1946, 0.2053, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20095930993556976 tensor([0.2010, 0.2042, 0.1917, 0.2072, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2058718204498291 tensor([0.1961, 0.2092, 0.1942, 0.2059, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20537111163139343 tensor([0.1962, 0.2054, 0.1968, 0.2055, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2065223902463913 tensor([0.1972, 0.2067, 0.1945, 0.2065, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2051820605993271 tensor([0.1968, 0.2059, 0.1935, 0.2052, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20120230317115784 tensor([0.2012, 0.2051, 0.1910, 0.2077, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2057332694530487 tensor([0.1974, 0.2059, 0.1941, 0.2057, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20521660149097443 tensor([0.1966, 0.2065, 0.1956, 0.2052, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2056933343410492 tensor([0.1972, 0.2057, 0.1949, 0.2077, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20531946420669556 tensor([0.1967, 0.2061, 0.1944, 0.2053, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19914942979812622 tensor([0.1991, 0.2054, 0.1930, 0.2060, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20512321591377258 tensor([0.1967, 0.2082, 0.1941, 0.2051, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2048683762550354 tensor([0.1963, 0.2059, 0.1966, 0.2049, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20576387643814087 tensor([0.1961, 0.2058, 0.1947, 0.2075, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20472796261310577 tensor([0.1979, 0.2047, 0.1931, 0.2059, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19883379340171814 tensor([0.1988, 0.2060, 0.1923, 0.2067, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19883227348327637 tensor([0.1988, 0.2061, 0.1929, 0.2066, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20565488934516907 tensor([0.1978, 0.2057, 0.1936, 0.2072, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20483693480491638 tensor([0.1964, 0.2048, 0.1939, 0.2053, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20001061260700226 tensor([0.2000, 0.2040, 0.1923, 0.2070, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20565873384475708 tensor([0.1964, 0.2091, 0.1939, 0.2057, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20499029755592346 tensor([0.1969, 0.2050, 0.1966, 0.2057, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20578180253505707 tensor([0.1964, 0.2063, 0.1953, 0.2058, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20516493916511536 tensor([0.1977, 0.2052, 0.1933, 0.2052, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20046693086624146 tensor([0.2005, 0.2051, 0.1918, 0.2072, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2049376219511032 tensor([0.1960, 0.2089, 0.1945, 0.2049, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19803103804588318 tensor([0.1950, 0.2062, 0.1980, 0.2044, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20576947927474976 tensor([0.1959, 0.2058, 0.1940, 0.2089, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19947712123394012 tensor([0.1995, 0.2051, 0.1933, 0.2062, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19917717576026917 tensor([0.1992, 0.2048, 0.1931, 0.2066, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20493367314338684 tensor([0.1956, 0.2069, 0.1966, 0.2049, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20508772134780884 tensor([0.1963, 0.2051, 0.1966, 0.2055, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2048562616109848 tensor([0.1964, 0.2049, 0.1931, 0.2089, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2055920660495758 tensor([0.1976, 0.2056, 0.1939, 0.2072, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20544536411762238 tensor([0.1960, 0.2076, 0.1945, 0.2054, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20473763346672058 tensor([0.1959, 0.2072, 0.1971, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19862093031406403 tensor([0.1986, 0.2054, 0.1937, 0.2062, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19880497455596924 tensor([0.1988, 0.2054, 0.1926, 0.2065, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19917914271354675 tensor([0.1992, 0.2047, 0.1928, 0.2071, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2055494487285614 tensor([0.1972, 0.2089, 0.1935, 0.2055, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19828741252422333 tensor([0.1950, 0.2074, 0.1983, 0.2034, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20519547164440155 tensor([0.1962, 0.2052, 0.1941, 0.2085, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20583300292491913 tensor([0.1973, 0.2058, 0.1939, 0.2060, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20066459476947784 tensor([0.2007, 0.2058, 0.1917, 0.2066, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20527417957782745 tensor([0.1957, 0.2085, 0.1949, 0.2053, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20535200834274292 tensor([0.1955, 0.2064, 0.1972, 0.2054, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2056070864200592 tensor([0.1957, 0.2056, 0.1965, 0.2066, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20078878104686737 tensor([0.2008, 0.2050, 0.1918, 0.2073, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20488837361335754 tensor([0.1964, 0.2089, 0.1953, 0.2049, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19851426780223846 tensor([0.1946, 0.2056, 0.1985, 0.2046, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19949358701705933 tensor([0.1995, 0.2055, 0.1925, 0.2065, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20514027774333954 tensor([0.1967, 0.2051, 0.1943, 0.2056, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2003898024559021 tensor([0.2004, 0.2052, 0.1918, 0.2072, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20568515360355377 tensor([0.1965, 0.2091, 0.1937, 0.2057, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1981092095375061 tensor([0.1958, 0.2061, 0.1981, 0.2039, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2058490514755249 tensor([0.1960, 0.2058, 0.1954, 0.2085, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20462915301322937 tensor([0.1980, 0.2046, 0.1933, 0.2048, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1991051584482193 tensor([0.1991, 0.2054, 0.1927, 0.2061, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20570138096809387 tensor([0.1952, 0.2062, 0.1964, 0.2057, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20578955113887787 tensor([0.1954, 0.2065, 0.1965, 0.2058, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20527584850788116 tensor([0.1968, 0.2062, 0.1947, 0.2053, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1993914693593979 tensor([0.1994, 0.2052, 0.1917, 0.2063, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1996905505657196 tensor([0.1997, 0.2053, 0.1924, 0.2069, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20519550144672394 tensor([0.1957, 0.2084, 0.1953, 0.2052, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19821549952030182 tensor([0.1943, 0.2073, 0.1982, 0.2051, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20560306310653687 tensor([0.1962, 0.2056, 0.1949, 0.2062, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043159008026123 tensor([0.1973, 0.2043, 0.1930, 0.2067, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19911974668502808 tensor([0.1991, 0.2055, 0.1928, 0.2070, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20620959997177124 tensor([0.1967, 0.2067, 0.1944, 0.2062, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2047416716814041 tensor([0.1953, 0.2066, 0.1970, 0.2047, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20516137778759003 tensor([0.1962, 0.2064, 0.1970, 0.2052, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20155447721481323 tensor([0.2016, 0.2058, 0.1912, 0.2071, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20584622025489807 tensor([0.1966, 0.2063, 0.1960, 0.2058, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20554691553115845 tensor([0.1964, 0.2055, 0.1967, 0.2056, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20532363653182983 tensor([0.1960, 0.2055, 0.1944, 0.2053, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20515233278274536 tensor([0.1956, 0.2052, 0.1955, 0.2059, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20501279830932617 tensor([0.1967, 0.2070, 0.1954, 0.2050, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19927354156970978 tensor([0.1943, 0.2061, 0.1993, 0.2038, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206015944480896 tensor([0.1947, 0.2060, 0.1954, 0.2071, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20559656620025635 tensor([0.1981, 0.2056, 0.1935, 0.2066, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.205768883228302 tensor([0.1962, 0.2069, 0.1954, 0.2058, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20555831491947174 tensor([0.1957, 0.2057, 0.1970, 0.2056, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20601274073123932 tensor([0.1953, 0.2060, 0.1949, 0.2086, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20011067390441895 tensor([0.2001, 0.2059, 0.1933, 0.2063, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20548225939273834 tensor([0.1970, 0.2067, 0.1947, 0.2055, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2056783139705658 tensor([0.1970, 0.2057, 0.1950, 0.2063, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20624665915966034 tensor([0.1946, 0.2067, 0.1967, 0.2062, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20461294054985046 tensor([0.1961, 0.2046, 0.1934, 0.2067, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19979867339134216 tensor([0.1998, 0.2053, 0.1919, 0.2072, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20580749213695526 tensor([0.1969, 0.2084, 0.1942, 0.2058, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19807739555835724 tensor([0.1943, 0.2065, 0.1981, 0.2052, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20580263435840607 tensor([0.1970, 0.2058, 0.1930, 0.2089, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19942143559455872 tensor([0.1994, 0.2039, 0.1917, 0.2064, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20136582851409912 tensor([0.2014, 0.2054, 0.1912, 0.2066, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20594319701194763 tensor([0.1970, 0.2077, 0.1935, 0.2059, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20572838187217712 tensor([0.1974, 0.2062, 0.1955, 0.2057, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20531485974788666 tensor([0.1971, 0.2053, 0.1931, 0.2090, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1993223875761032 tensor([0.1993, 0.2059, 0.1921, 0.2070, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20558230578899384 tensor([0.1961, 0.2080, 0.1950, 0.2056, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20513567328453064 tensor([0.1949, 0.2063, 0.1973, 0.2051, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20618849992752075 tensor([0.1954, 0.2062, 0.1959, 0.2072, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20536106824874878 tensor([0.1978, 0.2054, 0.1939, 0.2060, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20035073161125183 tensor([0.2004, 0.2054, 0.1919, 0.2069, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2053261250257492 tensor([0.1960, 0.2096, 0.1943, 0.2053, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20581397414207458 tensor([0.1978, 0.2058, 0.1957, 0.2058, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20573890209197998 tensor([0.1984, 0.2057, 0.1933, 0.2078, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19897250831127167 tensor([0.1990, 0.2045, 0.1933, 0.2055, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20064546167850494 tensor([0.2006, 0.2052, 0.1920, 0.2067, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20468507707118988 tensor([0.1961, 0.2072, 0.1957, 0.2047, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20430631935596466 tensor([0.1957, 0.2059, 0.1975, 0.2043, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20561407506465912 tensor([0.1975, 0.2064, 0.1947, 0.2056, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20548032224178314 tensor([0.1959, 0.2055, 0.1937, 0.2057, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19932013750076294 tensor([0.1993, 0.2057, 0.1931, 0.2063, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20559129118919373 tensor([0.1964, 0.2069, 0.1959, 0.2056, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19790293276309967 tensor([0.1944, 0.2068, 0.1979, 0.2046, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2059868723154068 tensor([0.1975, 0.2067, 0.1932, 0.2060, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20208987593650818 tensor([0.2021, 0.2050, 0.1909, 0.2072, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2063404619693756 tensor([0.1968, 0.2072, 0.1936, 0.2063, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19855153560638428 tensor([0.1948, 0.2065, 0.1986, 0.2050, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20530055463314056 tensor([0.1962, 0.2053, 0.1942, 0.2082, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20077133178710938 tensor([0.2008, 0.2053, 0.1913, 0.2068, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2050100564956665 tensor([0.1967, 0.2072, 0.1952, 0.2050, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19772233068943024 tensor([0.1946, 0.2068, 0.1977, 0.2053, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20558534562587738 tensor([0.1961, 0.2056, 0.1950, 0.2074, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20453277230262756 tensor([0.1979, 0.2045, 0.1934, 0.2071, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20052087306976318 tensor([0.2005, 0.2053, 0.1915, 0.2064, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2054060399532318 tensor([0.1957, 0.2057, 0.1959, 0.2054, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1980743110179901 tensor([0.1945, 0.2066, 0.1981, 0.2052, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2054930478334427 tensor([0.1976, 0.2055, 0.1938, 0.2082, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2046145647764206 tensor([0.1976, 0.2046, 0.1933, 0.2049, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19894777238368988 tensor([0.1989, 0.2054, 0.1930, 0.2065, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20551349222660065 tensor([0.1962, 0.2089, 0.1942, 0.2055, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.206019788980484 tensor([0.1954, 0.2060, 0.1957, 0.2060, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20581772923469543 tensor([0.1966, 0.2058, 0.1951, 0.2066, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2053861767053604 tensor([0.1955, 0.2064, 0.1961, 0.2054, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2009703665971756 tensor([0.2010, 0.2053, 0.1918, 0.2070, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20475973188877106 tensor([0.1961, 0.2095, 0.1946, 0.2048, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20515885949134827 tensor([0.1952, 0.2069, 0.1975, 0.2052, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20621876418590546 tensor([0.1964, 0.2062, 0.1933, 0.2086, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20562513172626495 tensor([0.1980, 0.2058, 0.1932, 0.2056, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19944985210895538 tensor([0.1994, 0.2059, 0.1920, 0.2069, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20634427666664124 tensor([0.1971, 0.2073, 0.1940, 0.2063, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2054121196269989 tensor([0.1956, 0.2054, 0.1963, 0.2057, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20476537942886353 tensor([0.1971, 0.2048, 0.1949, 0.2074, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20540748536586761 tensor([0.1975, 0.2054, 0.1931, 0.2057, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20221304893493652 tensor([0.2022, 0.2048, 0.1914, 0.2072, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20563086867332458 tensor([0.1969, 0.2091, 0.1940, 0.2056, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19835244119167328 tensor([0.1942, 0.2074, 0.1984, 0.2046, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20677492022514343 tensor([0.1968, 0.2074, 0.1940, 0.2068, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20547857880592346 tensor([0.1980, 0.2055, 0.1928, 0.2057, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2001236379146576 tensor([0.2001, 0.2056, 0.1923, 0.2066, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20523123443126678 tensor([0.1969, 0.2060, 0.1957, 0.2052, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20539674162864685 tensor([0.1956, 0.2059, 0.1969, 0.2054, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20591531693935394 tensor([0.1967, 0.2059, 0.1947, 0.2071, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20493179559707642 tensor([0.1979, 0.2049, 0.1929, 0.2061, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2011694461107254 tensor([0.2012, 0.2048, 0.1909, 0.2075, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20524513721466064 tensor([0.1966, 0.2080, 0.1943, 0.2052, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20468802750110626 tensor([0.1948, 0.2058, 0.1975, 0.2047, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20493833720684052 tensor([0.1950, 0.2049, 0.1965, 0.2067, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19969195127487183 tensor([0.1997, 0.2049, 0.1923, 0.2055, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20618362724781036 tensor([0.1968, 0.2078, 0.1938, 0.2062, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20502883195877075 tensor([0.1965, 0.2059, 0.1973, 0.2050, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2061484009027481 tensor([0.1958, 0.2061, 0.1940, 0.2087, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2052859365940094 tensor([0.1979, 0.2056, 0.1930, 0.2053, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2050585001707077 tensor([0.1974, 0.2067, 0.1950, 0.2051, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20580779016017914 tensor([0.1961, 0.2058, 0.1955, 0.2067, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20611870288848877 tensor([0.1956, 0.2061, 0.1963, 0.2063, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2004549205303192 tensor([0.2005, 0.2053, 0.1915, 0.2069, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20638157427310944 tensor([0.1972, 0.2073, 0.1936, 0.2064, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.198290154337883 tensor([0.1936, 0.2067, 0.1983, 0.2049, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.205366849899292 tensor([0.1975, 0.2054, 0.1930, 0.2098, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19915449619293213 tensor([0.1992, 0.2044, 0.1927, 0.2073, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19960546493530273 tensor([0.1996, 0.2060, 0.1919, 0.2066, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20574364066123962 tensor([0.1977, 0.2078, 0.1934, 0.2057, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1986294537782669 tensor([0.1947, 0.2069, 0.1986, 0.2049, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20656532049179077 tensor([0.1960, 0.2071, 0.1950, 0.2066, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19862493872642517 tensor([0.1986, 0.2055, 0.1935, 0.2064, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20631860196590424 tensor([0.1974, 0.2076, 0.1937, 0.2063, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19862127304077148 tensor([0.1940, 0.2065, 0.1986, 0.2052, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20542165637016296 tensor([0.1960, 0.2065, 0.1965, 0.2054, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20494642853736877 tensor([0.1978, 0.2049, 0.1935, 0.2058, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2003682404756546 tensor([0.2004, 0.2053, 0.1918, 0.2068, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20533308386802673 tensor([0.1958, 0.2070, 0.1956, 0.2053, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20552662014961243 tensor([0.1955, 0.2055, 0.1960, 0.2064, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20653951168060303 tensor([0.1969, 0.2067, 0.1947, 0.2065, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20547792315483093 tensor([0.1964, 0.2055, 0.1943, 0.2071, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2008526623249054 tensor([0.2009, 0.2042, 0.1915, 0.2071, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2060766965150833 tensor([0.1969, 0.2087, 0.1943, 0.2061, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2052800953388214 tensor([0.1961, 0.2063, 0.1955, 0.2053, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20573961734771729 tensor([0.1971, 0.2057, 0.1941, 0.2066, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20039796829223633 tensor([0.2004, 0.2053, 0.1909, 0.2066, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2052432894706726 tensor([0.1969, 0.2076, 0.1944, 0.2052, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20487473905086517 tensor([0.1961, 0.2065, 0.1960, 0.2049, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206064373254776 tensor([0.1968, 0.2061, 0.1950, 0.2066, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2054120898246765 tensor([0.1977, 0.2054, 0.1949, 0.2058, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2053111493587494 tensor([0.1968, 0.2071, 0.1953, 0.2053, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20619022846221924 tensor([0.1969, 0.2062, 0.1945, 0.2063, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20533689856529236 tensor([0.1978, 0.2058, 0.1934, 0.2053, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1997022032737732 tensor([0.1997, 0.2053, 0.1933, 0.2065, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20551446080207825 tensor([0.1957, 0.2086, 0.1950, 0.2055, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20467747747898102 tensor([0.1957, 0.2066, 0.1968, 0.2047, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20528477430343628 tensor([0.1969, 0.2053, 0.1933, 0.2091, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2051224410533905 tensor([0.1971, 0.2051, 0.1934, 0.2056, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2002270519733429 tensor([0.2002, 0.2054, 0.1927, 0.2064, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2053876668214798 tensor([0.1980, 0.2054, 0.1939, 0.2066, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19912448525428772 tensor([0.1991, 0.2056, 0.1941, 0.2052, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20604103803634644 tensor([0.1963, 0.2060, 0.1956, 0.2063, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2054116278886795 tensor([0.1972, 0.2055, 0.1936, 0.2054, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1986362636089325 tensor([0.1986, 0.2053, 0.1941, 0.2058, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20569320023059845 tensor([0.1962, 0.2057, 0.1951, 0.2059, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20437811315059662 tensor([0.1961, 0.2066, 0.1975, 0.2044, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2055724561214447 tensor([0.1966, 0.2056, 0.1948, 0.2074, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20520128309726715 tensor([0.1977, 0.2052, 0.1934, 0.2054, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2003108412027359 tensor([0.2003, 0.2048, 0.1926, 0.2058, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20438215136528015 tensor([0.1962, 0.2096, 0.1950, 0.2044, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20588622987270355 tensor([0.1961, 0.2059, 0.1944, 0.2072, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20623669028282166 tensor([0.1983, 0.2062, 0.1933, 0.2066, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4], [0, 4, 2], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 4], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4], [2, 4, 0], [4, 0, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [3, 0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4], [0, 4, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [1, 2, 4, 0], [4, 2, 0], [0, 2, 4], [4, 0, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [2, 4], [4, 0, 2], [4, 0, 2], [0, 2, 4], [0, 2, 4], [1, 4, 2, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [1, 0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [3, 0, 4, 2], [4, 2], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [2, 4, 0], [2, 1, 4, 0], [2, 4], [2, 4, 0], [4, 0, 2], [2, 4, 0], [3, 2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4], [4, 0, 2], [4, 2, 0], [4, 2, 0], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 0, 4], [2, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [0, 4, 2], [2, 4], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 2, 0], [4, 0, 2], [4, 0, 2], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 4, 2, 0], [1, 2, 4, 0], [2, 4], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [3, 2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 4], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4], [4, 2, 0], [0, 4, 2], [2, 4, 0], [3, 2, 4, 0], [2, 4], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [4, 2, 0], [2, 4, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4], [4, 2, 0], [4, 2, 0], [4, 0, 2], [2, 0, 4], [4, 2, 0], [0, 2, 4], [4, 0, 2], [4, 0, 2], [2, 0, 4], [2, 4], [4, 0, 2], [0, 4, 2], [2, 4, 0], [3, 0, 4, 2]]\n",
      "[[1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1], [0, 1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 3], [1, 3], [1]]\n",
      "NL_pred of 4th iteration [[2, 4, 0], [0, 4, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [0, 4, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [3, 0, 4, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [2, 1, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [2, 4, 0], [0, 4, 2], [4, 2, 0], [4, 2, 0], [4, 2, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.02923467213457281  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.02923414652997797  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.02923313325101679  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.029231694611636074  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.029229871251366356  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.029227695681832054  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.029225208542563698  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.029222447763789784  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.02921942960132252  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.02921618656678633  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.029212745753201572  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.02920911799777638  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.029205330393531105  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.02920139648697593  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.029197335243225098  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.02919315208088268  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.029188868674364956  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.029184490442276  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.029180030931125988  Accuracy on Support set:0.0\n",
      "torch.Size([44, 2048]) torch.Size([44])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.029175498268821022  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.207578644156456 tensor([0.1940, 0.2076, 0.1931, 0.2080, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20780214667320251 tensor([0.1931, 0.2084, 0.1942, 0.2078, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20603862404823303 tensor([0.1924, 0.2086, 0.1949, 0.2060, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20776455104351044 tensor([0.1948, 0.2078, 0.1917, 0.2087, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2073041796684265 tensor([0.1960, 0.2083, 0.1917, 0.2073, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20692846179008484 tensor([0.1934, 0.2086, 0.1937, 0.2069, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20662519335746765 tensor([0.1934, 0.2071, 0.1952, 0.2066, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20689541101455688 tensor([0.1944, 0.2069, 0.1914, 0.2106, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20732149481773376 tensor([0.1929, 0.2073, 0.1932, 0.2074, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1973314881324768 tensor([0.1973, 0.2071, 0.1911, 0.2084, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20763199031352997 tensor([0.1927, 0.2081, 0.1939, 0.2076, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20601704716682434 tensor([0.1919, 0.2076, 0.1977, 0.2060, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20758362114429474 tensor([0.1936, 0.2078, 0.1941, 0.2076, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20612390339374542 tensor([0.1927, 0.2061, 0.1935, 0.2067, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19763728976249695 tensor([0.1976, 0.2057, 0.1907, 0.2085, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20723453164100647 tensor([0.1928, 0.2107, 0.1932, 0.2072, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20683637261390686 tensor([0.1930, 0.2069, 0.1957, 0.2068, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20790334045886993 tensor([0.1939, 0.2082, 0.1935, 0.2079, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20653951168060303 tensor([0.1935, 0.2074, 0.1924, 0.2065, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1978483349084854 tensor([0.1978, 0.2067, 0.1900, 0.2091, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20709428191184998 tensor([0.1941, 0.2074, 0.1930, 0.2071, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20658913254737854 tensor([0.1934, 0.2080, 0.1946, 0.2066, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.207217276096344 tensor([0.1939, 0.2072, 0.1938, 0.2091, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20669592916965485 tensor([0.1935, 0.2076, 0.1934, 0.2067, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069934904575348 tensor([0.1958, 0.2070, 0.1920, 0.2073, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2064773291349411 tensor([0.1934, 0.2097, 0.1931, 0.2065, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2062365561723709 tensor([0.1931, 0.2074, 0.1955, 0.2062, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20730330049991608 tensor([0.1929, 0.2073, 0.1937, 0.2089, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20626415312290192 tensor([0.1946, 0.2063, 0.1920, 0.2073, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075042724609375 tensor([0.1955, 0.2075, 0.1913, 0.2081, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20762939751148224 tensor([0.1955, 0.2076, 0.1919, 0.2080, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20719633996486664 tensor([0.1945, 0.2072, 0.1925, 0.2086, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20634354650974274 tensor([0.1932, 0.2063, 0.1929, 0.2066, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19669096171855927 tensor([0.1967, 0.2055, 0.1913, 0.2084, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20702317357063293 tensor([0.1932, 0.2107, 0.1928, 0.2070, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20650486648082733 tensor([0.1937, 0.2065, 0.1955, 0.2070, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2071559876203537 tensor([0.1932, 0.2078, 0.1942, 0.2072, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20653054118156433 tensor([0.1944, 0.2068, 0.1923, 0.2065, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19712629914283752 tensor([0.1971, 0.2066, 0.1908, 0.2086, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20629875361919403 tensor([0.1927, 0.2104, 0.1935, 0.2063, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20570765435695648 tensor([0.1918, 0.2077, 0.1969, 0.2057, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2072838842868805 tensor([0.1927, 0.2073, 0.1929, 0.2103, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20660386979579926 tensor([0.1962, 0.2066, 0.1922, 0.2076, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20637676119804382 tensor([0.1959, 0.2064, 0.1921, 0.2079, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20630769431591034 tensor([0.1924, 0.2084, 0.1956, 0.2063, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20660899579524994 tensor([0.1931, 0.2066, 0.1955, 0.2069, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20639345049858093 tensor([0.1932, 0.2064, 0.1921, 0.2103, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20714151859283447 tensor([0.1943, 0.2071, 0.1928, 0.2086, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20681723952293396 tensor([0.1928, 0.2091, 0.1934, 0.2068, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2061110883951187 tensor([0.1927, 0.2088, 0.1960, 0.2061, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20698575675487518 tensor([0.1953, 0.2070, 0.1927, 0.2076, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069140076637268 tensor([0.1955, 0.2069, 0.1916, 0.2079, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20627780258655548 tensor([0.1959, 0.2063, 0.1918, 0.2085, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20690445601940155 tensor([0.1939, 0.2104, 0.1925, 0.2069, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2048056721687317 tensor([0.1918, 0.2089, 0.1972, 0.2048, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20670086145401 tensor([0.1929, 0.2067, 0.1931, 0.2099, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20735065639019012 tensor([0.1940, 0.2074, 0.1928, 0.2074, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19731785356998444 tensor([0.1973, 0.2073, 0.1907, 0.2080, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20663908123970032 tensor([0.1925, 0.2100, 0.1939, 0.2066, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20672716200351715 tensor([0.1923, 0.2079, 0.1962, 0.2067, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20714178681373596 tensor([0.1925, 0.2071, 0.1954, 0.2080, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19749456644058228 tensor([0.1975, 0.2065, 0.1908, 0.2087, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20624341070652008 tensor([0.1932, 0.2104, 0.1942, 0.2062, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20599065721035004 tensor([0.1914, 0.2071, 0.1974, 0.2060, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20705154538154602 tensor([0.1962, 0.2071, 0.1915, 0.2079, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20664989948272705 tensor([0.1935, 0.2066, 0.1932, 0.2069, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19704516232013702 tensor([0.1970, 0.2067, 0.1908, 0.2086, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20706059038639069 tensor([0.1933, 0.2107, 0.1927, 0.2071, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20735220611095428 tensor([0.1928, 0.2074, 0.1943, 0.2099, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2061692327260971 tensor([0.1947, 0.2062, 0.1923, 0.2062, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069251537322998 tensor([0.1958, 0.2069, 0.1917, 0.2075, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2070707082748413 tensor([0.1920, 0.2078, 0.1953, 0.2071, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20716089010238647 tensor([0.1922, 0.2080, 0.1954, 0.2072, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2066396027803421 tensor([0.1936, 0.2078, 0.1936, 0.2066, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20674695074558258 tensor([0.1961, 0.2067, 0.1907, 0.2077, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20680204033851624 tensor([0.1964, 0.2068, 0.1914, 0.2083, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20656119287014008 tensor([0.1925, 0.2100, 0.1943, 0.2066, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2064487189054489 tensor([0.1911, 0.2088, 0.1971, 0.2064, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20713357627391815 tensor([0.1929, 0.2071, 0.1939, 0.2076, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20582959055900574 tensor([0.1940, 0.2058, 0.1920, 0.2081, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20709335803985596 tensor([0.1958, 0.2071, 0.1918, 0.2083, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.207584410905838 tensor([0.1935, 0.2083, 0.1934, 0.2076, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20611533522605896 tensor([0.1921, 0.2081, 0.1960, 0.2061, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20653411746025085 tensor([0.1930, 0.2079, 0.1959, 0.2065, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1982133984565735 tensor([0.1982, 0.2074, 0.1902, 0.2085, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20722191035747528 tensor([0.1934, 0.2079, 0.1950, 0.2072, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20701605081558228 tensor([0.1932, 0.2071, 0.1956, 0.2070, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20670312643051147 tensor([0.1927, 0.2070, 0.1934, 0.2067, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20666013658046722 tensor([0.1924, 0.2067, 0.1945, 0.2073, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2063768208026886 tensor([0.1935, 0.2085, 0.1944, 0.2064, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20520120859146118 tensor([0.1912, 0.2076, 0.1981, 0.2052, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20752522349357605 tensor([0.1915, 0.2075, 0.1944, 0.2084, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20714379847049713 tensor([0.1948, 0.2071, 0.1925, 0.2080, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20713597536087036 tensor([0.1930, 0.2085, 0.1943, 0.2071, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2069302350282669 tensor([0.1925, 0.2072, 0.1959, 0.2069, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075273096561432 tensor([0.1921, 0.2075, 0.1938, 0.2100, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19682735204696655 tensor([0.1968, 0.2075, 0.1923, 0.2077, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20685769617557526 tensor([0.1937, 0.2083, 0.1937, 0.2069, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20720279216766357 tensor([0.1938, 0.2072, 0.1940, 0.2077, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.207615926861763 tensor([0.1914, 0.2083, 0.1956, 0.2076, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20611578226089478 tensor([0.1929, 0.2061, 0.1924, 0.2080, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20682108402252197 tensor([0.1965, 0.2068, 0.1909, 0.2085, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2071831375360489 tensor([0.1936, 0.2100, 0.1932, 0.2072, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20657804608345032 tensor([0.1912, 0.2080, 0.1970, 0.2066, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20732513070106506 tensor([0.1937, 0.2073, 0.1919, 0.2103, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19799402356147766 tensor([0.1980, 0.2070, 0.1902, 0.2080, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2073116898536682 tensor([0.1938, 0.2092, 0.1924, 0.2073, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20711590349674225 tensor([0.1941, 0.2077, 0.1945, 0.2071, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068270891904831 tensor([0.1938, 0.2068, 0.1921, 0.2104, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2073846012353897 tensor([0.1960, 0.2074, 0.1911, 0.2084, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20694436132907867 tensor([0.1929, 0.2095, 0.1939, 0.2069, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20650246739387512 tensor([0.1917, 0.2079, 0.1962, 0.2065, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20773032307624817 tensor([0.1922, 0.2077, 0.1949, 0.2086, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20690520107746124 tensor([0.1945, 0.2069, 0.1929, 0.2074, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19701899588108063 tensor([0.1970, 0.2070, 0.1909, 0.2083, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20668193697929382 tensor([0.1928, 0.2111, 0.1933, 0.2067, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2071932852268219 tensor([0.1946, 0.2074, 0.1946, 0.2072, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2072593718767166 tensor([0.1951, 0.2073, 0.1923, 0.2092, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20606081187725067 tensor([0.1957, 0.2061, 0.1923, 0.2069, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19731827080249786 tensor([0.1973, 0.2068, 0.1910, 0.2081, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20606030523777008 tensor([0.1929, 0.2087, 0.1946, 0.2061, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20567670464515686 tensor([0.1925, 0.2074, 0.1964, 0.2057, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20698976516723633 tensor([0.1942, 0.2080, 0.1937, 0.2070, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20698381960391998 tensor([0.1927, 0.2070, 0.1926, 0.2070, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.207184299826622 tensor([0.1960, 0.2072, 0.1921, 0.2077, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20696435868740082 tensor([0.1931, 0.2084, 0.1949, 0.2070, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20592351257801056 tensor([0.1913, 0.2083, 0.1968, 0.2059, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2073516547679901 tensor([0.1943, 0.2083, 0.1922, 0.2074, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1987639218568802 tensor([0.1988, 0.2066, 0.1899, 0.2086, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20771294832229614 tensor([0.1936, 0.2087, 0.1926, 0.2077, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2063666582107544 tensor([0.1917, 0.2080, 0.1974, 0.2064, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20680591464042664 tensor([0.1930, 0.2068, 0.1932, 0.2095, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1974225789308548 tensor([0.1974, 0.2069, 0.1903, 0.2082, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2063703089952469 tensor([0.1935, 0.2087, 0.1942, 0.2064, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20667238533496857 tensor([0.1915, 0.2083, 0.1966, 0.2067, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20710134506225586 tensor([0.1929, 0.2071, 0.1939, 0.2088, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20604072511196136 tensor([0.1947, 0.2060, 0.1924, 0.2084, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1971774399280548 tensor([0.1972, 0.2068, 0.1905, 0.2078, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20677068829536438 tensor([0.1925, 0.2072, 0.1948, 0.2068, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20654767751693726 tensor([0.1913, 0.2081, 0.1970, 0.2065, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070169299840927 tensor([0.1944, 0.2070, 0.1928, 0.2096, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20611657202243805 tensor([0.1943, 0.2061, 0.1922, 0.2062, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20690016448497772 tensor([0.1956, 0.2069, 0.1920, 0.2079, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20687392354011536 tensor([0.1930, 0.2104, 0.1932, 0.2069, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20738725364208221 tensor([0.1922, 0.2076, 0.1946, 0.2074, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20735275745391846 tensor([0.1933, 0.2074, 0.1941, 0.2080, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20674894750118256 tensor([0.1923, 0.2080, 0.1950, 0.2067, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19766715168952942 tensor([0.1977, 0.2069, 0.1908, 0.2084, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20612293481826782 tensor([0.1928, 0.2110, 0.1935, 0.2061, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2065342515707016 tensor([0.1921, 0.2084, 0.1964, 0.2065, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20775026082992554 tensor([0.1932, 0.2078, 0.1922, 0.2100, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20699602365493774 tensor([0.1948, 0.2073, 0.1922, 0.2070, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20741009712219238 tensor([0.1961, 0.2074, 0.1910, 0.2083, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20772017538547516 tensor([0.1938, 0.2088, 0.1930, 0.2077, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20692475140094757 tensor([0.1924, 0.2069, 0.1952, 0.2070, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20628193020820618 tensor([0.1938, 0.2063, 0.1938, 0.2088, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20692437887191772 tensor([0.1943, 0.2069, 0.1921, 0.2070, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1988520622253418 tensor([0.1989, 0.2064, 0.1904, 0.2086, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2069927155971527 tensor([0.1937, 0.2107, 0.1930, 0.2070, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20593281090259552 tensor([0.1910, 0.2089, 0.1973, 0.2059, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20814944803714752 tensor([0.1935, 0.2089, 0.1930, 0.2081, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20702031254768372 tensor([0.1947, 0.2070, 0.1918, 0.2070, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1968059539794922 tensor([0.1968, 0.2071, 0.1913, 0.2079, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20660074055194855 tensor([0.1937, 0.2075, 0.1947, 0.2066, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20676106214523315 tensor([0.1924, 0.2074, 0.1959, 0.2068, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2074567675590515 tensor([0.1935, 0.2075, 0.1937, 0.2085, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20645610988140106 tensor([0.1947, 0.2065, 0.1919, 0.2074, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19780543446540833 tensor([0.1978, 0.2064, 0.1899, 0.2089, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20662128925323486 tensor([0.1933, 0.2096, 0.1932, 0.2066, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20606210827827454 tensor([0.1916, 0.2073, 0.1964, 0.2061, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064460664987564 tensor([0.1918, 0.2064, 0.1954, 0.2080, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20642702281475067 tensor([0.1964, 0.2064, 0.1913, 0.2069, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20755425095558167 tensor([0.1936, 0.2094, 0.1928, 0.2076, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20640963315963745 tensor([0.1933, 0.2074, 0.1963, 0.2064, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2076878696680069 tensor([0.1926, 0.2077, 0.1929, 0.2101, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2066490650177002 tensor([0.1946, 0.2071, 0.1920, 0.2066, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2064095288515091 tensor([0.1942, 0.2083, 0.1940, 0.2064, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20735324919223785 tensor([0.1929, 0.2074, 0.1944, 0.2081, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2076362520456314 tensor([0.1924, 0.2077, 0.1952, 0.2076, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19709903001785278 tensor([0.1971, 0.2069, 0.1905, 0.2083, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20775650441646576 tensor([0.1939, 0.2088, 0.1925, 0.2078, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2062903642654419 tensor([0.1905, 0.2082, 0.1972, 0.2063, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20690253376960754 tensor([0.1942, 0.2069, 0.1920, 0.2112, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20589889585971832 tensor([0.1958, 0.2059, 0.1917, 0.2087, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075875997543335 tensor([0.1963, 0.2076, 0.1909, 0.2080, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2071121782064438 tensor([0.1944, 0.2093, 0.1924, 0.2071, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20622983574867249 tensor([0.1915, 0.2084, 0.1975, 0.2062, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20794285833835602 tensor([0.1928, 0.2086, 0.1940, 0.2079, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070406675338745 tensor([0.1953, 0.2070, 0.1925, 0.2077, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2076873630285263 tensor([0.1941, 0.2092, 0.1927, 0.2077, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20656070113182068 tensor([0.1909, 0.2080, 0.1975, 0.2066, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2067945897579193 tensor([0.1928, 0.2080, 0.1954, 0.2068, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20648320019245148 tensor([0.1945, 0.2065, 0.1925, 0.2072, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19702011346817017 tensor([0.1970, 0.2068, 0.1908, 0.2082, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20669691264629364 tensor([0.1926, 0.2085, 0.1945, 0.2067, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20703069865703583 tensor([0.1923, 0.2070, 0.1949, 0.2078, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20792362093925476 tensor([0.1937, 0.2082, 0.1936, 0.2079, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20701070129871368 tensor([0.1931, 0.2070, 0.1932, 0.2085, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19751039147377014 tensor([0.1975, 0.2057, 0.1905, 0.2085, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2074456810951233 tensor([0.1936, 0.2102, 0.1932, 0.2074, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20664826035499573 tensor([0.1929, 0.2078, 0.1944, 0.2066, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20726044476032257 tensor([0.1938, 0.2073, 0.1931, 0.2080, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19704952836036682 tensor([0.1970, 0.2069, 0.1899, 0.2080, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20660510659217834 tensor([0.1936, 0.2092, 0.1934, 0.2066, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20623978972434998 tensor([0.1929, 0.2080, 0.1950, 0.2062, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075846642255783 tensor([0.1936, 0.2076, 0.1939, 0.2080, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20695699751377106 tensor([0.1944, 0.2070, 0.1938, 0.2072, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20670057833194733 tensor([0.1935, 0.2086, 0.1943, 0.2067, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20768480002880096 tensor([0.1937, 0.2077, 0.1934, 0.2077, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20670686662197113 tensor([0.1946, 0.2073, 0.1924, 0.2067, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20687220990657806 tensor([0.1964, 0.2069, 0.1923, 0.2079, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20687244832515717 tensor([0.1925, 0.2101, 0.1939, 0.2069, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20604555308818817 tensor([0.1925, 0.2082, 0.1957, 0.2060, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068149745464325 tensor([0.1936, 0.2068, 0.1923, 0.2105, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20661912858486176 tensor([0.1938, 0.2066, 0.1924, 0.2069, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1969015747308731 tensor([0.1969, 0.2069, 0.1917, 0.2078, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20691326260566711 tensor([0.1947, 0.2069, 0.1929, 0.2080, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20662571489810944 tensor([0.1958, 0.2071, 0.1931, 0.2066, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075609266757965 tensor([0.1931, 0.2076, 0.1945, 0.2077, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20677363872528076 tensor([0.1939, 0.2070, 0.1926, 0.2068, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068396806716919 tensor([0.1953, 0.2068, 0.1931, 0.2071, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20722664892673492 tensor([0.1930, 0.2072, 0.1940, 0.2073, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20574524998664856 tensor([0.1929, 0.2082, 0.1964, 0.2057, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20711496472358704 tensor([0.1933, 0.2071, 0.1938, 0.2088, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20673589408397675 tensor([0.1944, 0.2067, 0.1924, 0.2068, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19698837399482727 tensor([0.1970, 0.2064, 0.1916, 0.2072, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20574350655078888 tensor([0.1930, 0.2112, 0.1940, 0.2057, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20740137994289398 tensor([0.1929, 0.2074, 0.1933, 0.2086, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20776300132274628 tensor([0.1950, 0.2078, 0.1923, 0.2080, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "[[4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [2, 4, 0], [4, 0, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [3, 0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [1, 2, 4, 0], [4, 2, 0], [0, 2, 4], [4, 0, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [4, 0, 2], [0, 2, 4], [0, 2, 4], [1, 4, 2, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [1, 0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [3, 0, 4, 2], [4, 2, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [2, 4, 0], [2, 1, 4, 0], [2, 4, 0], [2, 4, 0], [4, 0, 2], [2, 4, 0], [3, 2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [4, 0, 2], [4, 2, 0], [4, 2, 0], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 0, 4], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [0, 4, 2], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 2, 0], [4, 0, 2], [4, 0, 2], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 4, 2, 0], [1, 2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [3, 2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [2, 4, 0], [3, 2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [4, 2, 0], [2, 4, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 2, 0], [4, 2, 0], [4, 0, 2], [2, 0, 4], [4, 2, 0], [0, 2, 4], [4, 0, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [3, 0, 4, 2]]\n",
      "[[1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1]]\n",
      "NL_pred of 5th iteration [[2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.04941079708246084  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.04940856878574078  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.04940431851607103  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.04939827093711266  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.04939059569285466  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.04938147159723135  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.04937104078439566  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.049359449973473184  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0493468321286715  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.04933326977949876  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.049318905060107894  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.049303797575143665  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.04928804819400494  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.04927171193636381  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.04925488050167377  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.04923759056971623  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.049219910915081315  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.04920188738749577  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.04918356125171368  Accuracy on Support set:0.0\n",
      "torch.Size([26, 2048]) torch.Size([26])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.049164969187516436  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [2, 4, 0], [4, 0, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [3, 0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [1, 2, 4, 0], [4, 2, 0], [0, 2, 4], [4, 0, 2], [2, 4, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [4, 0, 2], [0, 2, 4], [0, 2, 4], [1, 4, 2, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [1, 0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [3, 0, 4, 2], [4, 2, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [2, 4, 0], [2, 1, 4, 0], [2, 4, 0], [2, 4, 0], [4, 0, 2], [2, 4, 0], [3, 2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [4, 0, 2], [4, 2, 0], [4, 2, 0], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [1, 2, 0, 4], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [0, 4, 2], [2, 4, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [0, 4, 2], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 2, 4, 0], [2, 4, 0], [4, 2, 0], [4, 0, 2], [4, 0, 2], [2, 4, 0], [4, 2, 0], [0, 4, 2], [0, 4, 2], [1, 4, 2, 0], [1, 2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 2, 0], [2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [3, 2, 0, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 2, 0], [0, 2, 4], [2, 4, 0], [4, 2, 0], [0, 4, 2], [2, 4, 0], [3, 2, 4, 0], [2, 4, 0], [4, 2, 0], [0, 4, 2], [4, 0, 2], [1, 2, 4, 0], [4, 2, 0], [3, 0, 4, 2], [4, 0, 2], [4, 2, 0], [2, 4, 0], [4, 2, 0], [4, 0, 2], [0, 4, 2], [4, 2, 0], [2, 0, 4], [2, 4, 0], [4, 2, 0], [4, 2, 0], [4, 0, 2], [2, 0, 4], [4, 2, 0], [0, 2, 4], [4, 0, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 4, 0], [3, 0, 4, 2]]\n",
      "POSITION :  [[1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [3], [3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [3], [1, 3], [1], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.4\n",
      "tensor([3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1, 1])\n",
      "Start of final training\n",
      "Epoch: 0  Loss: 61.904761904761905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 61.904761904761905\n",
      "Epoch: 2  Loss: 61.904761904761905\n",
      "Epoch: 3  Loss: 61.904761904761905\n",
      "Epoch: 4  Loss: 61.904761904761905\n",
      "Epoch: 5  Loss: 61.904761904761905\n",
      "Epoch: 6  Loss: 61.904761904761905\n",
      "Epoch: 7  Loss: 61.904761904761905\n",
      "Epoch: 8  Loss: 61.904761904761905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 1/15 [01:04<14:57, 64.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 61.904761904761905\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 1.8352290725708007  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 1.7871694159507752  Accuracy on Support set:20.0\n",
      "Train_Epoch: 2  Train_Loss: 1.748128821849823  Accuracy on Support set:20.0\n",
      "Train_Epoch: 3  Train_Loss: 1.7199206781387328  Accuracy on Support set:20.0\n",
      "Train_Epoch: 4  Train_Loss: 1.6989399480819702  Accuracy on Support set:20.0\n",
      "Train_Epoch: 5  Train_Loss: 1.682819242477417  Accuracy on Support set:20.0\n",
      "Train_Epoch: 6  Train_Loss: 1.6701356887817382  Accuracy on Support set:20.0\n",
      "Train_Epoch: 7  Train_Loss: 1.6599395227432252  Accuracy on Support set:20.0\n",
      "Train_Epoch: 8  Train_Loss: 1.651593017578125  Accuracy on Support set:20.0\n",
      "Train_Epoch: 9  Train_Loss: 1.6446643257141114  Accuracy on Support set:20.0\n",
      "Train_Epoch: 10  Train_Loss: 1.6388371801376342  Accuracy on Support set:20.0\n",
      "Train_Epoch: 11  Train_Loss: 1.6338944816589356  Accuracy on Support set:20.0\n",
      "Train_Epoch: 12  Train_Loss: 1.6296680164337158  Accuracy on Support set:20.0\n",
      "Train_Epoch: 13  Train_Loss: 1.6260204744338989  Accuracy on Support set:20.0\n",
      "Train_Epoch: 14  Train_Loss: 1.6228552532196046  Accuracy on Support set:20.0\n",
      "Train_Epoch: 15  Train_Loss: 1.6200898170471192  Accuracy on Support set:20.0\n",
      "Train_Epoch: 16  Train_Loss: 1.6176446390151977  Accuracy on Support set:20.0\n",
      "Train_Epoch: 17  Train_Loss: 1.6154831457138061  Accuracy on Support set:20.0\n",
      "Train_Epoch: 18  Train_Loss: 1.6135542821884155  Accuracy on Support set:20.0\n",
      "Train_Epoch: 19  Train_Loss: 1.6118171262741088  Accuracy on Support set:20.0\n",
      "Train_Epoch: 20  Train_Loss: 1.6102437114715575  Accuracy on Support set:20.0\n",
      "Train_Epoch: 21  Train_Loss: 1.6087985515594483  Accuracy on Support set:20.0\n",
      "Train_Epoch: 22  Train_Loss: 1.6074647045135497  Accuracy on Support set:20.0\n",
      "Train_Epoch: 23  Train_Loss: 1.6062234497070313  Accuracy on Support set:20.0\n",
      "Train_Epoch: 24  Train_Loss: 1.6050707197189331  Accuracy on Support set:20.0\n",
      "Train_Epoch: 25  Train_Loss: 1.6039899682998657  Accuracy on Support set:20.0\n",
      "Train_Epoch: 26  Train_Loss: 1.6029576587677001  Accuracy on Support set:20.0\n",
      "Train_Epoch: 27  Train_Loss: 1.6019686841964722  Accuracy on Support set:20.0\n",
      "Train_Epoch: 28  Train_Loss: 1.601012053489685  Accuracy on Support set:20.0\n",
      "Train_Epoch: 29  Train_Loss: 1.6000794649124146  Accuracy on Support set:32.0\n",
      "Train_Epoch: 30  Train_Loss: 1.5992013835906982  Accuracy on Support set:32.0\n",
      "Train_Epoch: 31  Train_Loss: 1.5983336544036866  Accuracy on Support set:32.0\n",
      "Train_Epoch: 32  Train_Loss: 1.597484917640686  Accuracy on Support set:32.0\n",
      "Train_Epoch: 33  Train_Loss: 1.5966520118713379  Accuracy on Support set:32.0\n",
      "Train_Epoch: 34  Train_Loss: 1.5958123302459717  Accuracy on Support set:36.0\n",
      "Train_Epoch: 35  Train_Loss: 1.59497474193573  Accuracy on Support set:36.0\n",
      "Train_Epoch: 36  Train_Loss: 1.594140248298645  Accuracy on Support set:40.0\n",
      "Train_Epoch: 37  Train_Loss: 1.5932938957214355  Accuracy on Support set:40.0\n",
      "Train_Epoch: 38  Train_Loss: 1.592446241378784  Accuracy on Support set:40.0\n",
      "Train_Epoch: 39  Train_Loss: 1.591580581665039  Accuracy on Support set:40.0\n",
      "Train_Epoch: 40  Train_Loss: 1.5906978178024291  Accuracy on Support set:40.0\n",
      "Train_Epoch: 41  Train_Loss: 1.589808735847473  Accuracy on Support set:40.0\n",
      "Train_Epoch: 42  Train_Loss: 1.588876395225525  Accuracy on Support set:40.0\n",
      "Train_Epoch: 43  Train_Loss: 1.587883062362671  Accuracy on Support set:48.0\n",
      "Train_Epoch: 44  Train_Loss: 1.5868959093093873  Accuracy on Support set:48.0\n",
      "Train_Epoch: 45  Train_Loss: 1.5858731698989867  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 46  Train_Loss: 1.5848014068603515  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 47  Train_Loss: 1.583708519935608  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 48  Train_Loss: 1.5825743007659911  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 49  Train_Loss: 1.581390724182129  Accuracy on Support set:56.00000000000001\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  32.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.19665059447288513 tensor([0.1967, 0.2021, 0.1969, 0.2071, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19471339881420135 tensor([0.1947, 0.2029, 0.1987, 0.2063, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1918819099664688 tensor([0.1919, 0.2026, 0.2033, 0.2040, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19582705199718475 tensor([0.1963, 0.2039, 0.1958, 0.2079, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19476257264614105 tensor([0.1971, 0.1986, 0.1948, 0.2086, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19330476224422455 tensor([0.2004, 0.2047, 0.1933, 0.2063, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1945391595363617 tensor([0.1945, 0.2085, 0.1991, 0.2029, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19445200264453888 tensor([0.1945, 0.2046, 0.2005, 0.2045, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948821097612381 tensor([0.1978, 0.2040, 0.1949, 0.2073, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19457203149795532 tensor([0.1981, 0.2000, 0.1946, 0.2075, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1952705979347229 tensor([0.1985, 0.2032, 0.1953, 0.2062, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19480381906032562 tensor([0.1948, 0.2059, 0.1984, 0.2050, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19411258399486542 tensor([0.1941, 0.2032, 0.1991, 0.2055, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948557198047638 tensor([0.1976, 0.2051, 0.1949, 0.2067, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19502118229866028 tensor([0.1950, 0.2004, 0.1971, 0.2053, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19333568215370178 tensor([0.2002, 0.2034, 0.1933, 0.2070, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19556796550750732 tensor([0.1956, 0.2027, 0.1991, 0.2049, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1932346373796463 tensor([0.1932, 0.2030, 0.2002, 0.2059, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1945769041776657 tensor([0.1946, 0.2035, 0.1999, 0.2051, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19439923763275146 tensor([0.1977, 0.2010, 0.1944, 0.2066, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19319748878479004 tensor([0.2004, 0.2061, 0.1932, 0.2056, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1947551667690277 tensor([0.1948, 0.2065, 0.1982, 0.2042, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18979333341121674 tensor([0.1898, 0.2041, 0.2067, 0.2013, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19651146233081818 tensor([0.1965, 0.2022, 0.1982, 0.2064, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19591183960437775 tensor([0.1971, 0.2005, 0.1959, 0.2061, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19593636691570282 tensor([0.1980, 0.2061, 0.1959, 0.2036, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19478359818458557 tensor([0.1948, 0.2038, 0.1983, 0.2059, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1901232898235321 tensor([0.1901, 0.2013, 0.2038, 0.2035, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19418424367904663 tensor([0.1975, 0.2013, 0.1942, 0.2098, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19546563923358917 tensor([0.1980, 0.1996, 0.1955, 0.2080, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1934509128332138 tensor([0.2004, 0.2041, 0.1935, 0.2063, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1940782070159912 tensor([0.1941, 0.2064, 0.1999, 0.2031, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1939285695552826 tensor([0.1939, 0.2014, 0.1991, 0.2071, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1939886510372162 tensor([0.1940, 0.2025, 0.1992, 0.2068, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1944849044084549 tensor([0.1974, 0.2020, 0.1945, 0.2065, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1956407129764557 tensor([0.1976, 0.2045, 0.1956, 0.2052, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19570739567279816 tensor([0.1957, 0.2049, 0.1979, 0.2054, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.194306880235672 tensor([0.1943, 0.2044, 0.1986, 0.2049, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19524820148944855 tensor([0.1969, 0.2046, 0.1952, 0.2072, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19459368288516998 tensor([0.1946, 0.2034, 0.1986, 0.2039, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19227096438407898 tensor([0.2014, 0.2054, 0.1923, 0.2060, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19378262758255005 tensor([0.1938, 0.2034, 0.2000, 0.2045, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1918254792690277 tensor([0.1918, 0.2024, 0.2035, 0.2041, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19549377262592316 tensor([0.1955, 0.2032, 0.1975, 0.2066, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1942991465330124 tensor([0.1978, 0.1985, 0.1943, 0.2087, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1950215995311737 tensor([0.1979, 0.2035, 0.1950, 0.2069, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1950712651014328 tensor([0.1951, 0.2065, 0.1997, 0.2035, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19324998557567596 tensor([0.1932, 0.2049, 0.2028, 0.2030, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19519196450710297 tensor([0.1952, 0.2030, 0.1984, 0.2077, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19433775544166565 tensor([0.1978, 0.2005, 0.1943, 0.2078, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19613619148731232 tensor([0.1961, 0.2028, 0.1983, 0.2063, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19534125924110413 tensor([0.1953, 0.2068, 0.1973, 0.2047, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19440814852714539 tensor([0.1944, 0.2045, 0.1993, 0.2050, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1964445859193802 tensor([0.1972, 0.2021, 0.1964, 0.2069, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1947871893644333 tensor([0.1975, 0.1994, 0.1948, 0.2079, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19290730357170105 tensor([0.2002, 0.2042, 0.1929, 0.2060, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1950748860836029 tensor([0.1952, 0.2074, 0.1977, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19146910309791565 tensor([0.1915, 0.2048, 0.2043, 0.2013, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1963626891374588 tensor([0.1975, 0.2015, 0.1964, 0.2080, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19499458372592926 tensor([0.1964, 0.1999, 0.1950, 0.2075, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19265149533748627 tensor([0.2012, 0.2054, 0.1927, 0.2051, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19624881446361542 tensor([0.1962, 0.2039, 0.1980, 0.2050, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1913890540599823 tensor([0.1914, 0.2039, 0.2026, 0.2039, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19585156440734863 tensor([0.1959, 0.2017, 0.1965, 0.2073, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19612297415733337 tensor([0.1965, 0.2007, 0.1961, 0.2070, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19447781145572662 tensor([0.1992, 0.2053, 0.1945, 0.2053, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19513154029846191 tensor([0.1951, 0.2028, 0.1988, 0.2063, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19138623774051666 tensor([0.1914, 0.2029, 0.2038, 0.2040, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1958743780851364 tensor([0.1963, 0.2047, 0.1996, 0.2035, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19561514258384705 tensor([0.1959, 0.1999, 0.1956, 0.2072, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19356435537338257 tensor([0.1999, 0.2060, 0.1936, 0.2047, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1954605132341385 tensor([0.1963, 0.2071, 0.1961, 0.2049, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19233374297618866 tensor([0.1923, 0.2013, 0.2026, 0.2044, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1958223581314087 tensor([0.1965, 0.2022, 0.1958, 0.2083, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1956639289855957 tensor([0.1968, 0.2017, 0.1957, 0.2065, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19300489127635956 tensor([0.1999, 0.2022, 0.1930, 0.2082, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19484484195709229 tensor([0.1948, 0.2047, 0.1982, 0.2056, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19387897849082947 tensor([0.1939, 0.2029, 0.2016, 0.2050, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1962689459323883 tensor([0.1963, 0.2017, 0.1966, 0.2091, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19614669680595398 tensor([0.1968, 0.2016, 0.1961, 0.2074, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1945962905883789 tensor([0.1988, 0.2057, 0.1946, 0.2045, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19454757869243622 tensor([0.1945, 0.2072, 0.2004, 0.2026, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1945360153913498 tensor([0.1945, 0.2026, 0.1987, 0.2062, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19629478454589844 tensor([0.1973, 0.2019, 0.1983, 0.2062, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1938479095697403 tensor([0.1993, 0.2030, 0.1938, 0.2061, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19526079297065735 tensor([0.1988, 0.2032, 0.1953, 0.2066, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1956372857093811 tensor([0.1956, 0.2042, 0.1971, 0.2072, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1926974058151245 tensor([0.1927, 0.2051, 0.2016, 0.2037, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1959126740694046 tensor([0.1967, 0.2045, 0.1972, 0.2057, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19634941220283508 tensor([0.1963, 0.2029, 0.1966, 0.2055, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1960972249507904 tensor([0.1974, 0.2038, 0.1961, 0.2059, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19571903347969055 tensor([0.1957, 0.2040, 0.1990, 0.2040, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19166408479213715 tensor([0.1917, 0.2027, 0.2045, 0.2023, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1959117352962494 tensor([0.1962, 0.2036, 0.1959, 0.2076, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19637036323547363 tensor([0.1964, 0.2020, 0.1968, 0.2052, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1936759203672409 tensor([0.1995, 0.2050, 0.1937, 0.2060, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1943860650062561 tensor([0.1957, 0.2068, 0.1991, 0.2040, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1943536251783371 tensor([0.1944, 0.2046, 0.1988, 0.2047, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19620558619499207 tensor([0.1962, 0.2047, 0.1965, 0.2058, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19493624567985535 tensor([0.1949, 0.1995, 0.1978, 0.2084, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19457091391086578 tensor([0.1983, 0.2034, 0.1946, 0.2070, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19486407935619354 tensor([0.1971, 0.2049, 0.1974, 0.2057, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1935614049434662 tensor([0.1936, 0.2037, 0.2011, 0.2047, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19485187530517578 tensor([0.1949, 0.2004, 0.1966, 0.2090, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1955908238887787 tensor([0.1957, 0.1991, 0.1956, 0.2081, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19474460184574127 tensor([0.1990, 0.2058, 0.1947, 0.2050, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19595934450626373 tensor([0.1960, 0.2049, 0.1978, 0.2050, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1929309368133545 tensor([0.1929, 0.2017, 0.2005, 0.2062, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19566144049167633 tensor([0.1981, 0.2034, 0.1957, 0.2067, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19590234756469727 tensor([0.1975, 0.2002, 0.1959, 0.2073, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19504526257514954 tensor([0.1986, 0.2050, 0.1950, 0.2048, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19603735208511353 tensor([0.1960, 0.2048, 0.1964, 0.2064, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19465959072113037 tensor([0.1947, 0.2031, 0.1984, 0.2057, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19561532139778137 tensor([0.1956, 0.2021, 0.1965, 0.2077, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19654375314712524 tensor([0.1965, 0.2026, 0.1974, 0.2046, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19357402622699738 tensor([0.2008, 0.2047, 0.1936, 0.2049, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19501785933971405 tensor([0.1955, 0.2079, 0.1971, 0.2044, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19283652305603027 tensor([0.1928, 0.2036, 0.2014, 0.2037, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19485227763652802 tensor([0.1970, 0.2059, 0.1970, 0.2053, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19477859139442444 tensor([0.1965, 0.2000, 0.1948, 0.2073, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19501668214797974 tensor([0.1979, 0.2027, 0.1950, 0.2082, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1935325562953949 tensor([0.1935, 0.2067, 0.1990, 0.2045, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1901814192533493 tensor([0.1902, 0.2026, 0.2061, 0.2026, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1962413489818573 tensor([0.1967, 0.2025, 0.1962, 0.2077, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19478194415569305 tensor([0.1965, 0.1998, 0.1948, 0.2066, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19216042757034302 tensor([0.2015, 0.2055, 0.1922, 0.2052, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1948077231645584 tensor([0.1948, 0.2043, 0.1990, 0.2055, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19274039566516876 tensor([0.1927, 0.2027, 0.2025, 0.2044, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1952158361673355 tensor([0.1966, 0.2037, 0.1962, 0.2084, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1959158033132553 tensor([0.1961, 0.1995, 0.1959, 0.2078, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19401945173740387 tensor([0.1988, 0.2061, 0.1940, 0.2050, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19526726007461548 tensor([0.1953, 0.2056, 0.1991, 0.2038, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1908944845199585 tensor([0.1909, 0.2025, 0.2044, 0.2037, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19497758150100708 tensor([0.1972, 0.2038, 0.1950, 0.2069, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19468720257282257 tensor([0.1976, 0.2016, 0.1947, 0.2081, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1944165974855423 tensor([0.1989, 0.2037, 0.1962, 0.2069, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19541886448860168 tensor([0.1966, 0.2061, 0.1967, 0.2052, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19156713783740997 tensor([0.1916, 0.2030, 0.2041, 0.2026, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19343331456184387 tensor([0.1934, 0.2051, 0.1997, 0.2049, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1960577517747879 tensor([0.1961, 0.1999, 0.1962, 0.2089, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19209584593772888 tensor([0.2020, 0.2044, 0.1921, 0.2058, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19465546309947968 tensor([0.1947, 0.2061, 0.1987, 0.2038, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1963508129119873 tensor([0.1964, 0.2015, 0.1981, 0.2063, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19549420475959778 tensor([0.1958, 0.2040, 0.1977, 0.2071, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19554686546325684 tensor([0.1955, 0.2027, 0.1985, 0.2050, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19471099972724915 tensor([0.1974, 0.2038, 0.1947, 0.2064, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1945362240076065 tensor([0.1945, 0.2056, 0.2003, 0.2039, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19457828998565674 tensor([0.1946, 0.2033, 0.1987, 0.2057, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1942785531282425 tensor([0.1983, 0.2035, 0.1943, 0.2072, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.193623349070549 tensor([0.1982, 0.2003, 0.1936, 0.2075, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19230104982852936 tensor([0.2009, 0.2070, 0.1923, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1955586075782776 tensor([0.1956, 0.2046, 0.1986, 0.2050, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19068071246147156 tensor([0.1907, 0.2042, 0.2065, 0.2018, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19470718502998352 tensor([0.1962, 0.2023, 0.1947, 0.2096, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19632868468761444 tensor([0.1965, 0.2021, 0.1963, 0.2056, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19608472287654877 tensor([0.1974, 0.2010, 0.1961, 0.2078, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19449642300605774 tensor([0.1947, 0.2087, 0.1992, 0.2029, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1937352567911148 tensor([0.1937, 0.2028, 0.2006, 0.2049, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19552338123321533 tensor([0.1955, 0.2034, 0.1976, 0.2075, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19351443648338318 tensor([0.1983, 0.2012, 0.1935, 0.2072, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19509157538414001 tensor([0.1988, 0.2050, 0.1951, 0.2045, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19578050076961517 tensor([0.1958, 0.2056, 0.1986, 0.2036, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19615769386291504 tensor([0.1962, 0.2010, 0.1965, 0.2071, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19464214146137238 tensor([0.1946, 0.2030, 0.1975, 0.2070, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19360744953155518 tensor([0.1985, 0.2009, 0.1936, 0.2066, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1959311068058014 tensor([0.1976, 0.2029, 0.1959, 0.2068, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19610951840877533 tensor([0.1962, 0.2038, 0.1961, 0.2076, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18869371712207794 tensor([0.1887, 0.2014, 0.2083, 0.2021, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19583384692668915 tensor([0.1966, 0.2022, 0.1974, 0.2080, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19582077860832214 tensor([0.1958, 0.2011, 0.1967, 0.2062, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19507357478141785 tensor([0.1990, 0.2068, 0.1951, 0.2040, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19515694677829742 tensor([0.1967, 0.2062, 0.1975, 0.2044, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19625189900398254 tensor([0.1963, 0.2029, 0.1982, 0.2055, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19585955142974854 tensor([0.1959, 0.2034, 0.1974, 0.2060, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19444893300533295 tensor([0.1983, 0.2015, 0.1944, 0.2074, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19610217213630676 tensor([0.1970, 0.2028, 0.1961, 0.2058, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19481918215751648 tensor([0.1948, 0.2058, 0.1980, 0.2048, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19490304589271545 tensor([0.1949, 0.2019, 0.1974, 0.2069, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19578954577445984 tensor([0.1964, 0.2022, 0.1958, 0.2076, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19582049548625946 tensor([0.1958, 0.1995, 0.1958, 0.2062, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1914220005273819 tensor([0.2019, 0.2049, 0.1914, 0.2061, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1939985305070877 tensor([0.1940, 0.2043, 0.1987, 0.2049, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19292758405208588 tensor([0.1929, 0.2048, 0.2017, 0.2041, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19574707746505737 tensor([0.1960, 0.2043, 0.1978, 0.2062, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19555550813674927 tensor([0.1956, 0.2033, 0.1981, 0.2048, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19645635783672333 tensor([0.1974, 0.2011, 0.1965, 0.2085, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19658902287483215 tensor([0.1966, 0.2027, 0.1974, 0.2061, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1895153969526291 tensor([0.1895, 0.2040, 0.2067, 0.2013, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19437380135059357 tensor([0.1944, 0.2043, 0.1986, 0.2054, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19529442489147186 tensor([0.1953, 0.2003, 0.1969, 0.2057, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19400520622730255 tensor([0.1996, 0.2040, 0.1940, 0.2062, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19573092460632324 tensor([0.1964, 0.2046, 0.1970, 0.2063, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1935817301273346 tensor([0.1936, 0.2032, 0.2002, 0.2047, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19459228217601776 tensor([0.1946, 0.2045, 0.1980, 0.2065, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19630509614944458 tensor([0.1963, 0.2017, 0.1984, 0.2065, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19491179287433624 tensor([0.1991, 0.2013, 0.1949, 0.2084, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19414274394512177 tensor([0.1941, 0.2015, 0.1993, 0.2064, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19472436606884003 tensor([0.1947, 0.2000, 0.2004, 0.2060, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19509157538414001 tensor([0.1951, 0.2039, 0.1980, 0.2055, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1961350440979004 tensor([0.1967, 0.2014, 0.1961, 0.2069, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19537247717380524 tensor([0.1954, 0.2041, 0.1984, 0.2058, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19535057246685028 tensor([0.1954, 0.2074, 0.1971, 0.2044, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1917722076177597 tensor([0.1918, 0.2029, 0.2032, 0.2038, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19522805511951447 tensor([0.1952, 0.2027, 0.1983, 0.2061, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1953173726797104 tensor([0.1963, 0.1995, 0.1953, 0.2060, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19255635142326355 tensor([0.2015, 0.2049, 0.1926, 0.2062, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1952902376651764 tensor([0.1968, 0.2056, 0.1971, 0.2052, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1928415596485138 tensor([0.1928, 0.2020, 0.1996, 0.2068, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19567011296749115 tensor([0.1964, 0.2051, 0.1971, 0.2058, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948309689760208 tensor([0.1981, 0.2002, 0.1948, 0.2068, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1951369345188141 tensor([0.1992, 0.2041, 0.1951, 0.2055, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19455403089523315 tensor([0.1946, 0.2060, 0.1994, 0.2044, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18973679840564728 tensor([0.1897, 0.2029, 0.2057, 0.2029, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1962517946958542 tensor([0.1963, 0.2026, 0.1968, 0.2073, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1944759339094162 tensor([0.1975, 0.1997, 0.1945, 0.2066, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19644157588481903 tensor([0.1972, 0.2040, 0.1975, 0.2048, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1950160264968872 tensor([0.1950, 0.2053, 0.1977, 0.2057, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19458399713039398 tensor([0.1946, 0.2014, 0.1994, 0.2061, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.195033460855484 tensor([0.1979, 0.2040, 0.1958, 0.2073, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19440925121307373 tensor([0.1973, 0.2014, 0.1944, 0.2067, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19605493545532227 tensor([0.1979, 0.2048, 0.1963, 0.2049, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19494126737117767 tensor([0.1963, 0.2075, 0.1962, 0.2051, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19553697109222412 tensor([0.1955, 0.2011, 0.1975, 0.2079, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1954314410686493 tensor([0.1954, 0.2048, 0.1974, 0.2069, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19567987322807312 tensor([0.1974, 0.1997, 0.1957, 0.2062, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19576063752174377 tensor([0.1958, 0.2053, 0.1983, 0.2037, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19572195410728455 tensor([0.1957, 0.2057, 0.1983, 0.2038, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19079020619392395 tensor([0.1908, 0.2021, 0.2047, 0.2039, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1933891922235489 tensor([0.1977, 0.2017, 0.1934, 0.2103, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19384917616844177 tensor([0.1986, 0.2031, 0.1938, 0.2056, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19426070153713226 tensor([0.1995, 0.2046, 0.1943, 0.2052, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19577178359031677 tensor([0.1964, 0.2058, 0.1974, 0.2046, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1935635358095169 tensor([0.1936, 0.2024, 0.1998, 0.2055, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1952458769083023 tensor([0.1963, 0.2037, 0.1952, 0.2081, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19506029784679413 tensor([0.1977, 0.2020, 0.1951, 0.2072, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19558557868003845 tensor([0.1981, 0.2049, 0.1963, 0.2051, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19563747942447662 tensor([0.1956, 0.2065, 0.1977, 0.2043, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19223745167255402 tensor([0.1922, 0.2047, 0.2015, 0.2039, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19357627630233765 tensor([0.1936, 0.2052, 0.1988, 0.2048, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19634714722633362 tensor([0.1963, 0.2008, 0.1967, 0.2060, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19555611908435822 tensor([0.1970, 0.2038, 0.1956, 0.2066, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1952332854270935 tensor([0.1952, 0.2058, 0.2000, 0.2034, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19460895657539368 tensor([0.1946, 0.2040, 0.2007, 0.2046, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19574816524982452 tensor([0.1957, 0.2035, 0.1974, 0.2070, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19514505565166473 tensor([0.1975, 0.2006, 0.1951, 0.2078, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1922140121459961 tensor([0.2013, 0.2050, 0.1922, 0.2062, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19563673436641693 tensor([0.1956, 0.2049, 0.1978, 0.2050, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1935124695301056 tensor([0.1935, 0.2035, 0.2004, 0.2045, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19384855031967163 tensor([0.1970, 0.2031, 0.1938, 0.2091, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19644273817539215 tensor([0.1966, 0.2024, 0.1964, 0.2062, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "[[0], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [2], [0], [0], [0], [2], [2], [2], [4], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [4], [2], [2], [4], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [4], [2], [2], [0], [0], [4], [0], [2], [0], [0], [2], [0], [2], [4], [0], [0], [0], [2], [4], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [0], [2], [4], [0], [4], [2], [2], [0], [0], [2], [2], [2], [0], [0], [4], [2], [2], [0], [0], [2], [2], [4], [4], [0], [0], [0], [2], [0], [0], [4], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [4], [0], [0], [2], [2], [0], [0], [0], [2], [2], [2], [0], [4], [0], [2], [4], [0], [0], [2], [2], [0], [0], [2], [0], [2], [0], [0], [4], [0], [2], [0], [0], [0], [0], [2], [4], [0], [0], [0], [2], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [4], [0], [4], [2], [2], [0], [0], [0], [2], [4], [0], [0], [4], [2], [4], [4], [0], [0], [2], [0], [0], [0], [2], [2], [2], [4], [0], [2], [2], [4], [0], [0], [0], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2]]\n",
      "[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4]]\n",
      "NL_pred of 0th iteration [[0], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [2], [0], [0], [0], [2], [2], [2], [4], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [4], [2], [2], [4], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [4], [2], [2], [0], [0], [4], [0], [2], [0], [0], [2], [0], [2], [4], [0], [0], [0], [2], [4], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [0], [2], [4], [0], [4], [2], [2], [0], [0], [2], [2], [2], [0], [0], [4], [2], [2], [0], [0], [2], [2], [4], [4], [0], [0], [0], [2], [0], [0], [4], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [4], [0], [0], [2], [2], [0], [0], [0], [2], [2], [2], [0], [4], [0], [2], [4], [0], [0], [2], [2], [0], [0], [2], [0], [2], [0], [0], [4], [0], [2], [0], [0], [0], [0], [2], [4], [0], [0], [0], [2], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [4], [0], [4], [2], [2], [0], [0], [0], [2], [4], [0], [0], [4], [2], [4], [4], [0], [0], [2], [0], [0], [0], [2], [2], [2], [4], [0], [2], [2], [4], [0], [0], [0], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.00512600040435791  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005125943660736084  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.00512583589553833  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005125681877136231  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005125486373901368  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005125253200531006  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.00512498664855957  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005124691009521484  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005124367237091064  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005124019622802734  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005123650550842285  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005123262405395508  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005122855186462403  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005122432231903076  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005121995449066162  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005121546268463135  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00512108564376831  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005120614528656006  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005120134353637696  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005119646549224853  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.19553181529045105 tensor([0.1945, 0.2034, 0.1955, 0.2085, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19726799428462982 tensor([0.1926, 0.2042, 0.1973, 0.2077, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19879817962646484 tensor([0.1898, 0.2040, 0.2019, 0.2054, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19420483708381653 tensor([0.1942, 0.2053, 0.1944, 0.2094, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19503121078014374 tensor([0.1950, 0.1999, 0.1934, 0.2101, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19593046605587006 tensor([0.1983, 0.2061, 0.1919, 0.2078, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1955665946006775 tensor([0.1925, 0.2099, 0.1977, 0.2044, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19650909304618835 tensor([0.1924, 0.2060, 0.1991, 0.2060, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19570113718509674 tensor([0.1957, 0.2054, 0.1935, 0.2088, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19600200653076172 tensor([0.1960, 0.2014, 0.1932, 0.2089, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1963677555322647 tensor([0.1964, 0.2046, 0.1939, 0.2077, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1965256929397583 tensor([0.1927, 0.2073, 0.1970, 0.2065, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976674199104309 tensor([0.1920, 0.2046, 0.1977, 0.2069, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19546271860599518 tensor([0.1955, 0.2065, 0.1935, 0.2081, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19568316638469696 tensor([0.1929, 0.2018, 0.1957, 0.2068, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19661512970924377 tensor([0.1981, 0.2048, 0.1920, 0.2085, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19768130779266357 tensor([0.1935, 0.2040, 0.1977, 0.2064, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982172280550003 tensor([0.1912, 0.2044, 0.1988, 0.2074, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.197549507021904 tensor([0.1925, 0.2049, 0.1985, 0.2065, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19558262825012207 tensor([0.1956, 0.2024, 0.1930, 0.2081, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19531647861003876 tensor([0.1983, 0.2075, 0.1918, 0.2071, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19678276777267456 tensor([0.1927, 0.2079, 0.1968, 0.2057, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19878174364566803 tensor([0.1878, 0.2055, 0.2053, 0.2027, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19676555693149567 tensor([0.1944, 0.2035, 0.1968, 0.2079, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19498103857040405 tensor([0.1950, 0.2018, 0.1945, 0.2076, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19587688148021698 tensor([0.1959, 0.2075, 0.1945, 0.2051, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1968829482793808 tensor([0.1927, 0.2052, 0.1969, 0.2074, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20187854766845703 tensor([0.1881, 0.2027, 0.2024, 0.2050, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19533462822437286 tensor([0.1953, 0.2027, 0.1928, 0.2113, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19585871696472168 tensor([0.1959, 0.2009, 0.1941, 0.2095, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19638009369373322 tensor([0.1983, 0.2055, 0.1921, 0.2077, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19711656868457794 tensor([0.1920, 0.2078, 0.1985, 0.2046, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19772835075855255 tensor([0.1918, 0.2027, 0.1977, 0.2086, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978275179862976 tensor([0.1919, 0.2038, 0.1978, 0.2083, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1953428089618683 tensor([0.1953, 0.2033, 0.1931, 0.2079, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19554294645786285 tensor([0.1955, 0.2059, 0.1943, 0.2066, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19653503596782684 tensor([0.1936, 0.2063, 0.1965, 0.2069, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19719715416431427 tensor([0.1922, 0.2058, 0.1972, 0.2064, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19475029408931732 tensor([0.1948, 0.2060, 0.1939, 0.2087, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971873790025711 tensor([0.1925, 0.2048, 0.1972, 0.2053, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1955670565366745 tensor([0.1992, 0.2068, 0.1909, 0.2075, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1985664814710617 tensor([0.1917, 0.2047, 0.1986, 0.2060, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19882266223430634 tensor([0.1898, 0.2038, 0.2021, 0.2055, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19612190127372742 tensor([0.1934, 0.2045, 0.1961, 0.2081, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1956699788570404 tensor([0.1957, 0.1998, 0.1929, 0.2102, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19574426114559174 tensor([0.1957, 0.2049, 0.1936, 0.2084, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19593758881092072 tensor([0.1930, 0.2079, 0.1983, 0.2049, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1966845840215683 tensor([0.1912, 0.2063, 0.2014, 0.2044, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19640658795833588 tensor([0.1931, 0.2044, 0.1970, 0.2092, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19567526876926422 tensor([0.1957, 0.2018, 0.1930, 0.2093, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19695225358009338 tensor([0.1940, 0.2042, 0.1970, 0.2077, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19585992395877838 tensor([0.1932, 0.2082, 0.1959, 0.2062, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19742846488952637 tensor([0.1923, 0.2059, 0.1980, 0.2064, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19506655633449554 tensor([0.1951, 0.2035, 0.1951, 0.2084, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1954246610403061 tensor([0.1954, 0.2008, 0.1934, 0.2094, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972610503435135 tensor([0.1981, 0.2056, 0.1915, 0.2075, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19308465719223022 tensor([0.1931, 0.2088, 0.1963, 0.2061, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19878165423870087 tensor([0.1894, 0.2062, 0.2029, 0.2028, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19540487229824066 tensor([0.1954, 0.2029, 0.1950, 0.2095, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19433695077896118 tensor([0.1943, 0.2013, 0.1936, 0.2090, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1963185966014862 tensor([0.1990, 0.2068, 0.1913, 0.2066, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19657708704471588 tensor([0.1941, 0.2053, 0.1966, 0.2065, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19891764223575592 tensor([0.1893, 0.2052, 0.2012, 0.2053, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1951521635055542 tensor([0.1937, 0.2031, 0.1952, 0.2088, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1944435089826584 tensor([0.1944, 0.2021, 0.1947, 0.2085, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19627812504768372 tensor([0.1971, 0.2067, 0.1931, 0.2068, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19741612672805786 tensor([0.1930, 0.2042, 0.1974, 0.2078, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19855889678001404 tensor([0.1893, 0.2043, 0.2024, 0.2054, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19418269395828247 tensor([0.1942, 0.2061, 0.1982, 0.2050, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19377459585666656 tensor([0.1938, 0.2013, 0.1942, 0.2086, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964251697063446 tensor([0.1978, 0.2074, 0.1922, 0.2062, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19422638416290283 tensor([0.1942, 0.2085, 0.1948, 0.2064, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.200057715177536 tensor([0.1903, 0.2027, 0.2012, 0.2059, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19438011944293976 tensor([0.1944, 0.2036, 0.1945, 0.2097, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19473758339881897 tensor([0.1947, 0.2031, 0.1943, 0.2080, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19739337265491486 tensor([0.1978, 0.2036, 0.1916, 0.2097, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19684168696403503 tensor([0.1927, 0.2060, 0.1968, 0.2070, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19721169769763947 tensor([0.1918, 0.2043, 0.2002, 0.2065, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19518575072288513 tensor([0.1941, 0.2031, 0.1952, 0.2106, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19465692341327667 tensor([0.1947, 0.2030, 0.1948, 0.2089, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19672562181949615 tensor([0.1967, 0.2071, 0.1932, 0.2059, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19576704502105713 tensor([0.1925, 0.2087, 0.1991, 0.2041, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19728775322437286 tensor([0.1924, 0.2040, 0.1973, 0.2076, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19514739513397217 tensor([0.1951, 0.2033, 0.1970, 0.2077, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1971251666545868 tensor([0.1971, 0.2044, 0.1925, 0.2075, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19670289754867554 tensor([0.1967, 0.2046, 0.1939, 0.2081, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19567634165287018 tensor([0.1935, 0.2056, 0.1957, 0.2087, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974637359380722 tensor([0.1906, 0.2065, 0.2002, 0.2051, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1946057677268982 tensor([0.1946, 0.2058, 0.1958, 0.2072, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19519278407096863 tensor([0.1942, 0.2043, 0.1952, 0.2070, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1953142285346985 tensor([0.1953, 0.2052, 0.1947, 0.2074, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19760939478874207 tensor([0.1936, 0.2054, 0.1976, 0.2055, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19948291778564453 tensor([0.1896, 0.2040, 0.2031, 0.2038, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1940799504518509 tensor([0.1941, 0.2050, 0.1945, 0.2091, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19538934528827667 tensor([0.1943, 0.2034, 0.1954, 0.2067, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19643351435661316 tensor([0.1974, 0.2064, 0.1923, 0.2075, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1936171054840088 tensor([0.1936, 0.2082, 0.1977, 0.2054, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19743604958057404 tensor([0.1923, 0.2060, 0.1974, 0.2061, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19508309662342072 tensor([0.1941, 0.2061, 0.1951, 0.2073, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19636164605617523 tensor([0.1928, 0.2009, 0.1964, 0.2099, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1961619108915329 tensor([0.1962, 0.2048, 0.1932, 0.2084, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19496600329875946 tensor([0.1950, 0.2063, 0.1961, 0.2072, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19764582812786102 tensor([0.1915, 0.2051, 0.1997, 0.2061, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19524964690208435 tensor([0.1928, 0.2018, 0.1952, 0.2105, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1936243772506714 tensor([0.1936, 0.2005, 0.1942, 0.2096, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19616566598415375 tensor([0.1968, 0.2072, 0.1934, 0.2064, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19639849662780762 tensor([0.1938, 0.2063, 0.1964, 0.2064, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19910520315170288 tensor([0.1909, 0.2030, 0.1991, 0.2077, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19602060317993164 tensor([0.1960, 0.2048, 0.1943, 0.2082, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1954258382320404 tensor([0.1954, 0.2015, 0.1945, 0.2088, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19650930166244507 tensor([0.1965, 0.2064, 0.1937, 0.2062, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19505849480628967 tensor([0.1939, 0.2062, 0.1951, 0.2079, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1970490664243698 tensor([0.1926, 0.2045, 0.1970, 0.2072, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19510267674922943 tensor([0.1935, 0.2035, 0.1951, 0.2092, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19596143066883087 tensor([0.1944, 0.2040, 0.1960, 0.2061, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19666790962219238 tensor([0.1986, 0.2061, 0.1922, 0.2064, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19340106844902039 tensor([0.1934, 0.2094, 0.1957, 0.2059, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1990514099597931 tensor([0.1908, 0.2050, 0.2000, 0.2052, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19491536915302277 tensor([0.1949, 0.2073, 0.1956, 0.2068, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19442319869995117 tensor([0.1944, 0.2014, 0.1934, 0.2088, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19575566053390503 tensor([0.1958, 0.2041, 0.1936, 0.2097, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19683152437210083 tensor([0.1915, 0.2081, 0.1976, 0.2060, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1991044580936432 tensor([0.1881, 0.2040, 0.2047, 0.2040, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19461040198802948 tensor([0.1946, 0.2039, 0.1948, 0.2092, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19445116817951202 tensor([0.1945, 0.2011, 0.1934, 0.2080, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19627539813518524 tensor([0.1994, 0.2069, 0.1908, 0.2067, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19699013233184814 tensor([0.1927, 0.2056, 0.1976, 0.2070, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19825893640518188 tensor([0.1907, 0.2041, 0.2011, 0.2058, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19444090127944946 tensor([0.1944, 0.2051, 0.1948, 0.2099, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19396468997001648 tensor([0.1940, 0.2008, 0.1945, 0.2093, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19664661586284637 tensor([0.1967, 0.2075, 0.1926, 0.2065, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19682005047798157 tensor([0.1932, 0.2070, 0.1977, 0.2053, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19915896654129028 tensor([0.1888, 0.2038, 0.2030, 0.2052, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19507132470607758 tensor([0.1951, 0.2052, 0.1936, 0.2084, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19550831615924835 tensor([0.1955, 0.2030, 0.1933, 0.2096, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19481408596038818 tensor([0.1968, 0.2050, 0.1948, 0.2083, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19452735781669617 tensor([0.1945, 0.2074, 0.1953, 0.2067, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.199355810880661 tensor([0.1895, 0.2044, 0.2027, 0.2040, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19745351374149323 tensor([0.1914, 0.2065, 0.1983, 0.2063, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948036551475525 tensor([0.1940, 0.2012, 0.1948, 0.2104, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1963273584842682 tensor([0.1999, 0.2058, 0.1907, 0.2072, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19726331532001495 tensor([0.1926, 0.2075, 0.1973, 0.2053, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19668947160243988 tensor([0.1942, 0.2029, 0.1967, 0.2077, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1936534196138382 tensor([0.1937, 0.2054, 0.1963, 0.2085, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1970902979373932 tensor([0.1935, 0.2041, 0.1971, 0.2064, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19526508450508118 tensor([0.1953, 0.2052, 0.1933, 0.2079, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962936669588089 tensor([0.1924, 0.2070, 0.1989, 0.2053, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1973380297422409 tensor([0.1925, 0.2047, 0.1973, 0.2071, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19614993035793304 tensor([0.1961, 0.2048, 0.1929, 0.2087, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1960502415895462 tensor([0.1961, 0.2016, 0.1922, 0.2090, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19573411345481873 tensor([0.1988, 0.2084, 0.1909, 0.2062, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967957764863968 tensor([0.1935, 0.2060, 0.1972, 0.2065, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1974387913942337 tensor([0.1886, 0.2056, 0.2051, 0.2033, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19406500458717346 tensor([0.1941, 0.2037, 0.1933, 0.2111, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1944502890110016 tensor([0.1945, 0.2035, 0.1949, 0.2071, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19532272219657898 tensor([0.1953, 0.2023, 0.1947, 0.2092, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19262202084064484 tensor([0.1926, 0.2101, 0.1978, 0.2044, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1985296607017517 tensor([0.1917, 0.2042, 0.1993, 0.2064, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1962236762046814 tensor([0.1934, 0.2048, 0.1962, 0.2090, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19621139764785767 tensor([0.1962, 0.2026, 0.1921, 0.2087, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19666026532649994 tensor([0.1967, 0.2063, 0.1937, 0.2060, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19715797901153564 tensor([0.1937, 0.2070, 0.1972, 0.2050, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19509044289588928 tensor([0.1941, 0.2023, 0.1951, 0.2086, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19614675641059875 tensor([0.1925, 0.2044, 0.1961, 0.2085, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19640180468559265 tensor([0.1964, 0.2023, 0.1922, 0.2081, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1955006718635559 tensor([0.1955, 0.2043, 0.1945, 0.2082, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19413302838802338 tensor([0.1941, 0.2052, 0.1947, 0.2091, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20008960366249084 tensor([0.1867, 0.2028, 0.2069, 0.2036, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19445328414440155 tensor([0.1945, 0.2036, 0.1960, 0.2095, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1953488290309906 tensor([0.1937, 0.2025, 0.1953, 0.2077, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1957312822341919 tensor([0.1969, 0.2082, 0.1937, 0.2054, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1946154236793518 tensor([0.1946, 0.2076, 0.1961, 0.2059, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19679932296276093 tensor([0.1941, 0.2042, 0.1968, 0.2070, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1960321068763733 tensor([0.1938, 0.2048, 0.1960, 0.2075, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19618146121501923 tensor([0.1962, 0.2028, 0.1931, 0.2089, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19485154747962952 tensor([0.1949, 0.2042, 0.1947, 0.2073, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1965792328119278 tensor([0.1927, 0.2072, 0.1966, 0.2063, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1960301548242569 tensor([0.1928, 0.2033, 0.1960, 0.2084, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19432306289672852 tensor([0.1943, 0.2036, 0.1944, 0.2091, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19445198774337769 tensor([0.1937, 0.2009, 0.1945, 0.2077, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19630658626556396 tensor([0.1998, 0.2063, 0.1900, 0.2075, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19730766117572784 tensor([0.1919, 0.2057, 0.1973, 0.2064, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1970965713262558 tensor([0.1909, 0.2062, 0.2003, 0.2056, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19386053085327148 tensor([0.1939, 0.2057, 0.1964, 0.2077, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19668470323085785 tensor([0.1935, 0.2047, 0.1967, 0.2063, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1952488124370575 tensor([0.1952, 0.2025, 0.1951, 0.2100, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1960420161485672 tensor([0.1945, 0.2041, 0.1960, 0.2075, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1991473287343979 tensor([0.1875, 0.2054, 0.2053, 0.2027, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19715499877929688 tensor([0.1923, 0.2057, 0.1972, 0.2069, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19549870491027832 tensor([0.1932, 0.2017, 0.1955, 0.2072, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19678640365600586 tensor([0.1975, 0.2055, 0.1926, 0.2077, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1942603588104248 tensor([0.1943, 0.2060, 0.1956, 0.2078, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1987779289484024 tensor([0.1915, 0.2046, 0.1988, 0.2062, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19665974378585815 tensor([0.1925, 0.2059, 0.1967, 0.2080, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19700488448143005 tensor([0.1942, 0.2031, 0.1970, 0.2079, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1968143880367279 tensor([0.1970, 0.2027, 0.1935, 0.2099, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19793039560317993 tensor([0.1921, 0.2029, 0.1979, 0.2078, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19898894429206848 tensor([0.1926, 0.2013, 0.1990, 0.2075, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19658207893371582 tensor([0.1930, 0.2053, 0.1966, 0.2069, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19463196396827698 tensor([0.1946, 0.2027, 0.1947, 0.2084, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19695965945720673 tensor([0.1933, 0.2055, 0.1970, 0.2073, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1957060545682907 tensor([0.1932, 0.2088, 0.1957, 0.2059, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19897355139255524 tensor([0.1897, 0.2042, 0.2018, 0.2053, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19689719378948212 tensor([0.1931, 0.2040, 0.1969, 0.2076, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19421908259391785 tensor([0.1942, 0.2009, 0.1939, 0.2075, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19556118547916412 tensor([0.1993, 0.2063, 0.1912, 0.2076, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19467376172542572 tensor([0.1947, 0.2070, 0.1957, 0.2067, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19817686080932617 tensor([0.1908, 0.2033, 0.1982, 0.2082, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.194266214966774 tensor([0.1943, 0.2065, 0.1957, 0.2072, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1959448754787445 tensor([0.1959, 0.2016, 0.1934, 0.2083, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19679832458496094 tensor([0.1970, 0.2055, 0.1937, 0.2069, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19622579216957092 tensor([0.1925, 0.2074, 0.1980, 0.2059, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1994197964668274 tensor([0.1877, 0.2043, 0.2043, 0.2043, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19542208313941956 tensor([0.1941, 0.2040, 0.1954, 0.2088, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19542545080184937 tensor([0.1954, 0.2010, 0.1931, 0.2080, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1950959861278534 tensor([0.1951, 0.2054, 0.1962, 0.2063, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19629999995231628 tensor([0.1929, 0.2067, 0.1963, 0.2071, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19800245761871338 tensor([0.1925, 0.2028, 0.1980, 0.2076, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19442328810691833 tensor([0.1958, 0.2054, 0.1944, 0.2088, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19520969688892365 tensor([0.1952, 0.2028, 0.1930, 0.2082, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19494013488292694 tensor([0.1958, 0.2062, 0.1949, 0.2064, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19419187307357788 tensor([0.1942, 0.2089, 0.1948, 0.2065, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19610683619976044 tensor([0.1934, 0.2024, 0.1961, 0.2094, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1960187554359436 tensor([0.1933, 0.2062, 0.1960, 0.2083, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19530156254768372 tensor([0.1953, 0.2010, 0.1943, 0.2076, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969248503446579 tensor([0.1937, 0.2067, 0.1969, 0.2052, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969492882490158 tensor([0.1936, 0.2071, 0.1969, 0.2053, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19924908876419067 tensor([0.1887, 0.2034, 0.2032, 0.2053, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19559453427791595 tensor([0.1956, 0.2031, 0.1920, 0.2118, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19652608036994934 tensor([0.1965, 0.2045, 0.1925, 0.2071, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19711127877235413 tensor([0.1973, 0.2060, 0.1929, 0.2067, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19426372647285461 tensor([0.1943, 0.2072, 0.1961, 0.2061, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1984536051750183 tensor([0.1915, 0.2038, 0.1985, 0.2070, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1941271275281906 tensor([0.1941, 0.2051, 0.1939, 0.2096, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19561433792114258 tensor([0.1956, 0.2034, 0.1937, 0.2086, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948874145746231 tensor([0.1960, 0.2063, 0.1949, 0.2066, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19632209837436676 tensor([0.1935, 0.2079, 0.1963, 0.2057, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19835183024406433 tensor([0.1902, 0.2060, 0.2001, 0.2053, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1973896324634552 tensor([0.1915, 0.2066, 0.1974, 0.2063, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19526459276676178 tensor([0.1942, 0.2021, 0.1953, 0.2075, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19491390883922577 tensor([0.1949, 0.2052, 0.1942, 0.2081, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19616177678108215 tensor([0.1931, 0.2072, 0.1986, 0.2049, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967739760875702 tensor([0.1925, 0.2054, 0.1993, 0.2060, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19599474966526031 tensor([0.1937, 0.2049, 0.1960, 0.2085, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19535577297210693 tensor([0.1954, 0.2020, 0.1938, 0.2093, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19596868753433228 tensor([0.1992, 0.2064, 0.1908, 0.2076, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1963571012020111 tensor([0.1935, 0.2063, 0.1964, 0.2064, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987699568271637 tensor([0.1914, 0.2049, 0.1990, 0.2059, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19483733177185059 tensor([0.1948, 0.2045, 0.1925, 0.2106, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19447769224643707 tensor([0.1945, 0.2038, 0.1950, 0.2077, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 2], [0, 2], [0, 4], [2, 0], [2, 0], [2, 4], [0, 4], [0, 4], [2, 0], [2, 0], [2, 0], [0, 4], [0, 2], [2, 0], [0, 2], [2, 4], [0, 2], [0, 4], [0, 4], [2, 0], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 0], [0, 2], [0], [2, 0], [2, 0], [2, 4], [0, 4], [0, 2], [0, 2], [2, 0], [2, 0], [0, 2], [0, 2], [2, 0], [0, 2], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 0], [0, 4], [0, 4], [0, 4], [2, 0], [0, 2], [0, 2], [0, 4], [2, 0], [2, 0], [2, 4], [4, 0], [0, 4], [2, 0], [2, 0], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 4], [0, 2], [0, 4], [4, 0], [2, 0], [2, 4], [4, 0], [0], [2, 0], [2, 0], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 0], [0, 4], [0, 2], [4, 0], [2, 0], [2, 0], [0, 2], [0, 4], [4, 0], [0, 2], [2, 0], [0, 2], [0, 4], [2, 0], [0, 2], [2, 4], [4, 0], [0, 2], [0, 2], [0, 2], [2, 0], [4, 0], [0, 4], [0, 2], [2, 0], [2, 4], [0, 2], [0, 2], [2, 0], [2, 0], [2, 0], [0, 2], [0, 2], [0, 2], [0, 2], [2, 4], [4, 0], [0, 4], [4, 0], [2, 0], [2, 0], [0, 4], [0, 4], [2, 0], [2, 0], [2, 4], [0, 4], [0, 4], [4, 0], [2, 0], [2, 4], [0, 4], [0, 4], [2, 0], [2, 0], [4, 2], [4, 0], [0, 4], [0, 4], [0, 2], [2, 4], [0, 2], [0, 2], [4, 0], [0, 2], [2, 0], [0, 4], [0, 2], [2, 0], [2, 0], [2, 4], [0, 4], [0, 4], [2, 0], [2, 0], [2, 0], [4, 0], [0, 4], [0, 2], [2, 0], [2, 0], [0, 4], [0, 2], [0, 2], [2, 0], [2, 0], [2, 0], [0], [4, 0], [0, 2], [2, 4], [4, 0], [0, 2], [0, 2], [2, 0], [2, 0], [0, 2], [0, 2], [2, 0], [0, 2], [2, 4], [0, 2], [0, 4], [4, 0], [0, 2], [2, 0], [0, 2], [0, 4], [0, 2], [0, 2], [2, 4], [4, 0], [0, 2], [0, 2], [0, 2], [2, 4], [0, 2], [0, 2], [0, 2], [2, 0], [0, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 4], [4, 0], [0, 2], [4, 0], [2, 0], [2, 4], [0, 4], [0, 4], [0, 2], [2, 0], [4, 0], [0, 2], [0, 2], [4, 2], [2, 0], [4, 2], [4, 0], [0, 2], [0, 2], [2, 0], [0, 2], [0, 2], [0, 4], [2, 0], [2, 0], [2, 4], [4, 0], [0, 2], [2, 0], [2, 0], [4, 2], [0, 2], [0, 4], [0, 2], [0, 2], [2, 0], [0, 4], [0, 4], [0, 2], [2, 0], [2, 4], [0, 2], [0, 4], [2, 0], [2, 0]]\n",
      "[[1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3, 4], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 3], [1, 2, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3], [1, 3, 4], [1, 3, 4]]\n",
      "NL_pred of 1th iteration [[0, 2], [0, 2], [0, 4], [2, 0], [2, 0], [2, 4], [0, 4], [0, 4], [2, 0], [2, 0], [2, 0], [0, 4], [0, 2], [2, 0], [0, 2], [2, 4], [0, 2], [0, 4], [0, 4], [2, 0], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 0], [0, 2], [2, 0], [2, 0], [2, 4], [0, 4], [0, 2], [0, 2], [2, 0], [2, 0], [0, 2], [0, 2], [2, 0], [0, 2], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 0], [0, 4], [0, 4], [0, 4], [2, 0], [0, 2], [0, 2], [0, 4], [2, 0], [2, 0], [2, 4], [4, 0], [0, 4], [2, 0], [2, 0], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 4], [0, 2], [0, 4], [4, 0], [2, 0], [2, 4], [4, 0], [2, 0], [2, 0], [2, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 0], [0, 4], [0, 2], [4, 0], [2, 0], [2, 0], [0, 2], [0, 4], [4, 0], [0, 2], [2, 0], [0, 2], [0, 4], [2, 0], [0, 2], [2, 4], [4, 0], [0, 2], [0, 2], [0, 2], [2, 0], [4, 0], [0, 4], [0, 2], [2, 0], [2, 4], [0, 2], [0, 2], [2, 0], [2, 0], [2, 0], [0, 2], [0, 2], [0, 2], [0, 2], [2, 4], [4, 0], [0, 4], [4, 0], [2, 0], [2, 0], [0, 4], [0, 4], [2, 0], [2, 0], [2, 4], [0, 4], [0, 4], [4, 0], [2, 0], [2, 4], [0, 4], [0, 4], [2, 0], [2, 0], [4, 2], [4, 0], [0, 4], [0, 4], [0, 2], [2, 4], [0, 2], [0, 2], [4, 0], [0, 2], [2, 0], [0, 4], [0, 2], [2, 0], [2, 0], [2, 4], [0, 4], [0, 4], [2, 0], [2, 0], [2, 0], [4, 0], [0, 4], [0, 2], [2, 0], [2, 0], [0, 4], [0, 2], [0, 2], [2, 0], [2, 0], [2, 0], [4, 0], [0, 2], [2, 4], [4, 0], [0, 2], [0, 2], [2, 0], [2, 0], [0, 2], [0, 2], [2, 0], [0, 2], [2, 4], [0, 2], [0, 4], [4, 0], [0, 2], [2, 0], [0, 2], [0, 4], [0, 2], [0, 2], [2, 4], [4, 0], [0, 2], [0, 2], [0, 2], [2, 4], [0, 2], [0, 2], [0, 2], [2, 0], [0, 4], [0, 2], [0, 4], [0, 2], [2, 0], [2, 4], [4, 0], [0, 2], [4, 0], [2, 0], [2, 4], [0, 4], [0, 4], [0, 2], [2, 0], [4, 0], [0, 2], [0, 2], [4, 2], [2, 0], [4, 2], [4, 0], [0, 2], [0, 2], [2, 0], [0, 2], [0, 2], [0, 4], [2, 0], [2, 0], [2, 4], [4, 0], [0, 2], [2, 0], [2, 0], [4, 2], [0, 2], [0, 4], [0, 2], [0, 2], [2, 0], [0, 4], [0, 4], [0, 2], [2, 0], [2, 4], [0, 2], [0, 4], [2, 0], [2, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005195349334222585  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005195310241297672  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005195237846992277  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005195134564449913  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005195002806814094  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005194844504599629  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005194664484093546  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0051944651584393585  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005194246045008362  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0051940114874588815  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005193762451048322  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005193497970519278  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005193222389530074  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005192936190709411  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005192640339314696  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005192335317974631  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005192022574575324  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005191703557002882  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005191376817371199  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005191045734081191  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.1972084492444992 tensor([0.1934, 0.2049, 0.1944, 0.2101, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1973915696144104 tensor([0.1915, 0.2057, 0.1962, 0.2093, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20082156360149384 tensor([0.1887, 0.2055, 0.2008, 0.2069, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19586284458637238 tensor([0.1930, 0.2068, 0.1934, 0.2109, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20080848038196564 tensor([0.1938, 0.2014, 0.1923, 0.2116, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1970839947462082 tensor([0.1971, 0.2076, 0.1908, 0.2093, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19662292301654816 tensor([0.1913, 0.2114, 0.1966, 0.2059, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19801554083824158 tensor([0.1912, 0.2075, 0.1980, 0.2075, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19589366018772125 tensor([0.1945, 0.2069, 0.1924, 0.2104, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19974292814731598 tensor([0.1948, 0.2029, 0.1921, 0.2105, 0.1997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19673778116703033 tensor([0.1952, 0.2061, 0.1928, 0.2092, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19590309262275696 tensor([0.1915, 0.2088, 0.1959, 0.2080, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19806808233261108 tensor([0.1909, 0.2061, 0.1966, 0.2084, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19573047757148743 tensor([0.1943, 0.2079, 0.1924, 0.2097, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2020796686410904 tensor([0.1918, 0.2032, 0.1946, 0.2083, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19687175750732422 tensor([0.1969, 0.2063, 0.1909, 0.2100, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19770538806915283 tensor([0.1923, 0.2055, 0.1966, 0.2079, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19768565893173218 tensor([0.1900, 0.2059, 0.1977, 0.2089, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19737772643566132 tensor([0.1913, 0.2064, 0.1974, 0.2081, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2002258598804474 tensor([0.1944, 0.2038, 0.1920, 0.2096, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19707469642162323 tensor([0.1971, 0.2090, 0.1907, 0.2086, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962515264749527 tensor([0.1915, 0.2094, 0.1957, 0.2072, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20413818955421448 tensor([0.1867, 0.2070, 0.2041, 0.2042, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19664128124713898 tensor([0.1932, 0.2050, 0.1957, 0.2095, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20035724341869354 tensor([0.1938, 0.2033, 0.1935, 0.2091, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962997019290924 tensor([0.1947, 0.2090, 0.1935, 0.2066, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1970929652452469 tensor([0.1915, 0.2067, 0.1958, 0.2089, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20113195478916168 tensor([0.1870, 0.2041, 0.2013, 0.2065, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971580982208252 tensor([0.1941, 0.2042, 0.1917, 0.2128, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19884765148162842 tensor([0.1947, 0.2024, 0.1930, 0.2110, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19709521532058716 tensor([0.1971, 0.2070, 0.1910, 0.2092, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19737215340137482 tensor([0.1908, 0.2094, 0.1974, 0.2061, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19835008680820465 tensor([0.1907, 0.2042, 0.1966, 0.2101, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19740945100784302 tensor([0.1907, 0.2053, 0.1967, 0.2098, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19956403970718384 tensor([0.1942, 0.2048, 0.1920, 0.2094, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1969713270664215 tensor([0.1944, 0.2074, 0.1932, 0.2081, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19600988924503326 tensor([0.1924, 0.2078, 0.1954, 0.2084, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976301521062851 tensor([0.1911, 0.2073, 0.1961, 0.2079, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19596987962722778 tensor([0.1936, 0.2075, 0.1928, 0.2102, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19943071901798248 tensor([0.1914, 0.2063, 0.1961, 0.2068, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19797469675540924 tensor([0.1980, 0.2083, 0.1898, 0.2090, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19828097522258759 tensor([0.1906, 0.2062, 0.1975, 0.2075, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20098593831062317 tensor([0.1886, 0.2053, 0.2010, 0.2070, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19708968698978424 tensor([0.1922, 0.2060, 0.1950, 0.2096, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2006278783082962 tensor([0.1945, 0.2013, 0.1919, 0.2117, 0.2006], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.196613609790802 tensor([0.1945, 0.2064, 0.1926, 0.2099, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19717340171337128 tensor([0.1918, 0.2094, 0.1972, 0.2064, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20029427111148834 tensor([0.1900, 0.2078, 0.2003, 0.2059, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19586887955665588 tensor([0.1919, 0.2058, 0.1959, 0.2107, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19955140352249146 tensor([0.1945, 0.2033, 0.1919, 0.2108, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19635167717933655 tensor([0.1929, 0.2057, 0.1959, 0.2093, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19574616849422455 tensor([0.1921, 0.2097, 0.1948, 0.2077, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19684582948684692 tensor([0.1911, 0.2074, 0.1968, 0.2079, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1972435563802719 tensor([0.1939, 0.2050, 0.1940, 0.2099, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20028766989707947 tensor([0.1942, 0.2022, 0.1924, 0.2109, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1968960464000702 tensor([0.1969, 0.2071, 0.1905, 0.2090, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19520476460456848 tensor([0.1919, 0.2103, 0.1952, 0.2076, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2017427533864975 tensor([0.1883, 0.2077, 0.2017, 0.2043, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19647209346294403 tensor([0.1942, 0.2044, 0.1939, 0.2111, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2010144144296646 tensor([0.1932, 0.2027, 0.1925, 0.2106, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19780108332633972 tensor([0.1978, 0.2084, 0.1902, 0.2081, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19675078988075256 tensor([0.1930, 0.2068, 0.1955, 0.2080, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000841498374939 tensor([0.1882, 0.2067, 0.2001, 0.2068, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19847343862056732 tensor([0.1925, 0.2046, 0.1941, 0.2103, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19944697618484497 tensor([0.1933, 0.2036, 0.1937, 0.2101, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19590574502944946 tensor([0.1959, 0.2082, 0.1920, 0.2083, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19681434333324432 tensor([0.1919, 0.2057, 0.1963, 0.2093, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20129269361495972 tensor([0.1882, 0.2058, 0.2013, 0.2069, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1971384733915329 tensor([0.1930, 0.2076, 0.1971, 0.2065, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2013637125492096 tensor([0.1926, 0.2027, 0.1932, 0.2102, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.196584090590477 tensor([0.1966, 0.2089, 0.1911, 0.2077, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19366176426410675 tensor([0.1930, 0.2100, 0.1937, 0.2079, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19932402670383453 tensor([0.1891, 0.2041, 0.2000, 0.2074, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971224993467331 tensor([0.1932, 0.2050, 0.1934, 0.2113, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19911539554595947 tensor([0.1935, 0.2046, 0.1932, 0.2095, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19654737412929535 tensor([0.1965, 0.2050, 0.1906, 0.2112, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19659066200256348 tensor([0.1916, 0.2075, 0.1957, 0.2086, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19905267655849457 tensor([0.1907, 0.2058, 0.1991, 0.2080, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19620375335216522 tensor([0.1930, 0.2046, 0.1941, 0.2122, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19804294407367706 tensor([0.1935, 0.2045, 0.1937, 0.2104, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.196280837059021 tensor([0.1955, 0.2086, 0.1921, 0.2074, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19795897603034973 tensor([0.1913, 0.2102, 0.1980, 0.2055, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19790171086788177 tensor([0.1913, 0.2055, 0.1962, 0.2092, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19586670398712158 tensor([0.1940, 0.2047, 0.1959, 0.2092, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977889984846115 tensor([0.1959, 0.2058, 0.1914, 0.2090, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1959991455078125 tensor([0.1955, 0.2061, 0.1928, 0.2096, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19579094648361206 tensor([0.1923, 0.2071, 0.1946, 0.2102, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19912268221378326 tensor([0.1895, 0.2080, 0.1991, 0.2067, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19473737478256226 tensor([0.1934, 0.2073, 0.1947, 0.2087, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19849398732185364 tensor([0.1931, 0.2058, 0.1941, 0.2085, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1966283917427063 tensor([0.1941, 0.2067, 0.1936, 0.2089, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19713343679904938 tensor([0.1925, 0.2069, 0.1965, 0.2070, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20199361443519592 tensor([0.1885, 0.2055, 0.2020, 0.2053, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19657357037067413 tensor([0.1929, 0.2065, 0.1934, 0.2106, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19947068393230438 tensor([0.1931, 0.2049, 0.1943, 0.2082, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19621853530406952 tensor([0.1962, 0.2079, 0.1912, 0.2090, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19660095870494843 tensor([0.1924, 0.2097, 0.1966, 0.2069, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19739975035190582 tensor([0.1911, 0.2075, 0.1963, 0.2077, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967150866985321 tensor([0.1929, 0.2076, 0.1940, 0.2088, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19929999113082886 tensor([0.1917, 0.2023, 0.1953, 0.2114, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668766856193542 tensor([0.1950, 0.2063, 0.1921, 0.2100, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19497503340244293 tensor([0.1938, 0.2078, 0.1950, 0.2087, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19854947924613953 tensor([0.1903, 0.2065, 0.1985, 0.2077, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1989966630935669 tensor([0.1916, 0.2033, 0.1942, 0.2120, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20139606297016144 tensor([0.1924, 0.2019, 0.1931, 0.2111, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19563782215118408 tensor([0.1956, 0.2087, 0.1923, 0.2079, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962454617023468 tensor([0.1927, 0.2078, 0.1953, 0.2079, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19854140281677246 tensor([0.1897, 0.2045, 0.1980, 0.2092, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19597631692886353 tensor([0.1948, 0.2063, 0.1932, 0.2097, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19898323714733124 tensor([0.1942, 0.2030, 0.1935, 0.2103, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19646506011486053 tensor([0.1953, 0.2079, 0.1926, 0.2077, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19626057147979736 tensor([0.1927, 0.2077, 0.1940, 0.2094, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1978982537984848 tensor([0.1914, 0.2060, 0.1959, 0.2087, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19800032675266266 tensor([0.1923, 0.2050, 0.1940, 0.2107, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987777203321457 tensor([0.1933, 0.2055, 0.1949, 0.2076, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1974385529756546 tensor([0.1974, 0.2076, 0.1911, 0.2079, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19460724294185638 tensor([0.1922, 0.2109, 0.1946, 0.2074, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1988951563835144 tensor([0.1896, 0.2065, 0.1989, 0.2067, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19447416067123413 tensor([0.1937, 0.2088, 0.1945, 0.2083, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2012440413236618 tensor([0.1932, 0.2028, 0.1924, 0.2103, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19615328311920166 tensor([0.1946, 0.2055, 0.1925, 0.2112, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19653068482875824 tensor([0.1903, 0.2096, 0.1965, 0.2075, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2035963237285614 tensor([0.1870, 0.2055, 0.2036, 0.2055, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19672715663909912 tensor([0.1934, 0.2054, 0.1938, 0.2107, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2022133320569992 tensor([0.1933, 0.2026, 0.1924, 0.2096, 0.2022], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19816158711910248 tensor([0.1982, 0.2083, 0.1897, 0.2082, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19649872183799744 tensor([0.1916, 0.2071, 0.1965, 0.2085, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20003259181976318 tensor([0.1895, 0.2056, 0.2000, 0.2073, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19364887475967407 tensor([0.1932, 0.2066, 0.1936, 0.2114, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20066972076892853 tensor([0.1928, 0.2023, 0.1935, 0.2108, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19553570449352264 tensor([0.1955, 0.2090, 0.1916, 0.2080, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19662663340568542 tensor([0.1920, 0.2085, 0.1966, 0.2068, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20187363028526306 tensor([0.1877, 0.2053, 0.2019, 0.2067, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19702763855457306 tensor([0.1939, 0.2067, 0.1925, 0.2099, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1979130357503891 tensor([0.1943, 0.2044, 0.1922, 0.2111, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.195548877120018 tensor([0.1955, 0.2065, 0.1937, 0.2099, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1942107379436493 tensor([0.1933, 0.2089, 0.1942, 0.2082, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2015964239835739 tensor([0.1884, 0.2059, 0.2016, 0.2055, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19719639420509338 tensor([0.1902, 0.2080, 0.1972, 0.2078, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19889239966869354 tensor([0.1928, 0.2027, 0.1937, 0.2119, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1986536681652069 tensor([0.1987, 0.2073, 0.1897, 0.2087, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19666971266269684 tensor([0.1914, 0.2090, 0.1961, 0.2068, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19771921634674072 tensor([0.1931, 0.2044, 0.1956, 0.2093, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1952204406261444 tensor([0.1925, 0.2069, 0.1952, 0.2101, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19822588562965393 tensor([0.1923, 0.2055, 0.1960, 0.2079, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19756115972995758 tensor([0.1941, 0.2067, 0.1923, 0.2094, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197794571518898 tensor([0.1913, 0.2085, 0.1978, 0.2068, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976466178894043 tensor([0.1913, 0.2062, 0.1962, 0.2087, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19665850698947906 tensor([0.1949, 0.2063, 0.1918, 0.2102, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20039163529872894 tensor([0.1948, 0.2031, 0.1912, 0.2105, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19756987690925598 tensor([0.1976, 0.2099, 0.1899, 0.2077, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19614021480083466 tensor([0.1923, 0.2075, 0.1961, 0.2080, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20391833782196045 tensor([0.1875, 0.2071, 0.2039, 0.2047, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19707795977592468 tensor([0.1929, 0.2052, 0.1922, 0.2127, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19928528368473053 tensor([0.1933, 0.2050, 0.1939, 0.2086, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19764667749404907 tensor([0.1941, 0.2038, 0.1936, 0.2108, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19666102528572083 tensor([0.1915, 0.2116, 0.1967, 0.2059, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1981460303068161 tensor([0.1905, 0.2057, 0.1981, 0.2079, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19584296643733978 tensor([0.1922, 0.2063, 0.1951, 0.2105, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1996319741010666 tensor([0.1950, 0.2041, 0.1911, 0.2102, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19661347568035126 tensor([0.1955, 0.2078, 0.1926, 0.2075, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19607511162757874 tensor([0.1925, 0.2085, 0.1961, 0.2065, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19917254149913788 tensor([0.1929, 0.2038, 0.1940, 0.2101, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976245939731598 tensor([0.1914, 0.2059, 0.1950, 0.2101, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20023491978645325 tensor([0.1952, 0.2038, 0.1912, 0.2096, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967085897922516 tensor([0.1943, 0.2058, 0.1934, 0.2098, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1961621195077896 tensor([0.1929, 0.2066, 0.1936, 0.2106, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19935499131679535 tensor([0.1856, 0.2042, 0.2058, 0.2051, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948903501033783 tensor([0.1933, 0.2051, 0.1949, 0.2110, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20000934600830078 tensor([0.1925, 0.2040, 0.1943, 0.2092, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19572150707244873 tensor([0.1957, 0.2097, 0.1926, 0.2069, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19504737854003906 tensor([0.1934, 0.2091, 0.1950, 0.2074, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19712628424167633 tensor([0.1930, 0.2057, 0.1957, 0.2085, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971990168094635 tensor([0.1926, 0.2063, 0.1949, 0.2090, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982772946357727 tensor([0.1950, 0.2043, 0.1920, 0.2104, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.198220357298851 tensor([0.1937, 0.2057, 0.1936, 0.2088, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19645005464553833 tensor([0.1916, 0.2087, 0.1955, 0.2078, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19866853952407837 tensor([0.1916, 0.2048, 0.1949, 0.2100, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19783587753772736 tensor([0.1931, 0.2051, 0.1933, 0.2106, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20234259963035583 tensor([0.1926, 0.2023, 0.1934, 0.2092, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19854238629341125 tensor([0.1985, 0.2078, 0.1890, 0.2091, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19796505570411682 tensor([0.1908, 0.2072, 0.1962, 0.2079, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19914138317108154 tensor([0.1897, 0.2077, 0.1991, 0.2071, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19526804983615875 tensor([0.1927, 0.2072, 0.1953, 0.2092, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19817550480365753 tensor([0.1923, 0.2062, 0.1956, 0.2078, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964890956878662 tensor([0.1941, 0.2040, 0.1940, 0.2115, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19716714322566986 tensor([0.1933, 0.2055, 0.1950, 0.2090, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2041536420583725 tensor([0.1864, 0.2069, 0.2042, 0.2042, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19728593528270721 tensor([0.1911, 0.2072, 0.1960, 0.2084, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20167647302150726 tensor([0.1920, 0.2031, 0.1944, 0.2087, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1962553709745407 tensor([0.1963, 0.2069, 0.1915, 0.2092, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1945372074842453 tensor([0.1931, 0.2075, 0.1945, 0.2093, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19815179705619812 tensor([0.1904, 0.2061, 0.1977, 0.2077, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19622497260570526 tensor([0.1913, 0.2074, 0.1956, 0.2095, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19699734449386597 tensor([0.1930, 0.2046, 0.1959, 0.2095, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19580630958080292 tensor([0.1958, 0.2042, 0.1925, 0.2115, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19853493571281433 tensor([0.1909, 0.2044, 0.1968, 0.2094, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988074630498886 tensor([0.1915, 0.2028, 0.1979, 0.2090, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19741521775722504 tensor([0.1918, 0.2068, 0.1955, 0.2085, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987374871969223 tensor([0.1934, 0.2042, 0.1937, 0.2099, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19589641690254211 tensor([0.1921, 0.2070, 0.1959, 0.2088, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1956167221069336 tensor([0.1921, 0.2103, 0.1946, 0.2074, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20069430768489838 tensor([0.1886, 0.2057, 0.2007, 0.2068, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19762717187404633 tensor([0.1920, 0.2055, 0.1958, 0.2091, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20236088335514069 tensor([0.1930, 0.2024, 0.1929, 0.2090, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19810758531093597 tensor([0.1981, 0.2078, 0.1901, 0.2091, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19463683664798737 tensor([0.1935, 0.2085, 0.1946, 0.2082, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19875629246234894 tensor([0.1896, 0.2048, 0.1971, 0.2098, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19455738365650177 tensor([0.1931, 0.2080, 0.1946, 0.2087, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20000247657299042 tensor([0.1947, 0.2030, 0.1924, 0.2098, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1958383321762085 tensor([0.1958, 0.2070, 0.1927, 0.2085, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19692683219909668 tensor([0.1913, 0.2089, 0.1969, 0.2074, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20314966142177582 tensor([0.1866, 0.2058, 0.2031, 0.2058, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.196952223777771 tensor([0.1929, 0.2055, 0.1943, 0.2103, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20166324079036713 tensor([0.1942, 0.2025, 0.1921, 0.2095, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19505533576011658 tensor([0.1939, 0.2069, 0.1951, 0.2078, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19620129466056824 tensor([0.1917, 0.2082, 0.1952, 0.2087, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19843831658363342 tensor([0.1913, 0.2042, 0.1969, 0.2091, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19459308683872223 tensor([0.1946, 0.2068, 0.1933, 0.2103, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19999440014362335 tensor([0.1940, 0.2043, 0.1920, 0.2098, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19462120532989502 tensor([0.1946, 0.2077, 0.1939, 0.2079, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19367821514606476 tensor([0.1930, 0.2104, 0.1937, 0.2080, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19793307781219482 tensor([0.1923, 0.2039, 0.1950, 0.2109, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1954190731048584 tensor([0.1921, 0.2077, 0.1949, 0.2099, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20101162791252136 tensor([0.1941, 0.2025, 0.1932, 0.2091, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19678258895874023 tensor([0.1925, 0.2082, 0.1958, 0.2067, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19630418717861176 tensor([0.1925, 0.2086, 0.1959, 0.2068, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20212110877037048 tensor([0.1876, 0.2049, 0.2021, 0.2068, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19672702252864838 tensor([0.1944, 0.2046, 0.1909, 0.2134, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1986742615699768 tensor([0.1953, 0.2060, 0.1914, 0.2086, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19614262878894806 tensor([0.1961, 0.2075, 0.1918, 0.2082, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19496652483940125 tensor([0.1931, 0.2087, 0.1950, 0.2076, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19853973388671875 tensor([0.1903, 0.2053, 0.1973, 0.2085, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.196627676486969 tensor([0.1929, 0.2066, 0.1928, 0.2111, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19797579944133759 tensor([0.1944, 0.2048, 0.1926, 0.2102, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19475840032100677 tensor([0.1948, 0.2078, 0.1938, 0.2081, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1958230435848236 tensor([0.1924, 0.2094, 0.1952, 0.2072, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19896861910820007 tensor([0.1890, 0.2075, 0.1990, 0.2068, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19748450815677643 tensor([0.1903, 0.2081, 0.1963, 0.2078, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20014144480228424 tensor([0.1931, 0.2036, 0.1942, 0.2090, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1968933343887329 tensor([0.1937, 0.2067, 0.1931, 0.2096, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19748668372631073 tensor([0.1920, 0.2087, 0.1975, 0.2064, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19821131229400635 tensor([0.1914, 0.2069, 0.1982, 0.2075, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962379366159439 tensor([0.1925, 0.2064, 0.1949, 0.2100, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1988595575094223 tensor([0.1942, 0.2034, 0.1927, 0.2108, 0.1989], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19794495403766632 tensor([0.1979, 0.2079, 0.1898, 0.2092, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19668614864349365 tensor([0.1924, 0.2078, 0.1953, 0.2080, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1979162096977234 tensor([0.1903, 0.2064, 0.1979, 0.2074, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19684621691703796 tensor([0.1936, 0.2060, 0.1914, 0.2121, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19830726087093353 tensor([0.1933, 0.2052, 0.1940, 0.2092, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [0, 2], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 4, 2], [2, 0], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0], [2, 0, 4], [0, 2, 4], [0], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0], [2, 0, 4], [0, 4, 2], [0, 4], [0, 4, 2], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0], [2, 4, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 0], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [4, 0, 2], [2, 0], [2, 4, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 0], [2, 0, 4], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0], [2, 4, 0], [0, 4, 2], [0, 4], [4, 0, 2], [2, 0], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 4], [0, 4, 2], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0], [2, 0, 4], [2, 0, 4], [0, 4], [4, 0, 2], [0, 2], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2], [2, 4, 0], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [0, 2, 4], [0, 2], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0], [2, 4, 0], [4, 0, 2], [0, 2, 4], [4, 0, 2], [2, 0], [2, 4, 0], [0, 4, 2], [0, 4], [0, 2, 4], [2, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [4, 2, 0], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0], [0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [0, 2], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4]]\n",
      "[[1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3, 4], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3]]\n",
      "NL_pred of 2th iteration [[0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 4, 2], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 4, 0], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [4, 0, 2], [2, 4, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [0, 4, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [4, 0, 2], [2, 4, 0], [0, 4, 2], [2, 0, 4], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4], [4, 0, 2], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [4, 0, 2], [2, 4, 0], [0, 4, 2], [0, 2, 4], [4, 0, 2], [0, 2, 4], [0, 2, 4], [4, 2, 0], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.006115742524464925  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00611565113067627  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.006115477425711496  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.006115229924519857  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.006114917142050607  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.006114543619610014  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.006114116736820766  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0061136404673258465  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.006113122190747942  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.006112564745403472  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.006111973240261986  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00611134994597662  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00611069883619036  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.006110022749219622  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.006109324523380824  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.006108605861663818  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.006107869034721738  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.006107117448534284  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.006106350535438175  Accuracy on Support set:0.0\n",
      "torch.Size([210, 2048]) torch.Size([210])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.006105571701413109  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.2065742015838623 tensor([0.1940, 0.2066, 0.1938, 0.2116, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20738448202610016 tensor([0.1921, 0.2074, 0.1955, 0.2108, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20018944144248962 tensor([0.1893, 0.2072, 0.2002, 0.2085, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20847390592098236 tensor([0.1936, 0.2085, 0.1927, 0.2125, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19758765399456024 tensor([0.1944, 0.2030, 0.1918, 0.2132, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2092389464378357 tensor([0.1977, 0.2092, 0.1902, 0.2108, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20739494264125824 tensor([0.1919, 0.2131, 0.1960, 0.2074, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20900624990463257 tensor([0.1918, 0.2092, 0.1974, 0.2090, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20851203799247742 tensor([0.1951, 0.2085, 0.1918, 0.2119, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20450671017169952 tensor([0.1954, 0.2045, 0.1916, 0.2120, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20771978795528412 tensor([0.1958, 0.2077, 0.1922, 0.2108, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20953324437141418 tensor([0.1921, 0.2104, 0.1953, 0.2095, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20772919058799744 tensor([0.1915, 0.2077, 0.1960, 0.2100, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2096041738986969 tensor([0.1949, 0.2096, 0.1918, 0.2112, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19881118834018707 tensor([0.1924, 0.2049, 0.1940, 0.2099, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2079440951347351 tensor([0.1974, 0.2079, 0.1903, 0.2116, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20718610286712646 tensor([0.1929, 0.2072, 0.1960, 0.2094, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20758603513240814 tensor([0.1906, 0.2076, 0.1970, 0.2105, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20808392763137817 tensor([0.1919, 0.2081, 0.1967, 0.2096, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19699034094810486 tensor([0.1950, 0.2055, 0.1914, 0.2112, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21016448736190796 tensor([0.1976, 0.2106, 0.1901, 0.2102, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20873162150382996 tensor([0.1921, 0.2111, 0.1950, 0.2087, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20350664854049683 tensor([0.1872, 0.2087, 0.2035, 0.2057, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20666897296905518 tensor([0.1938, 0.2067, 0.1950, 0.2110, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971118450164795 tensor([0.1944, 0.2049, 0.1929, 0.2107, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20808348059654236 tensor([0.1953, 0.2106, 0.1929, 0.2081, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20838750898838043 tensor([0.1921, 0.2084, 0.1952, 0.2105, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19789369404315948 tensor([0.1876, 0.2058, 0.2007, 0.2080, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20583346486091614 tensor([0.1947, 0.2058, 0.1911, 0.2144, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040555477142334 tensor([0.1952, 0.2041, 0.1925, 0.2126, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20863722264766693 tensor([0.1977, 0.2086, 0.1904, 0.2108, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2075885534286499 tensor([0.1914, 0.2110, 0.1967, 0.2076, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20588575303554535 tensor([0.1913, 0.2059, 0.1960, 0.2117, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070053070783615 tensor([0.1913, 0.2070, 0.1961, 0.2114, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20645156502723694 tensor([0.1948, 0.2065, 0.1914, 0.2110, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.209025040268898 tensor([0.1949, 0.2090, 0.1926, 0.2097, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20943811535835266 tensor([0.1930, 0.2094, 0.1948, 0.2099, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2089848816394806 tensor([0.1916, 0.2090, 0.1955, 0.2095, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20917202532291412 tensor([0.1941, 0.2092, 0.1922, 0.2118, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20797109603881836 tensor([0.1919, 0.2080, 0.1955, 0.2084, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2100091576576233 tensor([0.1985, 0.2100, 0.1892, 0.2105, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20786398649215698 tensor([0.1911, 0.2079, 0.1969, 0.2090, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20036302506923676 tensor([0.1892, 0.2070, 0.2004, 0.2086, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2076999545097351 tensor([0.1928, 0.2077, 0.1944, 0.2112, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1973753124475479 tensor([0.1951, 0.2029, 0.1913, 0.2133, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20801380276679993 tensor([0.1951, 0.2080, 0.1920, 0.2115, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20794038474559784 tensor([0.1924, 0.2111, 0.1965, 0.2079, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1996583193540573 tensor([0.1906, 0.2095, 0.1997, 0.2075, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20749104022979736 tensor([0.1925, 0.2075, 0.1952, 0.2123, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2049531787633896 tensor([0.1951, 0.2050, 0.1913, 0.2124, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2073267549276352 tensor([0.1934, 0.2073, 0.1952, 0.2108, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2092418074607849 tensor([0.1926, 0.2114, 0.1941, 0.2092, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20906633138656616 tensor([0.1917, 0.2091, 0.1962, 0.2095, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20664571225643158 tensor([0.1945, 0.2066, 0.1934, 0.2115, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19704245030879974 tensor([0.1948, 0.2039, 0.1918, 0.2125, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2087220698595047 tensor([0.1975, 0.2087, 0.1899, 0.2106, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.209184929728508 tensor([0.1925, 0.2120, 0.1945, 0.2092, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2011166363954544 tensor([0.1889, 0.2094, 0.2011, 0.2058, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2060142606496811 tensor([0.1948, 0.2060, 0.1933, 0.2126, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1977481096982956 tensor([0.1938, 0.2044, 0.1920, 0.2122, 0.1977], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20959192514419556 tensor([0.1984, 0.2100, 0.1896, 0.2096, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2085096687078476 tensor([0.1935, 0.2085, 0.1949, 0.2095, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19946694374084473 tensor([0.1888, 0.2084, 0.1995, 0.2084, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20627422630786896 tensor([0.1931, 0.2063, 0.1935, 0.2119, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2052292674779892 tensor([0.1938, 0.2052, 0.1931, 0.2116, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20987653732299805 tensor([0.1965, 0.2099, 0.1914, 0.2099, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20732803642749786 tensor([0.1925, 0.2073, 0.1957, 0.2109, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2006792426109314 tensor([0.1888, 0.2074, 0.2007, 0.2085, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2080071121454239 tensor([0.1936, 0.2093, 0.1965, 0.2080, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19810235500335693 tensor([0.1932, 0.2044, 0.1926, 0.2118, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2092016637325287 tensor([0.1972, 0.2106, 0.1905, 0.2092, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20945794880390167 tensor([0.1936, 0.2117, 0.1930, 0.2095, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994459182024002 tensor([0.1897, 0.2058, 0.1994, 0.2089, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20668776333332062 tensor([0.1938, 0.2067, 0.1927, 0.2129, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20626504719257355 tensor([0.1941, 0.2063, 0.1926, 0.2111, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206669881939888 tensor([0.1971, 0.2067, 0.1900, 0.2128, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20919986069202423 tensor([0.1922, 0.2092, 0.1951, 0.2101, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20747946202754974 tensor([0.1912, 0.2075, 0.1984, 0.2096, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20625948905944824 tensor([0.1935, 0.2063, 0.1935, 0.2138, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20609760284423828 tensor([0.1941, 0.2061, 0.1931, 0.2119, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2089788019657135 tensor([0.1961, 0.2103, 0.1915, 0.2090, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20706439018249512 tensor([0.1919, 0.2119, 0.1973, 0.2071, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20712418854236603 tensor([0.1919, 0.2071, 0.1956, 0.2107, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064010500907898 tensor([0.1946, 0.2064, 0.1952, 0.2108, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20750898122787476 tensor([0.1965, 0.2075, 0.1908, 0.2106, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20779292285442352 tensor([0.1961, 0.2078, 0.1922, 0.2111, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2087901383638382 tensor([0.1929, 0.2088, 0.1939, 0.2117, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2082114964723587 tensor([0.1901, 0.2097, 0.1985, 0.2082, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2089623063802719 tensor([0.1940, 0.2090, 0.1941, 0.2103, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20747707784175873 tensor([0.1936, 0.2075, 0.1935, 0.2101, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20835882425308228 tensor([0.1947, 0.2084, 0.1930, 0.2105, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2085120528936386 tensor([0.1930, 0.2086, 0.1959, 0.2085, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20138433575630188 tensor([0.1891, 0.2072, 0.2014, 0.2068, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20818157494068146 tensor([0.1935, 0.2082, 0.1928, 0.2122, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20655034482479095 tensor([0.1937, 0.2066, 0.1937, 0.2098, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20954513549804688 tensor([0.1968, 0.2095, 0.1906, 0.2105, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20847098529338837 tensor([0.1930, 0.2114, 0.1959, 0.2085, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20918063819408417 tensor([0.1917, 0.2092, 0.1957, 0.2092, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.209299698472023 tensor([0.1935, 0.2093, 0.1934, 0.2103, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20399776101112366 tensor([0.1923, 0.2040, 0.1947, 0.2130, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20793579518795013 tensor([0.1955, 0.2079, 0.1915, 0.2115, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20942503213882446 tensor([0.1944, 0.2094, 0.1943, 0.2102, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082316279411316 tensor([0.1909, 0.2082, 0.1979, 0.2092, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20490552484989166 tensor([0.1922, 0.2049, 0.1936, 0.2136, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19812820851802826 tensor([0.1930, 0.2036, 0.1926, 0.2127, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2094627469778061 tensor([0.1962, 0.2104, 0.1917, 0.2095, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20948712527751923 tensor([0.1933, 0.2095, 0.1947, 0.2095, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2061784267425537 tensor([0.1903, 0.2062, 0.1974, 0.2108, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2079509198665619 tensor([0.1954, 0.2080, 0.1926, 0.2113, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20464780926704407 tensor([0.1948, 0.2046, 0.1929, 0.2119, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2092764675617218 tensor([0.1959, 0.2096, 0.1920, 0.2093, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2093406617641449 tensor([0.1933, 0.2093, 0.1934, 0.2110, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20767593383789062 tensor([0.1920, 0.2077, 0.1953, 0.2103, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20665086805820465 tensor([0.1929, 0.2067, 0.1934, 0.2123, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20716850459575653 tensor([0.1939, 0.2072, 0.1943, 0.2091, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20917156338691711 tensor([0.1980, 0.2092, 0.1905, 0.2094, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20894069969654083 tensor([0.1928, 0.2125, 0.1940, 0.2089, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20818302035331726 tensor([0.1902, 0.2082, 0.1983, 0.2082, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20979860424995422 tensor([0.1943, 0.2105, 0.1938, 0.2098, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19798335433006287 tensor([0.1938, 0.2045, 0.1918, 0.2119, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071906328201294 tensor([0.1951, 0.2072, 0.1919, 0.2128, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20900240540504456 tensor([0.1909, 0.2113, 0.1959, 0.2090, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20296822488307953 tensor([0.1876, 0.2072, 0.2030, 0.2071, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20705252885818481 tensor([0.1940, 0.2071, 0.1932, 0.2123, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19896307587623596 tensor([0.1939, 0.2042, 0.1918, 0.2112, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20974397659301758 tensor([0.1987, 0.2100, 0.1892, 0.2097, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20881430804729462 tensor([0.1921, 0.2088, 0.1959, 0.2101, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994047611951828 tensor([0.1901, 0.2073, 0.1994, 0.2089, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082439363002777 tensor([0.1938, 0.2082, 0.1930, 0.2130, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19741664826869965 tensor([0.1934, 0.2039, 0.1929, 0.2124, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2095213383436203 tensor([0.1961, 0.2107, 0.1910, 0.2095, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20835483074188232 tensor([0.1926, 0.2102, 0.1960, 0.2084, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2012586146593094 tensor([0.1883, 0.2070, 0.2013, 0.2082, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20836447179317474 tensor([0.1944, 0.2084, 0.1919, 0.2115, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20607182383537292 tensor([0.1949, 0.2061, 0.1916, 0.2127, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20817236602306366 tensor([0.1961, 0.2082, 0.1931, 0.2114, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20974184572696686 tensor([0.1939, 0.2106, 0.1936, 0.2097, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2010067105293274 tensor([0.1889, 0.2076, 0.2010, 0.2071, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20939020812511444 tensor([0.1908, 0.2097, 0.1965, 0.2094, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043612152338028 tensor([0.1934, 0.2044, 0.1931, 0.2135, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2089422345161438 tensor([0.1992, 0.2089, 0.1891, 0.2103, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20832858979701996 tensor([0.1920, 0.2107, 0.1955, 0.2083, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20604662597179413 tensor([0.1937, 0.2060, 0.1950, 0.2108, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20850452780723572 tensor([0.1931, 0.2085, 0.1946, 0.2116, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2072114497423172 tensor([0.1929, 0.2072, 0.1954, 0.2095, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20836952328681946 tensor([0.1947, 0.2084, 0.1917, 0.2109, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2083614617586136 tensor([0.1919, 0.2102, 0.1971, 0.2084, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20783096551895142 tensor([0.1919, 0.2078, 0.1956, 0.2102, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20800402760505676 tensor([0.1955, 0.2080, 0.1912, 0.2118, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19714312255382538 tensor([0.1954, 0.2048, 0.1906, 0.2120, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20919424295425415 tensor([0.1981, 0.2116, 0.1893, 0.2092, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2091847062110901 tensor([0.1929, 0.2092, 0.1955, 0.2095, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2032483071088791 tensor([0.1881, 0.2088, 0.2032, 0.2063, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20682846009731293 tensor([0.1934, 0.2068, 0.1916, 0.2143, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066456526517868 tensor([0.1939, 0.2066, 0.1933, 0.2101, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20546558499336243 tensor([0.1947, 0.2055, 0.1930, 0.2124, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20740558207035065 tensor([0.1920, 0.2133, 0.1960, 0.2074, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20737652480602264 tensor([0.1911, 0.2074, 0.1975, 0.2094, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2079874724149704 tensor([0.1928, 0.2080, 0.1945, 0.2121, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20573928952217102 tensor([0.1956, 0.2057, 0.1905, 0.2118, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20898596942424774 tensor([0.1961, 0.2095, 0.1920, 0.2090, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20806662738323212 tensor([0.1931, 0.2101, 0.1955, 0.2081, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2054886668920517 tensor([0.1935, 0.2055, 0.1934, 0.2117, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20759275555610657 tensor([0.1919, 0.2076, 0.1944, 0.2116, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1970008909702301 tensor([0.1958, 0.2054, 0.1906, 0.2112, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20743264257907867 tensor([0.1949, 0.2074, 0.1928, 0.2113, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082865834236145 tensor([0.1935, 0.2083, 0.1930, 0.2122, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2051934450864792 tensor([0.1861, 0.2059, 0.2052, 0.2066, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20672500133514404 tensor([0.1939, 0.2067, 0.1943, 0.2126, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19677849113941193 tensor([0.1931, 0.2057, 0.1937, 0.2108, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20847629010677338 tensor([0.1963, 0.2114, 0.1920, 0.2085, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.208919957280159 tensor([0.1940, 0.2107, 0.1944, 0.2089, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2074047327041626 tensor([0.1936, 0.2074, 0.1951, 0.2100, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20793308317661285 tensor([0.1932, 0.2079, 0.1943, 0.2106, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20596937835216522 tensor([0.1956, 0.2060, 0.1914, 0.2120, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20731985569000244 tensor([0.1943, 0.2073, 0.1931, 0.2103, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20935726165771484 tensor([0.1921, 0.2104, 0.1949, 0.2094, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20647335052490234 tensor([0.1922, 0.2065, 0.1943, 0.2116, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20674914121627808 tensor([0.1937, 0.2067, 0.1927, 0.2122, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19923953711986542 tensor([0.1932, 0.2040, 0.1928, 0.2108, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20951148867607117 tensor([0.1991, 0.2095, 0.1884, 0.2106, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20885778963565826 tensor([0.1914, 0.2089, 0.1956, 0.2094, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20865753293037415 tensor([0.1903, 0.2093, 0.1985, 0.2087, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2088632881641388 tensor([0.1933, 0.2089, 0.1946, 0.2108, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20781493186950684 tensor([0.1929, 0.2078, 0.1950, 0.2093, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2056380808353424 tensor([0.1946, 0.2056, 0.1934, 0.2131, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20720304548740387 tensor([0.1939, 0.2072, 0.1943, 0.2106, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20356260240077972 tensor([0.1869, 0.2086, 0.2036, 0.2057, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20885111391544342 tensor([0.1917, 0.2089, 0.1954, 0.2100, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19841298460960388 tensor([0.1926, 0.2048, 0.1938, 0.2103, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.208602175116539 tensor([0.1968, 0.2086, 0.1910, 0.2107, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2091204971075058 tensor([0.1937, 0.2091, 0.1939, 0.2108, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2077891230583191 tensor([0.1909, 0.2078, 0.1971, 0.2093, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20903360843658447 tensor([0.1919, 0.2090, 0.1949, 0.2111, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20623742043972015 tensor([0.1936, 0.2062, 0.1953, 0.2110, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2058037370443344 tensor([0.1964, 0.2058, 0.1919, 0.2130, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20604920387268066 tensor([0.1915, 0.2060, 0.1962, 0.2109, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20441003143787384 tensor([0.1921, 0.2044, 0.1973, 0.2106, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20852094888687134 tensor([0.1924, 0.2085, 0.1949, 0.2100, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20587261021137238 tensor([0.1940, 0.2059, 0.1931, 0.2115, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20867207646369934 tensor([0.1927, 0.2087, 0.1953, 0.2103, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20891202986240387 tensor([0.1927, 0.2120, 0.1939, 0.2089, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000911831855774 tensor([0.1892, 0.2074, 0.2001, 0.2083, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20719921588897705 tensor([0.1926, 0.2072, 0.1952, 0.2106, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19945429265499115 tensor([0.1936, 0.2040, 0.1923, 0.2106, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2094361037015915 tensor([0.1987, 0.2094, 0.1895, 0.2107, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20969240367412567 tensor([0.1941, 0.2102, 0.1940, 0.2097, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20647688210010529 tensor([0.1902, 0.2065, 0.1965, 0.2113, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097102403640747 tensor([0.1937, 0.2097, 0.1939, 0.2103, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19677644968032837 tensor([0.1953, 0.2047, 0.1918, 0.2114, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20863358676433563 tensor([0.1964, 0.2086, 0.1921, 0.2100, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20896078646183014 tensor([0.1919, 0.2105, 0.1963, 0.2090, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2025604099035263 tensor([0.1871, 0.2075, 0.2026, 0.2074, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071439027786255 tensor([0.1935, 0.2071, 0.1937, 0.2119, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19840548932552338 tensor([0.1948, 0.2041, 0.1915, 0.2111, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20853649079799652 tensor([0.1945, 0.2085, 0.1944, 0.2094, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20988015830516815 tensor([0.1923, 0.2099, 0.1946, 0.2102, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2059057503938675 tensor([0.1919, 0.2059, 0.1963, 0.2107, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20848147571086884 tensor([0.1952, 0.2085, 0.1927, 0.2118, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20594243705272675 tensor([0.1946, 0.2059, 0.1914, 0.2113, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20935428142547607 tensor([0.1952, 0.2094, 0.1933, 0.2094, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20960085093975067 tensor([0.1936, 0.2121, 0.1930, 0.2096, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2055433690547943 tensor([0.1929, 0.2055, 0.1944, 0.2125, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2093464434146881 tensor([0.1927, 0.2093, 0.1943, 0.2114, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19776396453380585 tensor([0.1947, 0.2041, 0.1927, 0.2107, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20819610357284546 tensor([0.1931, 0.2099, 0.1952, 0.2082, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20833884179592133 tensor([0.1930, 0.2103, 0.1952, 0.2083, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20151619613170624 tensor([0.1882, 0.2066, 0.2015, 0.2084, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20622064173221588 tensor([0.1950, 0.2062, 0.1904, 0.2150, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20764009654521942 tensor([0.1959, 0.2076, 0.1908, 0.2102, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20917722582817078 tensor([0.1967, 0.2092, 0.1912, 0.2097, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2090952843427658 tensor([0.1937, 0.2104, 0.1943, 0.2091, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069411277770996 tensor([0.1909, 0.2069, 0.1967, 0.2101, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082442194223404 tensor([0.1935, 0.2082, 0.1922, 0.2127, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20650510489940643 tensor([0.1950, 0.2065, 0.1920, 0.2117, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2095087617635727 tensor([0.1953, 0.2095, 0.1932, 0.2097, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20875957608222961 tensor([0.1930, 0.2111, 0.1946, 0.2088, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20838572084903717 tensor([0.1896, 0.2092, 0.1983, 0.2084, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20935410261154175 tensor([0.1909, 0.2098, 0.1956, 0.2094, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19688117504119873 tensor([0.1937, 0.2053, 0.1936, 0.2106, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20834802091121674 tensor([0.1943, 0.2083, 0.1925, 0.2112, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20789982378482819 tensor([0.1926, 0.2104, 0.1968, 0.2079, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20854444801807404 tensor([0.1919, 0.2085, 0.1976, 0.2091, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20803673565387726 tensor([0.1931, 0.2080, 0.1943, 0.2116, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20508137345314026 tensor([0.1947, 0.2051, 0.1921, 0.2124, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20955422520637512 tensor([0.1985, 0.2096, 0.1892, 0.2107, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2094200998544693 tensor([0.1930, 0.2094, 0.1946, 0.2095, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20802877843379974 tensor([0.1909, 0.2080, 0.1973, 0.2090, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2076931595802307 tensor([0.1942, 0.2077, 0.1908, 0.2137, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.206892192363739 tensor([0.1939, 0.2069, 0.1934, 0.2107, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 4, 2], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [4, 0, 2], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 4], [0, 4, 2], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4], [4, 0, 2], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 2, 4], [0, 2, 4], [4, 2, 0], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4]]\n",
      "[[1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3]]\n",
      "NL_pred of 3th iteration [[2, 0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4], [2, 0, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4], [0, 4, 2], [2, 0, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.04760653884322555  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.047605059765003344  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.04760226055427834  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0475982736658167  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.04759320947859022  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.047587187201888474  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.04758031279952438  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.04757264808372215  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.047564299018294724  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.04755534066094293  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.04754581716325548  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.04753580799809209  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.04752534407156485  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.04751449161105686  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0475032991833157  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.04749179327929461  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.04748002246574119  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.04746800440329092  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.047455783243532536  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 2/15 [01:38<10:04, 46.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Epoch_NL: 19  Train_Loss: 0.04744335898646602  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.2082858681678772 tensor([0.1955, 0.2083, 0.1938, 0.2132, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20912288129329681 tensor([0.1936, 0.2091, 0.1954, 0.2124, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2001025229692459 tensor([0.1908, 0.2089, 0.2001, 0.2100, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2102167159318924 tensor([0.1952, 0.2102, 0.1927, 0.2140, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20472557842731476 tensor([0.1960, 0.2047, 0.1917, 0.2148, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21095526218414307 tensor([0.1993, 0.2110, 0.1902, 0.2124, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20889045298099518 tensor([0.1934, 0.2149, 0.1958, 0.2089, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21052832901477814 tensor([0.1933, 0.2109, 0.1973, 0.2105, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21022877097129822 tensor([0.1967, 0.2102, 0.1917, 0.2135, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20624448359012604 tensor([0.1970, 0.2062, 0.1916, 0.2136, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20942583680152893 tensor([0.1973, 0.2094, 0.1922, 0.2123, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2110702246427536 tensor([0.1936, 0.2121, 0.1952, 0.2111, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20948660373687744 tensor([0.1930, 0.2095, 0.1959, 0.2116, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21132799983024597 tensor([0.1964, 0.2113, 0.1917, 0.2127, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20670557022094727 tensor([0.1940, 0.2067, 0.1941, 0.2115, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20964457094669342 tensor([0.1990, 0.2096, 0.1902, 0.2131, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20892685651779175 tensor([0.1945, 0.2089, 0.1959, 0.2110, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20935189723968506 tensor([0.1922, 0.2094, 0.1970, 0.2121, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097981721162796 tensor([0.1935, 0.2098, 0.1967, 0.2112, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20723260939121246 tensor([0.1966, 0.2072, 0.1914, 0.2127, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21171753108501434 tensor([0.1992, 0.2122, 0.1900, 0.2117, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21024326980113983 tensor([0.1936, 0.2129, 0.1950, 0.2102, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2034548670053482 tensor([0.1887, 0.2104, 0.2035, 0.2073, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2083854079246521 tensor([0.1954, 0.2084, 0.1950, 0.2126, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20670755207538605 tensor([0.1960, 0.2067, 0.1929, 0.2123, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20959502458572388 tensor([0.1968, 0.2124, 0.1928, 0.2096, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2101367563009262 tensor([0.1937, 0.2101, 0.1951, 0.2121, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20066756010055542 tensor([0.1891, 0.2076, 0.2007, 0.2096, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20755186676979065 tensor([0.1963, 0.2076, 0.1911, 0.2159, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2057909220457077 tensor([0.1968, 0.2058, 0.1924, 0.2142, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21032485365867615 tensor([0.1992, 0.2103, 0.1903, 0.2123, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2091285139322281 tensor([0.1930, 0.2127, 0.1966, 0.2091, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20764124393463135 tensor([0.1928, 0.2076, 0.1960, 0.2133, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20873723924160004 tensor([0.1929, 0.2087, 0.1960, 0.2129, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082032859325409 tensor([0.1964, 0.2082, 0.1914, 0.2126, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21076913177967072 tensor([0.1965, 0.2108, 0.1925, 0.2112, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21118980646133423 tensor([0.1946, 0.2112, 0.1947, 0.2114, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21075358986854553 tensor([0.1932, 0.2108, 0.1954, 0.2110, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2109028398990631 tensor([0.1957, 0.2109, 0.1921, 0.2133, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097584307193756 tensor([0.1935, 0.2098, 0.1955, 0.2100, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21166305243968964 tensor([0.2001, 0.2117, 0.1892, 0.2121, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2096022069454193 tensor([0.1927, 0.2096, 0.1969, 0.2106, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20032241940498352 tensor([0.1908, 0.2087, 0.2003, 0.2102, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20942525565624237 tensor([0.1944, 0.2094, 0.1944, 0.2128, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20472335815429688 tensor([0.1967, 0.2047, 0.1913, 0.2149, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20973728597164154 tensor([0.1967, 0.2097, 0.1919, 0.2130, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20945513248443604 tensor([0.1940, 0.2128, 0.1965, 0.2095, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20901678502559662 tensor([0.1921, 0.2113, 0.1996, 0.2090, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20921534299850464 tensor([0.1940, 0.2092, 0.1952, 0.2138, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066892385482788 tensor([0.1967, 0.2067, 0.1913, 0.2140, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20903436839580536 tensor([0.1950, 0.2090, 0.1952, 0.2124, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21076741814613342 tensor([0.1942, 0.2132, 0.1941, 0.2108, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21080882847309113 tensor([0.1933, 0.2108, 0.1962, 0.2111, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20837944746017456 tensor([0.1960, 0.2084, 0.1933, 0.2131, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20561325550079346 tensor([0.1964, 0.2056, 0.1918, 0.2141, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21043336391448975 tensor([0.1991, 0.2104, 0.1898, 0.2121, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2107076346874237 tensor([0.1941, 0.2138, 0.1945, 0.2107, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20108215510845184 tensor([0.1904, 0.2111, 0.2011, 0.2074, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20773696899414062 tensor([0.1963, 0.2077, 0.1932, 0.2142, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20614060759544373 tensor([0.1954, 0.2061, 0.1919, 0.2138, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21112369000911713 tensor([0.1999, 0.2117, 0.1896, 0.2111, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21024008095264435 tensor([0.1951, 0.2102, 0.1948, 0.2111, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2099510133266449 tensor([0.1903, 0.2102, 0.1994, 0.2100, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.208022803068161 tensor([0.1947, 0.2080, 0.1935, 0.2135, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2069757729768753 tensor([0.1954, 0.2070, 0.1931, 0.2132, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21140991151332855 tensor([0.1981, 0.2116, 0.1913, 0.2114, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2090669572353363 tensor([0.1940, 0.2091, 0.1956, 0.2124, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20062829554080963 tensor([0.1903, 0.2092, 0.2006, 0.2100, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20952579379081726 tensor([0.1951, 0.2109, 0.1964, 0.2095, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20614226162433624 tensor([0.1948, 0.2061, 0.1926, 0.2134, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21073363721370697 tensor([0.1988, 0.2123, 0.1904, 0.2107, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21099288761615753 tensor([0.1952, 0.2135, 0.1929, 0.2110, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20753780007362366 tensor([0.1913, 0.2075, 0.1994, 0.2105, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20841030776500702 tensor([0.1953, 0.2084, 0.1926, 0.2144, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20800848305225372 tensor([0.1957, 0.2080, 0.1925, 0.2127, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2083664983510971 tensor([0.1987, 0.2084, 0.1899, 0.2143, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21095159649848938 tensor([0.1937, 0.2110, 0.1950, 0.2116, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20924067497253418 tensor([0.1928, 0.2092, 0.1983, 0.2111, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20798039436340332 tensor([0.1951, 0.2080, 0.1934, 0.2153, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20782119035720825 tensor([0.1957, 0.2078, 0.1930, 0.2135, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21050389111042023 tensor([0.1977, 0.2120, 0.1915, 0.2105, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2085913121700287 tensor([0.1934, 0.2135, 0.1972, 0.2086, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20887473225593567 tensor([0.1934, 0.2089, 0.1955, 0.2123, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20812886953353882 tensor([0.1961, 0.2081, 0.1951, 0.2123, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2092621624469757 tensor([0.1981, 0.2093, 0.1908, 0.2122, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20951169729232788 tensor([0.1977, 0.2095, 0.1921, 0.2127, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21052011847496033 tensor([0.1945, 0.2105, 0.1938, 0.2133, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20976172387599945 tensor([0.1916, 0.2115, 0.1984, 0.2098, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21066085994243622 tensor([0.1956, 0.2107, 0.1940, 0.2118, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20922721922397614 tensor([0.1952, 0.2092, 0.1935, 0.2117, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21008983254432678 tensor([0.1963, 0.2101, 0.1930, 0.2120, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21005555987358093 tensor([0.1946, 0.2103, 0.1958, 0.2101, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2013659030199051 tensor([0.1906, 0.2090, 0.2014, 0.2084, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20991170406341553 tensor([0.1950, 0.2099, 0.1928, 0.2138, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2083156406879425 tensor([0.1953, 0.2083, 0.1937, 0.2113, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21123242378234863 tensor([0.1984, 0.2112, 0.1906, 0.2121, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2099718451499939 tensor([0.1946, 0.2132, 0.1959, 0.2100, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21076403558254242 tensor([0.1933, 0.2109, 0.1957, 0.2108, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2110319584608078 tensor([0.1951, 0.2110, 0.1933, 0.2119, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20573411881923676 tensor([0.1938, 0.2057, 0.1947, 0.2146, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20970407128334045 tensor([0.1971, 0.2097, 0.1915, 0.2131, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.211105614900589 tensor([0.1959, 0.2111, 0.1943, 0.2118, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20998071134090424 tensor([0.1925, 0.2100, 0.1979, 0.2108, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066388726234436 tensor([0.1937, 0.2066, 0.1935, 0.2152, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2053164690732956 tensor([0.1946, 0.2053, 0.1926, 0.2143, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21098685264587402 tensor([0.1978, 0.2121, 0.1916, 0.2110, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2110462486743927 tensor([0.1948, 0.2112, 0.1946, 0.2110, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20792824029922485 tensor([0.1918, 0.2079, 0.1974, 0.2124, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20967204868793488 tensor([0.1970, 0.2097, 0.1925, 0.2128, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20639532804489136 tensor([0.1964, 0.2064, 0.1929, 0.2135, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21082080900669098 tensor([0.1975, 0.2113, 0.1919, 0.2108, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21107064187526703 tensor([0.1949, 0.2111, 0.1933, 0.2125, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2094334363937378 tensor([0.1936, 0.2094, 0.1953, 0.2119, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20840699970722198 tensor([0.1945, 0.2084, 0.1934, 0.2139, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2089054137468338 tensor([0.1954, 0.2089, 0.1943, 0.2107, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2107953429222107 tensor([0.1996, 0.2108, 0.1904, 0.2110, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21044990420341492 tensor([0.1943, 0.2143, 0.1939, 0.2104, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20980267226696014 tensor([0.1917, 0.2099, 0.1983, 0.2098, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2113335281610489 tensor([0.1959, 0.2122, 0.1938, 0.2113, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20626698434352875 tensor([0.1954, 0.2063, 0.1918, 0.2135, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20889431238174438 tensor([0.1967, 0.2089, 0.1919, 0.2143, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21054939925670624 tensor([0.1924, 0.2130, 0.1958, 0.2105, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20292650163173676 tensor([0.1891, 0.2089, 0.2029, 0.2087, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20876289904117584 tensor([0.1955, 0.2088, 0.1932, 0.2138, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20599690079689026 tensor([0.1955, 0.2060, 0.1918, 0.2128, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21130309998989105 tensor([0.2003, 0.2117, 0.1891, 0.2113, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21054966747760773 tensor([0.1937, 0.2105, 0.1958, 0.2116, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20902936160564423 tensor([0.1917, 0.2090, 0.1993, 0.2105, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.209953173995018 tensor([0.1954, 0.2100, 0.1930, 0.2145, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20567382872104645 tensor([0.1950, 0.2057, 0.1929, 0.2140, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21105347573757172 tensor([0.1977, 0.2124, 0.1909, 0.2111, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20990948379039764 tensor([0.1942, 0.2119, 0.1959, 0.2099, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2012317180633545 tensor([0.1898, 0.2088, 0.2012, 0.2098, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21009977161884308 tensor([0.1960, 0.2101, 0.1919, 0.2130, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2077888548374176 tensor([0.1965, 0.2078, 0.1916, 0.2142, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20987293124198914 tensor([0.1977, 0.2099, 0.1930, 0.2129, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21125376224517822 tensor([0.1955, 0.2124, 0.1935, 0.2113, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20101211965084076 tensor([0.1904, 0.2093, 0.2010, 0.2086, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21093228459358215 tensor([0.1923, 0.2115, 0.1965, 0.2109, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20610620081424713 tensor([0.1950, 0.2061, 0.1931, 0.2150, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21056954562664032 tensor([0.2008, 0.2106, 0.1890, 0.2118, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2098633497953415 tensor([0.1935, 0.2125, 0.1955, 0.2099, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20780529081821442 tensor([0.1952, 0.2078, 0.1950, 0.2124, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21022485196590424 tensor([0.1946, 0.2102, 0.1945, 0.2132, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20896443724632263 tensor([0.1944, 0.2090, 0.1954, 0.2111, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21010304987430573 tensor([0.1962, 0.2101, 0.1916, 0.2125, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20990994572639465 tensor([0.1934, 0.2118, 0.1970, 0.2099, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20956635475158691 tensor([0.1934, 0.2096, 0.1956, 0.2118, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20974282920360565 tensor([0.1971, 0.2097, 0.1911, 0.2133, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2065010964870453 tensor([0.1970, 0.2065, 0.1906, 0.2136, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21073521673679352 tensor([0.1997, 0.2132, 0.1892, 0.2107, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21093714237213135 tensor([0.1945, 0.2109, 0.1954, 0.2110, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20314182341098785 tensor([0.1896, 0.2106, 0.2031, 0.2078, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2085546851158142 tensor([0.1950, 0.2086, 0.1916, 0.2159, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20840632915496826 tensor([0.1955, 0.2084, 0.1932, 0.2117, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20718955993652344 tensor([0.1963, 0.2072, 0.1930, 0.2139, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2089323252439499 tensor([0.1936, 0.2150, 0.1959, 0.2089, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20913328230381012 tensor([0.1927, 0.2091, 0.1974, 0.2110, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097114771604538 tensor([0.1944, 0.2097, 0.1944, 0.2136, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20749671757221222 tensor([0.1972, 0.2075, 0.1905, 0.2134, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21051368117332458 tensor([0.1976, 0.2112, 0.1920, 0.2105, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2096060812473297 tensor([0.1947, 0.2118, 0.1954, 0.2096, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20724238455295563 tensor([0.1950, 0.2072, 0.1934, 0.2133, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20932139456272125 tensor([0.1935, 0.2093, 0.1944, 0.2132, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071504145860672 tensor([0.1974, 0.2072, 0.1906, 0.2128, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20915177464485168 tensor([0.1965, 0.2092, 0.1928, 0.2129, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20999637246131897 tensor([0.1951, 0.2100, 0.1930, 0.2137, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20519131422042847 tensor([0.1876, 0.2077, 0.2052, 0.2082, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20844386518001556 tensor([0.1954, 0.2084, 0.1942, 0.2141, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20741669833660126 tensor([0.1948, 0.2074, 0.1936, 0.2123, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21001484990119934 tensor([0.1979, 0.2130, 0.1919, 0.2100, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2104257494211197 tensor([0.1956, 0.2125, 0.1943, 0.2104, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20913127064704895 tensor([0.1951, 0.2091, 0.1950, 0.2116, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20968115329742432 tensor([0.1947, 0.2097, 0.1943, 0.2122, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20769640803337097 tensor([0.1972, 0.2077, 0.1914, 0.2136, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20904268324375153 tensor([0.1958, 0.2090, 0.1930, 0.2119, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.210887610912323 tensor([0.1937, 0.2121, 0.1948, 0.2109, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082340568304062 tensor([0.1938, 0.2082, 0.1943, 0.2132, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20847557485103607 tensor([0.1953, 0.2085, 0.1927, 0.2137, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20579257607460022 tensor([0.1948, 0.2058, 0.1928, 0.2124, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.211218923330307 tensor([0.2007, 0.2112, 0.1883, 0.2121, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21061398088932037 tensor([0.1929, 0.2106, 0.1955, 0.2110, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.210185706615448 tensor([0.1918, 0.2111, 0.1984, 0.2102, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21058636903762817 tensor([0.1949, 0.2106, 0.1945, 0.2123, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2095479518175125 tensor([0.1945, 0.2095, 0.1950, 0.2109, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20733976364135742 tensor([0.1962, 0.2073, 0.1933, 0.2146, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20892538130283356 tensor([0.1954, 0.2089, 0.1943, 0.2121, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2035575658082962 tensor([0.1884, 0.2104, 0.2036, 0.2072, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21061871945858002 tensor([0.1933, 0.2106, 0.1954, 0.2115, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20656222105026245 tensor([0.1942, 0.2066, 0.1939, 0.2119, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21031896770000458 tensor([0.1984, 0.2103, 0.1909, 0.2123, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21082985401153564 tensor([0.1952, 0.2108, 0.1938, 0.2124, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2095411717891693 tensor([0.1925, 0.2095, 0.1970, 0.2108, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21078148484230042 tensor([0.1935, 0.2108, 0.1949, 0.2126, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20794790983200073 tensor([0.1952, 0.2079, 0.1953, 0.2126, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2074911743402481 tensor([0.1980, 0.2075, 0.1918, 0.2146, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20779384672641754 tensor([0.1930, 0.2078, 0.1962, 0.2125, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20612740516662598 tensor([0.1936, 0.2061, 0.1973, 0.2122, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2102581411600113 tensor([0.1940, 0.2103, 0.1948, 0.2116, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20762969553470612 tensor([0.1956, 0.2076, 0.1931, 0.2131, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2104053795337677 tensor([0.1942, 0.2104, 0.1952, 0.2119, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21044199168682098 tensor([0.1942, 0.2138, 0.1939, 0.2104, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20007768273353577 tensor([0.1907, 0.2091, 0.2001, 0.2099, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2089260071516037 tensor([0.1941, 0.2089, 0.1951, 0.2122, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20579937100410461 tensor([0.1952, 0.2058, 0.1923, 0.2122, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21113291382789612 tensor([0.2002, 0.2111, 0.1895, 0.2122, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21119560301303864 tensor([0.1956, 0.2119, 0.1939, 0.2112, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20823872089385986 tensor([0.1918, 0.2082, 0.1964, 0.2129, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21142953634262085 tensor([0.1952, 0.2114, 0.1939, 0.2118, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064322531223297 tensor([0.1970, 0.2064, 0.1918, 0.2130, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21035218238830566 tensor([0.1980, 0.2104, 0.1920, 0.2115, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21050558984279633 tensor([0.1934, 0.2123, 0.1962, 0.2105, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20255401730537415 tensor([0.1886, 0.2092, 0.2026, 0.2089, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20889948308467865 tensor([0.1951, 0.2089, 0.1937, 0.2135, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2059101015329361 tensor([0.1964, 0.2059, 0.1915, 0.2127, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21026131510734558 tensor([0.1961, 0.2103, 0.1944, 0.2109, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21161803603172302 tensor([0.1939, 0.2116, 0.1945, 0.2117, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20766578614711761 tensor([0.1935, 0.2077, 0.1962, 0.2122, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21017694473266602 tensor([0.1967, 0.2102, 0.1927, 0.2134, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20769715309143066 tensor([0.1962, 0.2077, 0.1914, 0.2129, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21093162894248962 tensor([0.1968, 0.2111, 0.1932, 0.2109, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21115142107009888 tensor([0.1951, 0.2138, 0.1929, 0.2112, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20726889371871948 tensor([0.1944, 0.2073, 0.1943, 0.2140, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2110682725906372 tensor([0.1943, 0.2111, 0.1942, 0.2130, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20589004456996918 tensor([0.1963, 0.2059, 0.1927, 0.2123, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20973578095436096 tensor([0.1946, 0.2117, 0.1951, 0.2097, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20985962450504303 tensor([0.1946, 0.2120, 0.1951, 0.2099, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2015073001384735 tensor([0.1897, 0.2083, 0.2015, 0.2100, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20791929960250854 tensor([0.1965, 0.2079, 0.1903, 0.2165, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20940035581588745 tensor([0.1975, 0.2094, 0.1908, 0.2117, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21084275841712952 tensor([0.1983, 0.2108, 0.1911, 0.2113, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2106219232082367 tensor([0.1952, 0.2121, 0.1943, 0.2106, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2086952030658722 tensor([0.1925, 0.2087, 0.1966, 0.2116, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20995600521564484 tensor([0.1951, 0.2100, 0.1921, 0.2142, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2082454413175583 tensor([0.1966, 0.2082, 0.1920, 0.2133, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21115992963314056 tensor([0.1969, 0.2112, 0.1931, 0.2112, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21029207110404968 tensor([0.1945, 0.2128, 0.1945, 0.2103, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20990514755249023 tensor([0.1912, 0.2110, 0.1983, 0.2099, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.210895836353302 tensor([0.1925, 0.2115, 0.1956, 0.2109, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070562094449997 tensor([0.1953, 0.2071, 0.1936, 0.2122, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21007321774959564 tensor([0.1959, 0.2101, 0.1924, 0.2127, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20942354202270508 tensor([0.1941, 0.2121, 0.1968, 0.2094, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21026988327503204 tensor([0.1935, 0.2103, 0.1975, 0.2106, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097625583410263 tensor([0.1946, 0.2098, 0.1942, 0.2131, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20685245096683502 tensor([0.1963, 0.2069, 0.1921, 0.2140, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2112538367509842 tensor([0.2001, 0.2113, 0.1891, 0.2122, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2110481858253479 tensor([0.1945, 0.2111, 0.1946, 0.2110, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097756415605545 tensor([0.1924, 0.2098, 0.1973, 0.2106, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2094145566225052 tensor([0.1958, 0.2094, 0.1907, 0.2153, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20862150192260742 tensor([0.1955, 0.2086, 0.1934, 0.2123, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 4, 2], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [4, 0, 2], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 4], [0, 4, 2], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4], [4, 0, 2], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 2, 4], [0, 2, 4], [4, 2, 0], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4]]\n",
      "[[1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3]]\n",
      "NL_pred of 4th iteration []\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 4, 2], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 4, 2], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4], [4, 0, 2], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4, 2], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 4], [0, 4, 2], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [4, 0, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [2, 0, 4], [0, 4], [4, 0, 2], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [4, 0, 2], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 4, 0], [0, 2, 4], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 2, 4], [0, 4], [0, 2, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [4, 0, 2], [2, 0, 4], [2, 4, 0], [0, 4, 2], [0, 4], [0, 2, 4], [2, 0, 4], [4, 0, 2], [0, 2, 4], [0, 2, 4], [4, 2, 0], [2, 0, 4], [4, 2, 0], [4, 0, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 2, 4], [0, 2, 4], [0, 4], [2, 0, 4], [2, 0, 4], [2, 4, 0], [4, 0, 2], [0, 2, 4], [2, 0, 4], [2, 0, 4], [4, 2, 0], [0, 2, 4], [0, 4, 2], [0, 2, 4], [0, 2, 4], [2, 0, 4], [0, 4, 2], [0, 4, 2], [0, 2, 4], [2, 0, 4], [2, 4, 0], [0, 2, 4], [0, 4, 2], [2, 0, 4], [2, 0, 4]]\n",
      "POSITION :  [[1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 2, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.464\n",
      "tensor([], dtype=torch.int64)\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  30.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 1.6056349229812623  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 1.6042524909973144  Accuracy on Support set:24.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6029585123062133  Accuracy on Support set:24.0\n",
      "Train_Epoch: 3  Train_Loss: 1.6016813850402831  Accuracy on Support set:24.0\n",
      "Train_Epoch: 4  Train_Loss: 1.6004195070266725  Accuracy on Support set:24.0\n",
      "Train_Epoch: 5  Train_Loss: 1.599179801940918  Accuracy on Support set:32.0\n",
      "Train_Epoch: 6  Train_Loss: 1.5979621982574463  Accuracy on Support set:36.0\n",
      "Train_Epoch: 7  Train_Loss: 1.596738681793213  Accuracy on Support set:36.0\n",
      "Train_Epoch: 8  Train_Loss: 1.5955134725570679  Accuracy on Support set:36.0\n",
      "Train_Epoch: 9  Train_Loss: 1.5942725467681884  Accuracy on Support set:36.0\n",
      "Train_Epoch: 10  Train_Loss: 1.5930160140991212  Accuracy on Support set:36.0\n",
      "Train_Epoch: 11  Train_Loss: 1.5917496538162232  Accuracy on Support set:36.0\n",
      "Train_Epoch: 12  Train_Loss: 1.5904542922973632  Accuracy on Support set:40.0\n",
      "Train_Epoch: 13  Train_Loss: 1.5891325187683105  Accuracy on Support set:48.0\n",
      "Train_Epoch: 14  Train_Loss: 1.5877759075164795  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 15  Train_Loss: 1.586380205154419  Accuracy on Support set:60.0\n",
      "Train_Epoch: 16  Train_Loss: 1.584953818321228  Accuracy on Support set:64.0\n",
      "Train_Epoch: 17  Train_Loss: 1.583477621078491  Accuracy on Support set:64.0\n",
      "Train_Epoch: 18  Train_Loss: 1.581950454711914  Accuracy on Support set:72.0\n",
      "Train_Epoch: 19  Train_Loss: 1.5803711462020873  Accuracy on Support set:84.0\n",
      "Train_Epoch: 20  Train_Loss: 1.5787322473526002  Accuracy on Support set:88.0\n",
      "Train_Epoch: 21  Train_Loss: 1.5770348834991454  Accuracy on Support set:92.0\n",
      "Train_Epoch: 22  Train_Loss: 1.575280966758728  Accuracy on Support set:92.0\n",
      "Train_Epoch: 23  Train_Loss: 1.5734318780899048  Accuracy on Support set:96.0\n",
      "Train_Epoch: 24  Train_Loss: 1.5715479898452758  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 1.5695665645599366  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 1.5675133228302003  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 1.5653380060195923  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 1.563107590675354  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 1.5607397985458373  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 1.5582506322860719  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 1.555608835220337  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 1.552766466140747  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 1.5498180532455443  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 1.546797103881836  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 1.543593029975891  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 1.540266695022583  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 1.536717987060547  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 1.5330054378509521  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 1.5290698099136353  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 1.524865050315857  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 1.5204264736175537  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 1.515706377029419  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 1.5108160066604615  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 1.5056002712249756  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 1.500130205154419  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 1.494332137107849  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 1.488238263130188  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 1.4817597770690918  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 1.4748951292037964  Accuracy on Support set:96.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  75.33333333333333\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.1698983758687973 tensor([0.2305, 0.2025, 0.1699, 0.2181, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19122791290283203 tensor([0.2007, 0.2024, 0.1974, 0.2083, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19290383160114288 tensor([0.1929, 0.1990, 0.2071, 0.2048, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1728796809911728 tensor([0.2242, 0.1973, 0.1729, 0.2273, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19000846147537231 tensor([0.2038, 0.1919, 0.1900, 0.2104, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16871821880340576 tensor([0.2313, 0.2014, 0.1687, 0.2196, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19452373683452606 tensor([0.2020, 0.1981, 0.1945, 0.2108, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1901579648256302 tensor([0.1902, 0.1987, 0.2070, 0.2027, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18041875958442688 tensor([0.2140, 0.1947, 0.1804, 0.2252, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18635301291942596 tensor([0.1875, 0.1864, 0.2034, 0.2019, 0.2208], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17159712314605713 tensor([0.2287, 0.2017, 0.1716, 0.2191, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18811288475990295 tensor([0.2073, 0.2040, 0.1881, 0.2109, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18887795507907867 tensor([0.1950, 0.2058, 0.2057, 0.2046, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1792895346879959 tensor([0.2172, 0.1961, 0.1793, 0.2224, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1931445598602295 tensor([0.1966, 0.1931, 0.1978, 0.2093, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1859823316335678 tensor([0.2112, 0.1985, 0.1860, 0.2114, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19167880713939667 tensor([0.1996, 0.2071, 0.1987, 0.2030, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18402962386608124 tensor([0.2047, 0.2098, 0.1948, 0.2067, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17594321072101593 tensor([0.2237, 0.1960, 0.1759, 0.2206, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18762312829494476 tensor([0.1951, 0.1876, 0.1965, 0.2031, 0.2177], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16624845564365387 tensor([0.2347, 0.2044, 0.1662, 0.2178, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19367043673992157 tensor([0.1975, 0.2035, 0.1990, 0.2063, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18041366338729858 tensor([0.1804, 0.1983, 0.2223, 0.1981, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1841263324022293 tensor([0.2120, 0.2030, 0.1855, 0.2154, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19028307497501373 tensor([0.1903, 0.1928, 0.2013, 0.2030, 0.2126], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17094707489013672 tensor([0.2293, 0.2082, 0.1709, 0.2161, 0.1755], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18632031977176666 tensor([0.2108, 0.2091, 0.1879, 0.2059, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18826906383037567 tensor([0.1883, 0.1995, 0.2108, 0.2022, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17326946556568146 tensor([0.2240, 0.1924, 0.1733, 0.2275, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1804172247648239 tensor([0.1804, 0.1861, 0.2163, 0.1991, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1675485372543335 tensor([0.2332, 0.2045, 0.1675, 0.2172, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1919005960226059 tensor([0.1957, 0.2107, 0.2035, 0.1981, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1846836507320404 tensor([0.1847, 0.1987, 0.2157, 0.1996, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1664455085992813 tensor([0.2311, 0.1961, 0.1664, 0.2301, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18735641241073608 tensor([0.1969, 0.1874, 0.1962, 0.2077, 0.2118], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17955073714256287 tensor([0.2202, 0.2086, 0.1796, 0.2104, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18935835361480713 tensor([0.1894, 0.2045, 0.2114, 0.2006, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1866634637117386 tensor([0.2023, 0.2042, 0.1981, 0.2088, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17765918374061584 tensor([0.2224, 0.2019, 0.1777, 0.2154, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17968569695949554 tensor([0.1797, 0.1851, 0.2150, 0.1969, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1810029000043869 tensor([0.2174, 0.2041, 0.1810, 0.2132, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18568386137485504 tensor([0.2062, 0.2110, 0.1903, 0.2069, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18220844864845276 tensor([0.1822, 0.1994, 0.2173, 0.1987, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18613822758197784 tensor([0.2093, 0.1988, 0.1861, 0.2157, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1872979700565338 tensor([0.1873, 0.1920, 0.2067, 0.1999, 0.2141], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17128872871398926 tensor([0.2303, 0.1979, 0.1713, 0.2187, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17997519671916962 tensor([0.2122, 0.2162, 0.1880, 0.2036, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18737182021141052 tensor([0.1988, 0.2071, 0.2007, 0.2059, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18103815615177155 tensor([0.2186, 0.2019, 0.1810, 0.2142, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19743402302265167 tensor([0.1974, 0.1981, 0.1980, 0.2051, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1873074471950531 tensor([0.2068, 0.2064, 0.1923, 0.2072, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19255506992340088 tensor([0.1926, 0.2088, 0.2046, 0.1999, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18217350542545319 tensor([0.1822, 0.1987, 0.2207, 0.2001, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19199971854686737 tensor([0.2020, 0.2055, 0.1948, 0.2058, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19079014658927917 tensor([0.1937, 0.1908, 0.1976, 0.2025, 0.2155], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16994261741638184 tensor([0.2310, 0.2027, 0.1699, 0.2181, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18896707892417908 tensor([0.2063, 0.2011, 0.1936, 0.2101, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1907951831817627 tensor([0.2070, 0.1993, 0.1908, 0.2120, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17360703647136688 tensor([0.2218, 0.1972, 0.1736, 0.2265, 0.1809], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18208420276641846 tensor([0.1821, 0.1866, 0.2112, 0.1974, 0.2226], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19216057658195496 tensor([0.2077, 0.1962, 0.1922, 0.2084, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17379845678806305 tensor([0.2202, 0.2195, 0.1795, 0.2070, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19315217435359955 tensor([0.1932, 0.1980, 0.2039, 0.2057, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17844527959823608 tensor([0.2172, 0.1947, 0.1784, 0.2286, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19421972334384918 tensor([0.2008, 0.1969, 0.1942, 0.2079, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18990178406238556 tensor([0.2062, 0.1983, 0.1912, 0.2144, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1966990828514099 tensor([0.1967, 0.1997, 0.2036, 0.2020, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18651728332042694 tensor([0.1865, 0.1956, 0.2133, 0.2019, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.176653191447258 tensor([0.2183, 0.1933, 0.1767, 0.2286, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18662236630916595 tensor([0.1866, 0.1915, 0.2090, 0.2007, 0.2121], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19004495441913605 tensor([0.1906, 0.1900, 0.2049, 0.2075, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17380686104297638 tensor([0.2238, 0.2122, 0.1738, 0.2137, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1848158985376358 tensor([0.1848, 0.1996, 0.2157, 0.2008, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17895251512527466 tensor([0.2186, 0.1969, 0.1790, 0.2223, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1908007115125656 tensor([0.1908, 0.1945, 0.2021, 0.2015, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17018187046051025 tensor([0.2306, 0.2046, 0.1702, 0.2173, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19038043916225433 tensor([0.2012, 0.2091, 0.1961, 0.2032, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19490821659564972 tensor([0.1949, 0.1997, 0.2021, 0.2032, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18504127860069275 tensor([0.2128, 0.1946, 0.1850, 0.2215, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19046247005462646 tensor([0.1978, 0.1905, 0.1945, 0.2074, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1659032255411148 tensor([0.2334, 0.2007, 0.1659, 0.2201, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19109052419662476 tensor([0.1993, 0.2095, 0.1990, 0.2011, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17574448883533478 tensor([0.1757, 0.1957, 0.2273, 0.1929, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17304334044456482 tensor([0.2271, 0.1975, 0.1730, 0.2224, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18238022923469543 tensor([0.1824, 0.1869, 0.2089, 0.1965, 0.2253], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1570429503917694 tensor([0.2469, 0.2045, 0.1570, 0.2219, 0.1696], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18984803557395935 tensor([0.2090, 0.1960, 0.1902, 0.2149, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18811453878879547 tensor([0.2058, 0.2027, 0.1922, 0.2111, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17675670981407166 tensor([0.2200, 0.2006, 0.1768, 0.2190, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18997064232826233 tensor([0.1925, 0.1900, 0.2017, 0.2083, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19612213969230652 tensor([0.1976, 0.2021, 0.2027, 0.2014, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1805112659931183 tensor([0.2154, 0.2111, 0.1819, 0.2111, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1836937963962555 tensor([0.1837, 0.2009, 0.2197, 0.1997, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.172167107462883 tensor([0.2231, 0.1980, 0.1722, 0.2271, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18574810028076172 tensor([0.1892, 0.1857, 0.2038, 0.2012, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18076279759407043 tensor([0.2188, 0.2046, 0.1808, 0.2137, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1888790726661682 tensor([0.2078, 0.2008, 0.1889, 0.2101, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18957115709781647 tensor([0.1896, 0.2018, 0.2105, 0.2023, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17950570583343506 tensor([0.2183, 0.1994, 0.1795, 0.2163, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1876867264509201 tensor([0.1877, 0.1966, 0.2074, 0.1981, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18699149787425995 tensor([0.2122, 0.2002, 0.1870, 0.2128, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19035126268863678 tensor([0.1961, 0.2088, 0.2024, 0.2024, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18987463414669037 tensor([0.1899, 0.2059, 0.2108, 0.2003, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18197908997535706 tensor([0.2168, 0.1988, 0.1820, 0.2172, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19266332685947418 tensor([0.1935, 0.1927, 0.2009, 0.2050, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18528684973716736 tensor([0.2112, 0.2018, 0.1853, 0.2118, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1721515953540802 tensor([0.2250, 0.2155, 0.1751, 0.2122, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18613484501838684 tensor([0.1861, 0.2026, 0.2151, 0.1989, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1741049736738205 tensor([0.2211, 0.1976, 0.1741, 0.2233, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19681909680366516 tensor([0.1968, 0.1982, 0.1974, 0.2018, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17786940932273865 tensor([0.2240, 0.2073, 0.1791, 0.2118, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18736769258975983 tensor([0.2046, 0.2058, 0.1935, 0.2088, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19337856769561768 tensor([0.1934, 0.2026, 0.2065, 0.2032, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17183279991149902 tensor([0.2254, 0.1991, 0.1718, 0.2259, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19285470247268677 tensor([0.1929, 0.1942, 0.2037, 0.2021, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17239972949028015 tensor([0.2303, 0.2034, 0.1724, 0.2193, 0.1746], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17694967985153198 tensor([0.2192, 0.2154, 0.1800, 0.2084, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19300958514213562 tensor([0.1960, 0.1930, 0.2008, 0.2091, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1841810941696167 tensor([0.2120, 0.1931, 0.1842, 0.2217, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17691975831985474 tensor([0.1769, 0.1846, 0.2153, 0.1975, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1696636825799942 tensor([0.2293, 0.2000, 0.1697, 0.2223, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1909857541322708 tensor([0.2008, 0.2111, 0.1944, 0.2027, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1829071044921875 tensor([0.1829, 0.1969, 0.2174, 0.2018, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17848294973373413 tensor([0.2194, 0.2058, 0.1785, 0.2177, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17932595312595367 tensor([0.1793, 0.1916, 0.2144, 0.1967, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.164597749710083 tensor([0.2373, 0.2026, 0.1646, 0.2207, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19128535687923431 tensor([0.1913, 0.2040, 0.2083, 0.1964, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19456620514392853 tensor([0.1946, 0.1970, 0.2050, 0.2070, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17728008329868317 tensor([0.2173, 0.1968, 0.1773, 0.2242, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1876385509967804 tensor([0.1876, 0.1928, 0.2063, 0.1994, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1661960333585739 tensor([0.2372, 0.2019, 0.1662, 0.2212, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18540963530540466 tensor([0.2112, 0.2070, 0.1854, 0.2105, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19502156972885132 tensor([0.1974, 0.2010, 0.2017, 0.2049, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17420431971549988 tensor([0.2224, 0.1956, 0.1742, 0.2259, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18796668946743011 tensor([0.1880, 0.1907, 0.2053, 0.1990, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16687127947807312 tensor([0.2332, 0.2079, 0.1669, 0.2176, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18208639323711395 tensor([0.2093, 0.2177, 0.1878, 0.2031, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1847507804632187 tensor([0.1848, 0.1994, 0.2162, 0.2035, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18155629932880402 tensor([0.2151, 0.1931, 0.1816, 0.2211, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.191181480884552 tensor([0.1936, 0.1912, 0.2010, 0.2051, 0.2091], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17679552733898163 tensor([0.2218, 0.2094, 0.1768, 0.2131, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19280250370502472 tensor([0.1941, 0.2060, 0.2036, 0.2036, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19399422407150269 tensor([0.1945, 0.2047, 0.2040, 0.2029, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16939254105091095 tensor([0.2280, 0.1936, 0.1694, 0.2272, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1905709058046341 tensor([0.1935, 0.1906, 0.1993, 0.2031, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17963506281375885 tensor([0.2223, 0.2043, 0.1796, 0.2133, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17380981147289276 tensor([0.2257, 0.2094, 0.1739, 0.2172, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19412539899349213 tensor([0.1941, 0.2050, 0.2028, 0.2026, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17704890668392181 tensor([0.2236, 0.2027, 0.1770, 0.2175, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17979243397712708 tensor([0.1798, 0.1873, 0.2163, 0.1987, 0.2178], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17185690999031067 tensor([0.2290, 0.2012, 0.1719, 0.2193, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1813976913690567 tensor([0.2163, 0.2059, 0.1842, 0.2122, 0.1814], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18862372636795044 tensor([0.1886, 0.2014, 0.2148, 0.2024, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17933401465415955 tensor([0.2193, 0.1993, 0.1793, 0.2219, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19466544687747955 tensor([0.2017, 0.1969, 0.1947, 0.2092, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18576692044734955 tensor([0.2113, 0.1983, 0.1899, 0.2147, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18932117521762848 tensor([0.2060, 0.2087, 0.1911, 0.2049, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18489894270896912 tensor([0.1849, 0.2087, 0.2166, 0.1981, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18041536211967468 tensor([0.2168, 0.2011, 0.1804, 0.2169, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1908956617116928 tensor([0.1909, 0.1913, 0.2005, 0.1998, 0.2175], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1829792559146881 tensor([0.2107, 0.2113, 0.1904, 0.2045, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18005388975143433 tensor([0.2157, 0.2166, 0.1828, 0.2049, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19262106716632843 tensor([0.1944, 0.1992, 0.2065, 0.2074, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1717042624950409 tensor([0.2250, 0.1972, 0.1717, 0.2281, 0.1780], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18287187814712524 tensor([0.1829, 0.1846, 0.2098, 0.1995, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1702079176902771 tensor([0.2303, 0.1987, 0.1702, 0.2222, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19624538719654083 tensor([0.1962, 0.1963, 0.2018, 0.2039, 0.2018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19564220309257507 tensor([0.1961, 0.1999, 0.2007, 0.2076, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17332568764686584 tensor([0.2246, 0.1990, 0.1733, 0.2192, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18312443792819977 tensor([0.1831, 0.1904, 0.2109, 0.1970, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15868216753005981 tensor([0.2437, 0.1996, 0.1587, 0.2252, 0.1729], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1932975798845291 tensor([0.1933, 0.2067, 0.2045, 0.2017, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19159086048603058 tensor([0.1997, 0.2018, 0.1990, 0.2079, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1866157352924347 tensor([0.2108, 0.1996, 0.1866, 0.2149, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1901938021183014 tensor([0.1935, 0.1902, 0.2003, 0.2039, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1791210174560547 tensor([0.2195, 0.2041, 0.1791, 0.2137, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19065214693546295 tensor([0.2035, 0.2039, 0.1936, 0.2083, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.187549889087677 tensor([0.1875, 0.2017, 0.2129, 0.2001, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18460775911808014 tensor([0.2091, 0.1918, 0.1846, 0.2236, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18587812781333923 tensor([0.1890, 0.1859, 0.2014, 0.2061, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1766987144947052 tensor([0.2227, 0.1995, 0.1767, 0.2136, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1907961517572403 tensor([0.1908, 0.2013, 0.2068, 0.1995, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904149204492569 tensor([0.2027, 0.2067, 0.1961, 0.2041, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1896081566810608 tensor([0.2055, 0.1959, 0.1898, 0.2192, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18434374034404755 tensor([0.1843, 0.1877, 0.2092, 0.1992, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18958373367786407 tensor([0.2088, 0.2019, 0.1896, 0.2084, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.175838440656662 tensor([0.2255, 0.2052, 0.1758, 0.2158, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18685601651668549 tensor([0.1869, 0.1976, 0.2144, 0.2031, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19591587781906128 tensor([0.1986, 0.1976, 0.1966, 0.2112, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1923311948776245 tensor([0.1923, 0.1944, 0.2016, 0.2002, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17610853910446167 tensor([0.2235, 0.2025, 0.1761, 0.2170, 0.1809], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1812790483236313 tensor([0.2075, 0.2186, 0.1905, 0.2021, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18792003393173218 tensor([0.1879, 0.2024, 0.2109, 0.2001, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17144937813282013 tensor([0.2268, 0.2019, 0.1714, 0.2226, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18767867982387543 tensor([0.1888, 0.1877, 0.2028, 0.2014, 0.2194], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17546899616718292 tensor([0.2245, 0.2015, 0.1755, 0.2173, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18581734597682953 tensor([0.2063, 0.2070, 0.1912, 0.2096, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.193883016705513 tensor([0.1939, 0.2026, 0.2060, 0.2023, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17997398972511292 tensor([0.2172, 0.1986, 0.1800, 0.2220, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1940407156944275 tensor([0.1986, 0.1940, 0.1967, 0.2075, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17268826067447662 tensor([0.2280, 0.2040, 0.1727, 0.2148, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18966130912303925 tensor([0.1922, 0.2120, 0.2069, 0.1993, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19181017577648163 tensor([0.1918, 0.1967, 0.2060, 0.2046, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1729985475540161 tensor([0.2252, 0.1935, 0.1730, 0.2259, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18658502399921417 tensor([0.1866, 0.1953, 0.2095, 0.1998, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16212864220142365 tensor([0.2377, 0.2058, 0.1621, 0.2208, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19613084197044373 tensor([0.1964, 0.2030, 0.2020, 0.2025, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1897846907377243 tensor([0.1898, 0.2030, 0.2132, 0.2012, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17987945675849915 tensor([0.2161, 0.1957, 0.1799, 0.2240, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18352766335010529 tensor([0.1835, 0.1895, 0.2099, 0.2001, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15956415235996246 tensor([0.2430, 0.2016, 0.1596, 0.2240, 0.1719], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1949298083782196 tensor([0.1949, 0.2033, 0.2030, 0.2032, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18689966201782227 tensor([0.1869, 0.1992, 0.2126, 0.2021, 0.1992], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17221270501613617 tensor([0.2302, 0.1998, 0.1722, 0.2189, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1853954792022705 tensor([0.1880, 0.1854, 0.2038, 0.2010, 0.2217], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19209936261177063 tensor([0.2059, 0.1976, 0.1921, 0.2118, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1860724836587906 tensor([0.1861, 0.2061, 0.2130, 0.1976, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18795262277126312 tensor([0.1880, 0.2037, 0.2135, 0.2000, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17197786271572113 tensor([0.2246, 0.2025, 0.1720, 0.2263, 0.1746], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18310467898845673 tensor([0.1831, 0.1885, 0.2107, 0.2022, 0.2156], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17213942110538483 tensor([0.2269, 0.1983, 0.1721, 0.2181, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19279257953166962 tensor([0.1973, 0.2058, 0.2017, 0.2024, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19023336470127106 tensor([0.1902, 0.2056, 0.2113, 0.2019, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17698721587657928 tensor([0.2181, 0.1939, 0.1770, 0.2290, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19084639847278595 tensor([0.1964, 0.1908, 0.1962, 0.2050, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16484126448631287 tensor([0.2358, 0.2063, 0.1648, 0.2191, 0.1740], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18264873325824738 tensor([0.2138, 0.2084, 0.1882, 0.2070, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18007953464984894 tensor([0.1801, 0.1974, 0.2195, 0.1996, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19153623282909393 tensor([0.2043, 0.1937, 0.1915, 0.2148, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18380920588970184 tensor([0.1838, 0.1891, 0.2093, 0.1981, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16449473798274994 tensor([0.2353, 0.2014, 0.1645, 0.2211, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17889109253883362 tensor([0.2165, 0.2154, 0.1816, 0.2076, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1820569485425949 tensor([0.1821, 0.2029, 0.2213, 0.1984, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17853876948356628 tensor([0.2203, 0.2038, 0.1785, 0.2137, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18539094924926758 tensor([0.1854, 0.1863, 0.2083, 0.2008, 0.2193], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16846692562103271 tensor([0.2334, 0.2041, 0.1685, 0.2181, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1933804303407669 tensor([0.1997, 0.2038, 0.1960, 0.2070, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19147300720214844 tensor([0.1963, 0.2079, 0.2016, 0.2027, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17512445151805878 tensor([0.2240, 0.1966, 0.1751, 0.2229, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1916663497686386 tensor([0.1929, 0.1917, 0.2010, 0.2031, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17178314924240112 tensor([0.2307, 0.2076, 0.1718, 0.2151, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18601484596729279 tensor([0.2123, 0.2006, 0.1860, 0.2142, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1932794749736786 tensor([0.1953, 0.2049, 0.2065, 0.2000, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17440694570541382 tensor([0.2220, 0.1997, 0.1744, 0.2205, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1830061674118042 tensor([0.1851, 0.1830, 0.2055, 0.2006, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1696399301290512 tensor([0.2308, 0.2054, 0.1696, 0.2183, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18358372151851654 tensor([0.2078, 0.2122, 0.1907, 0.2056, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18519580364227295 tensor([0.1852, 0.1934, 0.2106, 0.2017, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18352878093719482 tensor([0.2118, 0.1999, 0.1835, 0.2144, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18462567031383514 tensor([0.1846, 0.1868, 0.2086, 0.1982, 0.2218], grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [4], [0], [2], [2], [2], [2], [0], [2], [1], [2], [2], [4], [2], [1], [2], [4], [4], [2], [1], [2], [4], [0], [4], [0], [2], [4], [0], [2], [0], [2], [4], [0], [2], [1], [2], [0], [4], [2], [0], [2], [4], [0], [2], [0], [2], [4], [4], [2], [0], [4], [0], [0], [4], [1], [2], [4], [2], [2], [0], [2], [4], [0], [2], [2], [4], [0], [0], [2], [0], [1], [2], [0], [2], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [4], [4], [2], [1], [4], [4], [0], [2], [1], [2], [2], [0], [2], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [4], [4], [0], [2], [0], [2], [4], [1], [2], [0], [2], [4], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [4], [2], [0], [2], [4], [0], [2], [1], [2], [4], [4], [2], [1], [2], [4], [0], [2], [0], [2], [4], [0], [2], [2], [4], [4], [0], [2], [0], [4], [4], [4], [2], [0], [2], [0], [4], [2], [0], [2], [0], [4], [2], [1], [2], [4], [0], [2], [1], [2], [0], [4], [4], [0], [2], [2], [0], [4], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [4], [0], [2], [0], [2], [0], [0], [2], [1], [2], [0], [0], [2], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [4], [0], [2], [0], [2], [4], [4], [2], [1], [2], [2], [4], [2], [1], [2], [4], [0], [2], [0]]\n",
      "[[0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [4], [0], [2], [2], [2], [2], [0], [2], [1], [2], [2], [4], [2], [1], [2], [4], [4], [2], [1], [2], [4], [0], [4], [0], [2], [4], [0], [2], [0], [2], [4], [0], [2], [1], [2], [0], [4], [2], [0], [2], [4], [0], [2], [0], [2], [4], [4], [2], [0], [4], [0], [0], [4], [1], [2], [4], [2], [2], [0], [2], [4], [0], [2], [2], [4], [0], [0], [2], [0], [1], [2], [0], [2], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [4], [4], [2], [1], [4], [4], [0], [2], [1], [2], [2], [0], [2], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [4], [4], [0], [2], [0], [2], [4], [1], [2], [0], [2], [4], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [4], [2], [0], [2], [4], [0], [2], [1], [2], [4], [4], [2], [1], [2], [4], [0], [2], [0], [2], [4], [0], [2], [2], [4], [4], [0], [2], [0], [4], [4], [4], [2], [0], [2], [0], [4], [2], [0], [2], [0], [4], [2], [1], [2], [4], [0], [2], [1], [2], [0], [4], [4], [0], [2], [2], [0], [4], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [4], [0], [2], [0], [2], [0], [0], [2], [1], [2], [0], [0], [2], [0], [2], [4], [0], [2], [1], [2], [4], [0], [2], [0], [2], [4], [0], [2], [0], [2], [4], [4], [2], [1], [2], [2], [4], [2], [1], [2], [4], [0], [2], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005075691699981689  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005075650215148926  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005075573444366455  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005075462818145752  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.00507532262802124  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005075156211853027  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0050749659538269045  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005074752807617188  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005074521541595459  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005074273586273193  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005074008464813233  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005073729991912842  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.0050734391212463375  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005073136329650879  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005072822570800781  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005072502136230469  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0050721726417541505  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005071835517883301  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005071492195129395  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005071143627166748  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.17780502140522003 tensor([0.2311, 0.2035, 0.1673, 0.2204, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19459198415279388 tensor([0.2013, 0.2035, 0.1946, 0.2105, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19517390429973602 tensor([0.1934, 0.2001, 0.2043, 0.2070, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1772807389497757 tensor([0.2247, 0.1982, 0.1702, 0.2296, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1928912252187729 tensor([0.2043, 0.1929, 0.1874, 0.2127, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17774800956249237 tensor([0.2319, 0.2024, 0.1661, 0.2219, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19348877668380737 tensor([0.2025, 0.1991, 0.1918, 0.2131, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19973929226398468 tensor([0.1906, 0.1997, 0.2043, 0.2049, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1844777762889862 tensor([0.2146, 0.1957, 0.1776, 0.2276, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1879395693540573 tensor([0.1879, 0.1873, 0.2008, 0.2041, 0.2198], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17775845527648926 tensor([0.2293, 0.2027, 0.1689, 0.2214, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18862205743789673 tensor([0.2078, 0.2050, 0.1854, 0.2132, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19543807208538055 tensor([0.1954, 0.2069, 0.2029, 0.2068, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18376988172531128 tensor([0.2178, 0.1971, 0.1765, 0.2248, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19514374434947968 tensor([0.1971, 0.1941, 0.1951, 0.2116, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19170865416526794 tensor([0.2118, 0.1995, 0.1833, 0.2137, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19599810242652893 tensor([0.2001, 0.2082, 0.1960, 0.2052, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19199737906455994 tensor([0.2053, 0.2109, 0.1920, 0.2090, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1825951337814331 tensor([0.2243, 0.1970, 0.1732, 0.2229, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1939074695110321 tensor([0.1955, 0.1886, 0.1939, 0.2054, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17571192979812622 tensor([0.2353, 0.2053, 0.1636, 0.2200, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19627022743225098 tensor([0.1980, 0.2046, 0.1963, 0.2086, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19945968687534332 tensor([0.1809, 0.1995, 0.2195, 0.2003, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1826895773410797 tensor([0.2125, 0.2040, 0.1827, 0.2177, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19384148716926575 tensor([0.1908, 0.1938, 0.1987, 0.2052, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17439287900924683 tensor([0.2298, 0.2091, 0.1683, 0.2183, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18524570763111115 tensor([0.2113, 0.2101, 0.1852, 0.2081, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19822058081626892 tensor([0.1887, 0.2006, 0.2080, 0.2044, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18161660432815552 tensor([0.2246, 0.1934, 0.1706, 0.2298, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18707990646362305 tensor([0.1809, 0.1871, 0.2136, 0.2013, 0.2172], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17645610868930817 tensor([0.2337, 0.2054, 0.1649, 0.2195, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19627059996128082 tensor([0.1963, 0.2118, 0.2007, 0.2003, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19981665909290314 tensor([0.1852, 0.1998, 0.2129, 0.2018, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17515112459659576 tensor([0.2316, 0.1970, 0.1638, 0.2324, 0.1752], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19355221092700958 tensor([0.1974, 0.1883, 0.1936, 0.2100, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1800948828458786 tensor([0.2208, 0.2096, 0.1768, 0.2127, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19311250746250153 tensor([0.1898, 0.2056, 0.2087, 0.2028, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.195306196808815 tensor([0.2028, 0.2053, 0.1953, 0.2111, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18155543506145477 tensor([0.2229, 0.2029, 0.1750, 0.2177, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18610166013240814 tensor([0.1801, 0.1861, 0.2124, 0.1990, 0.2224], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18313369154930115 tensor([0.2179, 0.2051, 0.1783, 0.2155, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18755950033664703 tensor([0.2067, 0.2121, 0.1876, 0.2091, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20052722096443176 tensor([0.1827, 0.2005, 0.2145, 0.2009, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18896572291851044 tensor([0.2099, 0.1998, 0.1834, 0.2180, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19302381575107574 tensor([0.1878, 0.1930, 0.2040, 0.2021, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18074233829975128 tensor([0.2308, 0.1988, 0.1687, 0.2209, 0.1807], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1853123903274536 tensor([0.2127, 0.2173, 0.1853, 0.2057, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19798478484153748 tensor([0.1993, 0.2082, 0.1980, 0.2082, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18314632773399353 tensor([0.2192, 0.2029, 0.1783, 0.2165, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19533738493919373 tensor([0.1979, 0.1991, 0.1953, 0.2073, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18963173031806946 tensor([0.2073, 0.2074, 0.1896, 0.2094, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19304953515529633 tensor([0.1931, 0.2100, 0.2018, 0.2021, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19735506176948547 tensor([0.1827, 0.1998, 0.2178, 0.2024, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19198346138000488 tensor([0.2026, 0.2066, 0.1920, 0.2080, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19416414201259613 tensor([0.1942, 0.1918, 0.1950, 0.2047, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17704421281814575 tensor([0.2316, 0.2037, 0.1673, 0.2204, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1908610761165619 tensor([0.2068, 0.2021, 0.1909, 0.2124, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1898118555545807 tensor([0.2075, 0.2003, 0.1881, 0.2143, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17969565093517303 tensor([0.2224, 0.1981, 0.1709, 0.2289, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1876450628042221 tensor([0.1825, 0.1876, 0.2086, 0.1996, 0.2216], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1944516897201538 tensor([0.2083, 0.1972, 0.1895, 0.2107, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1768300086259842 tensor([0.2207, 0.2206, 0.1768, 0.2092, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982315480709076 tensor([0.1936, 0.1991, 0.2011, 0.2079, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17996294796466827 tensor([0.2178, 0.1956, 0.1756, 0.2310, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19794344902038574 tensor([0.2013, 0.1979, 0.1915, 0.2102, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18843305110931396 tensor([0.2068, 0.1993, 0.1884, 0.2167, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19684883952140808 tensor([0.1972, 0.2008, 0.2009, 0.2042, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1966952681541443 tensor([0.1870, 0.1967, 0.2105, 0.2042, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18202577531337738 tensor([0.2189, 0.1942, 0.1739, 0.2310, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19256795942783356 tensor([0.1871, 0.1926, 0.2063, 0.2030, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19106325507164001 tensor([0.1911, 0.1911, 0.2021, 0.2098, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1754414439201355 tensor([0.2243, 0.2132, 0.1711, 0.2159, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19803935289382935 tensor([0.1853, 0.2007, 0.2129, 0.2030, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18219225108623505 tensor([0.2191, 0.1978, 0.1762, 0.2246, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19557760655879974 tensor([0.1913, 0.1956, 0.1995, 0.2038, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17619699239730835 tensor([0.2311, 0.2055, 0.1676, 0.2196, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19332654774188995 tensor([0.2017, 0.2102, 0.1933, 0.2054, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19909431040287018 tensor([0.1954, 0.2008, 0.1993, 0.2054, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18491131067276 tensor([0.2134, 0.1955, 0.1822, 0.2239, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19181141257286072 tensor([0.1983, 0.1915, 0.1918, 0.2097, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1786457747220993 tensor([0.2340, 0.2017, 0.1633, 0.2224, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19625061750411987 tensor([0.1999, 0.2106, 0.1963, 0.2033, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19504059851169586 tensor([0.1762, 0.1969, 0.2246, 0.1950, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17873185873031616 tensor([0.2277, 0.1985, 0.1703, 0.2248, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1879161149263382 tensor([0.1828, 0.1879, 0.2063, 0.1986, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16859662532806396 tensor([0.2473, 0.2053, 0.1547, 0.2242, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18739283084869385 tensor([0.2096, 0.1970, 0.1874, 0.2173, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18948619067668915 tensor([0.2064, 0.2037, 0.1895, 0.2134, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1825464367866516 tensor([0.2206, 0.2015, 0.1741, 0.2213, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19302710890769958 tensor([0.1930, 0.1910, 0.1990, 0.2105, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19815151393413544 tensor([0.1982, 0.2032, 0.2000, 0.2036, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17919281125068665 tensor([0.2160, 0.2121, 0.1792, 0.2133, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19500260055065155 tensor([0.1842, 0.2020, 0.2169, 0.2019, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17851880192756653 tensor([0.2237, 0.1989, 0.1695, 0.2294, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18967321515083313 tensor([0.1897, 0.1867, 0.2012, 0.2034, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18100497126579285 tensor([0.2194, 0.2056, 0.1780, 0.2160, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19133654236793518 tensor([0.2084, 0.2018, 0.1862, 0.2123, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1948845237493515 tensor([0.1900, 0.2029, 0.2077, 0.2045, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1853228509426117 tensor([0.2189, 0.2004, 0.1768, 0.2186, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19768016040325165 tensor([0.1882, 0.1977, 0.2047, 0.2003, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18672940135002136 tensor([0.2127, 0.2012, 0.1843, 0.2151, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19660323858261108 tensor([0.1966, 0.2099, 0.1996, 0.2046, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19213560223579407 tensor([0.1904, 0.2070, 0.2081, 0.2025, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18413083255290985 tensor([0.2173, 0.1998, 0.1792, 0.2195, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1939936727285385 tensor([0.1940, 0.1937, 0.1982, 0.2073, 0.2068], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18865518271923065 tensor([0.2118, 0.2028, 0.1826, 0.2141, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1724269837141037 tensor([0.2256, 0.2165, 0.1724, 0.2144, 0.1711], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1963263899087906 tensor([0.1866, 0.2037, 0.2123, 0.2011, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1826225072145462 tensor([0.2217, 0.1986, 0.1714, 0.2257, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19478027522563934 tensor([0.1973, 0.1992, 0.1948, 0.2039, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17639897763729095 tensor([0.2246, 0.2083, 0.1764, 0.2140, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1907125860452652 tensor([0.2051, 0.2069, 0.1907, 0.2110, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19331783056259155 tensor([0.1939, 0.2037, 0.2038, 0.2054, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17660966515541077 tensor([0.2260, 0.2001, 0.1691, 0.2282, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19527219235897064 tensor([0.1933, 0.1953, 0.2010, 0.2043, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17349494993686676 tensor([0.2308, 0.2044, 0.1697, 0.2215, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17733614146709442 tensor([0.2198, 0.2165, 0.1773, 0.2106, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19645459949970245 tensor([0.1965, 0.1940, 0.1981, 0.2114, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1878279447555542 tensor([0.2126, 0.1941, 0.1814, 0.2240, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1855831742286682 tensor([0.1774, 0.1856, 0.2126, 0.1998, 0.2247], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1775549203157425 tensor([0.2299, 0.2009, 0.1670, 0.2246, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19170233607292175 tensor([0.2013, 0.2122, 0.1917, 0.2049, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19795139133930206 tensor([0.1834, 0.1980, 0.2146, 0.2040, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17754635214805603 tensor([0.2200, 0.2068, 0.1757, 0.2200, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19269859790802002 tensor([0.1798, 0.1927, 0.2117, 0.1989, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17374049127101898 tensor([0.2378, 0.2035, 0.1620, 0.2229, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19860973954200745 tensor([0.1918, 0.2051, 0.2056, 0.1986, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19534718990325928 tensor([0.1951, 0.1981, 0.2022, 0.2093, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833076924085617 tensor([0.2178, 0.1977, 0.1745, 0.2266, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19381935894489288 tensor([0.1881, 0.1938, 0.2036, 0.2016, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17234982550144196 tensor([0.2378, 0.2028, 0.1636, 0.2235, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18476077914237976 tensor([0.2117, 0.2081, 0.1827, 0.2127, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19789472222328186 tensor([0.1979, 0.2021, 0.1989, 0.2072, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18066781759262085 tensor([0.2230, 0.1966, 0.1715, 0.2283, 0.1807], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19175635278224945 tensor([0.1884, 0.1918, 0.2027, 0.2012, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17324775457382202 tensor([0.2338, 0.2089, 0.1642, 0.2199, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18507975339889526 tensor([0.2099, 0.2188, 0.1851, 0.2053, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19511860609054565 tensor([0.1852, 0.2005, 0.2133, 0.2058, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18792355060577393 tensor([0.2157, 0.1941, 0.1788, 0.2235, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19409769773483276 tensor([0.1941, 0.1922, 0.1983, 0.2074, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17773602902889252 tensor([0.2224, 0.2104, 0.1741, 0.2154, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19459304213523865 tensor([0.1946, 0.2071, 0.2008, 0.2058, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1950215995311737 tensor([0.1950, 0.2058, 0.2012, 0.2051, 0.1929], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18061377108097076 tensor([0.2286, 0.1945, 0.1667, 0.2295, 0.1806], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.193961039185524 tensor([0.1940, 0.1916, 0.1967, 0.2053, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.179353266954422 tensor([0.2229, 0.2053, 0.1769, 0.2156, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17125362157821655 tensor([0.2262, 0.2104, 0.1713, 0.2194, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19430769979953766 tensor([0.1946, 0.2061, 0.2001, 0.2049, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17797881364822388 tensor([0.2242, 0.2036, 0.1743, 0.2198, 0.1780], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18838317692279816 tensor([0.1803, 0.1884, 0.2136, 0.2010, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17749425768852234 tensor([0.2296, 0.2021, 0.1692, 0.2216, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1814545840024948 tensor([0.2168, 0.2070, 0.1815, 0.2145, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19180424511432648 tensor([0.1891, 0.2025, 0.2120, 0.2046, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1789519339799881 tensor([0.2199, 0.2003, 0.1766, 0.2243, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1963539719581604 tensor([0.2023, 0.1979, 0.1919, 0.2115, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18708789348602295 tensor([0.2119, 0.1994, 0.1871, 0.2171, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18837003409862518 tensor([0.2065, 0.2098, 0.1884, 0.2071, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19079676270484924 tensor([0.1853, 0.2098, 0.2138, 0.2002, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18363526463508606 tensor([0.2174, 0.2021, 0.1777, 0.2192, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19232386350631714 tensor([0.1914, 0.1923, 0.1979, 0.2020, 0.2164], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18769624829292297 tensor([0.2113, 0.2124, 0.1877, 0.2067, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1801355630159378 tensor([0.2162, 0.2177, 0.1801, 0.2070, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19493789970874786 tensor([0.1949, 0.2003, 0.2036, 0.2096, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1767999827861786 tensor([0.2256, 0.1981, 0.1690, 0.2305, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18553896248340607 tensor([0.1833, 0.1855, 0.2072, 0.2017, 0.2223], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17737998068332672 tensor([0.2309, 0.1997, 0.1675, 0.2245, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19732101261615753 tensor([0.1967, 0.1973, 0.1991, 0.2061, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19664260745048523 tensor([0.1966, 0.2010, 0.1979, 0.2099, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1826496422290802 tensor([0.2252, 0.2000, 0.1706, 0.2215, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19146406650543213 tensor([0.1836, 0.1915, 0.2082, 0.1992, 0.2175], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17177535593509674 tensor([0.2441, 0.2005, 0.1562, 0.2274, 0.1718], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19283679127693176 tensor([0.1938, 0.2078, 0.2017, 0.2039, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19622071087360382 tensor([0.2003, 0.2029, 0.1962, 0.2101, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.186944380402565 tensor([0.2113, 0.2006, 0.1839, 0.2173, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19399511814117432 tensor([0.1940, 0.1912, 0.1976, 0.2061, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18245582282543182 tensor([0.2200, 0.2051, 0.1764, 0.2159, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1908528059720993 tensor([0.2041, 0.2050, 0.1909, 0.2105, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19674280285835266 tensor([0.1880, 0.2028, 0.2101, 0.2023, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1896969974040985 tensor([0.2097, 0.1928, 0.1818, 0.2260, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1895015686750412 tensor([0.1895, 0.1869, 0.1987, 0.2084, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1862446367740631 tensor([0.2233, 0.2005, 0.1740, 0.2159, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2005288004875183 tensor([0.1913, 0.2024, 0.2041, 0.2016, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19339533150196075 tensor([0.2032, 0.2078, 0.1934, 0.2063, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1869766116142273 tensor([0.2061, 0.1969, 0.1870, 0.2216, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18867407739162445 tensor([0.1848, 0.1887, 0.2066, 0.2014, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19023387134075165 tensor([0.2094, 0.2029, 0.1869, 0.2106, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17653588950634003 tensor([0.2261, 0.2061, 0.1731, 0.2181, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19699101150035858 tensor([0.1873, 0.1987, 0.2116, 0.2053, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1938716173171997 tensor([0.1992, 0.1987, 0.1939, 0.2135, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19544446468353271 tensor([0.1928, 0.1954, 0.1989, 0.2024, 0.2104], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17977158725261688 tensor([0.2241, 0.2035, 0.1734, 0.2193, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1877736747264862 tensor([0.2080, 0.2198, 0.1878, 0.2043, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1975763887166977 tensor([0.1884, 0.2035, 0.2082, 0.2023, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17607180774211884 tensor([0.2274, 0.2028, 0.1688, 0.2250, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1892402172088623 tensor([0.1892, 0.1887, 0.2001, 0.2036, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18014198541641235 tensor([0.2251, 0.2024, 0.1728, 0.2196, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1884603649377823 tensor([0.2069, 0.2080, 0.1885, 0.2119, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19413521885871887 tensor([0.1944, 0.2037, 0.2032, 0.2045, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18105056881904602 tensor([0.2178, 0.1996, 0.1772, 0.2244, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19402188062667847 tensor([0.1991, 0.1951, 0.1940, 0.2098, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17939253151416779 tensor([0.2286, 0.2049, 0.1700, 0.2171, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19268083572387695 tensor([0.1927, 0.2131, 0.2041, 0.2015, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19773560762405396 tensor([0.1923, 0.1977, 0.2032, 0.2069, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1812724620103836 tensor([0.2258, 0.1944, 0.1703, 0.2283, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1963822841644287 tensor([0.1871, 0.1964, 0.2067, 0.2020, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17239318788051605 tensor([0.2382, 0.2067, 0.1596, 0.2231, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19692614674568176 tensor([0.1969, 0.2040, 0.1993, 0.2047, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19175533950328827 tensor([0.1903, 0.2042, 0.2104, 0.2034, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18311361968517303 tensor([0.2167, 0.1966, 0.1771, 0.2264, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1905641108751297 tensor([0.1840, 0.1906, 0.2072, 0.2023, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17079953849315643 tensor([0.2434, 0.2024, 0.1572, 0.2262, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19444136321544647 tensor([0.1955, 0.2044, 0.2002, 0.2055, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19819127023220062 tensor([0.1874, 0.2003, 0.2098, 0.2043, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17780375480651855 tensor([0.2307, 0.2007, 0.1696, 0.2212, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18847933411598206 tensor([0.1885, 0.1864, 0.2012, 0.2032, 0.2207], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19144682586193085 tensor([0.2064, 0.1987, 0.1893, 0.2141, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1962389349937439 tensor([0.1865, 0.2072, 0.2103, 0.1997, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1939326375722885 tensor([0.1884, 0.2048, 0.2106, 0.2022, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17351126670837402 tensor([0.2251, 0.2034, 0.1693, 0.2287, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1894860565662384 tensor([0.1836, 0.1895, 0.2080, 0.2044, 0.2145], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1833469122648239 tensor([0.2275, 0.1992, 0.1695, 0.2204, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19781582057476044 tensor([0.1978, 0.2069, 0.1989, 0.2046, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19001036882400513 tensor([0.1907, 0.2067, 0.2085, 0.2041, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18087150156497955 tensor([0.2187, 0.1948, 0.1742, 0.2314, 0.1809], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19357264041900635 tensor([0.1969, 0.1918, 0.1936, 0.2073, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1727800816297531 tensor([0.2364, 0.2072, 0.1622, 0.2214, 0.1728], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18543805181980133 tensor([0.2144, 0.2094, 0.1854, 0.2092, 0.1815], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1985308825969696 tensor([0.1805, 0.1985, 0.2167, 0.2019, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19455492496490479 tensor([0.2048, 0.1947, 0.1887, 0.2171, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19015184044837952 tensor([0.1843, 0.1902, 0.2066, 0.2002, 0.2187], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17652666568756104 tensor([0.2359, 0.2023, 0.1619, 0.2234, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1789121776819229 tensor([0.2171, 0.2165, 0.1789, 0.2098, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19445140659809113 tensor([0.1825, 0.2040, 0.2185, 0.2006, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18248791992664337 tensor([0.2209, 0.2048, 0.1759, 0.2160, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18729351460933685 tensor([0.1859, 0.1873, 0.2056, 0.2030, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1747387945652008 tensor([0.2340, 0.2051, 0.1658, 0.2204, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19329501688480377 tensor([0.2003, 0.2049, 0.1933, 0.2092, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19683776795864105 tensor([0.1968, 0.2090, 0.1988, 0.2050, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18016855418682098 tensor([0.2246, 0.1976, 0.1724, 0.2253, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19336673617362976 tensor([0.1934, 0.1927, 0.1984, 0.2053, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17357653379440308 tensor([0.2313, 0.2086, 0.1691, 0.2174, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18566808104515076 tensor([0.2129, 0.2016, 0.1833, 0.2165, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1958514302968979 tensor([0.1959, 0.2060, 0.2037, 0.2022, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1822228729724884 tensor([0.2225, 0.2007, 0.1717, 0.2228, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18555116653442383 tensor([0.1856, 0.1840, 0.2029, 0.2028, 0.2248], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17470219731330872 tensor([0.2314, 0.2063, 0.1670, 0.2206, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18795904517173767 tensor([0.2084, 0.2133, 0.1880, 0.2079, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19446539878845215 tensor([0.1856, 0.1945, 0.2079, 0.2039, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1892492175102234 tensor([0.2123, 0.2009, 0.1808, 0.2167, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18783465027809143 tensor([0.1851, 0.1878, 0.2060, 0.2003, 0.2207], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [4, 2], [0, 4], [2, 4], [2, 1], [2, 4], [2, 4], [0, 1], [2, 4], [1, 0], [2, 4], [2, 4], [4, 0], [2, 4], [1, 2], [2, 4], [4, 2], [4, 2], [2, 4], [1, 2], [2, 4], [4, 2], [0, 1], [4, 2], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0], [0, 1], [2, 4], [1, 2], [2, 4], [0, 4], [4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [0], [2, 4], [0, 1], [2, 4], [4, 2], [4, 2], [2, 4], [0, 2], [4, 2], [0, 4], [0, 4], [4, 2], [1, 0], [2, 4], [4, 2], [2, 4], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [2, 1], [4, 2], [0, 4], [0, 1], [2, 4], [0, 1], [1, 0], [2, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [1, 2], [2, 4], [4, 2], [0, 3], [2, 4], [0, 1], [2, 4], [4, 2], [4, 2], [2, 4], [1, 0], [4, 0], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [2, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0], [0, 4], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [0, 2], [4, 2], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [1, 0], [2, 4], [0, 1], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1], [2, 4], [0, 3], [0, 4], [2, 4], [0, 1], [2, 4], [2, 4], [4, 0], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [4, 0], [4, 0], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [2, 4], [4, 2], [4, 2], [0, 4], [2, 4], [0, 1], [4, 2], [4, 2], [4, 0], [2, 4], [0, 1], [2, 4], [0, 1], [4, 0], [2, 4], [0, 1], [2, 4], [0, 4], [4, 2], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [0], [4, 2], [4, 2], [0, 1], [2, 4], [2, 4], [0, 4], [4, 2], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [1, 2], [2, 4], [4, 0], [0, 1], [2, 4], [0, 1], [2, 4], [4, 0], [0, 4], [2, 4], [0, 1], [2, 4], [0, 4], [0, 4], [2, 4], [1, 0], [2, 4], [0, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0], [0, 4], [2, 4], [1, 2], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [4, 0], [2, 4], [1, 0], [2, 4], [2, 4], [4, 0], [2, 4], [1, 0], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1]]\n",
      "[[0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [4, 2], [0, 4], [2, 4], [2, 1], [2, 4], [2, 4], [0, 1], [2, 4], [1, 0], [2, 4], [2, 4], [4, 0], [2, 4], [1, 2], [2, 4], [4, 2], [4, 2], [2, 4], [1, 2], [2, 4], [4, 2], [0, 1], [4, 2], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0], [0, 1], [2, 4], [1, 2], [2, 4], [0, 4], [4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [4, 2], [2, 4], [0, 2], [4, 2], [0, 4], [0, 4], [4, 2], [1, 0], [2, 4], [4, 2], [2, 4], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [2, 1], [4, 2], [0, 4], [0, 1], [2, 4], [0, 1], [1, 0], [2, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [1, 2], [2, 4], [4, 2], [0, 3], [2, 4], [0, 1], [2, 4], [4, 2], [4, 2], [2, 4], [1, 0], [4, 0], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [2, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0], [0, 4], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [0, 2], [4, 2], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [1, 0], [2, 4], [0, 1], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1], [2, 4], [0, 3], [0, 4], [2, 4], [0, 1], [2, 4], [2, 4], [4, 0], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [4, 0], [4, 0], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [2, 4], [4, 2], [4, 2], [0, 4], [2, 4], [0, 1], [4, 2], [4, 2], [4, 0], [2, 4], [0, 1], [2, 4], [0, 1], [4, 0], [2, 4], [0, 1], [2, 4], [0, 4], [4, 2], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [4, 2], [4, 2], [0, 1], [2, 4], [2, 4], [0, 4], [4, 2], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0], [2, 4], [4, 2], [0, 4], [2, 4], [1, 2], [2, 4], [4, 0], [0, 1], [2, 4], [0, 1], [2, 4], [4, 0], [0, 4], [2, 4], [0, 1], [2, 4], [0, 4], [0, 4], [2, 4], [1, 0], [2, 4], [0, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0], [0, 4], [2, 4], [1, 2], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [4, 0], [2, 4], [1, 0], [2, 4], [2, 4], [4, 0], [2, 4], [1, 0], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005136891238151058  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005136811444836278  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005136658587763386  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.005136442280584766  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005136166368761371  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005135838062532486  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005135462649406925  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005135045897576117  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005134590691135776  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0051341023175947124  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005133582219000786  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005133037605593281  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005132465593276485  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005131873872972304  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005131261963998118  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00513063371181488  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005129988635739972  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005129332023282205  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.005128661951711101  Accuracy on Support set:0.0\n",
      "torch.Size([248, 2048]) torch.Size([248])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005127981785804995  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.2043071687221527 tensor([0.2338, 0.2043, 0.1654, 0.2229, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20387747883796692 tensor([0.2039, 0.2045, 0.1926, 0.2132, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20113027095794678 tensor([0.1960, 0.2011, 0.2023, 0.2097, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1990251988172531 tensor([0.2275, 0.1990, 0.1683, 0.2322, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1982497125864029 tensor([0.2070, 0.1939, 0.1855, 0.2154, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20321258902549744 tensor([0.2347, 0.2032, 0.1642, 0.2245, 0.1734], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20011070370674133 tensor([0.2052, 0.2001, 0.1899, 0.2157, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19608749449253082 tensor([0.1932, 0.2008, 0.2024, 0.2076, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19656480848789215 tensor([0.2173, 0.1966, 0.1757, 0.2304, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1991017758846283 tensor([0.1905, 0.1884, 0.1991, 0.2068, 0.2152], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20351439714431763 tensor([0.2320, 0.2035, 0.1670, 0.2240, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20600299537181854 tensor([0.2105, 0.2060, 0.1834, 0.2158, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20090366899967194 tensor([0.1980, 0.2080, 0.2009, 0.2095, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1979488879442215 tensor([0.2206, 0.1979, 0.1746, 0.2275, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1976049393415451 tensor([0.1996, 0.1952, 0.1933, 0.2143, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2004825919866562 tensor([0.2145, 0.2005, 0.1813, 0.2164, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20264804363250732 tensor([0.2026, 0.2092, 0.1940, 0.2077, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20790380239486694 tensor([0.2079, 0.2119, 0.1899, 0.2115, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1978500485420227 tensor([0.2271, 0.1979, 0.1713, 0.2255, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19814848899841309 tensor([0.1981, 0.1896, 0.1921, 0.2081, 0.2120], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20614580810070038 tensor([0.2381, 0.2061, 0.1618, 0.2225, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20057202875614166 tensor([0.2006, 0.2057, 0.1943, 0.2112, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19570060074329376 tensor([0.1833, 0.2006, 0.2175, 0.2029, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20497097074985504 tensor([0.2152, 0.2050, 0.1807, 0.2204, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19684414565563202 tensor([0.1933, 0.1949, 0.1968, 0.2079, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20997758209705353 tensor([0.2326, 0.2100, 0.1664, 0.2208, 0.1702], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21065287292003632 tensor([0.2139, 0.2111, 0.1833, 0.2107, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017209678888321 tensor([0.1912, 0.2017, 0.2061, 0.2071, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19423387944698334 tensor([0.2274, 0.1942, 0.1687, 0.2325, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20401014387607574 tensor([0.1833, 0.1882, 0.2117, 0.2040, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20625917613506317 tensor([0.2365, 0.2063, 0.1630, 0.2220, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19873997569084167 tensor([0.1989, 0.2130, 0.1987, 0.2029, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1959303766489029 tensor([0.1877, 0.2010, 0.2109, 0.2045, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.197804793715477 tensor([0.2343, 0.1978, 0.1620, 0.2350, 0.1709], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19998636841773987 tensor([0.2000, 0.1893, 0.1918, 0.2127, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2105645090341568 tensor([0.2236, 0.2106, 0.1749, 0.2152, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20540812611579895 tensor([0.1923, 0.2067, 0.2067, 0.2054, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.205442413687706 tensor([0.2054, 0.2063, 0.1933, 0.2137, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2037506401538849 tensor([0.2256, 0.2038, 0.1731, 0.2202, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20163583755493164 tensor([0.1824, 0.1871, 0.2108, 0.2016, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20606990158557892 tensor([0.2207, 0.2061, 0.1764, 0.2181, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2093430906534195 tensor([0.2093, 0.2131, 0.1856, 0.2116, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1971886307001114 tensor([0.1851, 0.2017, 0.2126, 0.2035, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20078535377979279 tensor([0.2125, 0.2008, 0.1815, 0.2206, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20224300026893616 tensor([0.1903, 0.1941, 0.2022, 0.2048, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19963547587394714 tensor([0.2335, 0.1996, 0.1669, 0.2234, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20818300545215607 tensor([0.2153, 0.2183, 0.1834, 0.2082, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20186971127986908 tensor([0.2019, 0.2093, 0.1960, 0.2108, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20384842157363892 tensor([0.2219, 0.2038, 0.1764, 0.2190, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19593848288059235 tensor([0.2005, 0.2001, 0.1934, 0.2100, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20839948952198029 tensor([0.2098, 0.2084, 0.1878, 0.2119, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19986096024513245 tensor([0.1956, 0.2111, 0.1999, 0.2047, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20093826949596405 tensor([0.1852, 0.2009, 0.2158, 0.2051, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20521163940429688 tensor([0.2052, 0.2076, 0.1900, 0.2106, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19316662847995758 tensor([0.1968, 0.1929, 0.1932, 0.2074, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20454749464988708 tensor([0.2344, 0.2045, 0.1653, 0.2230, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20310159027576447 tensor([0.2094, 0.2031, 0.1889, 0.2150, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20130951702594757 tensor([0.2101, 0.2013, 0.1862, 0.2169, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1989849954843521 tensor([0.2251, 0.1990, 0.1690, 0.2316, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20226842164993286 tensor([0.1849, 0.1887, 0.2069, 0.2023, 0.2172], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19811511039733887 tensor([0.2108, 0.1981, 0.1877, 0.2132, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21166306734085083 tensor([0.2233, 0.2214, 0.1748, 0.2117, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19919461011886597 tensor([0.1962, 0.2002, 0.1992, 0.2106, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19649828970432281 tensor([0.2205, 0.1965, 0.1737, 0.2337, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1945955902338028 tensor([0.2040, 0.1990, 0.1896, 0.2129, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20024406909942627 tensor([0.2095, 0.2002, 0.1865, 0.2194, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19904689490795135 tensor([0.1997, 0.2018, 0.1990, 0.2068, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19734178483486176 tensor([0.1895, 0.1978, 0.2086, 0.2068, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19506044685840607 tensor([0.2216, 0.1951, 0.1720, 0.2337, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20443418622016907 tensor([0.1896, 0.1937, 0.2044, 0.2057, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20027048885822296 tensor([0.1936, 0.1921, 0.2003, 0.2125, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21410423517227173 tensor([0.2270, 0.2141, 0.1692, 0.2184, 0.1712], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20188960433006287 tensor([0.1877, 0.2019, 0.2109, 0.2057, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1987271010875702 tensor([0.2218, 0.1987, 0.1743, 0.2272, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976018100976944 tensor([0.1939, 0.1967, 0.1976, 0.2064, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20638476312160492 tensor([0.2338, 0.2064, 0.1657, 0.2221, 0.1720], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2043306976556778 tensor([0.2043, 0.2113, 0.1914, 0.2080, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19742079079151154 tensor([0.1980, 0.2019, 0.1974, 0.2080, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19650951027870178 tensor([0.2160, 0.1965, 0.1803, 0.2266, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20089304447174072 tensor([0.2009, 0.1925, 0.1900, 0.2124, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20251134037971497 tensor([0.2368, 0.2025, 0.1614, 0.2250, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20249268412590027 tensor([0.2025, 0.2117, 0.1942, 0.2059, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19807299971580505 tensor([0.1785, 0.1981, 0.2227, 0.1976, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19932423532009125 tensor([0.2304, 0.1993, 0.1684, 0.2274, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2012769877910614 tensor([0.1852, 0.1890, 0.2046, 0.2013, 0.2199], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20585018396377563 tensor([0.2497, 0.2059, 0.1532, 0.2266, 0.1647], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19799716770648956 tensor([0.2123, 0.1980, 0.1854, 0.2199, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20471513271331787 tensor([0.2090, 0.2047, 0.1875, 0.2161, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2024194449186325 tensor([0.2233, 0.2024, 0.1722, 0.2239, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19716805219650269 tensor([0.1956, 0.1920, 0.1972, 0.2133, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19814668595790863 tensor([0.2007, 0.2042, 0.1981, 0.2061, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21309636533260345 tensor([0.2187, 0.2131, 0.1772, 0.2159, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2031896859407425 tensor([0.1866, 0.2032, 0.2148, 0.2045, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19973261654376984 tensor([0.2265, 0.1997, 0.1675, 0.2321, 0.1742], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.199447900056839 tensor([0.1922, 0.1878, 0.1994, 0.2061, 0.2145], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20655414462089539 tensor([0.2221, 0.2066, 0.1761, 0.2186, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20275545120239258 tensor([0.2110, 0.2028, 0.1843, 0.2150, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2039826661348343 tensor([0.1925, 0.2040, 0.2057, 0.2071, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20130525529384613 tensor([0.2216, 0.2013, 0.1749, 0.2212, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20283867418766022 tensor([0.1907, 0.1988, 0.2028, 0.2029, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202164888381958 tensor([0.2154, 0.2022, 0.1823, 0.2177, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19760818779468536 tensor([0.1992, 0.2110, 0.1976, 0.2072, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2050577998161316 tensor([0.1929, 0.2081, 0.2060, 0.2051, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20075549185276031 tensor([0.2201, 0.2008, 0.1773, 0.2221, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19638419151306152 tensor([0.1966, 0.1947, 0.1964, 0.2100, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20381054282188416 tensor([0.2145, 0.2038, 0.1806, 0.2168, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21692100167274475 tensor([0.2282, 0.2172, 0.1705, 0.2169, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20365789532661438 tensor([0.1890, 0.2048, 0.2104, 0.2037, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1994294375181198 tensor([0.2244, 0.1994, 0.1695, 0.2283, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20026342570781708 tensor([0.1999, 0.2003, 0.1930, 0.2065, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.209126278758049 tensor([0.2272, 0.2091, 0.1745, 0.2165, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20775018632411957 tensor([0.2078, 0.2079, 0.1887, 0.2136, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.201802060008049 tensor([0.1964, 0.2047, 0.2018, 0.2080, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20091122388839722 tensor([0.2287, 0.2009, 0.1672, 0.2309, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1991630345582962 tensor([0.1959, 0.1963, 0.1992, 0.2069, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20521138608455658 tensor([0.2335, 0.2052, 0.1679, 0.2240, 0.1694], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2130538821220398 tensor([0.2224, 0.2174, 0.1754, 0.2131, 0.1718], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19569790363311768 tensor([0.1990, 0.1950, 0.1962, 0.2141, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19504009187221527 tensor([0.2154, 0.1950, 0.1794, 0.2268, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20249398052692413 tensor([0.1797, 0.1867, 0.2109, 0.2025, 0.2202], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20179049670696259 tensor([0.2327, 0.2018, 0.1651, 0.2272, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20387083292007446 tensor([0.2039, 0.2133, 0.1898, 0.2074, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1958068162202835 tensor([0.1858, 0.1991, 0.2126, 0.2067, 0.1958], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2076747566461563 tensor([0.2227, 0.2077, 0.1737, 0.2226, 0.1733], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20151835680007935 tensor([0.1821, 0.1938, 0.2099, 0.2015, 0.2126], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20425032079219818 tensor([0.2405, 0.2043, 0.1603, 0.2254, 0.1696], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19465599954128265 tensor([0.1943, 0.2062, 0.2036, 0.2012, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19913828372955322 tensor([0.1976, 0.1991, 0.2003, 0.2119, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1986469030380249 tensor([0.2205, 0.1986, 0.1726, 0.2293, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20182593166828156 tensor([0.1906, 0.1949, 0.2018, 0.2043, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2036256343126297 tensor([0.2406, 0.2036, 0.1617, 0.2260, 0.1681], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20903827250003815 tensor([0.2144, 0.2090, 0.1808, 0.2153, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19696833193302155 tensor([0.2005, 0.2031, 0.1970, 0.2098, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19744008779525757 tensor([0.2257, 0.1974, 0.1696, 0.2309, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20085518062114716 tensor([0.1910, 0.1928, 0.2009, 0.2039, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2097124606370926 tensor([0.2365, 0.2097, 0.1624, 0.2224, 0.1690], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2078014314174652 tensor([0.2125, 0.2198, 0.1831, 0.2078, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20162415504455566 tensor([0.1877, 0.2016, 0.2113, 0.2085, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19502167403697968 tensor([0.2184, 0.1950, 0.1769, 0.2262, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19645364582538605 tensor([0.1967, 0.1932, 0.1965, 0.2101, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21132014691829681 tensor([0.2251, 0.2113, 0.1722, 0.2179, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19880123436450958 tensor([0.1971, 0.2082, 0.1988, 0.2084, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19918234646320343 tensor([0.1976, 0.2069, 0.1992, 0.2077, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1953420490026474 tensor([0.2314, 0.1953, 0.1649, 0.2322, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19492632150650024 tensor([0.1965, 0.1926, 0.1949, 0.2080, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20617812871932983 tensor([0.2256, 0.2062, 0.1750, 0.2181, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21125736832618713 tensor([0.2289, 0.2113, 0.1693, 0.2219, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19809530675411224 tensor([0.1972, 0.2072, 0.1981, 0.2075, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.204494908452034 tensor([0.2269, 0.2045, 0.1724, 0.2224, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2036856710910797 tensor([0.1827, 0.1895, 0.2117, 0.2037, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20299124717712402 tensor([0.2323, 0.2030, 0.1673, 0.2242, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2078840285539627 tensor([0.2195, 0.2079, 0.1795, 0.2170, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20359322428703308 tensor([0.1916, 0.2036, 0.2099, 0.2073, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2011396437883377 tensor([0.2227, 0.2011, 0.1746, 0.2269, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.198927104473114 tensor([0.2049, 0.1989, 0.1900, 0.2142, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20030273497104645 tensor([0.2145, 0.2003, 0.1851, 0.2197, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20915555953979492 tensor([0.2092, 0.2108, 0.1864, 0.2097, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20278824865818024 tensor([0.1878, 0.2110, 0.2118, 0.2028, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20300233364105225 tensor([0.2201, 0.2030, 0.1757, 0.2219, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1961514800786972 tensor([0.1939, 0.1934, 0.1962, 0.2046, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20920348167419434 tensor([0.2139, 0.2134, 0.1857, 0.2092, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20948487520217896 tensor([0.2189, 0.2186, 0.1782, 0.2095, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2013198882341385 tensor([0.1975, 0.2013, 0.2016, 0.2123, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1989441066980362 tensor([0.2283, 0.1989, 0.1671, 0.2331, 0.1726], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20437254011631012 tensor([0.1857, 0.1866, 0.2055, 0.2044, 0.2179], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20049114525318146 tensor([0.2337, 0.2005, 0.1657, 0.2270, 0.1731], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1965031921863556 tensor([0.1992, 0.1983, 0.1973, 0.2087, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1959788203239441 tensor([0.1992, 0.2020, 0.1960, 0.2125, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2008235603570938 tensor([0.2280, 0.2008, 0.1687, 0.2241, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20189501345157623 tensor([0.1861, 0.1926, 0.2064, 0.2019, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2011900097131729 tensor([0.2467, 0.2012, 0.1546, 0.2299, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997353434562683 tensor([0.1963, 0.2089, 0.1997, 0.2065, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20288699865341187 tensor([0.2029, 0.2039, 0.1942, 0.2128, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20158186554908752 tensor([0.2140, 0.2016, 0.1819, 0.2199, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1958421766757965 tensor([0.1965, 0.1922, 0.1958, 0.2088, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2060665786266327 tensor([0.2227, 0.2061, 0.1745, 0.2185, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20602133870124817 tensor([0.2067, 0.2060, 0.1889, 0.2132, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20392818748950958 tensor([0.1905, 0.2039, 0.2081, 0.2049, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19368216395378113 tensor([0.2124, 0.1937, 0.1799, 0.2288, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19694653153419495 tensor([0.1921, 0.1879, 0.1969, 0.2111, 0.2120], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2013956606388092 tensor([0.2261, 0.2014, 0.1722, 0.2184, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19621282815933228 tensor([0.1938, 0.2035, 0.2022, 0.2042, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20576262474060059 tensor([0.2058, 0.2088, 0.1914, 0.2089, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19781328737735748 tensor([0.2088, 0.1978, 0.1850, 0.2243, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20404091477394104 tensor([0.1872, 0.1897, 0.2048, 0.2040, 0.2142], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038349211215973 tensor([0.2120, 0.2038, 0.1850, 0.2132, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2070043832063675 tensor([0.2289, 0.2070, 0.1712, 0.2206, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19984383881092072 tensor([0.1899, 0.1998, 0.2096, 0.2080, 0.1927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19970355927944183 tensor([0.2018, 0.1997, 0.1919, 0.2162, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19707754254341125 tensor([0.1954, 0.1965, 0.1971, 0.2050, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20433910191059113 tensor([0.2268, 0.2043, 0.1715, 0.2219, 0.1755], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20675447583198547 tensor([0.2106, 0.2208, 0.1858, 0.2068, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20465722680091858 tensor([0.1909, 0.2047, 0.2062, 0.2050, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20371495187282562 tensor([0.2301, 0.2037, 0.1668, 0.2276, 0.1718], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1983731985092163 tensor([0.1918, 0.1897, 0.1984, 0.2063, 0.2138], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20331396162509918 tensor([0.2279, 0.2033, 0.1708, 0.2222, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2090601772069931 tensor([0.2096, 0.2091, 0.1864, 0.2146, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2012365758419037 tensor([0.1970, 0.2048, 0.2012, 0.2071, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20047630369663239 tensor([0.2205, 0.2005, 0.1753, 0.2270, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19762283563613892 tensor([0.2017, 0.1961, 0.1921, 0.2125, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20580576360225677 tensor([0.2313, 0.2058, 0.1682, 0.2196, 0.1752], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20209576189517975 tensor([0.1952, 0.2142, 0.2021, 0.2040, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1953665316104889 tensor([0.1949, 0.1988, 0.2013, 0.2096, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.195265993475914 tensor([0.2285, 0.1953, 0.1685, 0.2309, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20347775518894196 tensor([0.1895, 0.1975, 0.2049, 0.2046, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20748494565486908 tensor([0.2410, 0.2075, 0.1578, 0.2256, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1973404884338379 tensor([0.1995, 0.2051, 0.1973, 0.2073, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2052614390850067 tensor([0.1928, 0.2053, 0.2083, 0.2061, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19756661355495453 tensor([0.2194, 0.1976, 0.1751, 0.2291, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2050127238035202 tensor([0.1865, 0.1916, 0.2054, 0.2050, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20312835276126862 tensor([0.2459, 0.2031, 0.1556, 0.2286, 0.1668], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19823172688484192 tensor([0.1981, 0.2055, 0.1982, 0.2081, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20141442120075226 tensor([0.1899, 0.2014, 0.2078, 0.2070, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20157256722450256 tensor([0.2334, 0.2016, 0.1678, 0.2236, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19949932396411896 tensor([0.1910, 0.1874, 0.1995, 0.2059, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19967225193977356 tensor([0.2091, 0.1997, 0.1874, 0.2168, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20231863856315613 tensor([0.1890, 0.2083, 0.2083, 0.2023, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20481781661510468 tensor([0.1910, 0.2059, 0.2086, 0.2048, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20427264273166656 tensor([0.2277, 0.2043, 0.1674, 0.2312, 0.1694], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20622429251670837 tensor([0.1860, 0.1905, 0.2062, 0.2072, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2001013457775116 tensor([0.2302, 0.2001, 0.1677, 0.2230, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19696572422981262 tensor([0.2003, 0.2080, 0.1970, 0.2072, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20643380284309387 tensor([0.1932, 0.2078, 0.2064, 0.2067, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19563128054141998 tensor([0.2214, 0.1956, 0.1723, 0.2341, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19957341253757477 tensor([0.1996, 0.1929, 0.1917, 0.2100, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20794419944286346 tensor([0.2391, 0.2079, 0.1604, 0.2239, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21038149297237396 tensor([0.2170, 0.2104, 0.1835, 0.2117, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1980985850095749 tensor([0.1830, 0.1997, 0.2147, 0.2045, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1957416832447052 tensor([0.2075, 0.1957, 0.1868, 0.2199, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.202930748462677 tensor([0.1867, 0.1912, 0.2049, 0.2029, 0.2142], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20314428210258484 tensor([0.2386, 0.2031, 0.1601, 0.2259, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2122785896062851 tensor([0.2197, 0.2174, 0.1770, 0.2123, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20316657423973083 tensor([0.1849, 0.2052, 0.2164, 0.2032, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20567497611045837 tensor([0.2235, 0.2057, 0.1741, 0.2185, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20384657382965088 tensor([0.1884, 0.1884, 0.2038, 0.2057, 0.2137], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20591382682323456 tensor([0.2367, 0.2059, 0.1639, 0.2229, 0.1705], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20286868512630463 tensor([0.2029, 0.2060, 0.1913, 0.2119, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1968291699886322 tensor([0.1994, 0.2101, 0.1968, 0.2076, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1984247863292694 tensor([0.2273, 0.1984, 0.1705, 0.2279, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19659475982189178 tensor([0.1959, 0.1937, 0.1966, 0.2079, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20940357446670532 tensor([0.2340, 0.2094, 0.1672, 0.2199, 0.1695], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20258913934230804 tensor([0.2155, 0.2026, 0.1814, 0.2191, 0.1814], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20175208151340485 tensor([0.1984, 0.2071, 0.2018, 0.2047, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20157985389232635 tensor([0.2253, 0.2016, 0.1698, 0.2255, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20115071535110474 tensor([0.1880, 0.1850, 0.2012, 0.2055, 0.2203], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20718330144882202 tensor([0.2342, 0.2072, 0.1651, 0.2231, 0.1704], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2103973627090454 tensor([0.2110, 0.2143, 0.1859, 0.2104, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2037964016199112 tensor([0.1881, 0.1955, 0.2060, 0.2065, 0.2038], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20183634757995605 tensor([0.2150, 0.2018, 0.1790, 0.2193, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20301733911037445 tensor([0.1876, 0.1889, 0.2043, 0.2030, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [4, 2], [0, 4], [2, 4, 1], [2, 1, 4], [2, 4], [2, 4], [0, 1, 4], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4], [4, 0], [2, 4, 1], [1, 2, 4], [2, 4], [4, 2], [4, 2], [2, 4, 1], [1, 2, 0], [2, 4], [4, 2], [0, 1, 4], [4, 2], [0, 1, 2], [2, 4], [4, 2], [0, 4], [2, 4, 1], [0, 1], [2, 4], [4, 0, 2], [0, 1, 4], [2, 4, 1], [1, 2, 0], [2, 4], [0, 4], [4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4, 1], [4, 2], [4, 2], [2, 4], [0, 2, 4], [4, 2], [0, 4, 2], [0, 4], [4, 2], [1, 0, 2], [2, 4], [4, 2], [2, 4], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2], [0, 4, 2], [2, 4, 1], [2, 1, 4], [4, 2], [0, 4, 2], [0, 1, 4], [2, 4, 1], [0, 1], [1, 0], [2, 4], [0, 4], [2, 4, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4, 2], [2, 4, 1], [1, 2], [2, 4], [4, 2], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4], [4, 2, 1], [4, 2], [2, 4], [1, 0, 2], [4, 0, 2], [4, 2], [0, 4], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0, 2], [0, 4], [2, 4], [1, 0, 2], [2, 4], [4, 2], [0, 4], [2, 4, 1], [0, 2], [4, 2], [4, 2], [0, 4], [2, 4], [0, 1, 2], [2, 4], [4, 2], [1, 0, 4], [2, 4, 1], [0, 1], [2, 4], [4, 2], [0, 1, 4], [2, 4], [0, 1], [2, 4], [0, 3, 4], [0, 4, 1], [2, 4, 1], [0, 1], [2, 4], [2, 4], [4, 0, 2], [2, 4, 1], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4, 1], [1, 0, 2], [2, 4], [4, 0, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4], [4, 2], [0, 4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [2, 4, 1], [4, 2], [4, 2], [0, 4], [2, 4], [0, 1, 2], [4, 2], [4, 2], [4, 0], [2, 4, 1], [0, 1], [2, 4], [0, 1, 4], [4, 0, 2], [2, 4], [0, 1], [2, 4], [0, 4, 2], [4, 2], [2, 4], [1, 0, 2], [2, 4], [4, 2], [0, 4], [2, 4, 1], [1, 0, 2], [2, 4], [0, 4], [4, 2], [4, 2, 1], [0, 1], [2, 4], [2, 4], [0, 4, 1], [4, 2, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0, 2], [2, 4], [4, 2], [0, 4], [2, 4], [1, 2, 4], [2, 4], [4, 0], [0, 1, 4], [2, 4, 1], [0, 1], [2, 4], [4, 0, 2], [0, 4], [2, 4, 1], [0, 1], [2, 4], [0, 4, 2], [0, 4], [2, 4], [1, 0, 2], [2, 4, 1], [0, 4], [0, 4], [2, 4], [0, 1], [2, 4], [4, 0, 2], [0, 4], [2, 4, 1], [1, 2, 0], [2, 4], [4, 2], [0, 1, 4], [2, 4, 1], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1], [2, 4], [4, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4], [4, 0], [2, 4], [1, 0], [2, 4], [4, 2], [0, 1], [2, 4], [0, 1]]\n",
      "[[0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [0, 3], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [0, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3], [0, 1, 3], [1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [1, 3], [0, 3], [0, 3], [0, 1, 3], [1, 3], [2, 3], [0, 3], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [0, 3], [2, 3, 4], [0, 1, 3], [0, 3], [0, 1, 3], [0, 1, 3], [3, 4], [1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2], [2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [2, 3], [1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3], [0, 1, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 3], [0, 1, 3], [1, 2, 3], [2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 1], [2, 1, 4], [0, 1, 4], [2, 4, 1], [1, 0, 2], [2, 4, 1], [1, 2, 4], [2, 4, 1], [1, 2, 0], [0, 1, 4], [0, 1, 2], [2, 4, 1], [4, 0, 2], [0, 1, 4], [2, 4, 1], [1, 2, 0], [0, 4], [2, 4, 1], [0, 2, 4], [0, 4, 2], [1, 0, 2], [2, 4, 1], [2, 4, 1], [0, 4, 2], [2, 4, 1], [2, 1, 4], [0, 4, 2], [0, 1, 4], [2, 4, 1], [2, 4, 1], [0, 1, 2], [0, 4, 2], [2, 4, 1], [0, 3, 1], [2, 4, 1], [4, 2, 1], [1, 0, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [4, 0, 2], [1, 0, 2], [2, 4, 1], [0, 1, 2], [1, 0, 4], [2, 4, 1], [0, 1, 4], [0, 3, 4], [0, 4, 1], [2, 4, 1], [4, 0, 2], [2, 4, 1], [2, 4, 1], [1, 0, 2], [4, 0, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [0, 4, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 1, 4], [4, 0, 2], [0, 4, 2], [1, 0, 2], [2, 4, 1], [1, 0, 2], [0, 4], [4, 2, 1], [0, 4, 1], [4, 2, 1], [0, 1, 2], [1, 0, 2], [1, 2, 4], [0, 1, 4], [2, 4, 1], [4, 0, 2], [2, 4, 1], [0, 4, 2], [1, 0, 2], [2, 4, 1], [4, 0, 2], [2, 4, 1], [1, 2, 0], [0, 1, 4], [2, 4, 1], [4, 0, 2], [2, 4, 1], [1, 0, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01427538659837511  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.014275113741556803  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.01427459716796875  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014273856745825873  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.014272920290629069  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.014271805021497939  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.014270529482099744  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.014269106917911105  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.014267557197146945  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.014265889591640896  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.014264119995964898  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.014262251059214274  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.014260301325056289  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.014258273442586263  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.014256179332733154  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.01425402561823527  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.014251814948187934  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.014249555269877116  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.014247250556945801  Accuracy on Support set:0.0\n",
      "torch.Size([90, 2048]) torch.Size([90])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.014244907432132296  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20150484144687653 tensor([0.2378, 0.2015, 0.1618, 0.2262, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20208655297756195 tensor([0.2077, 0.2021, 0.1885, 0.2167, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.198023721575737 tensor([0.1998, 0.1989, 0.1980, 0.2133, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2313503921031952 tensor([0.2314, 0.1962, 0.1646, 0.2357, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21082422137260437 tensor([0.2108, 0.1916, 0.1815, 0.2188, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20040863752365112 tensor([0.2387, 0.2004, 0.1605, 0.2278, 0.1725], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19772468507289886 tensor([0.2090, 0.1977, 0.1857, 0.2192, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1980804055929184 tensor([0.1969, 0.1986, 0.1981, 0.2111, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22120007872581482 tensor([0.2212, 0.1938, 0.1718, 0.2340, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21029193699359894 tensor([0.1942, 0.1864, 0.1949, 0.2103, 0.2143], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20072832703590393 tensor([0.2361, 0.2007, 0.1632, 0.2274, 0.1725], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20348891615867615 tensor([0.2144, 0.2035, 0.1795, 0.2193, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19669680297374725 tensor([0.2018, 0.2057, 0.1967, 0.2129, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22451995313167572 tensor([0.2245, 0.1952, 0.1707, 0.2310, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20340630412101746 tensor([0.2034, 0.1929, 0.1892, 0.2178, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19795852899551392 tensor([0.2186, 0.1980, 0.1773, 0.2199, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20647890865802765 tensor([0.2065, 0.2068, 0.1900, 0.2111, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20939816534519196 tensor([0.2118, 0.2094, 0.1859, 0.2150, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22894006967544556 tensor([0.2310, 0.1952, 0.1675, 0.2289, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.211081400513649 tensor([0.2019, 0.1875, 0.1880, 0.2115, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20328521728515625 tensor([0.2422, 0.2033, 0.1582, 0.2258, 0.1705], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20327633619308472 tensor([0.2044, 0.2033, 0.1901, 0.2147, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20643235743045807 tensor([0.1869, 0.1986, 0.2131, 0.2064, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20233489573001862 tensor([0.2191, 0.2023, 0.1768, 0.2238, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20613130927085876 tensor([0.1971, 0.1928, 0.1926, 0.2114, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20715218782424927 tensor([0.2366, 0.2072, 0.1628, 0.2241, 0.1693], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20857685804367065 tensor([0.2178, 0.2086, 0.1794, 0.2140, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19954155385494232 tensor([0.1950, 0.1995, 0.2017, 0.2106, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23123134672641754 tensor([0.2312, 0.1915, 0.1650, 0.2359, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2073240876197815 tensor([0.1870, 0.1862, 0.2073, 0.2075, 0.2120], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20344409346580505 tensor([0.2406, 0.2034, 0.1595, 0.2252, 0.1712], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2062971144914627 tensor([0.2028, 0.2106, 0.1945, 0.2063, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20648857951164246 tensor([0.1915, 0.1988, 0.2065, 0.2080, 0.1952], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23815621435642242 tensor([0.2382, 0.1950, 0.1585, 0.2383, 0.1700], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2053028643131256 tensor([0.2037, 0.1871, 0.1876, 0.2162, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20786821842193604 tensor([0.2276, 0.2079, 0.1710, 0.2186, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20242413878440857 tensor([0.1960, 0.2045, 0.2024, 0.2089, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20382192730903625 tensor([0.2093, 0.2038, 0.1891, 0.2172, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20110024511814117 tensor([0.2296, 0.2011, 0.1693, 0.2236, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20505216717720032 tensor([0.1860, 0.1852, 0.2065, 0.2051, 0.2173], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20341762900352478 tensor([0.2247, 0.2034, 0.1725, 0.2215, 0.1780], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21051190793514252 tensor([0.2132, 0.2105, 0.1817, 0.2150, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19962844252586365 tensor([0.1887, 0.1996, 0.2082, 0.2070, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19820311665534973 tensor([0.2164, 0.1982, 0.1775, 0.2242, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19797812402248383 tensor([0.1940, 0.1920, 0.1980, 0.2082, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22667372226715088 tensor([0.2373, 0.1969, 0.1634, 0.2267, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21142955124378204 tensor([0.2192, 0.2157, 0.1796, 0.2114, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20571710169315338 tensor([0.2057, 0.2069, 0.1918, 0.2142, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20121295750141144 tensor([0.2259, 0.2012, 0.1725, 0.2224, 0.1780], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19784791767597198 tensor([0.2043, 0.1978, 0.1893, 0.2134, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20595911145210266 tensor([0.2135, 0.2060, 0.1840, 0.2152, 0.1814], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20811772346496582 tensor([0.1994, 0.2088, 0.1957, 0.2081, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19881810247898102 tensor([0.1889, 0.1988, 0.2113, 0.2086, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20516179502010345 tensor([0.2091, 0.2052, 0.1859, 0.2141, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2088979035615921 tensor([0.2006, 0.1907, 0.1890, 0.2109, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017342746257782 tensor([0.2386, 0.2017, 0.1615, 0.2264, 0.1717], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20066139101982117 tensor([0.2133, 0.2007, 0.1849, 0.2184, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1988588571548462 tensor([0.2140, 0.1989, 0.1822, 0.2203, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22899843752384186 tensor([0.2290, 0.1962, 0.1652, 0.2351, 0.1745], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2026105523109436 tensor([0.1886, 0.1868, 0.2026, 0.2057, 0.2164], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21456092596054077 tensor([0.2146, 0.1957, 0.1838, 0.2165, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21491268277168274 tensor([0.2273, 0.2187, 0.1712, 0.2149, 0.1679], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19788824021816254 tensor([0.2000, 0.1979, 0.1950, 0.2141, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2243843674659729 tensor([0.2244, 0.1938, 0.1698, 0.2373, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20788493752479553 tensor([0.2079, 0.1966, 0.1855, 0.2163, 0.1937], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19774669408798218 tensor([0.2134, 0.1977, 0.1823, 0.2229, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19961923360824585 tensor([0.2034, 0.1996, 0.1950, 0.2101, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2042188197374344 tensor([0.1932, 0.1957, 0.2042, 0.2103, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22551144659519196 tensor([0.2255, 0.1923, 0.1681, 0.2373, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2000790536403656 tensor([0.1934, 0.1916, 0.2001, 0.2092, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19600549340248108 tensor([0.1973, 0.1899, 0.1960, 0.2161, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2112901210784912 tensor([0.2310, 0.2113, 0.1656, 0.2217, 0.1704], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19973188638687134 tensor([0.1914, 0.1997, 0.2066, 0.2092, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22565214335918427 tensor([0.2257, 0.1960, 0.1706, 0.2307, 0.1771], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2045464962720871 tensor([0.1977, 0.1945, 0.1933, 0.2099, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20358701050281525 tensor([0.2378, 0.2036, 0.1621, 0.2254, 0.1711], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2081456035375595 tensor([0.2081, 0.2088, 0.1874, 0.2113, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1995810568332672 tensor([0.2018, 0.1996, 0.1932, 0.2115, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2199137657880783 tensor([0.2199, 0.1939, 0.1763, 0.2302, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20335660874843597 tensor([0.2046, 0.1902, 0.1860, 0.2158, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19969649612903595 tensor([0.2408, 0.1997, 0.1578, 0.2283, 0.1733], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20642998814582825 tensor([0.2064, 0.2093, 0.1901, 0.2093, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20244982838630676 tensor([0.1821, 0.1962, 0.2182, 0.2011, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2308257520198822 tensor([0.2344, 0.1965, 0.1647, 0.2308, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20037692785263062 tensor([0.1889, 0.1871, 0.2004, 0.2047, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2029312402009964 tensor([0.2532, 0.2029, 0.1503, 0.2297, 0.1639], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2162061482667923 tensor([0.2162, 0.1955, 0.1813, 0.2235, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.202217698097229 tensor([0.2129, 0.2022, 0.1834, 0.2196, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19973069429397583 tensor([0.2272, 0.1997, 0.1684, 0.2273, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20115014910697937 tensor([0.1993, 0.1898, 0.1930, 0.2168, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2019437849521637 tensor([0.2044, 0.2019, 0.1941, 0.2095, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21040289103984833 tensor([0.2227, 0.2104, 0.1733, 0.2193, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20108479261398315 tensor([0.1903, 0.2011, 0.2104, 0.2080, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23042254149913788 tensor([0.2304, 0.1969, 0.1638, 0.2356, 0.1733], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20951692759990692 tensor([0.1959, 0.1857, 0.1952, 0.2095, 0.2136], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20389898121356964 tensor([0.2261, 0.2039, 0.1722, 0.2220, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20029456913471222 tensor([0.2149, 0.2003, 0.1803, 0.2184, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20148904621601105 tensor([0.1963, 0.2018, 0.2015, 0.2106, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19864003360271454 tensor([0.2256, 0.1986, 0.1710, 0.2247, 0.1801], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19856306910514832 tensor([0.1944, 0.1967, 0.1986, 0.2063, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19962510466575623 tensor([0.2193, 0.1996, 0.1784, 0.2211, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20862947404384613 tensor([0.2031, 0.2086, 0.1934, 0.2107, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20174627006053925 tensor([0.1967, 0.2059, 0.2017, 0.2085, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1981077790260315 tensor([0.2241, 0.1981, 0.1733, 0.2256, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20144765079021454 tensor([0.2004, 0.1925, 0.1922, 0.2135, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20125411450862885 tensor([0.2185, 0.2013, 0.1766, 0.2202, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2144259810447693 tensor([0.2321, 0.2144, 0.1669, 0.2202, 0.1664], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20264868438243866 tensor([0.1927, 0.2026, 0.2061, 0.2071, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22837884724140167 tensor([0.2284, 0.1966, 0.1658, 0.2318, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1980113685131073 tensor([0.2037, 0.1980, 0.1889, 0.2099, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20643390715122223 tensor([0.2311, 0.2064, 0.1708, 0.2197, 0.1719], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20536482334136963 tensor([0.2116, 0.2054, 0.1847, 0.2171, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19762742519378662 tensor([0.2001, 0.2025, 0.1976, 0.2114, 0.1884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19805482029914856 tensor([0.2326, 0.1981, 0.1635, 0.2343, 0.1715], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2008201628923416 tensor([0.1997, 0.1942, 0.1949, 0.2104, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20238825678825378 tensor([0.2375, 0.2024, 0.1642, 0.2273, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2146805077791214 tensor([0.2264, 0.2147, 0.1716, 0.2164, 0.1710], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19205322861671448 tensor([0.2027, 0.1928, 0.1921, 0.2176, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21928144991397858 tensor([0.2193, 0.1924, 0.1754, 0.2303, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20600713789463043 tensor([0.1834, 0.1848, 0.2064, 0.2060, 0.2194], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19899140298366547 tensor([0.2367, 0.1990, 0.1614, 0.2306, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20770837366580963 tensor([0.2077, 0.2108, 0.1858, 0.2108, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20820970833301544 tensor([0.1895, 0.1969, 0.2082, 0.2103, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20488885045051575 tensor([0.2267, 0.2049, 0.1699, 0.2261, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20497791469097137 tensor([0.1857, 0.1919, 0.2056, 0.2050, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2013777494430542 tensor([0.2444, 0.2014, 0.1569, 0.2287, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1993657946586609 tensor([0.1982, 0.2040, 0.1994, 0.2046, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19602616131305695 tensor([0.2014, 0.1969, 0.1960, 0.2155, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.224408358335495 tensor([0.2244, 0.1959, 0.1688, 0.2328, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19758343696594238 tensor([0.1943, 0.1928, 0.1976, 0.2077, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20074781775474548 tensor([0.2447, 0.2007, 0.1581, 0.2293, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2064308226108551 tensor([0.2183, 0.2064, 0.1768, 0.2188, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20077645778656006 tensor([0.2044, 0.2008, 0.1927, 0.2133, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2296907603740692 tensor([0.2297, 0.1947, 0.1658, 0.2344, 0.1755], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19659292697906494 tensor([0.1947, 0.1908, 0.1966, 0.2073, 0.2105], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20681551098823547 tensor([0.2406, 0.2068, 0.1588, 0.2257, 0.1681], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21112003922462463 tensor([0.2164, 0.2172, 0.1792, 0.2111, 0.1760], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19944195449352264 tensor([0.1915, 0.1994, 0.2069, 0.2121, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2222483605146408 tensor([0.2222, 0.1924, 0.1730, 0.2297, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2026727795600891 tensor([0.2004, 0.1911, 0.1922, 0.2136, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20857223868370056 tensor([0.2291, 0.2086, 0.1684, 0.2213, 0.1726], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20582523941993713 tensor([0.2010, 0.2058, 0.1946, 0.2119, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20453323423862457 tensor([0.2015, 0.2045, 0.1949, 0.2112, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23517349362373352 tensor([0.2352, 0.1927, 0.1614, 0.2355, 0.1752], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2070440798997879 tensor([0.2003, 0.1905, 0.1908, 0.2114, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20351536571979523 tensor([0.2296, 0.2035, 0.1712, 0.2214, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20842963457107544 tensor([0.2329, 0.2084, 0.1655, 0.2253, 0.1678], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20491275191307068 tensor([0.2010, 0.2049, 0.1939, 0.2110, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20177826285362244 tensor([0.2309, 0.2018, 0.1686, 0.2257, 0.1729], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20719856023788452 tensor([0.1864, 0.1875, 0.2073, 0.2072, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20022174715995789 tensor([0.2363, 0.2002, 0.1636, 0.2276, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20526692271232605 tensor([0.2234, 0.2053, 0.1757, 0.2204, 0.1753], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20138175785541534 tensor([0.1954, 0.2014, 0.2056, 0.2108, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19841019809246063 tensor([0.2266, 0.1984, 0.1707, 0.2304, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2087233066558838 tensor([0.2087, 0.1965, 0.1859, 0.2177, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19776707887649536 tensor([0.2184, 0.1978, 0.1811, 0.2232, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20833316445350647 tensor([0.2131, 0.2083, 0.1824, 0.2131, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20625491440296173 tensor([0.1915, 0.2088, 0.2075, 0.2063, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20033389329910278 tensor([0.2240, 0.2003, 0.1719, 0.2253, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20808710157871246 tensor([0.1978, 0.1913, 0.1919, 0.2081, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2109098583459854 tensor([0.2178, 0.2109, 0.1819, 0.2125, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21276113390922546 tensor([0.2228, 0.2159, 0.1744, 0.2128, 0.1740], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972702145576477 tensor([0.2014, 0.1990, 0.1973, 0.2159, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2321520894765854 tensor([0.2322, 0.1961, 0.1634, 0.2366, 0.1718], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20126137137413025 tensor([0.1893, 0.1846, 0.2013, 0.2078, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19771088659763336 tensor([0.2377, 0.1977, 0.1620, 0.2304, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19318406283855438 tensor([0.2029, 0.1961, 0.1932, 0.2121, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19970008730888367 tensor([0.2031, 0.1997, 0.1918, 0.2160, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19806472957134247 tensor([0.2321, 0.1981, 0.1650, 0.2276, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20206786692142487 tensor([0.1898, 0.1906, 0.2021, 0.2053, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19838476181030273 tensor([0.2504, 0.1984, 0.1515, 0.2329, 0.1668], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20655162632465363 tensor([0.2001, 0.2066, 0.1955, 0.2100, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2014998197555542 tensor([0.2068, 0.2015, 0.1900, 0.2163, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19901679456233978 tensor([0.2179, 0.1990, 0.1780, 0.2234, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2057390660047531 tensor([0.2003, 0.1901, 0.1916, 0.2122, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2033940553665161 tensor([0.2267, 0.2034, 0.1707, 0.2219, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2035619467496872 tensor([0.2107, 0.2036, 0.1848, 0.2167, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2017369270324707 tensor([0.1943, 0.2017, 0.2038, 0.2084, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21627606451511383 tensor([0.2163, 0.1911, 0.1759, 0.2324, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2110235095024109 tensor([0.1958, 0.1858, 0.1927, 0.2147, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19876883924007416 tensor([0.2300, 0.1988, 0.1685, 0.2217, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19796665012836456 tensor([0.1976, 0.2014, 0.1980, 0.2077, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20641174912452698 tensor([0.2096, 0.2064, 0.1874, 0.2122, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21260182559490204 tensor([0.2126, 0.1952, 0.1810, 0.2279, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20056979358196259 tensor([0.1909, 0.1877, 0.2006, 0.2075, 0.2134], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20137837529182434 tensor([0.2159, 0.2014, 0.1810, 0.2166, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2042207419872284 tensor([0.2329, 0.2042, 0.1674, 0.2240, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20521391928195953 tensor([0.1936, 0.1977, 0.2052, 0.2116, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.205536350607872 tensor([0.2055, 0.1973, 0.1879, 0.2197, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20509712398052216 tensor([0.1992, 0.1944, 0.1929, 0.2085, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20161306858062744 tensor([0.2308, 0.2016, 0.1677, 0.2253, 0.1746], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2100691795349121 tensor([0.2145, 0.2182, 0.1819, 0.2101, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20186418294906616 tensor([0.1947, 0.2024, 0.2019, 0.2084, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20089101791381836 tensor([0.2341, 0.2009, 0.1631, 0.2310, 0.1709], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20975755155086517 tensor([0.1955, 0.1877, 0.1941, 0.2098, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20059500634670258 tensor([0.2319, 0.2006, 0.1670, 0.2256, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20650963485240936 tensor([0.2135, 0.2065, 0.1824, 0.2180, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19698230922222137 tensor([0.2008, 0.2025, 0.1970, 0.2106, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19776155054569244 tensor([0.2244, 0.1978, 0.1714, 0.2305, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20552285015583038 tensor([0.2055, 0.1938, 0.1880, 0.2159, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20307084918022156 tensor([0.2353, 0.2031, 0.1645, 0.2228, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1978950798511505 tensor([0.1990, 0.2119, 0.1979, 0.2075, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969211995601654 tensor([0.1988, 0.1966, 0.1969, 0.2132, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23221054673194885 tensor([0.2322, 0.1927, 0.1650, 0.2341, 0.1760], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20063312351703644 tensor([0.1933, 0.1954, 0.2006, 0.2081, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20449422299861908 tensor([0.2449, 0.2045, 0.1543, 0.2289, 0.1674], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20281359553337097 tensor([0.2033, 0.2028, 0.1931, 0.2107, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20302803814411163 tensor([0.1967, 0.2030, 0.2039, 0.2096, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22334852814674377 tensor([0.2233, 0.1949, 0.1712, 0.2327, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2010866105556488 tensor([0.1902, 0.1896, 0.2011, 0.2085, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2002965658903122 tensor([0.2496, 0.2003, 0.1525, 0.2317, 0.1660], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20312488079071045 tensor([0.2019, 0.2031, 0.1940, 0.2116, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1992449015378952 tensor([0.1937, 0.1992, 0.2034, 0.2105, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19886821508407593 tensor([0.2372, 0.1989, 0.1642, 0.2269, 0.1728], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20935384929180145 tensor([0.1947, 0.1854, 0.1953, 0.2094, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21296565234661102 tensor([0.2130, 0.1972, 0.1833, 0.2203, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2040337324142456 tensor([0.1927, 0.2062, 0.2040, 0.2058, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2037353366613388 tensor([0.1948, 0.2037, 0.2042, 0.2083, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20139718055725098 tensor([0.2315, 0.2014, 0.1638, 0.2346, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2018747478723526 tensor([0.1897, 0.1885, 0.2019, 0.2107, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19740676879882812 tensor([0.2341, 0.1974, 0.1641, 0.2263, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20565417408943176 tensor([0.2042, 0.2057, 0.1929, 0.2106, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20220589637756348 tensor([0.1970, 0.2055, 0.2022, 0.2101, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22528685629367828 tensor([0.2253, 0.1928, 0.1685, 0.2377, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20487871766090393 tensor([0.2034, 0.1907, 0.1876, 0.2134, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20495334267616272 tensor([0.2432, 0.2050, 0.1569, 0.2273, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20782867074012756 tensor([0.2209, 0.2078, 0.1796, 0.2150, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2080947607755661 tensor([0.1866, 0.1976, 0.2103, 0.2081, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21137218177318573 tensor([0.2114, 0.1933, 0.1827, 0.2234, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20055241882801056 tensor([0.1905, 0.1892, 0.2006, 0.2064, 0.2134], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20026187598705292 tensor([0.2427, 0.2003, 0.1565, 0.2292, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21471713483333588 tensor([0.2237, 0.2147, 0.1732, 0.2156, 0.1728], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20303693413734436 tensor([0.1886, 0.2030, 0.2121, 0.2067, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2030193954706192 tensor([0.2274, 0.2030, 0.1704, 0.2218, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19953665137290955 tensor([0.1921, 0.1863, 0.1995, 0.2092, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20307083427906036 tensor([0.2408, 0.2031, 0.1603, 0.2262, 0.1697], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20355473458766937 tensor([0.2067, 0.2036, 0.1872, 0.2154, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20770257711410522 tensor([0.2033, 0.2077, 0.1926, 0.2110, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.231145977973938 tensor([0.2311, 0.1957, 0.1668, 0.2313, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20498445630073547 tensor([0.1997, 0.1916, 0.1924, 0.2114, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20657707750797272 tensor([0.2380, 0.2066, 0.1636, 0.2231, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.200019970536232 tensor([0.2194, 0.2000, 0.1775, 0.2226, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1976156085729599 tensor([0.2021, 0.2048, 0.1976, 0.2081, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19884450733661652 tensor([0.2292, 0.1988, 0.1661, 0.2289, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1969102919101715 tensor([0.1917, 0.1831, 0.1969, 0.2089, 0.2194], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2043100744485855 tensor([0.2383, 0.2043, 0.1613, 0.2265, 0.1695], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21180853247642517 tensor([0.2149, 0.2118, 0.1820, 0.2138, 0.1775], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2017914354801178 tensor([0.1917, 0.1935, 0.2018, 0.2100, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19927643239498138 tensor([0.2189, 0.1993, 0.1751, 0.2227, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19998259842395782 tensor([0.1913, 0.1869, 0.2000, 0.2065, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [4, 2], [0, 4, 2], [2, 4, 1], [2, 1, 4], [2, 4], [2, 4, 1], [0, 1, 4, 2], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4], [4, 0, 2], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 2], [4, 2], [2, 4, 1], [1, 2, 0], [2, 4], [4, 2], [0, 1, 4], [4, 2], [0, 1, 2], [2, 4], [4, 2], [0, 4, 1], [2, 4, 1], [0, 1], [2, 4], [4, 0, 2], [0, 1, 4], [2, 4, 1], [1, 2, 0], [2, 4], [0, 4], [4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [4, 2], [2, 4], [0, 2, 4, 1], [4, 2], [0, 4, 2], [0, 4, 1], [4, 2], [1, 0, 2], [2, 4], [4, 2], [2, 4, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [4, 2, 1], [0, 4, 2, 1], [0, 1, 4], [2, 4, 1], [0, 1], [1, 0, 2], [2, 4], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4, 2, 1], [2, 4, 1], [1, 2], [2, 4, 1], [4, 2], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4], [4, 2, 1], [4, 2], [2, 4, 1], [1, 0, 2], [4, 0, 2], [4, 2], [0, 4], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4], [0, 4], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2], [0, 4], [2, 4, 1], [1, 0, 2], [2, 4], [4, 2], [0, 4], [2, 4, 1], [0, 2, 1], [4, 2], [4, 2], [0, 4, 2], [2, 4, 1], [0, 1, 2], [2, 4], [4, 2], [1, 0, 4, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2], [0, 1, 4], [2, 4], [0, 1], [2, 4], [0, 3, 4, 2], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4], [2, 4], [4, 0, 2], [2, 4, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4], [4, 0, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4], [4, 2], [0, 4, 2], [2, 4], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4, 1], [2, 4, 1], [4, 2, 1], [4, 2], [0, 4], [2, 4], [0, 1, 2], [4, 2], [4, 2], [4, 0, 2], [2, 4, 1], [0, 1], [2, 4, 1], [0, 1, 4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 4, 2], [4, 2], [2, 4, 1], [1, 0, 2], [2, 4], [4, 2], [0, 4], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4, 2], [4, 2], [4, 2, 1], [0, 1], [2, 4], [2, 4], [0, 4, 1], [4, 2, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4], [2, 4], [1, 0, 2], [2, 4], [4, 2], [0, 4, 2], [2, 4, 1], [1, 2, 4], [2, 4], [4, 0, 2], [0, 1, 4, 2], [2, 4, 1], [0, 1], [2, 4], [4, 0, 2], [0, 4], [2, 4, 1], [0, 1], [2, 4], [0, 4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4], [0, 4], [2, 4], [0, 1], [2, 4, 1], [4, 0, 2], [0, 4], [2, 4, 1], [1, 2, 0], [2, 4], [4, 2], [0, 1, 4], [2, 4, 1], [0, 1], [2, 4], [4, 2], [0, 4], [2, 4], [0, 1, 2], [2, 4], [4, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4], [4, 2], [0, 1], [2, 4, 1], [0, 1, 2]]\n",
      "[[0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [0, 3], [0, 1, 3], [0, 3], [3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [0, 3], [0, 3], [0, 1, 3], [0, 1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 2, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [3], [0, 1, 3], [1, 3], [2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [0, 3], [0, 3], [3], [2, 3], [0, 3], [2, 3, 4], [3, 4], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [3], [0, 3], [0, 3, 4], [0, 3], [0, 1, 3], [2, 4], [0, 3], [2, 3, 4], [0, 1, 3], [0, 3], [0, 1, 3], [0, 3], [3, 4], [1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 3], [1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1], [3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [0, 3], [0, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [2, 3, 4], [0, 3], [3], [3], [0, 3], [2, 3, 4], [0, 3], [1, 3], [0, 1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 3], [1, 3], [0, 1, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [0, 3], [0, 1, 3], [1, 3], [3], [0, 3], [2, 3, 4], [0, 1, 3], [1, 3], [1, 2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [1, 2, 3], [1, 2, 3], [0, 1, 3], [2, 3, 4], [0, 3], [1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3], [3, 4]]\n",
      "NL_pred of 3th iteration [[0, 4, 2], [2, 4, 1], [0, 1, 4, 2], [4, 0, 2], [2, 4, 1], [0, 4, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [0, 2, 4, 1], [0, 4, 1], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [0, 4, 2, 1], [1, 0, 2], [0, 4, 1], [0, 4, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [0, 2, 1], [0, 4, 2], [2, 4, 1], [1, 0, 4, 2], [2, 4, 1], [0, 3, 4, 2], [0, 4, 1, 2], [0, 1, 2], [0, 1, 2], [0, 4, 1], [2, 4, 1], [4, 2, 1], [4, 0, 2], [2, 4, 1], [0, 1, 4, 2], [4, 0, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 4, 2], [0, 4, 2], [2, 4, 1], [4, 0, 2], [0, 1, 4, 2], [0, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1, 2], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 1, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.022165656089782715  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.02216488945073095  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.022163428109267663  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.022161344002033102  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.022158698789004623  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.022155558240824734  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.022151963464145004  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.022147965842279894  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.02214361470321129  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.022138932655597555  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.02213396697208799  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.022128736150675808  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.022123277187347412  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.022117606524763436  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.022111752937579977  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0221057390344554  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.022099570981387436  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.022093277553032184  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.022086862860054804  Accuracy on Support set:0.0\n",
      "torch.Size([58, 2048]) torch.Size([58])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.02208035156644624  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.19595786929130554 tensor([0.2416, 0.1960, 0.1586, 0.2297, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1968059092760086 tensor([0.2114, 0.1968, 0.1848, 0.2203, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19374676048755646 tensor([0.2034, 0.1937, 0.1940, 0.2170, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23495419323444366 tensor([0.2350, 0.1908, 0.1614, 0.2392, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2143930047750473 tensor([0.2144, 0.1865, 0.1778, 0.2224, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1947758048772812 tensor([0.2426, 0.1948, 0.1572, 0.2314, 0.1740], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21262919902801514 tensor([0.2126, 0.1925, 0.1820, 0.2229, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22489047050476074 tensor([0.2249, 0.1885, 0.1682, 0.2377, 0.1807], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2137167602777481 tensor([0.1975, 0.1816, 0.1910, 0.2137, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19516317546367645 tensor([0.2400, 0.1952, 0.1599, 0.2310, 0.1740], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1980840563774109 tensor([0.2181, 0.1981, 0.1758, 0.2229, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20048263669013977 tensor([0.2054, 0.2005, 0.1928, 0.2166, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2282862663269043 tensor([0.2283, 0.1898, 0.1671, 0.2347, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20692703127861023 tensor([0.2069, 0.1879, 0.1853, 0.2214, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22235578298568726 tensor([0.2224, 0.1926, 0.1736, 0.2235, 0.1880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20154030621051788 tensor([0.2102, 0.2015, 0.1863, 0.2147, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20394419133663177 tensor([0.2156, 0.2039, 0.1822, 0.2187, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2324705719947815 tensor([0.2347, 0.1898, 0.1641, 0.2325, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21285821497440338 tensor([0.2054, 0.1826, 0.1841, 0.2149, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19765715301036835 tensor([0.2460, 0.1977, 0.1550, 0.2293, 0.1720], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19797807931900024 tensor([0.2081, 0.1980, 0.1863, 0.2184, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20908161997795105 tensor([0.1903, 0.1937, 0.2091, 0.2100, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19688545167446136 tensor([0.2228, 0.1969, 0.1733, 0.2275, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20797425508499146 tensor([0.2006, 0.1878, 0.1886, 0.2150, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2014782875776291 tensor([0.2405, 0.2015, 0.1595, 0.2277, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20315657556056976 tensor([0.2216, 0.2032, 0.1759, 0.2175, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1977730095386505 tensor([0.1985, 0.1945, 0.1978, 0.2142, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23466920852661133 tensor([0.2347, 0.1864, 0.1619, 0.2392, 0.1778], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20315279066562653 tensor([0.1903, 0.1816, 0.2032, 0.2111, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19782212376594543 tensor([0.2446, 0.1978, 0.1562, 0.2288, 0.1726], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20527870953083038 tensor([0.2065, 0.2053, 0.1907, 0.2099, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20239417254924774 tensor([0.1950, 0.1938, 0.2024, 0.2117, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2414693385362625 tensor([0.2416, 0.1899, 0.1558, 0.2415, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20698267221450806 tensor([0.2072, 0.1823, 0.1838, 0.2196, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20225790143013 tensor([0.2315, 0.2023, 0.1676, 0.2221, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19852370023727417 tensor([0.1996, 0.1994, 0.1985, 0.2125, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19848528504371643 tensor([0.2131, 0.1985, 0.1853, 0.2209, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19564002752304077 tensor([0.2334, 0.1956, 0.1659, 0.2271, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2023027390241623 tensor([0.1893, 0.1807, 0.2023, 0.2085, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1979065239429474 tensor([0.2285, 0.1979, 0.1690, 0.2251, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20504041016101837 tensor([0.2170, 0.2050, 0.1781, 0.2186, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20422078669071198 tensor([0.1921, 0.1947, 0.2042, 0.2106, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22014254331588745 tensor([0.2201, 0.1928, 0.1738, 0.2278, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20968252420425415 tensor([0.1975, 0.1871, 0.1940, 0.2117, 0.2097], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23013585805892944 tensor([0.2410, 0.1915, 0.1601, 0.2301, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2102470099925995 tensor([0.2230, 0.2102, 0.1762, 0.2149, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20153279602527618 tensor([0.2094, 0.2015, 0.1880, 0.2179, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19574779272079468 tensor([0.2297, 0.1957, 0.1690, 0.2260, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20072409510612488 tensor([0.2170, 0.2007, 0.1806, 0.2186, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20352792739868164 tensor([0.2030, 0.2035, 0.1919, 0.2117, 0.1899], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20715411007404327 tensor([0.1924, 0.1938, 0.2072, 0.2123, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19983161985874176 tensor([0.2127, 0.1998, 0.1823, 0.2177, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21070757508277893 tensor([0.2041, 0.1857, 0.1851, 0.2143, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19607621431350708 tensor([0.2425, 0.1961, 0.1582, 0.2300, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1954045593738556 tensor([0.2169, 0.1954, 0.1812, 0.2220, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21761170029640198 tensor([0.2176, 0.1936, 0.1785, 0.2240, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23264986276626587 tensor([0.2326, 0.1908, 0.1619, 0.2388, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19858449697494507 tensor([0.1919, 0.1821, 0.1986, 0.2091, 0.2183], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21807095408439636 tensor([0.2181, 0.1907, 0.1803, 0.2200, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21311092376708984 tensor([0.2313, 0.2131, 0.1679, 0.2184, 0.1693], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2280595749616623 tensor([0.2281, 0.1884, 0.1663, 0.2410, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21153505146503448 tensor([0.2115, 0.1914, 0.1817, 0.2200, 0.1954], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21712234616279602 tensor([0.2171, 0.1924, 0.1786, 0.2266, 0.1852], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20018917322158813 tensor([0.1967, 0.1907, 0.2002, 0.2140, 0.1984], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22901643812656403 tensor([0.2290, 0.1872, 0.1648, 0.2408, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19599400460720062 tensor([0.1969, 0.1866, 0.1960, 0.2128, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20242635905742645 tensor([0.2009, 0.1850, 0.1920, 0.2197, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2055756151676178 tensor([0.2349, 0.2056, 0.1622, 0.2253, 0.1719], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20258207619190216 tensor([0.1949, 0.1947, 0.2026, 0.2128, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22917738556861877 tensor([0.2292, 0.1908, 0.1673, 0.2341, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20639894902706146 tensor([0.2012, 0.1895, 0.1894, 0.2135, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19798144698143005 tensor([0.2417, 0.1980, 0.1588, 0.2289, 0.1726], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20351120829582214 tensor([0.2118, 0.2035, 0.1838, 0.2149, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2235787957906723 tensor([0.2236, 0.1886, 0.1727, 0.2338, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20510922372341156 tensor([0.2081, 0.1853, 0.1822, 0.2193, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2316773384809494 tensor([0.2445, 0.1943, 0.1549, 0.2317, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2038746476173401 tensor([0.2102, 0.2039, 0.1863, 0.2129, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20446695387363434 tensor([0.1855, 0.1914, 0.2140, 0.2047, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2344512641429901 tensor([0.2383, 0.1910, 0.1613, 0.2345, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19637510180473328 tensor([0.1922, 0.1824, 0.1964, 0.2081, 0.2210], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19741182029247284 tensor([0.2567, 0.1974, 0.1477, 0.2330, 0.1651], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21990440785884857 tensor([0.2199, 0.1903, 0.1776, 0.2271, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19686324894428253 tensor([0.2167, 0.1969, 0.1797, 0.2232, 0.1835], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23093101382255554 tensor([0.2309, 0.1943, 0.1650, 0.2309, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20293129980564117 tensor([0.2027, 0.1849, 0.1891, 0.2204, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19684937596321106 tensor([0.2080, 0.1968, 0.1904, 0.2129, 0.1919], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2047836035490036 tensor([0.2266, 0.2048, 0.1699, 0.2229, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1961272954940796 tensor([0.1938, 0.1961, 0.2064, 0.2117, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23417410254478455 tensor([0.2342, 0.1913, 0.1604, 0.2393, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21296004951000214 tensor([0.1993, 0.1810, 0.1912, 0.2130, 0.2155], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19838345050811768 tensor([0.2299, 0.1984, 0.1687, 0.2256, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1950019747018814 tensor([0.2186, 0.1950, 0.1766, 0.2220, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19669094681739807 tensor([0.1998, 0.1967, 0.1976, 0.2142, 0.1917], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2282591015100479 tensor([0.2293, 0.1932, 0.1676, 0.2283, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20588378608226776 tensor([0.1979, 0.1917, 0.1946, 0.2099, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22304494678974152 tensor([0.2230, 0.1943, 0.1748, 0.2247, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20329828560352325 tensor([0.2068, 0.2033, 0.1896, 0.2143, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19781829416751862 tensor([0.2003, 0.2007, 0.1978, 0.2122, 0.1890], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22785331308841705 tensor([0.2279, 0.1926, 0.1698, 0.2293, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20324069261550903 tensor([0.2039, 0.1876, 0.1882, 0.2170, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1958278864622116 tensor([0.2223, 0.1958, 0.1730, 0.2239, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2088622748851776 tensor([0.2361, 0.2089, 0.1637, 0.2237, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19763024151325226 tensor([0.1963, 0.1976, 0.2021, 0.2107, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2319795787334442 tensor([0.2320, 0.1913, 0.1626, 0.2353, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20131877064704895 tensor([0.2072, 0.1929, 0.1851, 0.2134, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2009708285331726 tensor([0.2350, 0.2010, 0.1675, 0.2232, 0.1734], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19999712705612183 tensor([0.2153, 0.2000, 0.1811, 0.2207, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19733332097530365 tensor([0.2037, 0.1973, 0.1937, 0.2151, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23634254932403564 tensor([0.2363, 0.1925, 0.1602, 0.2380, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20264093577861786 tensor([0.2032, 0.1892, 0.1910, 0.2139, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19678862392902374 tensor([0.2414, 0.1968, 0.1608, 0.2309, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20904621481895447 tensor([0.2303, 0.2090, 0.1683, 0.2199, 0.1725], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2229774296283722 tensor([0.2230, 0.1871, 0.1717, 0.2341, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2022487223148346 tensor([0.1867, 0.1802, 0.2022, 0.2095, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2340339571237564 tensor([0.2404, 0.1936, 0.1583, 0.2340, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20540373027324677 tensor([0.2114, 0.2054, 0.1822, 0.2144, 0.1866], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20414717495441437 tensor([0.1930, 0.1920, 0.2041, 0.2139, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19924095273017883 tensor([0.2306, 0.1992, 0.1664, 0.2298, 0.1740], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20150358974933624 tensor([0.1890, 0.1871, 0.2015, 0.2085, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1959092915058136 tensor([0.2481, 0.1959, 0.1540, 0.2320, 0.1700], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.228103905916214 tensor([0.2281, 0.1904, 0.1654, 0.2365, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20943953096866608 tensor([0.1978, 0.1880, 0.1936, 0.2112, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19525839388370514 tensor([0.2485, 0.1953, 0.1550, 0.2326, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20092497766017914 tensor([0.2221, 0.2009, 0.1733, 0.2224, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19558070600032806 tensor([0.2080, 0.1956, 0.1889, 0.2170, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23339153826236725 tensor([0.2334, 0.1893, 0.1624, 0.2380, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21081392467021942 tensor([0.1982, 0.1860, 0.1926, 0.2108, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.201048344373703 tensor([0.2446, 0.2010, 0.1555, 0.2293, 0.1696], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21160559356212616 tensor([0.2203, 0.2116, 0.1757, 0.2147, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20283035933971405 tensor([0.1950, 0.1944, 0.2028, 0.2158, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22588518261909485 tensor([0.2259, 0.1872, 0.1695, 0.2333, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20447450876235962 tensor([0.2040, 0.1861, 0.1883, 0.2171, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20291069149971008 tensor([0.2330, 0.2029, 0.1650, 0.2250, 0.1742], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20058035850524902 tensor([0.2046, 0.2006, 0.1908, 0.2155, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.199271097779274 tensor([0.2052, 0.1993, 0.1910, 0.2149, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23866330087184906 tensor([0.2387, 0.1875, 0.1584, 0.2388, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20885725319385529 tensor([0.2038, 0.1855, 0.1869, 0.2149, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19804753363132477 tensor([0.2334, 0.1980, 0.1678, 0.2250, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20274260640144348 tensor([0.2369, 0.2027, 0.1622, 0.2290, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19966772198677063 tensor([0.2047, 0.1997, 0.1900, 0.2146, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19628794491291046 tensor([0.2347, 0.1963, 0.1653, 0.2293, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20316772162914276 tensor([0.1897, 0.1828, 0.2032, 0.2107, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1946655809879303 tensor([0.2401, 0.1947, 0.1603, 0.2311, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1997997909784317 tensor([0.2272, 0.1998, 0.1722, 0.2240, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19632263481616974 tensor([0.1990, 0.1963, 0.2015, 0.2145, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2304094135761261 tensor([0.2304, 0.1929, 0.1672, 0.2341, 0.1753], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.212361678481102 tensor([0.2124, 0.1914, 0.1822, 0.2213, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.222132608294487 tensor([0.2221, 0.1924, 0.1775, 0.2268, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20289015769958496 tensor([0.2169, 0.2029, 0.1787, 0.2167, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2035323977470398 tensor([0.1950, 0.2037, 0.2035, 0.2099, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19486384093761444 tensor([0.2278, 0.1949, 0.1684, 0.2290, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2115744799375534 tensor([0.2013, 0.1864, 0.1880, 0.2116, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20550765097141266 tensor([0.2215, 0.2055, 0.1784, 0.2160, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2103387415409088 tensor([0.2267, 0.2103, 0.1710, 0.2163, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19381582736968994 tensor([0.2050, 0.1938, 0.1933, 0.2197, 0.1882], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23572145402431488 tensor([0.2357, 0.1908, 0.1603, 0.2400, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1972358077764511 tensor([0.1926, 0.1800, 0.1972, 0.2112, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2337818592786789 tensor([0.2414, 0.1924, 0.1589, 0.2338, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23122520744800568 tensor([0.2359, 0.1925, 0.1615, 0.2312, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19803063571453094 tensor([0.1932, 0.1858, 0.1980, 0.2088, 0.2142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23612403869628906 tensor([0.2538, 0.1931, 0.1488, 0.2361, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20130454003810883 tensor([0.2038, 0.2013, 0.1917, 0.2136, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19622093439102173 tensor([0.2104, 0.1962, 0.1862, 0.2200, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2216183990240097 tensor([0.2216, 0.1936, 0.1743, 0.2271, 0.1834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2075263261795044 tensor([0.2038, 0.1852, 0.1877, 0.2157, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19785603880882263 tensor([0.2306, 0.1979, 0.1672, 0.2255, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19820371270179749 tensor([0.2144, 0.1982, 0.1810, 0.2203, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1966545134782791 tensor([0.1979, 0.1967, 0.1998, 0.2121, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21992631256580353 tensor([0.2199, 0.1858, 0.1721, 0.2362, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2128300666809082 tensor([0.1992, 0.1810, 0.1887, 0.2183, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22520212829113007 tensor([0.2338, 0.1934, 0.1651, 0.2252, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1962830126285553 tensor([0.2012, 0.1963, 0.1941, 0.2112, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20109274983406067 tensor([0.2133, 0.2011, 0.1837, 0.2158, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21623218059539795 tensor([0.2162, 0.1899, 0.1773, 0.2316, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1965462863445282 tensor([0.1942, 0.1830, 0.1965, 0.2109, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19611695408821106 tensor([0.2196, 0.1961, 0.1774, 0.2201, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1986154466867447 tensor([0.2368, 0.1986, 0.1640, 0.2277, 0.1729], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20113719999790192 tensor([0.1972, 0.1926, 0.2011, 0.2153, 0.1938], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2091049998998642 tensor([0.2091, 0.1921, 0.1841, 0.2233, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2069239616394043 tensor([0.2028, 0.1894, 0.1890, 0.2120, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19605502486228943 tensor([0.2346, 0.1961, 0.1643, 0.2289, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21266470849514008 tensor([0.2183, 0.2127, 0.1784, 0.2136, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19733324646949768 tensor([0.1983, 0.1973, 0.1979, 0.2121, 0.1944], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19529713690280914 tensor([0.2379, 0.1953, 0.1597, 0.2346, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2132200002670288 tensor([0.1990, 0.1829, 0.1902, 0.2132, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19503770768642426 tensor([0.2358, 0.1950, 0.1635, 0.2292, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2010335624217987 tensor([0.2173, 0.2010, 0.1787, 0.2217, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1973475217819214 tensor([0.2045, 0.1973, 0.1931, 0.2143, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2281816303730011 tensor([0.2282, 0.1923, 0.1678, 0.2343, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2090664505958557 tensor([0.2091, 0.1887, 0.1842, 0.2195, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19753600656986237 tensor([0.2391, 0.1975, 0.1612, 0.2263, 0.1758], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2066129744052887 tensor([0.2026, 0.2066, 0.1941, 0.2111, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23564817011356354 tensor([0.2356, 0.1877, 0.1620, 0.2373, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1967097371816635 tensor([0.1967, 0.1905, 0.1967, 0.2116, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1989198476076126 tensor([0.2487, 0.1989, 0.1514, 0.2322, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19763438403606415 tensor([0.2070, 0.1976, 0.1893, 0.2143, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19789116084575653 tensor([0.2003, 0.1979, 0.1999, 0.2133, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22706617414951324 tensor([0.2271, 0.1895, 0.1676, 0.2364, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19701901078224182 tensor([0.1936, 0.1848, 0.1970, 0.2120, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1948443204164505 tensor([0.2532, 0.1948, 0.1497, 0.2349, 0.1673], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1978873610496521 tensor([0.2056, 0.1979, 0.1901, 0.2153, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1994095742702484 tensor([0.1972, 0.1942, 0.1994, 0.2142, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23032443225383759 tensor([0.2409, 0.1934, 0.1610, 0.2303, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21278811991214752 tensor([0.1981, 0.1807, 0.1913, 0.2128, 0.2171], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2166515290737152 tensor([0.2167, 0.1919, 0.1796, 0.2239, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20012284815311432 tensor([0.1963, 0.2011, 0.2001, 0.2093, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19859492778778076 tensor([0.1984, 0.1986, 0.2002, 0.2120, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19575512409210205 tensor([0.2352, 0.1958, 0.1606, 0.2383, 0.1702], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19776861369609833 tensor([0.1931, 0.1837, 0.1978, 0.2143, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22967542707920074 tensor([0.2378, 0.1921, 0.1610, 0.2297, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2004677802324295 tensor([0.2078, 0.2005, 0.1891, 0.2141, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19835089147090912 tensor([0.2005, 0.2004, 0.1984, 0.2138, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22894924879074097 tensor([0.2289, 0.1875, 0.1650, 0.2414, 0.1772], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20663267374038696 tensor([0.2068, 0.1857, 0.1839, 0.2169, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19918175041675568 tensor([0.2471, 0.1992, 0.1537, 0.2308, 0.1692], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20241792500019073 tensor([0.2247, 0.2024, 0.1762, 0.2185, 0.1782], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20615644752979279 tensor([0.1901, 0.1927, 0.2062, 0.2118, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2150225043296814 tensor([0.2150, 0.1881, 0.1789, 0.2271, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19651103019714355 tensor([0.1939, 0.1844, 0.1965, 0.2099, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19484058022499084 tensor([0.2463, 0.1948, 0.1536, 0.2326, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20912078022956848 tensor([0.2275, 0.2091, 0.1698, 0.2191, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19803838431835175 tensor([0.1921, 0.1980, 0.2081, 0.2103, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.197588711977005 tensor([0.2311, 0.1976, 0.1670, 0.2253, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2127370685338974 tensor([0.1955, 0.1816, 0.1955, 0.2127, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1973889023065567 tensor([0.2447, 0.1974, 0.1570, 0.2298, 0.1711], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1982426792383194 tensor([0.2104, 0.1982, 0.1835, 0.2191, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20237720012664795 tensor([0.2070, 0.2024, 0.1888, 0.2147, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23484118282794952 tensor([0.2348, 0.1903, 0.1634, 0.2348, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20673713088035583 tensor([0.2032, 0.1867, 0.1886, 0.2148, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20095868408679962 tensor([0.2420, 0.2010, 0.1603, 0.2267, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19465778768062592 tensor([0.2231, 0.1947, 0.1740, 0.2261, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1997368335723877 tensor([0.2058, 0.1997, 0.1938, 0.2116, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2325378954410553 tensor([0.2330, 0.1933, 0.1626, 0.2325, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.212406724691391 tensor([0.1950, 0.1785, 0.1929, 0.2124, 0.2212], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19857162237167358 tensor([0.2423, 0.1986, 0.1580, 0.2301, 0.1710], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2063242793083191 tensor([0.2187, 0.2063, 0.1785, 0.2174, 0.1791], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19781892001628876 tensor([0.1951, 0.1886, 0.1978, 0.2135, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2225540727376938 tensor([0.2226, 0.1939, 0.1715, 0.2264, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2099095582962036 tensor([0.1946, 0.1822, 0.1959, 0.2099, 0.2173], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 1], [0, 1, 4, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 2], [4, 2], [2, 4, 1], [1, 2, 0], [2, 4, 1], [4, 2, 1], [0, 1, 4], [4, 2, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4, 1, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 0, 2], [0, 1, 4], [2, 4, 1], [1, 2, 0], [2, 4], [0, 4, 2], [4, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [4, 2], [2, 4, 1], [0, 2, 4, 1], [4, 2], [0, 4, 2], [0, 4, 1], [4, 2, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [2, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [4, 2, 1], [0, 4, 2, 1], [0, 1, 4], [2, 4, 1], [0, 1, 2], [1, 0, 2], [2, 4], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [1, 2], [2, 4, 1], [4, 2], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 0, 2], [4, 0, 2, 1], [4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2], [0, 4, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 2, 1], [4, 2], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [1, 0, 4, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2], [0, 1, 4], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 4, 2], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [2, 4], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4], [4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4], [4, 0, 2], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [4, 2], [0, 4], [2, 4, 1], [0, 1, 2], [4, 2], [4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 1, 4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2], [4, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4, 2, 1], [4, 2], [4, 2, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [0, 4, 1], [4, 2, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 0, 2], [0, 1, 4, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2], [0, 4, 2], [2, 4, 1], [1, 2, 0], [2, 4, 1], [4, 2], [0, 1, 4], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 0, 2], [2, 4, 1], [1, 0, 2], [2, 4], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2], [0, 1, 2], [2, 4, 1], [0, 1, 2]]\n",
      "[[0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [1, 3], [0, 3], [0, 3], [0, 3], [0, 1, 3], [0, 1, 3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [3], [0, 3], [2, 3, 4], [0, 3], [1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [0, 1, 3], [0, 3], [3], [0, 1, 3], [1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [0, 3], [0, 3], [3], [2, 3], [0, 3], [3, 4], [3, 4], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [0, 3, 4], [0, 3], [0, 1, 3], [2, 4], [0, 3], [3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [1, 3], [1, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [2, 3, 4], [0, 3], [1], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 1, 3], [1, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [0, 3], [0, 3], [0, 1, 3], [1, 2, 3], [0, 3], [3, 4], [0, 1, 3], [0, 1, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [1, 3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [3], [0, 1, 3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [0, 3], [0, 3], [1, 3], [3], [0, 3], [3, 4], [0, 3], [3], [2, 3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [1, 2, 3], [2, 3], [0, 3], [3, 4], [0, 3], [1, 3], [1, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [1, 3], [0, 3], [3, 4], [0, 1, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3, 4], [0, 3], [3, 4]]\n",
      "NL_pred of 4th iteration [[2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [0, 4, 2], [4, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [2, 4, 1], [4, 2, 1], [0, 1, 2], [0, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 0, 2, 1], [0, 4, 1], [2, 4, 1], [2, 4, 1], [0, 4, 1], [0, 4, 2], [2, 4, 1], [0, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [4, 0, 2, 1], [2, 4, 1], [0, 4, 2, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [4, 0, 2, 1], [0, 1, 2], [0, 1, 2], [4, 2, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [0, 4, 2, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 4, 1], [2, 4, 1], [2, 4, 1], [0, 4, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2, 1], [0, 4, 1, 2], [0, 4, 1], [2, 4, 1], [0, 1, 2], [0, 4, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.014597197825258429  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.014596525918353687  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.014595249837095087  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014593429186127403  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.014591127634048462  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.01458838311108676  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.014585251157934015  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.014581769704818726  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.014577975327318365  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.014573904601010408  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.01456958461891521  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.014565042474053123  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01456030932339755  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.014555400068109686  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.014550340446558866  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.014545149423859337  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.014539839191870256  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.014534426006403837  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.014528924768621271  Accuracy on Support set:0.0\n",
      "torch.Size([88, 2048]) torch.Size([88])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.014523344961079683  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.2330155223608017 tensor([0.2441, 0.1886, 0.1579, 0.2330, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2138189673423767 tensor([0.2138, 0.1895, 0.1837, 0.2237, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23727253079414368 tensor([0.2373, 0.1837, 0.1608, 0.2425, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21673327684402466 tensor([0.2167, 0.1794, 0.1766, 0.2257, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23486611247062683 tensor([0.2452, 0.1873, 0.1565, 0.2349, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21502076089382172 tensor([0.2150, 0.1853, 0.1809, 0.2263, 0.1924], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22725394368171692 tensor([0.2273, 0.1814, 0.1673, 0.2410, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21682274341583252 tensor([0.1997, 0.1750, 0.1898, 0.2168, 0.2188], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23438671231269836 tensor([0.2425, 0.1878, 0.1592, 0.2344, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22057582437992096 tensor([0.2206, 0.1906, 0.1749, 0.2264, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19316649436950684 tensor([0.2079, 0.1932, 0.1917, 0.2200, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23077386617660522 tensor([0.2308, 0.1825, 0.1661, 0.2383, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20920391380786896 tensor([0.2092, 0.1809, 0.1842, 0.2247, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22486388683319092 tensor([0.2249, 0.1852, 0.1725, 0.2270, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19417589902877808 tensor([0.2126, 0.1942, 0.1853, 0.2181, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1963510811328888 tensor([0.2182, 0.1964, 0.1812, 0.2222, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2357582300901413 tensor([0.2371, 0.1827, 0.1633, 0.2358, 0.1811], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21537715196609497 tensor([0.2077, 0.1759, 0.1829, 0.2181, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2326699197292328 tensor([0.2485, 0.1902, 0.1544, 0.2327, 0.1742], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21054057776927948 tensor([0.2105, 0.1906, 0.1853, 0.2218, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20797327160835266 tensor([0.1925, 0.1868, 0.2080, 0.2133, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22535164654254913 tensor([0.2254, 0.1894, 0.1723, 0.2310, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21061591804027557 tensor([0.2028, 0.1809, 0.1874, 0.2183, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19378145039081573 tensor([0.2432, 0.1938, 0.1586, 0.2313, 0.1731], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19563546776771545 tensor([0.2242, 0.1956, 0.1749, 0.2210, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23680658638477325 tensor([0.2368, 0.1796, 0.1613, 0.2423, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2017267644405365 tensor([0.1925, 0.1751, 0.2017, 0.2142, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23209810256958008 tensor([0.2471, 0.1904, 0.1556, 0.2321, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19778373837471008 tensor([0.2089, 0.1978, 0.1897, 0.2133, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20123173296451569 tensor([0.1973, 0.1867, 0.2012, 0.2150, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24375708401203156 tensor([0.2438, 0.1828, 0.1553, 0.2446, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2094157487154007 tensor([0.2095, 0.1756, 0.1827, 0.2228, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1945980191230774 tensor([0.2341, 0.1946, 0.1667, 0.2256, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19212031364440918 tensor([0.2019, 0.1921, 0.1974, 0.2159, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21553556621074677 tensor([0.2155, 0.1910, 0.1843, 0.2244, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23055897653102875 tensor([0.2359, 0.1882, 0.1650, 0.2306, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2009044587612152 tensor([0.1915, 0.1743, 0.2009, 0.2116, 0.2218], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22862033545970917 tensor([0.2311, 0.1904, 0.1680, 0.2286, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19745483994483948 tensor([0.2195, 0.1975, 0.1772, 0.2221, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2031058520078659 tensor([0.1943, 0.1877, 0.2031, 0.2139, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2226017862558365 tensor([0.2226, 0.1855, 0.1728, 0.2313, 0.1878], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21230611205101013 tensor([0.1997, 0.1803, 0.1928, 0.2149, 0.2123], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23350706696510315 tensor([0.2435, 0.1843, 0.1592, 0.2335, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2026475965976715 tensor([0.2256, 0.2026, 0.1753, 0.2183, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19407671689987183 tensor([0.2119, 0.1941, 0.1870, 0.2214, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22948488593101501 tensor([0.2323, 0.1883, 0.1680, 0.2295, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19344937801361084 tensor([0.2195, 0.1934, 0.1796, 0.2220, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1961420476436615 tensor([0.2054, 0.1961, 0.1909, 0.2151, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20592325925827026 tensor([0.1946, 0.1868, 0.2059, 0.2157, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21521320939064026 tensor([0.2152, 0.1924, 0.1813, 0.2211, 0.1900], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2132975459098816 tensor([0.2064, 0.1789, 0.1839, 0.2175, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23354646563529968 tensor([0.2452, 0.1885, 0.1573, 0.2335, 0.1755], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2193680703639984 tensor([0.2194, 0.1881, 0.1802, 0.2254, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2200319617986679 tensor([0.2200, 0.1863, 0.1775, 0.2274, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23489433526992798 tensor([0.2349, 0.1836, 0.1613, 0.2421, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21216672658920288 tensor([0.1941, 0.1756, 0.1972, 0.2122, 0.2209], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2204364538192749 tensor([0.2204, 0.1836, 0.1792, 0.2232, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20528694987297058 tensor([0.2341, 0.2053, 0.1671, 0.2219, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23048283159732819 tensor([0.2305, 0.1813, 0.1653, 0.2443, 0.1786], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21391387283802032 tensor([0.2139, 0.1843, 0.1805, 0.2233, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21956607699394226 tensor([0.2196, 0.1851, 0.1775, 0.2302, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1989816576242447 tensor([0.1990, 0.1837, 0.1990, 0.2173, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23119664192199707 tensor([0.2312, 0.1802, 0.1641, 0.2440, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21035458147525787 tensor([0.1992, 0.1798, 0.1947, 0.2160, 0.2104], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20498336851596832 tensor([0.2031, 0.1782, 0.1907, 0.2230, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19778256118297577 tensor([0.2376, 0.1978, 0.1614, 0.2289, 0.1743], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20143935084342957 tensor([0.1972, 0.1876, 0.2014, 0.2162, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23150043189525604 tensor([0.2315, 0.1837, 0.1665, 0.2374, 0.1808], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2090703397989273 tensor([0.2035, 0.1825, 0.1882, 0.2168, 0.2091], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23242898285388947 tensor([0.2444, 0.1904, 0.1580, 0.2324, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19607898592948914 tensor([0.2143, 0.1961, 0.1828, 0.2183, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2259744107723236 tensor([0.2260, 0.1815, 0.1717, 0.2372, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2076486200094223 tensor([0.2103, 0.1785, 0.1810, 0.2225, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23498180508613586 tensor([0.2469, 0.1869, 0.1543, 0.2350, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1963348388671875 tensor([0.2127, 0.1963, 0.1853, 0.2164, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20721033215522766 tensor([0.1877, 0.1845, 0.2127, 0.2078, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23781529068946838 tensor([0.2407, 0.1837, 0.1605, 0.2378, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21115927398204803 tensor([0.1943, 0.1759, 0.1951, 0.2112, 0.2236], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23625637590885162 tensor([0.2593, 0.1902, 0.1472, 0.2363, 0.1671], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22233454883098602 tensor([0.2223, 0.1831, 0.1765, 0.2306, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2191602885723114 tensor([0.2192, 0.1894, 0.1787, 0.2267, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23345065116882324 tensor([0.2335, 0.1868, 0.1641, 0.2344, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20546014606952667 tensor([0.2050, 0.1781, 0.1878, 0.2236, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19705314934253693 tensor([0.2292, 0.1971, 0.1690, 0.2265, 0.1783], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20525923371315002 tensor([0.1961, 0.1891, 0.2053, 0.2150, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23661164939403534 tensor([0.2366, 0.1840, 0.1596, 0.2428, 0.1770], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21605432033538818 tensor([0.2016, 0.1744, 0.1899, 0.2161, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22910302877426147 tensor([0.2325, 0.1908, 0.1678, 0.2291, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22112183272838593 tensor([0.2211, 0.1877, 0.1756, 0.2254, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1964922398328781 tensor([0.2021, 0.1895, 0.1965, 0.2175, 0.1943], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23173558712005615 tensor([0.2318, 0.1858, 0.1666, 0.2317, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20857352018356323 tensor([0.2002, 0.1847, 0.1934, 0.2131, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22554261982440948 tensor([0.2255, 0.1869, 0.1738, 0.2281, 0.1856], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19580808281898499 tensor([0.2092, 0.1958, 0.1886, 0.2178, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19334910809993744 tensor([0.2027, 0.1933, 0.1967, 0.2156, 0.1916], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23037981986999512 tensor([0.2304, 0.1852, 0.1687, 0.2328, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20584554970264435 tensor([0.2062, 0.1806, 0.1871, 0.2203, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2248174250125885 tensor([0.2248, 0.1884, 0.1720, 0.2274, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20110338926315308 tensor([0.2389, 0.2011, 0.1629, 0.2273, 0.1698], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20104481279850006 tensor([0.1985, 0.1905, 0.2010, 0.2140, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23430979251861572 tensor([0.2343, 0.1841, 0.1620, 0.2386, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20392310619354248 tensor([0.2096, 0.1859, 0.1840, 0.2167, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19352224469184875 tensor([0.2377, 0.1935, 0.1666, 0.2266, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21782581508159637 tensor([0.2178, 0.1925, 0.1801, 0.2242, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23869439959526062 tensor([0.2387, 0.1852, 0.1595, 0.2413, 0.1753], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20524054765701294 tensor([0.2055, 0.1823, 0.1898, 0.2172, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23446209728717804 tensor([0.2441, 0.1892, 0.1599, 0.2345, 0.1724], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20129413902759552 tensor([0.2330, 0.2013, 0.1675, 0.2234, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22536517679691315 tensor([0.2254, 0.1801, 0.1707, 0.2374, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20078758895397186 tensor([0.1888, 0.1737, 0.2008, 0.2126, 0.2241], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2373599410057068 tensor([0.2428, 0.1862, 0.1576, 0.2374, 0.1760], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19789083302021027 tensor([0.2139, 0.1979, 0.1813, 0.2178, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20297269523143768 tensor([0.1952, 0.1850, 0.2030, 0.2173, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23323681950569153 tensor([0.2332, 0.1916, 0.1654, 0.2335, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2002311795949936 tensor([0.1912, 0.1804, 0.2002, 0.2116, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23535440862178802 tensor([0.2507, 0.1884, 0.1533, 0.2354, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23044082522392273 tensor([0.2304, 0.1832, 0.1646, 0.2398, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21209053695201874 tensor([0.2000, 0.1811, 0.1924, 0.2144, 0.2121], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2359631210565567 tensor([0.2510, 0.1878, 0.1545, 0.2360, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19333750009536743 tensor([0.2247, 0.1933, 0.1723, 0.2259, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23576319217681885 tensor([0.2358, 0.1821, 0.1616, 0.2413, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21395447850227356 tensor([0.2004, 0.1792, 0.1913, 0.2140, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19327044486999512 tensor([0.2473, 0.1933, 0.1546, 0.2329, 0.1719], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20387762784957886 tensor([0.2229, 0.2039, 0.1749, 0.2182, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20166055858135223 tensor([0.1973, 0.1873, 0.2017, 0.2192, 0.1946], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2281453013420105 tensor([0.2281, 0.1802, 0.1687, 0.2365, 0.1865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20706698298454285 tensor([0.2062, 0.1792, 0.1871, 0.2204, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19518592953681946 tensor([0.2357, 0.1952, 0.1641, 0.2285, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19322353601455688 tensor([0.2070, 0.1932, 0.1898, 0.2189, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2408779263496399 tensor([0.2409, 0.1804, 0.1578, 0.2420, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2114330381155014 tensor([0.2060, 0.1787, 0.1857, 0.2181, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22839826345443726 tensor([0.2361, 0.1906, 0.1668, 0.2284, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19499856233596802 tensor([0.2396, 0.1950, 0.1614, 0.2326, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23284538090229034 tensor([0.2373, 0.1888, 0.1643, 0.2328, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20184364914894104 tensor([0.1919, 0.1762, 0.2018, 0.2139, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23445427417755127 tensor([0.2426, 0.1873, 0.1596, 0.2345, 0.1761], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2274731695652008 tensor([0.2297, 0.1923, 0.1713, 0.2275, 0.1792], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20041833817958832 tensor([0.2013, 0.1892, 0.2004, 0.2178, 0.1913], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23294949531555176 tensor([0.2329, 0.1855, 0.1662, 0.2377, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2147696316242218 tensor([0.2148, 0.1842, 0.1811, 0.2247, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22459086775779724 tensor([0.2246, 0.1852, 0.1764, 0.2302, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19533318281173706 tensor([0.2194, 0.1953, 0.1777, 0.2202, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19638964533805847 tensor([0.1974, 0.1964, 0.2025, 0.2133, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23029127717018127 tensor([0.2303, 0.1874, 0.1674, 0.2325, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21474424004554749 tensor([0.2035, 0.1796, 0.1868, 0.2147, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.197999969124794 tensor([0.2241, 0.1980, 0.1775, 0.2194, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20262999832630157 tensor([0.2295, 0.2026, 0.1701, 0.2198, 0.1780], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2380259484052658 tensor([0.2380, 0.1836, 0.1596, 0.2433, 0.1755], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21428512036800385 tensor([0.1948, 0.1736, 0.1959, 0.2143, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23699656128883362 tensor([0.2438, 0.1852, 0.1582, 0.2370, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23464545607566833 tensor([0.2384, 0.1851, 0.1607, 0.2346, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21197104454040527 tensor([0.1954, 0.1790, 0.1967, 0.2120, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23942157626152039 tensor([0.2560, 0.1860, 0.1484, 0.2394, 0.1702], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19392520189285278 tensor([0.2062, 0.1939, 0.1907, 0.2170, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21286813914775848 tensor([0.2129, 0.1889, 0.1852, 0.2234, 0.1896], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22410105168819427 tensor([0.2241, 0.1863, 0.1733, 0.2305, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21009983122348785 tensor([0.2061, 0.1784, 0.1865, 0.2189, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22896035015583038 tensor([0.2332, 0.1903, 0.1663, 0.2290, 0.1813], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2168971449136734 tensor([0.2169, 0.1908, 0.1800, 0.2238, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19866116344928741 tensor([0.2002, 0.1894, 0.1987, 0.2154, 0.1962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2223065048456192 tensor([0.2223, 0.1788, 0.1710, 0.2396, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21542330086231232 tensor([0.2014, 0.1743, 0.1874, 0.2215, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22860030829906464 tensor([0.2362, 0.1860, 0.1642, 0.2286, 0.1849], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1937001496553421 tensor([0.2158, 0.1937, 0.1827, 0.2192, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21863044798374176 tensor([0.2186, 0.1827, 0.1762, 0.2351, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2140413522720337 tensor([0.1965, 0.1764, 0.1952, 0.2140, 0.2179], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2220485508441925 tensor([0.2220, 0.1888, 0.1764, 0.2235, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23123018443584442 tensor([0.2395, 0.1910, 0.1631, 0.2312, 0.1752], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19996139407157898 tensor([0.1995, 0.1855, 0.2000, 0.2187, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21146486699581146 tensor([0.2115, 0.1849, 0.1830, 0.2268, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20954746007919312 tensor([0.2051, 0.1825, 0.1878, 0.2152, 0.2095], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23245133459568024 tensor([0.2373, 0.1885, 0.1633, 0.2325, 0.1785], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2049390822649002 tensor([0.2209, 0.2049, 0.1776, 0.2171, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19677917659282684 tensor([0.2006, 0.1901, 0.1968, 0.2155, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23803991079330444 tensor([0.2404, 0.1879, 0.1590, 0.2380, 0.1747], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21635250747203827 tensor([0.2012, 0.1762, 0.1890, 0.2164, 0.2173], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2326934039592743 tensor([0.2384, 0.1876, 0.1626, 0.2327, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19346708059310913 tensor([0.2198, 0.1935, 0.1777, 0.2252, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23067565262317657 tensor([0.2307, 0.1849, 0.1669, 0.2378, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21138671040534973 tensor([0.2114, 0.1817, 0.1831, 0.2228, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22979409992694855 tensor([0.2417, 0.1900, 0.1604, 0.2298, 0.1781], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1991608738899231 tensor([0.2051, 0.1992, 0.1931, 0.2145, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2378987818956375 tensor([0.2379, 0.1808, 0.1614, 0.2405, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20720985531806946 tensor([0.1989, 0.1836, 0.1955, 0.2148, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2356206625699997 tensor([0.2513, 0.1914, 0.1509, 0.2356, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19883352518081665 tensor([0.2027, 0.1906, 0.1988, 0.2167, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22949960827827454 tensor([0.2295, 0.1822, 0.1666, 0.2399, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21516388654708862 tensor([0.1958, 0.1781, 0.1957, 0.2152, 0.2152], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23817123472690582 tensor([0.2556, 0.1876, 0.1492, 0.2382, 0.1694], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23369266092777252 tensor([0.2434, 0.1861, 0.1601, 0.2337, 0.1766], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21586763858795166 tensor([0.2003, 0.1742, 0.1900, 0.2159, 0.2196], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21906986832618713 tensor([0.2191, 0.1847, 0.1785, 0.2273, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19384773075580597 tensor([0.1986, 0.1938, 0.1991, 0.2127, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19909542798995972 tensor([0.2008, 0.1913, 0.1991, 0.2154, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23773284256458282 tensor([0.2377, 0.1881, 0.1597, 0.2419, 0.1725], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2137209177017212 tensor([0.1953, 0.1770, 0.1964, 0.2175, 0.2137], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23286671936511993 tensor([0.2401, 0.1849, 0.1604, 0.2329, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19319112598896027 tensor([0.2102, 0.1932, 0.1882, 0.2175, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19313611090183258 tensor([0.2029, 0.1931, 0.1973, 0.2172, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23133158683776855 tensor([0.2313, 0.1805, 0.1641, 0.2447, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20919716358184814 tensor([0.2091, 0.1789, 0.1828, 0.2201, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23434387147426605 tensor([0.2498, 0.1916, 0.1529, 0.2343, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19493526220321655 tensor([0.2272, 0.1949, 0.1753, 0.2219, 0.1806], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.204958975315094 tensor([0.1923, 0.1857, 0.2050, 0.2151, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21738414466381073 tensor([0.2174, 0.1809, 0.1778, 0.2306, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21301452815532684 tensor([0.1961, 0.1777, 0.1952, 0.2130, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23597070574760437 tensor([0.2486, 0.1875, 0.1531, 0.2360, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20140035450458527 tensor([0.2303, 0.2014, 0.1690, 0.2226, 0.1768], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20694726705551147 tensor([0.1943, 0.1909, 0.2069, 0.2137, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2287539839744568 tensor([0.2336, 0.1902, 0.1661, 0.2288, 0.1814], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21588340401649475 tensor([0.1976, 0.1750, 0.1941, 0.2159, 0.2173], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2332869917154312 tensor([0.2474, 0.1897, 0.1562, 0.2333, 0.1734], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.212858185172081 tensor([0.2129, 0.1909, 0.1824, 0.2225, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19491790235042572 tensor([0.2095, 0.1949, 0.1878, 0.2182, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2372058779001236 tensor([0.2372, 0.1831, 0.1625, 0.2382, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20924769341945648 tensor([0.2054, 0.1799, 0.1874, 0.2180, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19332852959632874 tensor([0.2447, 0.1933, 0.1595, 0.2302, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22553399205207825 tensor([0.2255, 0.1873, 0.1730, 0.2296, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23541733622550964 tensor([0.2354, 0.1860, 0.1619, 0.2359, 0.1808], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2154902219772339 tensor([0.1971, 0.1720, 0.1917, 0.2155, 0.2237], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23371067643165588 tensor([0.2450, 0.1908, 0.1571, 0.2337, 0.1733], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19869883358478546 tensor([0.2213, 0.1987, 0.1776, 0.2208, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2075130045413971 tensor([0.1973, 0.1818, 0.1966, 0.2167, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22501006722450256 tensor([0.2250, 0.1866, 0.1705, 0.2298, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21300433576107025 tensor([0.1968, 0.1756, 0.1946, 0.2130, 0.2200], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 1], [0, 1, 4, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 2, 0], [2, 4, 1], [4, 2, 1], [0, 1, 4], [4, 2, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 0, 2, 1], [0, 1, 4], [2, 4, 1], [1, 2, 0], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [4, 2, 1], [2, 4, 1], [0, 2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [0, 4, 1], [4, 2, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [2, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [4, 2, 1], [0, 4, 2, 1], [0, 1, 4, 2], [2, 4, 1], [0, 1, 2], [1, 0, 2], [2, 4, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 2], [2, 4, 1], [4, 2, 1], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 0, 2], [4, 0, 2, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 2, 1], [4, 2, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [1, 0, 4, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 1, 4], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 4, 2], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 0, 2, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [4, 2, 1], [4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 1, 4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [4, 2, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [0, 4, 1, 2], [4, 2, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 0, 2, 1], [0, 1, 4, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 2, 0], [2, 4, 1], [4, 2, 1], [0, 1, 4], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 1, 2], [2, 4, 1], [0, 1, 2]]\n",
      "[[0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [2, 3, 4], [0, 3], [3], [2, 3], [0, 3], [3, 4], [0, 3], [3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [0, 3], [0, 3], [3], [0, 3], [3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [0, 3], [0, 3], [3], [3], [0, 3], [3, 4], [3, 4], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [0, 3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [2, 3, 4], [0, 3], [1], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [0, 3], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [2, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3, 4], [0, 3], [3, 4]]\n",
      "NL_pred of 5th iteration [[4, 0, 2, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [4, 2, 1], [4, 0, 2, 1], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [4, 2, 1], [4, 2, 1], [0, 4, 2, 1], [0, 1, 4, 2], [2, 4, 1], [4, 2, 1], [4, 2, 1], [4, 2, 1], [0, 4, 1, 2], [4, 0, 2, 1], [0, 4, 2, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [4, 2, 1], [4, 2, 1], [0, 4, 1], [4, 2, 1], [0, 4, 2, 1], [0, 4, 1, 2], [4, 2, 1], [0, 4, 1, 2], [0, 4, 1, 2], [4, 2, 1], [4, 0, 2, 1], [0, 4, 1, 2], [0, 4, 1], [0, 4, 1, 2], [4, 0, 2, 1], [0, 4, 2, 1], [4, 2, 1], [4, 0, 2, 1], [2, 4, 1], [4, 2, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.027886766454447872  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.02788542146268098  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.027882861054461937  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.027879214805105457  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.027874596740888512  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.02786909756453141  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.027862813161767048  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.027855839418328327  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.027848238530366318  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.027840077877044678  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.027831422246020775  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.02782232605892679  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.027812846328901207  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.027803014154019562  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.027792881364407745  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.027782468692116116  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.027771825375764267  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.027760969555896263  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.02774993492209393  Accuracy on Support set:0.0\n",
      "torch.Size([46, 2048]) torch.Size([46])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.027738737023395042  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 1], [0, 1, 4, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 2, 0], [2, 4, 1], [4, 2, 1], [0, 1, 4], [4, 2, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 0, 2, 1], [0, 1, 4], [2, 4, 1], [1, 2, 0], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [4, 2, 1], [2, 4, 1], [0, 2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [0, 4, 1], [4, 2, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [2, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 2, 1], [2, 4, 1], [2, 1, 4], [4, 2, 1], [0, 4, 2, 1], [0, 1, 4, 2], [2, 4, 1], [0, 1, 2], [1, 0, 2], [2, 4, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 2], [2, 4, 1], [4, 2, 1], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 0, 2], [4, 0, 2, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 2, 1], [4, 2, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [1, 0, 4, 2], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 1, 4], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 4, 2], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 0, 2, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [2, 4, 1], [4, 2, 1], [4, 2, 1], [0, 4, 1], [2, 4, 1], [0, 1, 2], [4, 2, 1], [4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 1, 4, 2], [4, 0, 2, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4, 2, 1], [4, 2, 1], [4, 2, 1], [0, 1, 2], [2, 4, 1], [2, 4, 1], [0, 4, 1, 2], [4, 2, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 2, 4], [2, 4, 1], [4, 0, 2, 1], [0, 1, 4, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [0, 4, 2, 1], [0, 4, 1, 2], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 4, 1], [0, 4, 1, 2], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 0, 2, 1], [0, 4, 2, 1], [2, 4, 1], [1, 2, 0], [2, 4, 1], [4, 2, 1], [0, 1, 4], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2], [0, 4, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [4, 2, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [4, 0, 2, 1], [2, 4, 1], [1, 0, 2], [2, 4, 1], [4, 2, 1], [0, 1, 2], [2, 4, 1], [0, 1, 2]]\n",
      "POSITION :  [[0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [2, 3, 4], [0, 3], [3], [2, 3], [0, 3], [3, 4], [0, 3], [3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [0, 3], [0, 3], [3], [0, 3], [3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [0, 3], [0, 3], [3], [3], [0, 3], [3, 4], [3, 4], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [0, 3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [2, 3, 4], [0, 3], [1], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [0, 3], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [3], [0, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [0, 3], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [2, 3], [3], [0, 3], [3, 4], [0, 3], [3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 1, 3], [2, 3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3], [0, 3], [3, 4], [0, 3], [0, 3], [3, 4], [0, 3], [3, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.66\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 97.72727272727273\n",
      "Epoch: 1  Loss: 97.72727272727273\n",
      "Epoch: 2  Loss: 97.72727272727273\n",
      "Epoch: 3  Loss: 97.72727272727273\n",
      "Epoch: 4  Loss: 97.72727272727273\n",
      "Epoch: 5  Loss: 97.72727272727273\n",
      "Epoch: 6  Loss: 97.72727272727273\n",
      "Epoch: 7  Loss: 97.72727272727273\n",
      "Epoch: 8  Loss: 97.72727272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 3/15 [02:16<08:30, 42.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 97.72727272727273\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 3.339193437099457  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 2.1739509320259094  Accuracy on Support set:20.0\n",
      "Train_Epoch: 2  Train_Loss: 1.8249311828613282  Accuracy on Support set:20.0\n",
      "Train_Epoch: 3  Train_Loss: 1.719768373966217  Accuracy on Support set:20.0\n",
      "Train_Epoch: 4  Train_Loss: 1.6721134996414184  Accuracy on Support set:20.0\n",
      "Train_Epoch: 5  Train_Loss: 1.6441881656646729  Accuracy on Support set:20.0\n",
      "Train_Epoch: 6  Train_Loss: 1.6255896663665772  Accuracy on Support set:20.0\n",
      "Train_Epoch: 7  Train_Loss: 1.612050437927246  Accuracy on Support set:20.0\n",
      "Train_Epoch: 8  Train_Loss: 1.6016011619567871  Accuracy on Support set:20.0\n",
      "Train_Epoch: 9  Train_Loss: 1.5931372404098512  Accuracy on Support set:20.0\n",
      "Train_Epoch: 10  Train_Loss: 1.5858819246292115  Accuracy on Support set:20.0\n",
      "Train_Epoch: 11  Train_Loss: 1.5793697357177734  Accuracy on Support set:20.0\n",
      "Train_Epoch: 12  Train_Loss: 1.573344531059265  Accuracy on Support set:20.0\n",
      "Train_Epoch: 13  Train_Loss: 1.5676137351989745  Accuracy on Support set:20.0\n",
      "Train_Epoch: 14  Train_Loss: 1.5621047401428223  Accuracy on Support set:20.0\n",
      "Train_Epoch: 15  Train_Loss: 1.5568972110748291  Accuracy on Support set:24.0\n",
      "Train_Epoch: 16  Train_Loss: 1.5518173456192017  Accuracy on Support set:24.0\n",
      "Train_Epoch: 17  Train_Loss: 1.546781940460205  Accuracy on Support set:24.0\n",
      "Train_Epoch: 18  Train_Loss: 1.541735019683838  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 19  Train_Loss: 1.536690492630005  Accuracy on Support set:32.0\n",
      "Train_Epoch: 20  Train_Loss: 1.5315101909637452  Accuracy on Support set:36.0\n",
      "Train_Epoch: 21  Train_Loss: 1.526258158683777  Accuracy on Support set:44.0\n",
      "Train_Epoch: 22  Train_Loss: 1.520753960609436  Accuracy on Support set:48.0\n",
      "Train_Epoch: 23  Train_Loss: 1.5151039218902589  Accuracy on Support set:48.0\n",
      "Train_Epoch: 24  Train_Loss: 1.5092091703414916  Accuracy on Support set:52.0\n",
      "Train_Epoch: 25  Train_Loss: 1.503099675178528  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 26  Train_Loss: 1.496859483718872  Accuracy on Support set:68.0\n",
      "Train_Epoch: 27  Train_Loss: 1.4903826713562012  Accuracy on Support set:68.0\n",
      "Train_Epoch: 28  Train_Loss: 1.4836199140548707  Accuracy on Support set:76.0\n",
      "Train_Epoch: 29  Train_Loss: 1.4764985990524293  Accuracy on Support set:76.0\n",
      "Train_Epoch: 30  Train_Loss: 1.4691701078414916  Accuracy on Support set:76.0\n",
      "Train_Epoch: 31  Train_Loss: 1.4614112758636475  Accuracy on Support set:76.0\n",
      "Train_Epoch: 32  Train_Loss: 1.4533829164505006  Accuracy on Support set:76.0\n",
      "Train_Epoch: 33  Train_Loss: 1.4448283052444457  Accuracy on Support set:76.0\n",
      "Train_Epoch: 34  Train_Loss: 1.4360438442230226  Accuracy on Support set:76.0\n",
      "Train_Epoch: 35  Train_Loss: 1.4266724300384521  Accuracy on Support set:76.0\n",
      "Train_Epoch: 36  Train_Loss: 1.4169608688354491  Accuracy on Support set:80.0\n",
      "Train_Epoch: 37  Train_Loss: 1.4067733192443848  Accuracy on Support set:80.0\n",
      "Train_Epoch: 38  Train_Loss: 1.3962532901763915  Accuracy on Support set:80.0\n",
      "Train_Epoch: 39  Train_Loss: 1.384980821609497  Accuracy on Support set:80.0\n",
      "Train_Epoch: 40  Train_Loss: 1.3735352897644042  Accuracy on Support set:84.0\n",
      "Train_Epoch: 41  Train_Loss: 1.361298632621765  Accuracy on Support set:84.0\n",
      "Train_Epoch: 42  Train_Loss: 1.348781623840332  Accuracy on Support set:88.0\n",
      "Train_Epoch: 43  Train_Loss: 1.335639979839325  Accuracy on Support set:88.0\n",
      "Train_Epoch: 44  Train_Loss: 1.3220138478279113  Accuracy on Support set:88.0\n",
      "Train_Epoch: 45  Train_Loss: 1.3079181003570557  Accuracy on Support set:88.0\n",
      "Train_Epoch: 46  Train_Loss: 1.293216609954834  Accuracy on Support set:88.0\n",
      "Train_Epoch: 47  Train_Loss: 1.278141884803772  Accuracy on Support set:88.0\n",
      "Train_Epoch: 48  Train_Loss: 1.2624024653434753  Accuracy on Support set:88.0\n",
      "Train_Epoch: 49  Train_Loss: 1.2463355350494385  Accuracy on Support set:88.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  54.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.09505375474691391 tensor([0.3590, 0.1722, 0.0951, 0.2437, 0.1300], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17815624177455902 tensor([0.2038, 0.2060, 0.1782, 0.2194, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13322222232818604 tensor([0.1332, 0.1975, 0.2487, 0.1920, 0.2285], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16304385662078857 tensor([0.1630, 0.1902, 0.2154, 0.2233, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1471882462501526 tensor([0.1472, 0.1936, 0.2328, 0.2084, 0.2179], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08984515815973282 tensor([0.3779, 0.1849, 0.0898, 0.2215, 0.1258], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13586056232452393 tensor([0.1359, 0.2029, 0.2328, 0.1968, 0.2317], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11202099174261093 tensor([0.1120, 0.1819, 0.2889, 0.1916, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1234954372048378 tensor([0.2977, 0.2066, 0.1235, 0.2245, 0.1476], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14969463646411896 tensor([0.2501, 0.2007, 0.1497, 0.2279, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14742901921272278 tensor([0.2556, 0.2123, 0.1474, 0.2156, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1339568942785263 tensor([0.1340, 0.2185, 0.2370, 0.1813, 0.2293], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14752236008644104 tensor([0.1475, 0.2093, 0.2253, 0.1986, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13913123309612274 tensor([0.1391, 0.2013, 0.2348, 0.2079, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1569882184267044 tensor([0.1570, 0.2140, 0.2181, 0.1999, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10150256007909775 tensor([0.3423, 0.1760, 0.1015, 0.2464, 0.1338], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16619615256786346 tensor([0.1662, 0.2107, 0.2055, 0.2050, 0.2126], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08263219892978668 tensor([0.0826, 0.1690, 0.3274, 0.1767, 0.2442], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15668438374996185 tensor([0.1567, 0.1970, 0.2233, 0.2115, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17197328805923462 tensor([0.1720, 0.2150, 0.2004, 0.2049, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11952868849039078 tensor([0.3044, 0.1907, 0.1195, 0.2341, 0.1514], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17741698026657104 tensor([0.1990, 0.2146, 0.1774, 0.2074, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10063637793064117 tensor([0.1006, 0.1875, 0.2891, 0.1813, 0.2415], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1747017353773117 tensor([0.1747, 0.1764, 0.2027, 0.2474, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17616894841194153 tensor([0.1762, 0.2122, 0.1982, 0.2099, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18330445885658264 tensor([0.2045, 0.1873, 0.1833, 0.2362, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17281748354434967 tensor([0.2038, 0.2101, 0.1728, 0.2176, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18269844353199005 tensor([0.1827, 0.2202, 0.1900, 0.2051, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12218116223812103 tensor([0.2927, 0.1846, 0.1222, 0.2481, 0.1525], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15878096222877502 tensor([0.1588, 0.1939, 0.2144, 0.2213, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08006110787391663 tensor([0.4010, 0.1664, 0.0801, 0.2362, 0.1163], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16719302535057068 tensor([0.1672, 0.2222, 0.2063, 0.1942, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09409814327955246 tensor([0.0941, 0.1946, 0.3010, 0.1738, 0.2365], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18618591129779816 tensor([0.1862, 0.1928, 0.1900, 0.2337, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12796159088611603 tensor([0.1280, 0.1948, 0.2558, 0.2000, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06485699862241745 tensor([0.4459, 0.1498, 0.0649, 0.2404, 0.0990], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.151309996843338 tensor([0.1513, 0.2106, 0.2206, 0.1978, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08785741776227951 tensor([0.0879, 0.1814, 0.3182, 0.1728, 0.2398], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16969256103038788 tensor([0.1697, 0.1873, 0.2016, 0.2369, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1760728806257248 tensor([0.1761, 0.2180, 0.1983, 0.2077, 0.1999], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14470894634723663 tensor([0.2584, 0.1780, 0.1447, 0.2528, 0.1661], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18290533125400543 tensor([0.1923, 0.2197, 0.1829, 0.2042, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08007282018661499 tensor([0.0801, 0.1713, 0.3345, 0.1690, 0.2451], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17396557331085205 tensor([0.2182, 0.1935, 0.1740, 0.2348, 0.1795], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14503850042819977 tensor([0.1450, 0.1952, 0.2347, 0.2134, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14395788311958313 tensor([0.2584, 0.1811, 0.1440, 0.2491, 0.1673], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17745694518089294 tensor([0.1775, 0.2247, 0.1913, 0.1943, 0.2123], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11379421502351761 tensor([0.1138, 0.1932, 0.2762, 0.1884, 0.2283], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14193961024284363 tensor([0.2562, 0.1810, 0.1419, 0.2571, 0.1637], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15330766141414642 tensor([0.1533, 0.2048, 0.2209, 0.2068, 0.2141], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10016900300979614 tensor([0.3403, 0.1806, 0.1002, 0.2417, 0.1372], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18261542916297913 tensor([0.1900, 0.2149, 0.1826, 0.2152, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0918726772069931 tensor([0.0919, 0.1796, 0.3078, 0.1829, 0.2378], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15894301235675812 tensor([0.2212, 0.1863, 0.1589, 0.2569, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16320529580116272 tensor([0.1632, 0.2189, 0.2063, 0.2050, 0.2066], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13018685579299927 tensor([0.2792, 0.1780, 0.1302, 0.2573, 0.1553], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13016130030155182 tensor([0.1302, 0.2099, 0.2412, 0.1868, 0.2319], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10411710292100906 tensor([0.1041, 0.1845, 0.2919, 0.1842, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14463438093662262 tensor([0.1446, 0.2033, 0.2300, 0.2090, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16061608493328094 tensor([0.2321, 0.2006, 0.1606, 0.2278, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12203671783208847 tensor([0.2980, 0.1780, 0.1220, 0.2537, 0.1483], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16731061041355133 tensor([0.1673, 0.2063, 0.2058, 0.2058, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15465223789215088 tensor([0.1547, 0.1938, 0.2299, 0.2165, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16405531764030457 tensor([0.2268, 0.1835, 0.1641, 0.2461, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18343470990657806 tensor([0.1994, 0.2082, 0.1834, 0.2187, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09524977952241898 tensor([0.3565, 0.1742, 0.0952, 0.2453, 0.1287], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17340227961540222 tensor([0.1992, 0.2257, 0.1734, 0.2001, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12770403921604156 tensor([0.1277, 0.2132, 0.2491, 0.1849, 0.2250], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09909626841545105 tensor([0.3497, 0.1911, 0.0991, 0.2312, 0.1289], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16953662037849426 tensor([0.1695, 0.2041, 0.2040, 0.2131, 0.2093], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1707513928413391 tensor([0.2242, 0.1952, 0.1708, 0.2293, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1279948204755783 tensor([0.1280, 0.1887, 0.2521, 0.2008, 0.2303], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10239235311746597 tensor([0.1024, 0.1858, 0.2901, 0.1868, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16749292612075806 tensor([0.2215, 0.2058, 0.1675, 0.2259, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1548926681280136 tensor([0.1549, 0.2061, 0.2209, 0.2084, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09470339864492416 tensor([0.3656, 0.1900, 0.0947, 0.2244, 0.1253], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17463456094264984 tensor([0.2062, 0.2029, 0.1746, 0.2254, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1028623953461647 tensor([0.1029, 0.1936, 0.2895, 0.1798, 0.2342], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1787995994091034 tensor([0.1788, 0.1950, 0.1939, 0.2315, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13923674821853638 tensor([0.1392, 0.2076, 0.2324, 0.1995, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11014846712350845 tensor([0.3308, 0.1733, 0.1101, 0.2453, 0.1404], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10951957106590271 tensor([0.1095, 0.1917, 0.2755, 0.1858, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07668201625347137 tensor([0.0767, 0.1681, 0.3405, 0.1690, 0.2457], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1869901865720749 tensor([0.1925, 0.1900, 0.1870, 0.2407, 0.1898], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12892751395702362 tensor([0.1289, 0.2000, 0.2503, 0.1967, 0.2240], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10972816497087479 tensor([0.3237, 0.1691, 0.1097, 0.2565, 0.1410], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1838102638721466 tensor([0.1838, 0.2099, 0.1932, 0.2125, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12604647874832153 tensor([0.1260, 0.2016, 0.2486, 0.1934, 0.2304], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1790478527545929 tensor([0.1991, 0.2034, 0.1790, 0.2270, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13467399775981903 tensor([0.1347, 0.1918, 0.2421, 0.2079, 0.2235], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10664153844118118 tensor([0.3283, 0.1765, 0.1066, 0.2441, 0.1444], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15833823382854462 tensor([0.1583, 0.2019, 0.2266, 0.2053, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10376681387424469 tensor([0.1038, 0.1959, 0.2914, 0.1778, 0.2311], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17870990931987762 tensor([0.1924, 0.1787, 0.1860, 0.2542, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1731468290090561 tensor([0.1731, 0.1855, 0.1979, 0.2373, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09919006377458572 tensor([0.3471, 0.1694, 0.0992, 0.2528, 0.1316], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1269593983888626 tensor([0.1270, 0.2085, 0.2437, 0.1841, 0.2367], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14166787266731262 tensor([0.1417, 0.1963, 0.2379, 0.2062, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15915820002555847 tensor([0.1592, 0.1911, 0.2147, 0.2246, 0.2104], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13750337064266205 tensor([0.1375, 0.1949, 0.2360, 0.2093, 0.2223], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1238599494099617 tensor([0.2986, 0.1911, 0.1239, 0.2374, 0.1491], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18444815278053284 tensor([0.1977, 0.2066, 0.1844, 0.2138, 0.1975], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0841093584895134 tensor([0.0841, 0.1729, 0.3303, 0.1677, 0.2450], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13535688817501068 tensor([0.1354, 0.1888, 0.2462, 0.2074, 0.2221], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1541549414396286 tensor([0.1542, 0.2169, 0.2182, 0.1974, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12633955478668213 tensor([0.2906, 0.1909, 0.1263, 0.2402, 0.1520], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17019236087799072 tensor([0.1702, 0.2140, 0.2015, 0.2015, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17984728515148163 tensor([0.2162, 0.2009, 0.1798, 0.2218, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1420436054468155 tensor([0.2498, 0.1872, 0.1420, 0.2550, 0.1660], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16169552505016327 tensor([0.2229, 0.1951, 0.1617, 0.2367, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08950376510620117 tensor([0.3729, 0.1625, 0.0895, 0.2522, 0.1229], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1702336072921753 tensor([0.1702, 0.2214, 0.1982, 0.1952, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11512286216020584 tensor([0.1151, 0.1933, 0.2677, 0.1938, 0.2300], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11917456984519958 tensor([0.2984, 0.2147, 0.1192, 0.2172, 0.1505], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1838122308254242 tensor([0.1838, 0.2131, 0.1928, 0.2072, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06770461052656174 tensor([0.4406, 0.1549, 0.0677, 0.2342, 0.1026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17863254249095917 tensor([0.1786, 0.2197, 0.1910, 0.2041, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18858975172042847 tensor([0.1900, 0.1886, 0.1937, 0.2295, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18793174624443054 tensor([0.1879, 0.1918, 0.1947, 0.2299, 0.1956], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1455543488264084 tensor([0.1456, 0.1955, 0.2363, 0.2093, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13100653886795044 tensor([0.2814, 0.1915, 0.1310, 0.2403, 0.1558], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16038204729557037 tensor([0.2150, 0.2259, 0.1604, 0.2058, 0.1930], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10059714317321777 tensor([0.1006, 0.1878, 0.2900, 0.1858, 0.2359], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17170777916908264 tensor([0.1717, 0.2052, 0.2050, 0.2137, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1533258706331253 tensor([0.1533, 0.2008, 0.2198, 0.2123, 0.2137], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11518474668264389 tensor([0.3130, 0.1829, 0.1152, 0.2421, 0.1468], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10743718594312668 tensor([0.1074, 0.1977, 0.2781, 0.1847, 0.2321], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1376304179430008 tensor([0.1376, 0.1844, 0.2488, 0.2093, 0.2199], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1598544418811798 tensor([0.1599, 0.1856, 0.2126, 0.2375, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1855582743883133 tensor([0.1856, 0.2132, 0.1905, 0.2119, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12945601344108582 tensor([0.2904, 0.1900, 0.1295, 0.2404, 0.1497], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1334434151649475 tensor([0.1334, 0.2067, 0.2397, 0.1889, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17092832922935486 tensor([0.1709, 0.1993, 0.2174, 0.2115, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16531185805797577 tensor([0.2122, 0.1912, 0.1653, 0.2437, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15710023045539856 tensor([0.1571, 0.1961, 0.2170, 0.2193, 0.2105], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07482670992612839 tensor([0.4147, 0.1597, 0.0748, 0.2395, 0.1112], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1678796410560608 tensor([0.2122, 0.2228, 0.1679, 0.2063, 0.1908], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13695822656154633 tensor([0.1370, 0.1991, 0.2388, 0.1966, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15825238823890686 tensor([0.1583, 0.2080, 0.2185, 0.2113, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1655120998620987 tensor([0.1655, 0.1864, 0.2153, 0.2268, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09354133903980255 tensor([0.3703, 0.1929, 0.0935, 0.2185, 0.1247], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1427287608385086 tensor([0.1427, 0.2056, 0.2263, 0.1979, 0.2275], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09541328251361847 tensor([0.0954, 0.1712, 0.3070, 0.1904, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16536033153533936 tensor([0.2167, 0.1745, 0.1654, 0.2603, 0.1831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1513749659061432 tensor([0.1514, 0.2072, 0.2198, 0.2057, 0.2160], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12394081801176071 tensor([0.2975, 0.1833, 0.1239, 0.2458, 0.1494], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18257908523082733 tensor([0.1955, 0.2015, 0.1826, 0.2236, 0.1969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18384362757205963 tensor([0.1838, 0.2083, 0.1984, 0.2140, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15424367785453796 tensor([0.1542, 0.1953, 0.2216, 0.2165, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1527945101261139 tensor([0.1528, 0.1931, 0.2239, 0.2177, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1194445788860321 tensor([0.3099, 0.1926, 0.1194, 0.2284, 0.1497], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1561109870672226 tensor([0.1561, 0.2082, 0.2187, 0.2020, 0.2149], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08366519957780838 tensor([0.0837, 0.1695, 0.3275, 0.1764, 0.2430], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16630038619041443 tensor([0.1663, 0.1864, 0.2159, 0.2238, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1685190498828888 tensor([0.2152, 0.2063, 0.1685, 0.2262, 0.1838], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09200159460306168 tensor([0.3683, 0.1797, 0.0920, 0.2320, 0.1280], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17177951335906982 tensor([0.2155, 0.2029, 0.1718, 0.2167, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1784103512763977 tensor([0.1784, 0.1971, 0.2024, 0.2172, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1726325899362564 tensor([0.2147, 0.1987, 0.1726, 0.2311, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18522076308727264 tensor([0.1852, 0.2012, 0.1930, 0.2233, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09383710473775864 tensor([0.3593, 0.1740, 0.0938, 0.2439, 0.1290], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17446522414684296 tensor([0.1745, 0.2347, 0.1920, 0.1883, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10762383043766022 tensor([0.1076, 0.1989, 0.2756, 0.1846, 0.2333], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15311306715011597 tensor([0.1531, 0.1894, 0.2178, 0.2279, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17409387230873108 tensor([0.2051, 0.2081, 0.1741, 0.2232, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11635711044073105 tensor([0.3095, 0.1770, 0.1164, 0.2495, 0.1476], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13249067962169647 tensor([0.1325, 0.2209, 0.2337, 0.1800, 0.2330], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09535885602235794 tensor([0.0954, 0.1811, 0.3037, 0.1801, 0.2398], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10715717822313309 tensor([0.1072, 0.1928, 0.2855, 0.1797, 0.2348], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14743687212467194 tensor([0.1474, 0.2041, 0.2240, 0.2040, 0.2204], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13729336857795715 tensor([0.2710, 0.1942, 0.1373, 0.2388, 0.1587], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14839580655097961 tensor([0.1484, 0.2051, 0.2278, 0.1986, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08943583071231842 tensor([0.0894, 0.1779, 0.3136, 0.1771, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18127115070819855 tensor([0.1963, 0.2039, 0.1813, 0.2298, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16465480625629425 tensor([0.1647, 0.1995, 0.2078, 0.2172, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13647142052650452 tensor([0.2756, 0.1911, 0.1365, 0.2377, 0.1592], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13925373554229736 tensor([0.1393, 0.2110, 0.2360, 0.1867, 0.2270], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15285161137580872 tensor([0.1529, 0.1952, 0.2283, 0.2075, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17097310721874237 tensor([0.1710, 0.1833, 0.2041, 0.2371, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1795065701007843 tensor([0.1967, 0.2012, 0.1795, 0.2314, 0.1912], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09770295768976212 tensor([0.3601, 0.1836, 0.0977, 0.2269, 0.1317], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1353999376296997 tensor([0.1354, 0.2038, 0.2422, 0.1942, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11381126195192337 tensor([0.1138, 0.1808, 0.2833, 0.1926, 0.2296], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.177408367395401 tensor([0.2058, 0.2058, 0.1774, 0.2289, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13647803664207458 tensor([0.2739, 0.2047, 0.1365, 0.2299, 0.1550], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09417732059955597 tensor([0.3595, 0.1667, 0.0942, 0.2506, 0.1291], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1673414260149002 tensor([0.2243, 0.2045, 0.1673, 0.2188, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17732959985733032 tensor([0.1773, 0.2010, 0.2018, 0.2155, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18187396228313446 tensor([0.2002, 0.1951, 0.1819, 0.2342, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13863252103328705 tensor([0.1386, 0.2026, 0.2407, 0.1991, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07595384865999222 tensor([0.4100, 0.1637, 0.0760, 0.2392, 0.1112], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15406617522239685 tensor([0.1541, 0.1969, 0.2182, 0.2138, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1497052162885666 tensor([0.1497, 0.1973, 0.2260, 0.2095, 0.2175], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18720147013664246 tensor([0.1901, 0.1972, 0.1872, 0.2289, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16718263924121857 tensor([0.1672, 0.1993, 0.2091, 0.2196, 0.2049], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16865676641464233 tensor([0.1687, 0.2132, 0.2082, 0.2044, 0.2055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16281981766223907 tensor([0.1628, 0.2109, 0.2062, 0.2057, 0.2143], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09174687415361404 tensor([0.0917, 0.1929, 0.3037, 0.1693, 0.2423], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15418516099452972 tensor([0.2360, 0.1868, 0.1542, 0.2524, 0.1706], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15107674896717072 tensor([0.2411, 0.2001, 0.1511, 0.2328, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12542353570461273 tensor([0.2911, 0.1823, 0.1254, 0.2443, 0.1568], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1842552274465561 tensor([0.1867, 0.2190, 0.1843, 0.2050, 0.2051], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09262242168188095 tensor([0.0926, 0.1819, 0.3139, 0.1765, 0.2351], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14686685800552368 tensor([0.2514, 0.1839, 0.1469, 0.2517, 0.1662], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14878001809120178 tensor([0.1488, 0.2087, 0.2216, 0.2028, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08430396020412445 tensor([0.3871, 0.1618, 0.0843, 0.2476, 0.1191], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17428848147392273 tensor([0.1743, 0.2213, 0.1947, 0.1984, 0.2113], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09843098372220993 tensor([0.0984, 0.1940, 0.3038, 0.1745, 0.2292], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17608542740345 tensor([0.2101, 0.1997, 0.1761, 0.2290, 0.1851], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15970204770565033 tensor([0.2317, 0.2041, 0.1597, 0.2281, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10838818550109863 tensor([0.3256, 0.1858, 0.1084, 0.2369, 0.1434], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1860806941986084 tensor([0.1871, 0.2185, 0.1861, 0.2016, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14460249245166779 tensor([0.1446, 0.1996, 0.2313, 0.2012, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16131021082401276 tensor([0.2267, 0.1846, 0.1613, 0.2515, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18370018899440765 tensor([0.1837, 0.2034, 0.1939, 0.2212, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10950730741024017 tensor([0.3192, 0.1840, 0.1095, 0.2422, 0.1451], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15215080976486206 tensor([0.1522, 0.2120, 0.2184, 0.1993, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12349190562963486 tensor([0.1235, 0.1878, 0.2677, 0.1940, 0.2270], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18763256072998047 tensor([0.1964, 0.1952, 0.1876, 0.2282, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13184581696987152 tensor([0.1318, 0.2021, 0.2432, 0.2002, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15115369856357574 tensor([0.2545, 0.1951, 0.1512, 0.2276, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17842650413513184 tensor([0.1784, 0.2254, 0.1879, 0.1938, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11262845993041992 tensor([0.1126, 0.1815, 0.2795, 0.1907, 0.2356], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18227213621139526 tensor([0.1860, 0.1823, 0.1892, 0.2477, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1408766508102417 tensor([0.1409, 0.1963, 0.2347, 0.2092, 0.2188], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10338882356882095 tensor([0.3412, 0.1761, 0.1034, 0.2432, 0.1361], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18268156051635742 tensor([0.1827, 0.2112, 0.1954, 0.2033, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12615704536437988 tensor([0.1262, 0.1987, 0.2629, 0.1906, 0.2217], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16079755127429962 tensor([0.1608, 0.1889, 0.2118, 0.2279, 0.2105], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17535370588302612 tensor([0.2102, 0.1896, 0.1754, 0.2378, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08242444694042206 tensor([0.3952, 0.1623, 0.0824, 0.2420, 0.1180], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17869128286838531 tensor([0.1787, 0.2103, 0.1930, 0.2124, 0.2056], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15136198699474335 tensor([0.1514, 0.1859, 0.2293, 0.2218, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16913501918315887 tensor([0.1691, 0.1784, 0.2124, 0.2374, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1526281237602234 tensor([0.1526, 0.2066, 0.2279, 0.2026, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1338094174861908 tensor([0.2863, 0.1859, 0.1338, 0.2347, 0.1592], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15209072828292847 tensor([0.1521, 0.2093, 0.2169, 0.2051, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1166493147611618 tensor([0.1166, 0.1759, 0.2729, 0.2076, 0.2269], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17632915079593658 tensor([0.1763, 0.2060, 0.1985, 0.2166, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17439821362495422 tensor([0.1744, 0.1975, 0.2075, 0.2179, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07445289194583893 tensor([0.4305, 0.1680, 0.0745, 0.2217, 0.1054], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1527099758386612 tensor([0.2323, 0.2053, 0.1527, 0.2294, 0.1803], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14476142823696136 tensor([0.1448, 0.2052, 0.2365, 0.1992, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15420649945735931 tensor([0.2397, 0.1945, 0.1542, 0.2413, 0.1703], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1309059113264084 tensor([0.2759, 0.1948, 0.1309, 0.2438, 0.1546], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1704036444425583 tensor([0.2128, 0.1924, 0.1704, 0.2370, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10619229078292847 tensor([0.1062, 0.1978, 0.2816, 0.1778, 0.2366], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19273899495601654 tensor([0.1975, 0.1957, 0.1927, 0.2193, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14450015127658844 tensor([0.1445, 0.1964, 0.2336, 0.2137, 0.2118], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16595426201820374 tensor([0.1660, 0.2109, 0.2110, 0.2085, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [2], [0], [0], [0], [2], [0], [0], [2], [2], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [1], [0], [2], [0], [0], [0], [0], [2], [2], [0], [0], [0], [2], [0], [2], [2], [2], [2], [0], [0], [2], [0], [2], [0], [1], [0], [0], [2], [2], [0], [0], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [0], [0], [0], [2], [2], [2], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [0], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [1], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [0], [2], [2], [2], [0], [2], [0], [0]]\n",
      "[[0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [2], [0], [0], [0], [2], [0], [0], [2], [2], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [1], [0], [2], [0], [0], [0], [0], [2], [2], [0], [0], [0], [2], [0], [2], [2], [2], [2], [0], [0], [2], [0], [2], [0], [1], [0], [0], [2], [2], [0], [0], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [0], [0], [0], [2], [2], [2], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [0], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [1], [0], [2], [0], [0], [0], [2], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [2], [2], [0], [2], [2], [2], [0], [2], [0], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004900709152221679  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004900568008422852  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004900301933288574  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004899921417236328  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004899441242218017  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004898871421813965  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004898222923278808  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004897504329681396  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004896724700927734  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004895890712738037  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004895009994506836  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004894088745117187  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00489313268661499  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004892147064208984  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004891135215759277  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004890101909637451  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004889052867889404  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004887989521026611  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004886914253234864  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004885831356048584  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.13514891266822815 tensor([0.3443, 0.1758, 0.0980, 0.2467, 0.1351], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19243457913398743 tensor([0.1924, 0.2078, 0.1823, 0.2194, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19061331450939178 tensor([0.1249, 0.1978, 0.2532, 0.1906, 0.2335], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19123148918151855 tensor([0.1532, 0.1912, 0.2198, 0.2225, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19428496062755585 tensor([0.1380, 0.1943, 0.2374, 0.2073, 0.2231], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13047920167446136 tensor([0.3641, 0.1887, 0.0923, 0.2244, 0.1305], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19540612399578094 tensor([0.1272, 0.2033, 0.2372, 0.1954, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18186183273792267 tensor([0.1047, 0.1819, 0.2936, 0.1898, 0.2301], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15288858115673065 tensor([0.2839, 0.2100, 0.1269, 0.2263, 0.1529], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17690064013004303 tensor([0.2378, 0.2032, 0.1532, 0.2289, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1744849532842636 tensor([0.2428, 0.2150, 0.1512, 0.2165, 0.1745], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18031376600265503 tensor([0.1257, 0.2186, 0.2415, 0.1803, 0.2340], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1974911391735077 tensor([0.1384, 0.2100, 0.2297, 0.1975, 0.2244], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20191733539104462 tensor([0.1303, 0.2019, 0.2393, 0.2065, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19899235665798187 tensor([0.1475, 0.2149, 0.2224, 0.1990, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13881029188632965 tensor([0.3282, 0.1794, 0.1044, 0.2492, 0.1388], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20411686599254608 tensor([0.1562, 0.2118, 0.2099, 0.2041, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16872476041316986 tensor([0.0780, 0.1687, 0.3307, 0.1753, 0.2473], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19781897962093353 tensor([0.1471, 0.1978, 0.2278, 0.2106, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20432902872562408 tensor([0.1619, 0.2162, 0.2046, 0.2043, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15673410892486572 tensor([0.2906, 0.1938, 0.1228, 0.2360, 0.1567], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18792517483234406 tensor([0.1879, 0.2163, 0.1814, 0.2072, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17982670664787292 tensor([0.0946, 0.1871, 0.2932, 0.1798, 0.2452], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17764121294021606 tensor([0.1647, 0.1776, 0.2067, 0.2470, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20248325169086456 tensor([0.1658, 0.2135, 0.2025, 0.2093, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18902869522571564 tensor([0.1936, 0.1890, 0.1871, 0.2363, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1922600120306015 tensor([0.1923, 0.2119, 0.1770, 0.2175, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19431735575199127 tensor([0.1719, 0.2217, 0.1943, 0.2045, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15795299410820007 tensor([0.2787, 0.1877, 0.1257, 0.2500, 0.1580], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19484613835811615 tensor([0.1491, 0.1948, 0.2188, 0.2203, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12118034064769745 tensor([0.3861, 0.1703, 0.0826, 0.2398, 0.1212], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.193389892578125 tensor([0.1571, 0.2233, 0.2107, 0.1934, 0.2155], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17247042059898376 tensor([0.0885, 0.1938, 0.3049, 0.1725, 0.2403], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19390062987804413 tensor([0.1758, 0.1942, 0.1939, 0.2335, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19512443244457245 tensor([0.1198, 0.1951, 0.2603, 0.1985, 0.2263], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10300008207559586 tensor([0.4322, 0.1533, 0.0666, 0.2449, 0.1030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19663238525390625 tensor([0.1418, 0.2114, 0.2252, 0.1966, 0.2250], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17120669782161713 tensor([0.0825, 0.1805, 0.3224, 0.1712, 0.2433], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18849694728851318 tensor([0.1594, 0.1885, 0.2060, 0.2361, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2026183307170868 tensor([0.1656, 0.2193, 0.2026, 0.2072, 0.2053], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1713925302028656 tensor([0.2459, 0.1804, 0.1482, 0.2541, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18147239089012146 tensor([0.1815, 0.2214, 0.1869, 0.2040, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16828472912311554 tensor([0.0761, 0.1708, 0.3370, 0.1683, 0.2478], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18478460609912872 tensor([0.2066, 0.1955, 0.1779, 0.2351, 0.1848], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1958959400653839 tensor([0.1361, 0.1959, 0.2392, 0.2123, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17281308770179749 tensor([0.2456, 0.1836, 0.1476, 0.2504, 0.1728], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19370250403881073 tensor([0.1672, 0.2260, 0.1953, 0.1937, 0.2178], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1866215318441391 tensor([0.1061, 0.1931, 0.2811, 0.1866, 0.2331], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1692376732826233 tensor([0.2431, 0.1835, 0.1457, 0.2584, 0.1692], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20559442043304443 tensor([0.1437, 0.2056, 0.2255, 0.2057, 0.2195], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14246226847171783 tensor([0.3258, 0.1842, 0.1031, 0.2444, 0.1425], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17913185060024261 tensor([0.1791, 0.2165, 0.1867, 0.2149, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17902950942516327 tensor([0.0862, 0.1790, 0.3121, 0.1812, 0.2415], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1820765733718872 tensor([0.2094, 0.1883, 0.1626, 0.2575, 0.1821], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20418106019496918 tensor([0.1532, 0.2200, 0.2107, 0.2042, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16064245998859406 tensor([0.2658, 0.1808, 0.1336, 0.2591, 0.1606], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18555155396461487 tensor([0.1219, 0.2101, 0.2457, 0.1856, 0.2368], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18229445815086365 tensor([0.0973, 0.1842, 0.2964, 0.1823, 0.2398], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20403750240802765 tensor([0.1355, 0.2040, 0.2346, 0.2078, 0.2181], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18446174263954163 tensor([0.2197, 0.2029, 0.1646, 0.2283, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1535225808620453 tensor([0.2843, 0.1810, 0.1254, 0.2559, 0.1535], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20509929955005646 tensor([0.1576, 0.2074, 0.2099, 0.2051, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19458061456680298 tensor([0.1451, 0.1946, 0.2345, 0.2155, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1849726289510727 tensor([0.2150, 0.1855, 0.1678, 0.2467, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1881803274154663 tensor([0.1882, 0.2100, 0.1876, 0.2186, 0.1957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1338411271572113 tensor([0.3417, 0.1778, 0.0982, 0.2484, 0.1338], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18799832463264465 tensor([0.1880, 0.2275, 0.1774, 0.1999, 0.2072], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18353453278541565 tensor([0.1195, 0.2133, 0.2539, 0.1835, 0.2298], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13376784324645996 tensor([0.3353, 0.1950, 0.1020, 0.2339, 0.1338], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2052561491727829 tensor([0.1594, 0.2053, 0.2083, 0.2124, 0.2146], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18591395020484924 tensor([0.2124, 0.1972, 0.1747, 0.2297, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18899811804294586 tensor([0.1198, 0.1890, 0.2566, 0.1993, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18499475717544556 tensor([0.0958, 0.1855, 0.2945, 0.1850, 0.2392], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18442071974277496 tensor([0.2103, 0.2079, 0.1710, 0.2264, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20694616436958313 tensor([0.1454, 0.2069, 0.2253, 0.2073, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13007858395576477 tensor([0.3513, 0.1939, 0.0975, 0.2272, 0.1301], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19487693905830383 tensor([0.1949, 0.2047, 0.1786, 0.2254, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1781492531299591 tensor([0.0962, 0.1931, 0.2941, 0.1781, 0.2384], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19634027779102325 tensor([0.1683, 0.1963, 0.1981, 0.2310, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1980954259634018 tensor([0.1303, 0.2081, 0.2370, 0.1981, 0.2265], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14564472436904907 tensor([0.3166, 0.1765, 0.1133, 0.2479, 0.1456], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18409138917922974 tensor([0.1024, 0.1914, 0.2801, 0.1841, 0.2420], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16787762939929962 tensor([0.0730, 0.1679, 0.3426, 0.1682, 0.2483], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1817924529314041 tensor([0.1818, 0.1916, 0.1910, 0.2406, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1952470988035202 tensor([0.1207, 0.2003, 0.2549, 0.1952, 0.2289], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14606964588165283 tensor([0.3101, 0.1721, 0.1127, 0.2591, 0.1461], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19756248593330383 tensor([0.1731, 0.2114, 0.1976, 0.2120, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1918392926454544 tensor([0.1179, 0.2017, 0.2533, 0.1918, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18807435035705566 tensor([0.1881, 0.2051, 0.1830, 0.2270, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19223947823047638 tensor([0.1260, 0.1922, 0.2467, 0.2064, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14977312088012695 tensor([0.3141, 0.1797, 0.1097, 0.2466, 0.1498], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20276963710784912 tensor([0.1491, 0.2028, 0.2308, 0.2044, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17627738416194916 tensor([0.0972, 0.1954, 0.2960, 0.1763, 0.2352], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18163840472698212 tensor([0.1816, 0.1802, 0.1900, 0.2542, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18664959073066711 tensor([0.1630, 0.1866, 0.2020, 0.2367, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13662634789943695 tensor([0.3326, 0.1728, 0.1021, 0.2558, 0.1366], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.182939812541008 tensor([0.1190, 0.2085, 0.2482, 0.1829, 0.2413], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1968604326248169 tensor([0.1327, 0.1969, 0.2425, 0.2049, 0.2231], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19207613170146942 tensor([0.1495, 0.1921, 0.2190, 0.2237, 0.2157], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19541716575622559 tensor([0.1287, 0.1954, 0.2406, 0.2079, 0.2275], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15432240068912506 tensor([0.2849, 0.1942, 0.1273, 0.2393, 0.1543], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18657992780208588 tensor([0.1866, 0.2082, 0.1886, 0.2136, 0.2030], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1668553352355957 tensor([0.0796, 0.1723, 0.3334, 0.1669, 0.2478], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18927474319934845 tensor([0.1268, 0.1893, 0.2508, 0.2060, 0.2272], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19645103812217712 tensor([0.1449, 0.2178, 0.2224, 0.1965, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.157218798995018 tensor([0.2771, 0.1939, 0.1297, 0.2420, 0.1572], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20065629482269287 tensor([0.1598, 0.2151, 0.2060, 0.2007, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18625958263874054 tensor([0.2052, 0.2028, 0.1837, 0.2222, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17127543687820435 tensor([0.2374, 0.1897, 0.1455, 0.2562, 0.1713], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1891295164823532 tensor([0.2109, 0.1972, 0.1656, 0.2371, 0.1891], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1278131753206253 tensor([0.3583, 0.1660, 0.0922, 0.2557, 0.1278], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19445323944091797 tensor([0.1600, 0.2226, 0.2024, 0.1945, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19206486642360687 tensor([0.1075, 0.1933, 0.2723, 0.1921, 0.2348], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15572720766067505 tensor([0.2848, 0.2182, 0.1224, 0.2189, 0.1557], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19693222641944885 tensor([0.1733, 0.2145, 0.1969, 0.2068, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10703881084918976 tensor([0.4260, 0.1587, 0.0698, 0.2385, 0.1070], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1952669620513916 tensor([0.1680, 0.2211, 0.1953, 0.2035, 0.2121], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17891451716423035 tensor([0.1789, 0.1900, 0.1981, 0.2291, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19325779378414154 tensor([0.1773, 0.1933, 0.1989, 0.2296, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19620533287525177 tensor([0.1364, 0.1962, 0.2409, 0.2080, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16102518141269684 tensor([0.2682, 0.1944, 0.1344, 0.2420, 0.1610], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1986750215291977 tensor([0.2032, 0.2280, 0.1642, 0.2059, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18402108550071716 tensor([0.0943, 0.1874, 0.2944, 0.1840, 0.2400], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20643873512744904 tensor([0.1618, 0.2064, 0.2091, 0.2131, 0.2095], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20163758099079132 tensor([0.1439, 0.2016, 0.2242, 0.2113, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15193578600883484 tensor([0.2996, 0.1860, 0.1182, 0.2443, 0.1519], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1831628382205963 tensor([0.1008, 0.1973, 0.2824, 0.1832, 0.2363], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18483291566371918 tensor([0.1291, 0.1848, 0.2532, 0.2080, 0.2249], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1866026520729065 tensor([0.1504, 0.1866, 0.2168, 0.2367, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1948501616716385 tensor([0.1747, 0.2147, 0.1949, 0.2114, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1547689139842987 tensor([0.2771, 0.1930, 0.1328, 0.2423, 0.1548], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18750055134296417 tensor([0.1248, 0.2071, 0.2442, 0.1875, 0.2364], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20046024024486542 tensor([0.1611, 0.2005, 0.2218, 0.2109, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1930653154850006 tensor([0.2008, 0.1931, 0.1691, 0.2440, 0.1931], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19699040055274963 tensor([0.1474, 0.1970, 0.2215, 0.2183, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11570166051387787 tensor([0.4006, 0.1633, 0.0770, 0.2434, 0.1157], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1964751034975052 tensor([0.2004, 0.2249, 0.1720, 0.2063, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19518058001995087 tensor([0.1283, 0.1996, 0.2432, 0.1952, 0.2337], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20899708569049835 tensor([0.1486, 0.2090, 0.2230, 0.2104, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18744264543056488 tensor([0.1555, 0.1874, 0.2198, 0.2259, 0.2113], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1293434202671051 tensor([0.3563, 0.1970, 0.0962, 0.2212, 0.1293], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19674260914325714 tensor([0.1340, 0.2062, 0.2305, 0.1967, 0.2327], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17090559005737305 tensor([0.0894, 0.1709, 0.3112, 0.1884, 0.2401], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1763468235731125 tensor([0.2051, 0.1763, 0.1692, 0.2608, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20460864901542664 tensor([0.1420, 0.2080, 0.2242, 0.2046, 0.2212], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15468862652778625 tensor([0.2839, 0.1864, 0.1273, 0.2477, 0.1547], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1845165491104126 tensor([0.1845, 0.2031, 0.1866, 0.2234, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2008618712425232 tensor([0.1731, 0.2097, 0.2028, 0.2135, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19610442221164703 tensor([0.1447, 0.1961, 0.2260, 0.2154, 0.2177], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19392164051532745 tensor([0.1434, 0.1939, 0.2284, 0.2167, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.154936745762825 tensor([0.2963, 0.1958, 0.1227, 0.2303, 0.1549], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20092521607875824 tensor([0.1465, 0.2091, 0.2232, 0.2009, 0.2203], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16897249221801758 tensor([0.0786, 0.1690, 0.3314, 0.1748, 0.2462], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18742239475250244 tensor([0.1566, 0.1874, 0.2202, 0.2231, 0.2128], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18926171958446503 tensor([0.2035, 0.2083, 0.1725, 0.2264, 0.1893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13302242755889893 tensor([0.3537, 0.1835, 0.0948, 0.2350, 0.1330], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1987805962562561 tensor([0.2039, 0.2048, 0.1758, 0.2168, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1983674168586731 tensor([0.1681, 0.1984, 0.2066, 0.2166, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18834663927555084 tensor([0.2031, 0.2006, 0.1766, 0.2314, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19724902510643005 tensor([0.1745, 0.2026, 0.1972, 0.2230, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1341155767440796 tensor([0.3446, 0.1776, 0.0967, 0.2469, 0.1341], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18772876262664795 tensor([0.1643, 0.2359, 0.1961, 0.1877, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18309468030929565 tensor([0.1010, 0.1984, 0.2801, 0.1831, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1903018206357956 tensor([0.1437, 0.1903, 0.2222, 0.2269, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19374488294124603 tensor([0.1937, 0.2100, 0.1781, 0.2232, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15281657874584198 tensor([0.2959, 0.1801, 0.1195, 0.2517, 0.1528], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17914505302906036 tensor([0.1245, 0.2209, 0.2378, 0.1791, 0.2376], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17839521169662476 tensor([0.0895, 0.1805, 0.3081, 0.1784, 0.2436], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17827777564525604 tensor([0.1004, 0.1924, 0.2900, 0.1783, 0.2389], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2028174102306366 tensor([0.1383, 0.2048, 0.2284, 0.2028, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16391591727733612 tensor([0.2582, 0.1970, 0.1407, 0.2403, 0.1639], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19742952287197113 tensor([0.1392, 0.2058, 0.2323, 0.1974, 0.2254], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1755676120519638 tensor([0.0841, 0.1773, 0.3176, 0.1756, 0.2454], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18546295166015625 tensor([0.1855, 0.2056, 0.1852, 0.2298, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20057612657546997 tensor([0.1547, 0.2006, 0.2121, 0.2164, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16453272104263306 tensor([0.2623, 0.1939, 0.1401, 0.2392, 0.1645], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18553058803081512 tensor([0.1304, 0.2114, 0.2405, 0.1855, 0.2321], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1959497332572937 tensor([0.1434, 0.1959, 0.2329, 0.2064, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18447960913181305 tensor([0.1609, 0.1845, 0.2083, 0.2365, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18568678200244904 tensor([0.1857, 0.2029, 0.1835, 0.2313, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1365981101989746 tensor([0.3461, 0.1872, 0.1004, 0.2296, 0.1366], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19286605715751648 tensor([0.1268, 0.2042, 0.2467, 0.1929, 0.2294], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18072496354579926 tensor([0.1064, 0.1807, 0.2879, 0.1908, 0.2341], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1873559206724167 tensor([0.1944, 0.2077, 0.1815, 0.2290, 0.1874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16031529009342194 tensor([0.2604, 0.2077, 0.1402, 0.2313, 0.1603], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13400675356388092 tensor([0.3453, 0.1701, 0.0969, 0.2537, 0.1340], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19048744440078735 tensor([0.2126, 0.2066, 0.1712, 0.2192, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2022562474012375 tensor([0.1669, 0.2023, 0.2062, 0.2149, 0.2097], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18877355754375458 tensor([0.1888, 0.1968, 0.1862, 0.2342, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19773688912391663 tensor([0.1298, 0.2031, 0.2454, 0.1977, 0.2240], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11571233719587326 tensor([0.3955, 0.1675, 0.0783, 0.2430, 0.1157], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1977524757385254 tensor([0.1446, 0.1978, 0.2226, 0.2127, 0.2223], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1979767084121704 tensor([0.1403, 0.1980, 0.2306, 0.2083, 0.2228], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1794179230928421 tensor([0.1794, 0.1987, 0.1912, 0.2287, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20042383670806885 tensor([0.1572, 0.2004, 0.2133, 0.2188, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2036886215209961 tensor([0.1586, 0.2144, 0.2126, 0.2037, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2047692984342575 tensor([0.1528, 0.2119, 0.2107, 0.2048, 0.2198], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16831575334072113 tensor([0.0868, 0.1921, 0.3071, 0.1683, 0.2456], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17592871189117432 tensor([0.2239, 0.1890, 0.1579, 0.2533, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18035107851028442 tensor([0.2286, 0.2025, 0.1549, 0.2336, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16223008930683136 tensor([0.2776, 0.1852, 0.1288, 0.2461, 0.1622], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1757911890745163 tensor([0.1758, 0.2205, 0.1884, 0.2046, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17508657276630402 tensor([0.0872, 0.1812, 0.3178, 0.1751, 0.2387], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17147499322891235 tensor([0.2389, 0.1863, 0.1505, 0.2529, 0.1715], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2016909420490265 tensor([0.1395, 0.2095, 0.2260, 0.2017, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12381076067686081 tensor([0.3727, 0.1654, 0.0868, 0.2513, 0.1238], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977618932723999 tensor([0.1640, 0.2226, 0.1989, 0.1978, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1730208396911621 tensor([0.0924, 0.1932, 0.3082, 0.1730, 0.2331], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1904272437095642 tensor([0.1988, 0.2015, 0.1801, 0.2292, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.181748166680336 tensor([0.2195, 0.2064, 0.1636, 0.2287, 0.1817], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14861489832401276 tensor([0.3117, 0.1891, 0.1114, 0.2392, 0.1486], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17649702727794647 tensor([0.1765, 0.2200, 0.1901, 0.2012, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19986696541309357 tensor([0.1354, 0.2002, 0.2360, 0.1999, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18119920790195465 tensor([0.2149, 0.1867, 0.1651, 0.2522, 0.1812], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19811266660690308 tensor([0.1731, 0.2048, 0.1981, 0.2209, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15027113258838654 tensor([0.3054, 0.1872, 0.1125, 0.2445, 0.1503], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19818086922168732 tensor([0.1427, 0.2128, 0.2227, 0.1982, 0.2235], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1879386007785797 tensor([0.1155, 0.1879, 0.2724, 0.1924, 0.2318], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1854298859834671 tensor([0.1854, 0.1968, 0.1918, 0.2281, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19873389601707458 tensor([0.1234, 0.2025, 0.2477, 0.1987, 0.2277], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17688454687595367 tensor([0.2423, 0.1975, 0.1547, 0.2286, 0.1769], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19219040870666504 tensor([0.1678, 0.2267, 0.1922, 0.1933, 0.2200], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18138831853866577 tensor([0.1051, 0.1814, 0.2844, 0.1888, 0.2404], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17550532519817352 tensor([0.1755, 0.1837, 0.1932, 0.2474, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19688037037849426 tensor([0.1319, 0.1969, 0.2393, 0.2079, 0.2240], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14128172397613525 tensor([0.3268, 0.1795, 0.1064, 0.2459, 0.1413], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1997050642967224 tensor([0.1720, 0.2126, 0.1997, 0.2027, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1890869289636612 tensor([0.1179, 0.1989, 0.2676, 0.1891, 0.2265], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1899205893278122 tensor([0.1511, 0.1899, 0.2160, 0.2271, 0.2159], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19137802720069885 tensor([0.1986, 0.1914, 0.1795, 0.2379, 0.1926], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12282347679138184 tensor([0.3806, 0.1660, 0.0850, 0.2457, 0.1228], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19731058180332184 tensor([0.1681, 0.2116, 0.1973, 0.2118, 0.2112], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18666481971740723 tensor([0.1420, 0.1867, 0.2338, 0.2207, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17945773899555206 tensor([0.1590, 0.1795, 0.2170, 0.2367, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2015482485294342 tensor([0.1432, 0.2074, 0.2324, 0.2015, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1646762192249298 tensor([0.2729, 0.1887, 0.1374, 0.2363, 0.1647], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20399408042430878 tensor([0.1427, 0.2101, 0.2212, 0.2040, 0.2220], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17599670588970184 tensor([0.1090, 0.1760, 0.2776, 0.2058, 0.2317], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20274077355861664 tensor([0.1660, 0.2073, 0.2027, 0.2161, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19870245456695557 tensor([0.1639, 0.1987, 0.2120, 0.2172, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10896165668964386 tensor([0.4184, 0.1717, 0.0763, 0.2247, 0.1090], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1858523190021515 tensor([0.2201, 0.2076, 0.1564, 0.2300, 0.1859], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1979205459356308 tensor([0.1356, 0.2058, 0.2412, 0.1979, 0.2195], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1756828874349594 tensor([0.2274, 0.1968, 0.1580, 0.2421, 0.1757], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15982452034950256 tensor([0.2627, 0.1978, 0.1343, 0.2454, 0.1598], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1927957683801651 tensor([0.2013, 0.1943, 0.1743, 0.2373, 0.1928], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17658235132694244 tensor([0.1000, 0.1975, 0.2854, 0.1766, 0.2405], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1866077184677124 tensor([0.1866, 0.1972, 0.1969, 0.2191, 0.2001], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19707298278808594 tensor([0.1355, 0.1971, 0.2381, 0.2125, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2077549695968628 tensor([0.1560, 0.2120, 0.2154, 0.2078, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [2, 0], [0, 3], [0, 1], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [0], [0, 3], [2, 4], [0], [0, 1], [0, 1], [0], [2, 4], [2, 0], [0, 3], [0, 1], [0], [2, 1], [2, 0], [0, 2], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [0, 2], [0, 1], [2, 4], [0, 3], [0, 3], [0, 1], [0], [2, 4], [2, 0], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0], [2, 4], [2, 0], [0, 1], [2, 4], [0], [2, 4], [0, 3], [0, 3], [0], [2, 4], [2, 4], [0], [0, 1], [2, 4], [2, 0], [2, 4], [2, 0], [0, 3], [2, 4], [0], [2, 4], [0, 1], [0, 3], [2, 4], [0], [2, 4], [2, 0], [0, 3], [0, 1], [0, 3], [2, 4], [0, 3], [0, 1], [2, 0], [0, 3], [2, 4], [0, 2], [0, 3], [2, 0], [0, 1], [2, 4], [0], [0, 3], [1, 0], [0, 1], [2, 4], [0, 3], [0, 1], [0, 1], [0, 1], [2, 4], [2, 0], [0, 3], [0, 1], [0, 3], [2, 4], [0], [2, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [2, 4], [0, 2], [1, 0], [0, 1], [0, 1], [2, 4], [2, 4], [0, 3], [0], [0], [2, 4], [0, 3], [0, 1], [0, 1], [0, 2], [2, 4], [0, 3], [0], [2, 1], [0, 1], [2, 4], [2, 4], [0, 3], [0], [0, 1], [2, 4], [0, 3], [0, 1], [2, 1], [0], [2, 4], [2, 0], [0], [0, 1], [0, 1], [2, 4], [0], [0, 1], [0, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [0, 2], [2, 4], [0, 3], [0, 3], [0, 1], [2, 0], [2, 4], [0, 3], [0, 3], [0, 3], [0], [2, 4], [0, 3], [0, 3], [2, 0], [0], [2, 4], [0, 3], [0, 1], [0, 1], [2, 0], [2, 4], [0, 3], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [0], [2, 0], [0, 3], [2, 4], [0, 1], [0, 1], [2, 0], [0], [0], [0], [0, 3], [2, 4], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [0], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [0, 2], [2, 4], [0, 3], [0, 1], [2, 0], [0, 3], [2, 4], [0, 2], [0, 1], [1, 0], [0, 1], [2, 4], [0, 2], [0, 3], [0, 1], [2, 1], [2, 4], [0, 2], [0, 1], [0, 1], [0], [2, 4], [0], [0, 1], [0], [0, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 4], [2, 4], [0, 3], [2, 0], [0, 1], [0]]\n",
      "[[0, 1, 3], [1, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 3, 4], [2, 3, 4], [2, 3, 4], [1, 2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 3, 4], [1, 2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [1, 2, 3, 4], [0, 1, 3], [1, 2, 3, 4], [2, 3, 4], [1, 2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 3, 4], [2, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [2, 0], [0, 3], [0, 1], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [2, 0], [0, 3], [0, 1], [2, 1], [2, 0], [0, 2], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [0, 2], [0, 1], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [2, 0], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [2, 0], [0, 1], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [2, 4], [2, 0], [2, 4], [2, 0], [0, 3], [2, 4], [2, 4], [0, 1], [0, 3], [2, 4], [2, 4], [2, 0], [0, 3], [0, 1], [0, 3], [2, 4], [0, 3], [0, 1], [2, 0], [0, 3], [2, 4], [0, 2], [0, 3], [2, 0], [0, 1], [2, 4], [0, 3], [1, 0], [0, 1], [2, 4], [0, 3], [0, 1], [0, 1], [0, 1], [2, 4], [2, 0], [0, 3], [0, 1], [0, 3], [2, 4], [2, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [2, 4], [0, 2], [1, 0], [0, 1], [0, 1], [2, 4], [2, 4], [0, 3], [2, 4], [0, 3], [0, 1], [0, 1], [0, 2], [2, 4], [0, 3], [2, 1], [0, 1], [2, 4], [2, 4], [0, 3], [0, 1], [2, 4], [0, 3], [0, 1], [2, 1], [2, 4], [2, 0], [0, 1], [0, 1], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [0, 2], [2, 4], [0, 3], [0, 3], [0, 1], [2, 0], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [2, 0], [2, 4], [0, 3], [0, 1], [0, 1], [2, 0], [2, 4], [0, 3], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [0, 1], [0, 1], [2, 0], [0, 3], [2, 4], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [0, 2], [2, 4], [0, 3], [0, 1], [2, 0], [0, 3], [2, 4], [0, 2], [0, 1], [1, 0], [0, 1], [2, 4], [0, 2], [0, 3], [0, 1], [2, 1], [2, 4], [0, 2], [0, 1], [0, 1], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 4], [2, 4], [0, 3], [2, 0], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005769571212873067  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0057695401857977046  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.005769480853320257  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0057693964814486565  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0057692897918561825  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.005769161873212143  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005769015991524474  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005768853235462485  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.005768675782364797  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005768484720900723  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005768282228408883  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005768067760554623  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00576784512768053  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005767612696782639  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005767374278203537  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005767127694604603  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0057668740346551485  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005766616564363105  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0057663525620551956  Accuracy on Support set:0.0\n",
      "torch.Size([219, 2048]) torch.Size([219])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.005766085293739354  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.17495903372764587 tensor([0.3460, 0.1750, 0.0987, 0.2468, 0.1336], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1966785341501236 tensor([0.1928, 0.2070, 0.1844, 0.2192, 0.1967], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.196832537651062 tensor([0.1250, 0.1968, 0.2559, 0.1902, 0.2320], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2116425484418869 tensor([0.1537, 0.1903, 0.2221, 0.2223, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20698079466819763 tensor([0.1384, 0.1933, 0.2399, 0.2070, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1879054605960846 tensor([0.3657, 0.1879, 0.0930, 0.2244, 0.1290], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20236364006996155 tensor([0.1273, 0.2024, 0.2399, 0.1950, 0.2354], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1892770677804947 tensor([0.1048, 0.1808, 0.2966, 0.1893, 0.2285], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2091359794139862 tensor([0.2850, 0.2091, 0.1282, 0.2263, 0.1514], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20236539840698242 tensor([0.2387, 0.2024, 0.1547, 0.2288, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2141408771276474 tensor([0.2438, 0.2141, 0.1527, 0.2164, 0.1730], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21763257682323456 tensor([0.1257, 0.2176, 0.2442, 0.1798, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20899535715579987 tensor([0.1387, 0.2090, 0.2323, 0.1971, 0.2229], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20090316236019135 tensor([0.1306, 0.2009, 0.2420, 0.2062, 0.2204], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21399624645709991 tensor([0.1479, 0.2140, 0.2247, 0.1987, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17854714393615723 tensor([0.3299, 0.1785, 0.1051, 0.2492, 0.1372], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20382246375083923 tensor([0.1564, 0.2109, 0.2123, 0.2038, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17458461225032806 tensor([0.0779, 0.1677, 0.3340, 0.1746, 0.2458], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21028995513916016 tensor([0.1475, 0.1969, 0.2303, 0.2103, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20413529872894287 tensor([0.1624, 0.2153, 0.2068, 0.2041, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19298987090587616 tensor([0.2920, 0.1930, 0.1239, 0.2360, 0.1551], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20573219656944275 tensor([0.1884, 0.2155, 0.1834, 0.2070, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18601806461811066 tensor([0.0945, 0.1860, 0.2965, 0.1791, 0.2439], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20235759019851685 tensor([0.1652, 0.1768, 0.2090, 0.2468, 0.2024], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20460739731788635 tensor([0.1663, 0.2126, 0.2046, 0.2092, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19226615130901337 tensor([0.1944, 0.1882, 0.1889, 0.2362, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19995220005512238 tensor([0.1927, 0.2110, 0.1790, 0.2173, 0.2000], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20434625446796417 tensor([0.1723, 0.2208, 0.1964, 0.2043, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18681225180625916 tensor([0.2798, 0.1868, 0.1269, 0.2501, 0.1564], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21541917324066162 tensor([0.1495, 0.1939, 0.2211, 0.2201, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16942919790744781 tensor([0.3879, 0.1694, 0.0832, 0.2398, 0.1196], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21314071118831635 tensor([0.1574, 0.2224, 0.2131, 0.1931, 0.2140], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19273263216018677 tensor([0.0884, 0.1927, 0.3082, 0.1718, 0.2388], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19334189593791962 tensor([0.1764, 0.1933, 0.1959, 0.2334, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19809739291667938 tensor([0.1200, 0.1941, 0.2631, 0.1981, 0.2247], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15240131318569183 tensor([0.4342, 0.1524, 0.0670, 0.2449, 0.1016], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21039986610412598 tensor([0.1421, 0.2104, 0.2277, 0.1963, 0.2235], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17943385243415833 tensor([0.0824, 0.1794, 0.3258, 0.1705, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20832182466983795 tensor([0.1598, 0.1876, 0.2083, 0.2359, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20374330878257751 tensor([0.1662, 0.2184, 0.2047, 0.2070, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17953483760356903 tensor([0.2470, 0.1795, 0.1496, 0.2542, 0.1697], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20379824936389923 tensor([0.1819, 0.2205, 0.1889, 0.2038, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16975732147693634 tensor([0.0760, 0.1698, 0.3404, 0.1675, 0.2464], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19464364647865295 tensor([0.2074, 0.1946, 0.1797, 0.2350, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21196167171001434 tensor([0.1364, 0.1949, 0.2417, 0.2120, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18276625871658325 tensor([0.2468, 0.1828, 0.1489, 0.2504, 0.1711], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.197552889585495 tensor([0.1675, 0.2252, 0.1976, 0.1934, 0.2164], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1920327991247177 tensor([0.1062, 0.1920, 0.2841, 0.1861, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18270640075206757 tensor([0.2439, 0.1827, 0.1472, 0.2584, 0.1677], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20463143289089203 tensor([0.1440, 0.2046, 0.2280, 0.2055, 0.2179], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1833265721797943 tensor([0.3274, 0.1833, 0.1040, 0.2444, 0.1409], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20131739974021912 tensor([0.1795, 0.2156, 0.1888, 0.2147, 0.2013], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18043671548366547 tensor([0.0861, 0.1779, 0.3155, 0.1804, 0.2400], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1874525547027588 tensor([0.2102, 0.1875, 0.1643, 0.2575, 0.1805], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20397453010082245 tensor([0.1537, 0.2190, 0.2129, 0.2040, 0.2104], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17995138466358185 tensor([0.2671, 0.1800, 0.1348, 0.2591, 0.1590], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20913296937942505 tensor([0.1220, 0.2091, 0.2484, 0.1851, 0.2354], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1831829845905304 tensor([0.0973, 0.1832, 0.2996, 0.1817, 0.2382], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20304074883460999 tensor([0.1358, 0.2030, 0.2371, 0.2075, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20202066004276276 tensor([0.2206, 0.2020, 0.1662, 0.2283, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18011018633842468 tensor([0.2857, 0.1801, 0.1264, 0.2559, 0.1519], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20489788055419922 tensor([0.1580, 0.2065, 0.2121, 0.2049, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20876622200012207 tensor([0.1454, 0.1936, 0.2371, 0.2152, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18467114865779877 tensor([0.2159, 0.1847, 0.1694, 0.2467, 0.1833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19416150450706482 tensor([0.1888, 0.2091, 0.1896, 0.2184, 0.1942], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17698843777179718 tensor([0.3433, 0.1770, 0.0990, 0.2484, 0.1323], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19972588121891022 tensor([0.1884, 0.2267, 0.1794, 0.1997, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21230830252170563 tensor([0.1196, 0.2123, 0.2566, 0.1831, 0.2284], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19411344826221466 tensor([0.3367, 0.1941, 0.1029, 0.2340, 0.1324], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20433402061462402 tensor([0.1600, 0.2043, 0.2105, 0.2122, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1963491290807724 tensor([0.2133, 0.1963, 0.1764, 0.2297, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1988479495048523 tensor([0.1200, 0.1880, 0.2594, 0.1988, 0.2338], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1844477653503418 tensor([0.0957, 0.1844, 0.2977, 0.1844, 0.2377], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20705915987491608 tensor([0.2111, 0.2071, 0.1727, 0.2263, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20597273111343384 tensor([0.1457, 0.2060, 0.2278, 0.2071, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19304490089416504 tensor([0.3529, 0.1930, 0.0982, 0.2273, 0.1286], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19493837654590607 tensor([0.1953, 0.2039, 0.1806, 0.2252, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1920747309923172 tensor([0.0962, 0.1921, 0.2974, 0.1775, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20025452971458435 tensor([0.1688, 0.1954, 0.2003, 0.2309, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20708894729614258 tensor([0.1305, 0.2071, 0.2396, 0.1978, 0.2250], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17567890882492065 tensor([0.3182, 0.1757, 0.1142, 0.2479, 0.1440], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1904449611902237 tensor([0.1024, 0.1904, 0.2831, 0.1835, 0.2405], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16741886734962463 tensor([0.0729, 0.1668, 0.3461, 0.1674, 0.2468], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19067133963108063 tensor([0.1823, 0.1907, 0.1930, 0.2405, 0.1935], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1993303894996643 tensor([0.1209, 0.1993, 0.2575, 0.1949, 0.2273], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1712547391653061 tensor([0.3117, 0.1713, 0.1135, 0.2592, 0.1444], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20447386801242828 tensor([0.1735, 0.2105, 0.1998, 0.2118, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20067080855369568 tensor([0.1179, 0.2007, 0.2562, 0.1914, 0.2339], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19532889127731323 tensor([0.1886, 0.2042, 0.1850, 0.2269, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20607852935791016 tensor([0.1262, 0.1913, 0.2494, 0.2061, 0.2270], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17892920970916748 tensor([0.3156, 0.1789, 0.1106, 0.2467, 0.1482], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20184731483459473 tensor([0.1494, 0.2018, 0.2332, 0.2041, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1943320482969284 tensor([0.0971, 0.1943, 0.2992, 0.1756, 0.2338], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19210876524448395 tensor([0.1822, 0.1793, 0.1921, 0.2541, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20413249731063843 tensor([0.1636, 0.1858, 0.2041, 0.2366, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1719208061695099 tensor([0.3343, 0.1719, 0.1029, 0.2559, 0.1350], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20753806829452515 tensor([0.1190, 0.2075, 0.2510, 0.1824, 0.2400], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2045140415430069 tensor([0.1329, 0.1959, 0.2451, 0.2045, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2141517698764801 tensor([0.1499, 0.1912, 0.2214, 0.2234, 0.2142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20754940807819366 tensor([0.1290, 0.1944, 0.2432, 0.2075, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19338035583496094 tensor([0.2862, 0.1934, 0.1284, 0.2393, 0.1527], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20146098732948303 tensor([0.1871, 0.2074, 0.1906, 0.2135, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1712406426668167 tensor([0.0795, 0.1712, 0.3368, 0.1661, 0.2464], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20565474033355713 tensor([0.1270, 0.1883, 0.2535, 0.2057, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2168450653553009 tensor([0.1454, 0.2169, 0.2246, 0.1962, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19308385252952576 tensor([0.2784, 0.1931, 0.1309, 0.2420, 0.1556], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20040161907672882 tensor([0.1601, 0.2142, 0.2083, 0.2004, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2018948644399643 tensor([0.2059, 0.2019, 0.1854, 0.2221, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18881838023662567 tensor([0.2382, 0.1888, 0.1470, 0.2562, 0.1698], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19632360339164734 tensor([0.2117, 0.1963, 0.1673, 0.2371, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16514703631401062 tensor([0.3601, 0.1651, 0.0929, 0.2557, 0.1262], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20469722151756287 tensor([0.1604, 0.2217, 0.2047, 0.1942, 0.2191], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19228225946426392 tensor([0.1076, 0.1923, 0.2752, 0.1916, 0.2332], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21732942759990692 tensor([0.2861, 0.2173, 0.1235, 0.2189, 0.1542], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2066701352596283 tensor([0.1739, 0.2136, 0.1989, 0.2067, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15779902040958405 tensor([0.4279, 0.1578, 0.0702, 0.2385, 0.1056], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20328964293003082 tensor([0.1684, 0.2202, 0.1975, 0.2033, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20027875900268555 tensor([0.1794, 0.1891, 0.2003, 0.2289, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19933092594146729 tensor([0.1779, 0.1924, 0.2009, 0.2295, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20769459009170532 tensor([0.1367, 0.1952, 0.2436, 0.2077, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19353294372558594 tensor([0.2695, 0.1935, 0.1355, 0.2420, 0.1594], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20373955368995667 tensor([0.2037, 0.2272, 0.1661, 0.2057, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18630217015743256 tensor([0.0942, 0.1863, 0.2975, 0.1834, 0.2385], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20551332831382751 tensor([0.1623, 0.2055, 0.2113, 0.2129, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20068544149398804 tensor([0.1443, 0.2007, 0.2266, 0.2111, 0.2173], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1851874589920044 tensor([0.3011, 0.1852, 0.1191, 0.2443, 0.1503], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19624494016170502 tensor([0.1008, 0.1962, 0.2856, 0.1826, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2076052576303482 tensor([0.1293, 0.1839, 0.2559, 0.2076, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20797789096832275 tensor([0.1507, 0.1856, 0.2191, 0.2365, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20277254283428192 tensor([0.1753, 0.2138, 0.1969, 0.2113, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19216851890087128 tensor([0.2784, 0.1922, 0.1340, 0.2423, 0.1532], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2060937136411667 tensor([0.1250, 0.2061, 0.2470, 0.1870, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19954921305179596 tensor([0.1615, 0.1995, 0.2240, 0.2106, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19145840406417847 tensor([0.2016, 0.1922, 0.1708, 0.2440, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21426890790462494 tensor([0.1478, 0.1960, 0.2239, 0.2180, 0.2143], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1624244600534439 tensor([0.4024, 0.1624, 0.0774, 0.2435, 0.1142], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2008586972951889 tensor([0.2009, 0.2240, 0.1739, 0.2062, 0.1950], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19860118627548218 tensor([0.1285, 0.1986, 0.2459, 0.1948, 0.2322], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20751383900642395 tensor([0.1489, 0.2080, 0.2255, 0.2101, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20967824757099152 tensor([0.1560, 0.1865, 0.2222, 0.2257, 0.2097], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1960875242948532 tensor([0.3577, 0.1961, 0.0970, 0.2212, 0.1279], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20522445440292358 tensor([0.1341, 0.2052, 0.2331, 0.1963, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18780215084552765 tensor([0.0894, 0.1699, 0.3144, 0.1878, 0.2385], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18699057400226593 tensor([0.2058, 0.1755, 0.1709, 0.2607, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20435015857219696 tensor([0.1424, 0.2070, 0.2266, 0.2044, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18552561104297638 tensor([0.2852, 0.1855, 0.1284, 0.2478, 0.1531], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20085696876049042 tensor([0.1850, 0.2023, 0.1887, 0.2232, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19934093952178955 tensor([0.1736, 0.2088, 0.2049, 0.2133, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21516522765159607 tensor([0.1452, 0.1951, 0.2284, 0.2152, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21602581441402435 tensor([0.1438, 0.1930, 0.2308, 0.2164, 0.2160], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19497253000736237 tensor([0.2977, 0.1950, 0.1237, 0.2303, 0.1534], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20061616599559784 tensor([0.1467, 0.2081, 0.2257, 0.2006, 0.2188], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1739557385444641 tensor([0.0785, 0.1679, 0.3349, 0.1740, 0.2447], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21113522350788116 tensor([0.1570, 0.1865, 0.2225, 0.2229, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20414264500141144 tensor([0.2041, 0.2074, 0.1743, 0.2264, 0.1877], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1826293170452118 tensor([0.3554, 0.1826, 0.0955, 0.2351, 0.1315], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20393890142440796 tensor([0.2046, 0.2039, 0.1776, 0.2167, 0.1972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2087181806564331 tensor([0.1686, 0.1975, 0.2088, 0.2164, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19974198937416077 tensor([0.2039, 0.1997, 0.1784, 0.2313, 0.1867], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2010161280632019 tensor([0.1752, 0.2017, 0.1992, 0.2228, 0.2010], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17678608000278473 tensor([0.3463, 0.1768, 0.0974, 0.2470, 0.1325], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19833461940288544 tensor([0.1646, 0.2351, 0.1983, 0.1874, 0.2146], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19739162921905518 tensor([0.1009, 0.1974, 0.2832, 0.1825, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21540074050426483 tensor([0.1440, 0.1894, 0.2247, 0.2266, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1934378743171692 tensor([0.1943, 0.2091, 0.1801, 0.2231, 0.1934], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17923922836780548 tensor([0.2973, 0.1792, 0.1205, 0.2518, 0.1512], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2199859321117401 tensor([0.1245, 0.2200, 0.2406, 0.1786, 0.2363], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1794111430644989 tensor([0.0893, 0.1794, 0.3114, 0.1777, 0.2422], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1914294958114624 tensor([0.1004, 0.1914, 0.2931, 0.1776, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20253264904022217 tensor([0.1386, 0.2039, 0.2308, 0.2025, 0.2241], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19608992338180542 tensor([0.2595, 0.1961, 0.1419, 0.2403, 0.1623], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2048121690750122 tensor([0.1394, 0.2048, 0.2349, 0.1971, 0.2239], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1762181669473648 tensor([0.0840, 0.1762, 0.3210, 0.1749, 0.2439], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19249679148197174 tensor([0.1860, 0.2047, 0.1872, 0.2296, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19965365529060364 tensor([0.1552, 0.1997, 0.2143, 0.2163, 0.2146], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19303441047668457 tensor([0.2635, 0.1930, 0.1413, 0.2392, 0.1629], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21047411859035492 tensor([0.1306, 0.2105, 0.2432, 0.1851, 0.2306], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20607411861419678 tensor([0.1436, 0.1950, 0.2355, 0.2061, 0.2198], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20823124051094055 tensor([0.1614, 0.1836, 0.2105, 0.2363, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19506137073040009 tensor([0.1863, 0.2020, 0.1855, 0.2312, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1864340901374817 tensor([0.3477, 0.1864, 0.1012, 0.2296, 0.1351], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.203279048204422 tensor([0.1271, 0.2033, 0.2493, 0.1925, 0.2278], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19031693041324615 tensor([0.1065, 0.1797, 0.2909, 0.1903, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19503535330295563 tensor([0.1950, 0.2068, 0.1834, 0.2289, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2068784385919571 tensor([0.2614, 0.2069, 0.1416, 0.2313, 0.1588], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16923938691616058 tensor([0.3470, 0.1692, 0.0976, 0.2538, 0.1324], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20573507249355316 tensor([0.2134, 0.2057, 0.1729, 0.2191, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20136758685112 tensor([0.1673, 0.2014, 0.2084, 0.2147, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19248048961162567 tensor([0.1894, 0.1959, 0.1881, 0.2341, 0.1925], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20206187665462494 tensor([0.1300, 0.2021, 0.2480, 0.1974, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16663454473018646 tensor([0.3973, 0.1666, 0.0788, 0.2430, 0.1142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21240484714508057 tensor([0.1449, 0.1968, 0.2251, 0.2124, 0.2208], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20799309015274048 tensor([0.1406, 0.1970, 0.2332, 0.2080, 0.2212], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19783715903759003 tensor([0.1799, 0.1978, 0.1933, 0.2285, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19949400424957275 tensor([0.1577, 0.1995, 0.2156, 0.2186, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20348043739795685 tensor([0.1591, 0.2134, 0.2148, 0.2035, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20446573197841644 tensor([0.1531, 0.2110, 0.2131, 0.2045, 0.2184], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1910603791475296 tensor([0.0867, 0.1911, 0.3104, 0.1676, 0.2443], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18819014728069305 tensor([0.2246, 0.1882, 0.1595, 0.2533, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20168103277683258 tensor([0.2295, 0.2017, 0.1564, 0.2336, 0.1788], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18437603116035461 tensor([0.2790, 0.1844, 0.1299, 0.2462, 0.1606], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20435823500156403 tensor([0.1761, 0.2197, 0.1906, 0.2044, 0.2093], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18017950654029846 tensor([0.0871, 0.1802, 0.3210, 0.1744, 0.2373], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18548311293125153 tensor([0.2397, 0.1855, 0.1520, 0.2529, 0.1699], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2014385610818863 tensor([0.1399, 0.2085, 0.2284, 0.2014, 0.2217], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16455025970935822 tensor([0.3745, 0.1646, 0.0874, 0.2513, 0.1223], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20111769437789917 tensor([0.1643, 0.2217, 0.2011, 0.1975, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19215607643127441 tensor([0.0923, 0.1922, 0.3115, 0.1723, 0.2317], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19950644671916962 tensor([0.1995, 0.2007, 0.1819, 0.2291, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20553424954414368 tensor([0.2204, 0.2055, 0.1652, 0.2287, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1882922351360321 tensor([0.3132, 0.1883, 0.1123, 0.2392, 0.1470], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20102515816688538 tensor([0.1769, 0.2191, 0.1922, 0.2010, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19922304153442383 tensor([0.1356, 0.1992, 0.2386, 0.1995, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1858055591583252 tensor([0.2156, 0.1858, 0.1668, 0.2522, 0.1796], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20149151980876923 tensor([0.1737, 0.2039, 0.2001, 0.2208, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18640170991420746 tensor([0.3069, 0.1864, 0.1134, 0.2446, 0.1486], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21187248826026917 tensor([0.1430, 0.2119, 0.2252, 0.1979, 0.2220], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19191239774227142 tensor([0.1156, 0.1869, 0.2753, 0.1919, 0.2303], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1959412693977356 tensor([0.1861, 0.1959, 0.1937, 0.2280, 0.1963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20149821043014526 tensor([0.1237, 0.2015, 0.2504, 0.1984, 0.2261], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19672824442386627 tensor([0.2434, 0.1967, 0.1561, 0.2285, 0.1753], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1929871141910553 tensor([0.1682, 0.2259, 0.1943, 0.1930, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18827000260353088 tensor([0.1051, 0.1803, 0.2875, 0.1883, 0.2388], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19535866379737854 tensor([0.1760, 0.1828, 0.1954, 0.2473, 0.1986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20756752789020538 tensor([0.1322, 0.1959, 0.2419, 0.2076, 0.2224], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1786653697490692 tensor([0.3284, 0.1787, 0.1073, 0.2460, 0.1397], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2025425285100937 tensor([0.1725, 0.2117, 0.2018, 0.2025, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19781143963336945 tensor([0.1181, 0.1978, 0.2705, 0.1886, 0.2249], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21429482102394104 tensor([0.1515, 0.1890, 0.2184, 0.2268, 0.2143], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19098693132400513 tensor([0.1993, 0.1905, 0.1814, 0.2379, 0.1910], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16514015197753906 tensor([0.3824, 0.1651, 0.0855, 0.2457, 0.1213], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20966529846191406 tensor([0.1685, 0.2107, 0.1995, 0.2116, 0.2097], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21519029140472412 tensor([0.1424, 0.1857, 0.2364, 0.2204, 0.2152], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20635762810707092 tensor([0.1593, 0.1786, 0.2193, 0.2364, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20126140117645264 tensor([0.1436, 0.2064, 0.2349, 0.2013, 0.2138], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18791250884532928 tensor([0.2740, 0.1879, 0.1387, 0.2363, 0.1631], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20369039475917816 tensor([0.1430, 0.2091, 0.2237, 0.2037, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20529180765151978 tensor([0.1091, 0.1750, 0.2806, 0.2053, 0.2300], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20492306351661682 tensor([0.1665, 0.2064, 0.2049, 0.2159, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2065291404724121 tensor([0.1644, 0.1978, 0.2143, 0.2170, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1709902584552765 tensor([0.4198, 0.1710, 0.0768, 0.2247, 0.1077], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067955583333969 tensor([0.2209, 0.2068, 0.1580, 0.2299, 0.1843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20484383404254913 tensor([0.1358, 0.2048, 0.2438, 0.1976, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19592835009098053 tensor([0.2282, 0.1959, 0.1596, 0.2421, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19688737392425537 tensor([0.2638, 0.1969, 0.1356, 0.2454, 0.1583], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19342593848705292 tensor([0.2023, 0.1934, 0.1759, 0.2373, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1964913308620453 tensor([0.1000, 0.1965, 0.2885, 0.1760, 0.2391], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19636614620685577 tensor([0.1872, 0.1964, 0.1989, 0.2190, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2121535986661911 tensor([0.1358, 0.1961, 0.2407, 0.2122, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20726601779460907 tensor([0.1565, 0.2110, 0.2176, 0.2076, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [2, 0, 4], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3], [0, 1, 3], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [0], [0, 3], [2, 4, 1], [0], [0, 1, 3], [0, 1], [0], [2, 4, 1], [2, 0], [0, 3, 1], [0, 1], [0], [2, 1, 4], [2, 0, 4], [0, 2], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 3], [0, 3, 1], [0, 1], [0], [2, 4, 1], [2, 0], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 4, 1], [0], [2, 4, 1], [2, 0], [0, 1, 3], [2, 4, 1], [0], [2, 4, 1], [0, 3], [0, 3, 1], [0], [2, 4], [2, 4, 1], [0], [0, 1], [2, 4, 1], [2, 0, 4], [2, 4, 1], [2, 0, 3], [0, 3], [2, 4, 1], [0], [2, 4, 1], [0, 1, 3], [0, 3, 1], [2, 4], [0], [2, 4, 1], [2, 0, 4], [0, 3, 1], [0, 1], [0, 3], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 2], [0, 3], [2, 0, 4], [0, 1], [2, 4, 1], [0], [0, 3, 1], [1, 0, 2], [0, 1], [2, 4, 1], [0, 3], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0], [0, 3, 1], [0, 1], [0, 3], [2, 4, 1], [0], [2, 4], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3], [0, 3, 1], [2, 4], [0, 2], [2, 4, 1], [0, 2], [1, 0], [0, 1, 4], [0, 1], [2, 4, 1], [2, 4], [0, 3, 1], [0], [0], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 2], [2, 4, 1], [0, 3], [0, 1], [2, 1, 4], [0, 1], [2, 4, 1], [2, 4], [0, 3, 1], [0], [0, 1], [2, 4, 1], [0, 3], [0, 1, 3], [2, 1, 4], [0], [2, 4, 1], [2, 0], [0, 4], [0, 1], [0, 1], [2, 4, 1], [0], [0, 1, 3], [0, 1], [2, 4], [2, 4, 1], [2, 4], [0, 1], [2, 4, 1], [0, 2], [2, 4, 1], [0, 3, 2], [0, 3, 1], [0, 1], [2, 0, 4], [2, 4, 1], [0, 3], [0, 3, 1], [0, 3, 1], [0], [2, 4, 1], [0, 3], [0, 3, 1], [2, 0, 4], [0, 1], [2, 4, 1], [0, 3], [0, 1], [0, 1], [2, 0, 4], [2, 4, 1], [0, 3], [0, 1, 3], [2, 4, 0], [2, 4], [2, 4, 1], [2, 4], [0], [2, 0, 4], [0, 3], [2, 4, 1], [0, 1], [0, 1], [2, 0, 1], [0, 1], [0], [0], [0, 3, 1], [2, 4, 1], [2, 4], [2, 4, 1], [2, 0], [0, 3, 1], [2, 4, 1], [0], [2, 4, 1], [0, 3], [0, 3, 1], [2, 4, 0], [2, 4], [2, 4, 1], [2, 0], [0, 3, 1], [2, 4, 1], [0, 2], [2, 4, 1], [0, 3], [0, 1, 3], [2, 0, 1], [0, 3], [2, 4, 1], [0, 2, 3], [0, 1, 3], [1, 0, 2], [0, 1], [2, 4, 1], [0, 2], [0, 3, 1], [0, 1], [2, 1, 4], [2, 4, 1], [0, 2], [0, 1], [0, 1], [0], [2, 4, 1], [0], [0, 1], [0], [0, 1], [2, 4, 1], [2, 4], [0, 3], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1], [0, 1], [0]]\n",
      "[[0, 3], [1, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 3, 4], [1, 2, 4], [0, 3], [1, 2, 3, 4], [2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3], [1, 3], [1, 3, 4], [0, 3], [2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [3, 4], [2, 4], [0, 3], [1, 2, 4], [2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [1, 4], [2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [1, 2, 3, 4], [0, 1, 3], [0, 3], [1, 2, 3, 4], [2, 3, 4], [0, 3], [1, 3], [0, 3], [1, 4], [1, 2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 1, 3], [1, 2, 3, 4], [0, 3], [1, 3], [2, 4], [2, 3, 4], [1, 2, 4], [0, 3], [2, 4], [2, 4], [3, 4], [2, 4], [0, 3], [1, 3, 4], [1, 2, 4], [1, 3], [2, 3, 4], [0, 3], [1, 2, 3, 4], [2, 4], [3, 4], [2, 3, 4], [0, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [2, 3, 4], [1, 2, 4], [0, 3], [1, 2, 3, 4], [0, 1, 3], [0, 3], [0, 3], [0, 3], [1, 2, 4], [2, 4], [0, 1, 3], [1, 3, 4], [0, 3], [1, 3, 4], [2, 3, 4], [2, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 3], [1, 2, 4], [2, 3, 4], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 4], [1, 2, 3, 4], [2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [1, 3, 4], [1, 2, 3], [2, 3, 4], [2, 3, 4], [0, 3], [1, 2, 3, 4], [2, 4], [2, 3, 4], [0, 1, 3], [0, 3], [0, 1, 3], [2, 3, 4], [0, 3], [1, 3, 4], [0, 3], [1, 4], [2, 4], [2, 3, 4], [1, 3], [0, 3], [1, 2, 4], [2, 4], [2, 4], [1, 2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [1, 3], [2, 3, 4], [0, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [1, 3], [0, 3], [1, 2, 4], [2, 4], [1, 3], [0, 1, 3], [0, 3], [0, 1, 3], [1, 2, 3, 4], [1, 3], [1, 2, 4], [0, 3], [2, 3, 4], [2, 3, 4], [3, 4], [2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [2, 4], [0, 3], [0, 1, 3], [0, 3], [1, 3, 4], [2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [1, 3], [0, 1, 3], [0, 3], [1, 3, 4], [2, 4], [0, 3], [1, 3, 4], [0, 3], [1, 2, 4], [2, 4], [3, 4], [1, 2, 4], [0, 3], [1, 4], [2, 4], [3, 4], [2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3], [1, 2, 3, 4], [2, 3, 4], [1, 2, 3, 4], [2, 3, 4], [0, 3], [0, 1, 3], [1, 2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [3, 4], [2, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 0, 4], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 4, 1], [2, 4, 1], [0, 1, 3], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 0, 4], [2, 4, 1], [2, 0, 3], [2, 4, 1], [2, 4, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1], [2, 0, 4], [2, 4, 1], [0, 3, 1], [1, 0, 2], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1, 4], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 1, 4], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1, 3], [2, 1, 4], [2, 4, 1], [0, 4], [2, 4, 1], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 0, 4], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 0, 4], [0, 1], [2, 4, 1], [2, 0, 4], [2, 4, 1], [0, 1, 3], [2, 4, 0], [2, 4, 1], [2, 0, 4], [2, 4, 1], [2, 0, 1], [0, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 0], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [0, 1, 3], [2, 0, 1], [2, 4, 1], [0, 2, 3], [0, 1, 3], [1, 0, 2], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.009813485695765569  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.009813008858607366  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.009812101034017709  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.009810809905712422  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0098091721534729  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.009807226291069617  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.009805005330305833  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.009802537698012133  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0097998518210191  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00979697062419011  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.009793917949383075  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0097907148874723  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.009787377944359412  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.00978392454294058  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.009780371189117431  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.009776728886824388  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.009773009556990403  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.009769226954533503  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.009765383830437293  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.009761495773608868  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.2519260048866272 tensor([0.3522, 0.1648, 0.0984, 0.2519, 0.1326], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19676131010055542 tensor([0.1971, 0.1968, 0.1850, 0.2246, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23226778209209442 tensor([0.1276, 0.1879, 0.2575, 0.1948, 0.2323], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21146200597286224 tensor([0.1571, 0.1808, 0.2228, 0.2278, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2121124416589737 tensor([0.1414, 0.1841, 0.2409, 0.2121, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2289627641439438 tensor([0.3722, 0.1777, 0.0928, 0.2290, 0.1283], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1929766833782196 tensor([0.1302, 0.1930, 0.2412, 0.2000, 0.2357], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22863565385341644 tensor([0.1069, 0.1725, 0.2982, 0.1937, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19815006852149963 tensor([0.2909, 0.1982, 0.1283, 0.2316, 0.1510], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19213926792144775 tensor([0.2436, 0.1921, 0.1551, 0.2342, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20346195995807648 tensor([0.2490, 0.2035, 0.1531, 0.2217, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20816081762313843 tensor([0.1283, 0.2082, 0.2459, 0.1841, 0.2336], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19934344291687012 tensor([0.1417, 0.1993, 0.2336, 0.2021, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1913224309682846 tensor([0.1335, 0.1913, 0.2432, 0.2115, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20415692031383514 tensor([0.1512, 0.2042, 0.2259, 0.2038, 0.2149], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2545087933540344 tensor([0.3360, 0.1683, 0.1049, 0.2545, 0.1363], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20100364089012146 tensor([0.1600, 0.2010, 0.2133, 0.2090, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24628038704395294 tensor([0.0793, 0.1603, 0.3357, 0.1784, 0.2463], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21516378223896027 tensor([0.1506, 0.1873, 0.2314, 0.2156, 0.2152], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20523600280284882 tensor([0.1659, 0.2052, 0.2079, 0.2093, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2413681149482727 tensor([0.2978, 0.1826, 0.1239, 0.2414, 0.1544], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20529450476169586 tensor([0.1926, 0.2053, 0.1842, 0.2122, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24474072456359863 tensor([0.0960, 0.1777, 0.2986, 0.1830, 0.2447], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2018696367740631 tensor([0.1686, 0.1677, 0.2093, 0.2525, 0.2019], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20247279107570648 tensor([0.1701, 0.2025, 0.2056, 0.2145, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19855432212352753 tensor([0.1986, 0.1785, 0.1893, 0.2419, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20074322819709778 tensor([0.1969, 0.2007, 0.1797, 0.2228, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20621390640735626 tensor([0.1762, 0.2104, 0.1975, 0.2096, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2557328939437866 tensor([0.2853, 0.1765, 0.1269, 0.2557, 0.1557], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2152974009513855 tensor([0.1528, 0.1844, 0.2220, 0.2255, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24456210434436798 tensor([0.3946, 0.1594, 0.0828, 0.2446, 0.1186], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21218480169773102 tensor([0.1610, 0.2122, 0.2144, 0.1981, 0.2144], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23965665698051453 tensor([0.0896, 0.1844, 0.3108, 0.1754, 0.2397], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20068544149398804 tensor([0.1801, 0.1837, 0.1965, 0.2390, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22492273151874542 tensor([0.1226, 0.1850, 0.2645, 0.2030, 0.2249], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2487756460905075 tensor([0.4395, 0.1440, 0.0669, 0.2488, 0.1009], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20055583119392395 tensor([0.1453, 0.2006, 0.2289, 0.2014, 0.2237], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24260768294334412 tensor([0.0836, 0.1715, 0.3282, 0.1741, 0.2426], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20810189843177795 tensor([0.1632, 0.1781, 0.2090, 0.2416, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2039141207933426 tensor([0.1699, 0.2081, 0.2058, 0.2123, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2518336772918701 tensor([0.2518, 0.1698, 0.1496, 0.2598, 0.1690], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20499159395694733 tensor([0.1861, 0.2101, 0.1898, 0.2090, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24724127352237701 tensor([0.0768, 0.1626, 0.3430, 0.1704, 0.2472], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21180430054664612 tensor([0.2118, 0.1847, 0.1801, 0.2406, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21509124338626862 tensor([0.1393, 0.1855, 0.2428, 0.2172, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2517665922641754 tensor([0.2518, 0.1728, 0.1489, 0.2561, 0.1704], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21498554944992065 tensor([0.1714, 0.2150, 0.1985, 0.1984, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23184554278850555 tensor([0.1084, 0.1831, 0.2859, 0.1907, 0.2318], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24871449172496796 tensor([0.2487, 0.1727, 0.1473, 0.2642, 0.1670], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19486166536808014 tensor([0.1473, 0.1949, 0.2290, 0.2107, 0.2180], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24969099462032318 tensor([0.3337, 0.1730, 0.1037, 0.2497, 0.1400], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20135332643985748 tensor([0.1836, 0.2052, 0.1896, 0.2202, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2406093180179596 tensor([0.0875, 0.1697, 0.3176, 0.1846, 0.2406], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21470722556114197 tensor([0.2147, 0.1774, 0.1645, 0.2636, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20867089927196503 tensor([0.1573, 0.2087, 0.2141, 0.2094, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2650148570537567 tensor([0.2725, 0.1698, 0.1346, 0.2650, 0.1581], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19975778460502625 tensor([0.1246, 0.1998, 0.2500, 0.1896, 0.2360], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2384830266237259 tensor([0.0993, 0.1747, 0.3014, 0.1861, 0.2385], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19336679577827454 tensor([0.1390, 0.1934, 0.2382, 0.2128, 0.2166], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19167904555797577 tensor([0.2253, 0.1917, 0.1666, 0.2338, 0.1825], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2614886462688446 tensor([0.2912, 0.1700, 0.1262, 0.2615, 0.1511], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19679595530033112 tensor([0.1615, 0.1968, 0.2130, 0.2100, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20875495672225952 tensor([0.1485, 0.1842, 0.2381, 0.2204, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22032009065151215 tensor([0.2203, 0.1750, 0.1696, 0.2524, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19874101877212524 tensor([0.1930, 0.1987, 0.1903, 0.2240, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2536259889602661 tensor([0.3496, 0.1668, 0.0987, 0.2536, 0.1314], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20597495138645172 tensor([0.1928, 0.2162, 0.1802, 0.2048, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2028006911277771 tensor([0.1222, 0.2028, 0.2583, 0.1877, 0.2290], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23905542492866516 tensor([0.3432, 0.1834, 0.1028, 0.2391, 0.1316], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19450290501117706 tensor([0.1636, 0.1945, 0.2114, 0.2175, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2177889347076416 tensor([0.2178, 0.1863, 0.1768, 0.2352, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2338588386774063 tensor([0.1225, 0.1792, 0.2607, 0.2037, 0.2339], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2380620241165161 tensor([0.0976, 0.1759, 0.2996, 0.1888, 0.2381], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19681665301322937 tensor([0.2156, 0.1968, 0.1732, 0.2317, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19617989659309387 tensor([0.1490, 0.1962, 0.2289, 0.2123, 0.2136], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23216164112091064 tensor([0.3595, 0.1824, 0.0980, 0.2322, 0.1279], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1937406212091446 tensor([0.1996, 0.1937, 0.1812, 0.2308, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2376765012741089 tensor([0.0979, 0.1834, 0.2993, 0.1817, 0.2377], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20092786848545074 tensor([0.1724, 0.1856, 0.2009, 0.2365, 0.2045], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19736027717590332 tensor([0.1335, 0.1974, 0.2410, 0.2029, 0.2252], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25317856669425964 tensor([0.3239, 0.1658, 0.1140, 0.2532, 0.1431], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24101710319519043 tensor([0.1044, 0.1817, 0.2848, 0.1880, 0.2410], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2475452572107315 tensor([0.0737, 0.1596, 0.3489, 0.1703, 0.2475], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19319921731948853 tensor([0.1861, 0.1809, 0.1935, 0.2462, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22756654024124146 tensor([0.1236, 0.1901, 0.2590, 0.1998, 0.2276], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26440271735191345 tensor([0.3173, 0.1616, 0.1132, 0.2644, 0.1435], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20041215419769287 tensor([0.1774, 0.2004, 0.2006, 0.2171, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19150160253047943 tensor([0.1204, 0.1915, 0.2577, 0.1962, 0.2342], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19403588771820068 tensor([0.1928, 0.1940, 0.1855, 0.2325, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2112669199705124 tensor([0.1290, 0.1821, 0.2506, 0.2113, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25185510516166687 tensor([0.3213, 0.1690, 0.1105, 0.2519, 0.1473], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1923961639404297 tensor([0.1527, 0.1924, 0.2343, 0.2091, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23460102081298828 tensor([0.0987, 0.1858, 0.3014, 0.1796, 0.2346], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1917976588010788 tensor([0.1860, 0.1700, 0.1923, 0.2599, 0.1918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20468838512897491 tensor([0.1671, 0.1763, 0.2047, 0.2423, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26120617985725403 tensor([0.3403, 0.1619, 0.1026, 0.2612, 0.1340], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19847488403320312 tensor([0.1214, 0.1985, 0.2526, 0.1866, 0.2409], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20965901017189026 tensor([0.1359, 0.1865, 0.2463, 0.2097, 0.2216], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21401113271713257 tensor([0.1531, 0.1818, 0.2223, 0.2289, 0.2140], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2127828598022461 tensor([0.1319, 0.1851, 0.2443, 0.2128, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24471236765384674 tensor([0.2918, 0.1829, 0.1284, 0.2447, 0.1522], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19729815423488617 tensor([0.1912, 0.1973, 0.1914, 0.2188, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2472277730703354 tensor([0.0804, 0.1640, 0.3392, 0.1692, 0.2472], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21074344217777252 tensor([0.1298, 0.1793, 0.2546, 0.2107, 0.2256], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20699109137058258 tensor([0.1487, 0.2070, 0.2260, 0.2012, 0.2172], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24756664037704468 tensor([0.2841, 0.1826, 0.1308, 0.2476, 0.1549], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20416846871376038 tensor([0.1638, 0.2042, 0.2094, 0.2056, 0.2171], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19198572635650635 tensor([0.2102, 0.1920, 0.1860, 0.2273, 0.1845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24312445521354675 tensor([0.2431, 0.1787, 0.1471, 0.2620, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21617035567760468 tensor([0.2162, 0.1862, 0.1677, 0.2428, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26059964299201965 tensor([0.3661, 0.1555, 0.0925, 0.2606, 0.1253], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.205796480178833 tensor([0.1641, 0.2115, 0.2058, 0.1992, 0.2193], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23345381021499634 tensor([0.1100, 0.1833, 0.2768, 0.1965, 0.2335], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20628969371318817 tensor([0.2922, 0.2063, 0.1236, 0.2241, 0.1538], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20359905064105988 tensor([0.1778, 0.2036, 0.1998, 0.2118, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24262762069702148 tensor([0.4336, 0.1488, 0.0702, 0.2426, 0.1047], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20856873691082 tensor([0.1723, 0.2098, 0.1985, 0.2086, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20089522004127502 tensor([0.1831, 0.1796, 0.2009, 0.2344, 0.2020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2014838010072708 tensor([0.1817, 0.1827, 0.2015, 0.2351, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2128460556268692 tensor([0.1396, 0.1858, 0.2448, 0.2128, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24765795469284058 tensor([0.2750, 0.1830, 0.1356, 0.2477, 0.1587], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2085057646036148 tensor([0.2085, 0.2165, 0.1667, 0.2109, 0.1973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2390318661928177 tensor([0.0959, 0.1777, 0.2994, 0.1878, 0.2390], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19566237926483154 tensor([0.1658, 0.1957, 0.2123, 0.2183, 0.2080], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19102609157562256 tensor([0.1475, 0.1910, 0.2277, 0.2164, 0.2174], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24962417781352997 tensor([0.3068, 0.1750, 0.1190, 0.2496, 0.1495], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23557716608047485 tensor([0.1026, 0.1873, 0.2875, 0.1870, 0.2356], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21249490976333618 tensor([0.1320, 0.1751, 0.2571, 0.2125, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20769113302230835 tensor([0.1540, 0.1763, 0.2199, 0.2422, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2027772068977356 tensor([0.1793, 0.2035, 0.1977, 0.2167, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24778097867965698 tensor([0.2839, 0.1818, 0.1340, 0.2478, 0.1525], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1966678649187088 tensor([0.1278, 0.1967, 0.2484, 0.1918, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20428511500358582 tensor([0.1649, 0.1900, 0.2251, 0.2158, 0.2043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.206013485789299 tensor([0.2060, 0.1822, 0.1710, 0.2498, 0.1909], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21423158049583435 tensor([0.1510, 0.1865, 0.2249, 0.2234, 0.2142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24779605865478516 tensor([0.4082, 0.1533, 0.0774, 0.2478, 0.1133], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20547689497470856 tensor([0.2055, 0.2133, 0.1747, 0.2115, 0.1951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23237359523773193 tensor([0.1313, 0.1894, 0.2473, 0.1997, 0.2324], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19802142679691315 tensor([0.1522, 0.1980, 0.2267, 0.2155, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20945711433887482 tensor([0.1593, 0.1772, 0.2229, 0.2312, 0.2095], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22586151957511902 tensor([0.3647, 0.1855, 0.0968, 0.2259, 0.1272], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19578561186790466 tensor([0.1370, 0.1958, 0.2345, 0.2012, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23866036534309387 tensor([0.0911, 0.1621, 0.3160, 0.1921, 0.2387], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2100650519132614 tensor([0.2101, 0.1661, 0.1710, 0.2667, 0.1862], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19719384610652924 tensor([0.1456, 0.1972, 0.2277, 0.2097, 0.2198], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25331252813339233 tensor([0.2907, 0.1753, 0.1284, 0.2533, 0.1524], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19225028157234192 tensor([0.1890, 0.1923, 0.1893, 0.2287, 0.2007], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19866052269935608 tensor([0.1775, 0.1987, 0.2058, 0.2187, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21605849266052246 tensor([0.1484, 0.1856, 0.2293, 0.2206, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2159697264432907 tensor([0.1469, 0.1836, 0.2318, 0.2218, 0.2160], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23551280796527863 tensor([0.3035, 0.1846, 0.1237, 0.2355, 0.1527], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19834958016872406 tensor([0.1500, 0.1983, 0.2270, 0.2057, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24521379172801971 tensor([0.0798, 0.1604, 0.3367, 0.1778, 0.2452], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21090401709079742 tensor([0.1604, 0.1773, 0.2231, 0.2282, 0.2109], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19701555371284485 tensor([0.2086, 0.1970, 0.1749, 0.2320, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23997117578983307 tensor([0.3620, 0.1723, 0.0952, 0.2400, 0.1306], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19392505288124084 tensor([0.2089, 0.1939, 0.1782, 0.2219, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20862242579460144 tensor([0.1721, 0.1879, 0.2097, 0.2217, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2082420438528061 tensor([0.2082, 0.1895, 0.1789, 0.2370, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19172196090221405 tensor([0.1790, 0.1917, 0.2000, 0.2284, 0.2009], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2521107494831085 tensor([0.3526, 0.1666, 0.0971, 0.2521, 0.1316], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21509124338626862 tensor([0.1686, 0.2248, 0.1994, 0.1922, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23683546483516693 tensor([0.1026, 0.1885, 0.2852, 0.1869, 0.2368], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21530118584632874 tensor([0.1471, 0.1800, 0.2256, 0.2321, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19873248040676117 tensor([0.1986, 0.1987, 0.1807, 0.2287, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2572300434112549 tensor([0.3030, 0.1692, 0.1203, 0.2572, 0.1503], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2105483114719391 tensor([0.1269, 0.2105, 0.2424, 0.1827, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24279442429542542 tensor([0.0908, 0.1713, 0.3134, 0.1817, 0.2428], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23820088803768158 tensor([0.1022, 0.1829, 0.2950, 0.1816, 0.2382], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19430860877037048 tensor([0.1417, 0.1943, 0.2320, 0.2077, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24593617022037506 tensor([0.2649, 0.1856, 0.1420, 0.2459, 0.1617], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19530418515205383 tensor([0.1425, 0.1953, 0.2360, 0.2021, 0.2241], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24469657242298126 tensor([0.0852, 0.1682, 0.3233, 0.1786, 0.2447], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19446448981761932 tensor([0.1902, 0.1945, 0.1877, 0.2353, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21453136205673218 tensor([0.1587, 0.1898, 0.2152, 0.2218, 0.2145], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24475479125976562 tensor([0.2688, 0.1827, 0.1414, 0.2448, 0.1623], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20093126595020294 tensor([0.1334, 0.2009, 0.2448, 0.1896, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21110211312770844 tensor([0.1467, 0.1858, 0.2366, 0.2111, 0.2198], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20789630711078644 tensor([0.1648, 0.1742, 0.2110, 0.2420, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19179010391235352 tensor([0.1905, 0.1918, 0.1859, 0.2370, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23435522615909576 tensor([0.3541, 0.1763, 0.1010, 0.2344, 0.1343], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19398580491542816 tensor([0.1299, 0.1940, 0.2507, 0.1973, 0.2281], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23267754912376404 tensor([0.1086, 0.1713, 0.2925, 0.1949, 0.2327], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1964007169008255 tensor([0.1992, 0.1964, 0.1841, 0.2346, 0.1857], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19606716930866241 tensor([0.2668, 0.1961, 0.1419, 0.2368, 0.1584], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.258732408285141 tensor([0.3530, 0.1595, 0.0973, 0.2587, 0.1314], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19552068412303925 tensor([0.2180, 0.1955, 0.1734, 0.2244, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19166499376296997 tensor([0.1709, 0.1917, 0.2094, 0.2199, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18588712811470032 tensor([0.1935, 0.1859, 0.1886, 0.2398, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19267645478248596 tensor([0.1329, 0.1927, 0.2494, 0.2023, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.247674360871315 tensor([0.4037, 0.1568, 0.0785, 0.2477, 0.1133], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2177066206932068 tensor([0.1481, 0.1873, 0.2261, 0.2177, 0.2208], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2132798284292221 tensor([0.1437, 0.1875, 0.2343, 0.2133, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20018261671066284 tensor([0.1838, 0.1879, 0.1939, 0.2341, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20849230885505676 tensor([0.1612, 0.1897, 0.2165, 0.2241, 0.2085], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20333655178546906 tensor([0.1626, 0.2033, 0.2159, 0.2087, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20107199251651764 tensor([0.1565, 0.2011, 0.2142, 0.2097, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24546672403812408 tensor([0.0875, 0.1829, 0.3133, 0.1708, 0.2455], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22930248081684113 tensor([0.2293, 0.1782, 0.1597, 0.2591, 0.1738], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19130398333072662 tensor([0.2344, 0.1913, 0.1567, 0.2392, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25167763233184814 tensor([0.2844, 0.1743, 0.1298, 0.2517, 0.1598], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20931527018547058 tensor([0.1801, 0.2093, 0.1916, 0.2096, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23790377378463745 tensor([0.0883, 0.1723, 0.3233, 0.1782, 0.2379], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2445095330476761 tensor([0.2445, 0.1755, 0.1521, 0.2586, 0.1693], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19870930910110474 tensor([0.1431, 0.1987, 0.2296, 0.2067, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25595059990882874 tensor([0.3806, 0.1550, 0.0871, 0.2560, 0.1214], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20219014585018158 tensor([0.1681, 0.2115, 0.2022, 0.2026, 0.2156], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23249202966690063 tensor([0.0936, 0.1838, 0.3140, 0.1761, 0.2325], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1906173974275589 tensor([0.2036, 0.1906, 0.1825, 0.2346, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1951279491186142 tensor([0.2251, 0.1951, 0.1656, 0.2342, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2444775253534317 tensor([0.3194, 0.1779, 0.1121, 0.2445, 0.1462], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20620191097259521 tensor([0.1810, 0.2089, 0.1931, 0.2062, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22727304697036743 tensor([0.1385, 0.1898, 0.2399, 0.2045, 0.2273], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22003889083862305 tensor([0.2200, 0.1760, 0.1670, 0.2580, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1938570737838745 tensor([0.1776, 0.1939, 0.2008, 0.2263, 0.2014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2499627321958542 tensor([0.3130, 0.1761, 0.1132, 0.2500, 0.1478], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20200394093990326 tensor([0.1463, 0.2020, 0.2265, 0.2030, 0.2223], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23045918345451355 tensor([0.1180, 0.1782, 0.2769, 0.1965, 0.2305], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1959601193666458 tensor([0.1901, 0.1860, 0.1943, 0.2336, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1920243799686432 tensor([0.1265, 0.1920, 0.2517, 0.2035, 0.2263], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23382468521595 tensor([0.2483, 0.1867, 0.1564, 0.2338, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21556434035301208 tensor([0.1721, 0.2156, 0.1954, 0.1979, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23897205293178558 tensor([0.1072, 0.1719, 0.2891, 0.1928, 0.2390], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19806675612926483 tensor([0.1798, 0.1733, 0.1957, 0.2531, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21281698346138 tensor([0.1351, 0.1865, 0.2431, 0.2128, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2512068450450897 tensor([0.3346, 0.1685, 0.1070, 0.2512, 0.1388], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20174621045589447 tensor([0.1763, 0.2017, 0.2028, 0.2076, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22524532675743103 tensor([0.1206, 0.1886, 0.2723, 0.1933, 0.2252], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2140943557024002 tensor([0.1548, 0.1796, 0.2192, 0.2323, 0.2141], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20346875488758087 tensor([0.2035, 0.1807, 0.1818, 0.2435, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25035569071769714 tensor([0.3886, 0.1555, 0.0853, 0.2504, 0.1203], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20057110488414764 tensor([0.1723, 0.2006, 0.2004, 0.2170, 0.2097], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21509510278701782 tensor([0.1453, 0.1767, 0.2373, 0.2256, 0.2151], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20598849654197693 tensor([0.1626, 0.1694, 0.2199, 0.2420, 0.2060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19669099152088165 tensor([0.1468, 0.1967, 0.2362, 0.2064, 0.2140], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24161341786384583 tensor([0.2792, 0.1779, 0.1388, 0.2416, 0.1625], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19929516315460205 tensor([0.1463, 0.1993, 0.2248, 0.2090, 0.2206], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21018235385417938 tensor([0.1113, 0.1665, 0.2819, 0.2102, 0.2300], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19633235037326813 tensor([0.1702, 0.1963, 0.2058, 0.2213, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20645353198051453 tensor([0.1680, 0.1880, 0.2151, 0.2224, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22863341867923737 tensor([0.4265, 0.1616, 0.0764, 0.2286, 0.1068], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1962614208459854 tensor([0.2258, 0.1963, 0.1584, 0.2356, 0.1840], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1951867640018463 tensor([0.1388, 0.1952, 0.2452, 0.2026, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23290102183818817 tensor([0.2329, 0.1856, 0.1599, 0.2478, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25107690691947937 tensor([0.2692, 0.1863, 0.1357, 0.2511, 0.1577], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20663613080978394 tensor([0.2066, 0.1834, 0.1762, 0.2431, 0.1906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24001865088939667 tensor([0.1016, 0.1879, 0.2906, 0.1798, 0.2400], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19829484820365906 tensor([0.1912, 0.1866, 0.1996, 0.2243, 0.1983], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21536025404930115 tensor([0.1387, 0.1866, 0.2418, 0.2175, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20088255405426025 tensor([0.1601, 0.2009, 0.2187, 0.2130, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4], [0, 3], [0, 3, 1], [0, 1], [0, 3], [2, 4, 1], [0], [0, 1, 3], [0, 1], [0], [2, 4, 1], [2, 0], [0, 3, 1], [0, 1], [0], [2, 1, 4, 0], [2, 0, 4], [0, 2], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 3], [0, 3, 1], [0, 1], [0], [2, 4, 1], [2, 0], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0], [0, 1, 3], [2, 4, 1], [0], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [2, 4, 1], [2, 0, 3], [0, 3], [2, 4, 1], [0, 1], [2, 4, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 1], [0, 3, 1], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 3], [2, 4, 1], [0], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3], [0, 3, 1], [2, 4], [0, 2], [2, 4, 1], [0, 2], [1, 0], [0, 1, 4], [0, 1], [2, 4, 1], [2, 4], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 2], [2, 4, 1], [0, 3, 1], [0, 1], [2, 1, 4], [0, 1], [2, 4, 1], [2, 4], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 1, 4], [0, 1], [2, 4, 1], [2, 0, 1], [0, 4, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 3], [0, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1], [2, 0, 1], [0, 1], [0], [0], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 0], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3], [0, 3, 1], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 0], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 3], [0, 1, 3], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 2], [0, 3, 1], [0, 1], [2, 1, 4], [2, 4, 1], [0, 2], [0, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1, 4], [0, 1], [0]]\n",
      "[[0, 3], [3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [0, 1, 3], [1, 2, 4], [2, 4], [2, 3, 4], [1, 2, 4], [0, 3], [1, 2, 3, 4], [2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [2, 3, 4], [1, 2, 3, 4], [3], [1, 3], [1, 3, 4], [0, 3], [2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [3, 4], [2, 4], [0, 3], [1, 2, 4], [2, 4], [2, 3, 4], [1, 2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [1, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [2, 3, 4], [2, 3, 4], [0, 3], [3], [0, 3], [1, 4], [1, 2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [1, 3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [1, 2, 4], [0, 3], [1, 2, 3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [1, 2, 4], [2, 4], [0, 1, 3], [1, 3, 4], [0, 3], [1, 3, 4], [2, 3, 4], [2, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 3], [2, 4], [2, 3, 4], [0, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 3], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [3, 4], [0, 3], [1, 4], [2, 4], [2, 3, 4], [3], [0, 3], [1, 2, 4], [2, 4], [2, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 3, 4], [0, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [0, 3], [2, 3, 4], [3], [2, 4], [0, 3], [2, 3, 4], [2, 3, 4], [3, 4], [2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [2, 4], [0, 3], [0, 3], [0, 3], [1, 3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [1, 2, 4], [2, 4], [3], [0, 3], [0, 3], [1, 3, 4], [2, 4], [0, 3], [3, 4], [0, 3], [1, 2, 4], [2, 4], [3], [2, 4], [0, 3], [1, 4], [2, 4], [3], [2, 3, 4], [0, 3], [1, 3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [3], [2, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 3th iteration [[2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1], [2, 1, 4, 0], [0, 1], [0, 3, 1], [0, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 3, 1], [2, 0, 1, 4], [0, 3, 1], [2, 0, 4, 1], [0, 1], [1, 0, 2, 4], [0, 3, 1], [2, 0, 1], [2, 4, 1], [0, 1], [0, 1], [0, 3, 1], [0, 1], [0, 3, 1], [0, 1], [2, 0, 1], [0, 4, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 2, 1], [2, 0, 4, 1], [0, 1], [0, 3, 1], [2, 0, 4, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 0, 1], [2, 4, 1], [0, 2, 1], [2, 0, 1, 4], [0, 3, 1], [1, 0, 2, 4], [0, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.02003878727555275  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.020036941394209862  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.02003343403339386  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.02002844028174877  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.020022111013531685  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.020014595240354538  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.020006021484732628  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.019996503368020058  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.01998615637421608  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.019975068047642708  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.019963333383202553  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.019951028749346733  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01993822306394577  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.019924987107515335  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.019911369308829308  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.019897429272532463  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.019883213564753532  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.01986876130104065  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.019854117184877396  Accuracy on Support set:0.0\n",
      "torch.Size([64, 2048]) torch.Size([64])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.01983931101858616  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.25999248027801514 tensor([0.3528, 0.1531, 0.1004, 0.2600, 0.1337], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2339870184659958 tensor([0.1279, 0.1751, 0.2623, 0.2008, 0.2340], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2129172533750534 tensor([0.1573, 0.1677, 0.2269, 0.2351, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21888040006160736 tensor([0.1417, 0.1710, 0.2454, 0.2189, 0.2230], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23638740181922913 tensor([0.3737, 0.1658, 0.0947, 0.2364, 0.1294], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23756569623947144 tensor([0.1305, 0.1794, 0.2460, 0.2066, 0.2376], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22989152371883392 tensor([0.1071, 0.1604, 0.3030, 0.1995, 0.2299], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23974406719207764 tensor([0.2924, 0.1844, 0.1310, 0.2397, 0.1525], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24196231365203857 tensor([0.2446, 0.1789, 0.1581, 0.2420, 0.1765], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1897522360086441 tensor([0.2502, 0.1898, 0.1563, 0.2293, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19396960735321045 tensor([0.1287, 0.1940, 0.2511, 0.1902, 0.2361], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22509236633777618 tensor([0.1421, 0.1856, 0.2383, 0.2089, 0.2251], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21851475536823273 tensor([0.1339, 0.1775, 0.2479, 0.2185, 0.2222], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19021716713905334 tensor([0.1519, 0.1902, 0.2305, 0.2106, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2626473903656006 tensor([0.3366, 0.1564, 0.1069, 0.2626, 0.1374], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1871146559715271 tensor([0.1605, 0.1871, 0.2177, 0.2160, 0.2186], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24741961061954498 tensor([0.0797, 0.1497, 0.3397, 0.1834, 0.2474], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2167915552854538 tensor([0.1510, 0.1739, 0.2357, 0.2226, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19116823375225067 tensor([0.1666, 0.1912, 0.2122, 0.2164, 0.2137], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24938735365867615 tensor([0.2987, 0.1698, 0.1264, 0.2494, 0.1557], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19146232306957245 tensor([0.1935, 0.1915, 0.1880, 0.2194, 0.2076], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24662593007087708 tensor([0.0958, 0.1653, 0.3039, 0.1884, 0.2466], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20283226668834686 tensor([0.1689, 0.1554, 0.2128, 0.2601, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18839608132839203 tensor([0.1707, 0.1884, 0.2098, 0.2218, 0.2093], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18679557740688324 tensor([0.1977, 0.1868, 0.1835, 0.2304, 0.2016], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1959802359342575 tensor([0.1771, 0.1960, 0.2018, 0.2170, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2642298638820648 tensor([0.2860, 0.1636, 0.1293, 0.2642, 0.1568], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21680420637130737 tensor([0.1531, 0.1711, 0.2262, 0.2328, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25238239765167236 tensor([0.3953, 0.1483, 0.0845, 0.2524, 0.1196], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1978253722190857 tensor([0.1617, 0.1978, 0.2190, 0.2050, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24173519015312195 tensor([0.0893, 0.1717, 0.3169, 0.1804, 0.2417], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20210187137126923 tensor([0.1806, 0.1707, 0.2001, 0.2466, 0.2021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22653113305568695 tensor([0.1228, 0.1719, 0.2694, 0.2094, 0.2265], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.256063848733902 tensor([0.4382, 0.1350, 0.0686, 0.2561, 0.1021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1865607649087906 tensor([0.1458, 0.1866, 0.2336, 0.2083, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24432985484600067 tensor([0.0833, 0.1595, 0.3338, 0.1791, 0.2443], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20940327644348145 tensor([0.1635, 0.1650, 0.2128, 0.2493, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19372737407684326 tensor([0.1706, 0.1937, 0.2102, 0.2196, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25212758779525757 tensor([0.2521, 0.1576, 0.1523, 0.2679, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19586165249347687 tensor([0.1869, 0.1959, 0.1939, 0.2163, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24881766736507416 tensor([0.0771, 0.1522, 0.3472, 0.1747, 0.2488], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21238316595554352 tensor([0.2124, 0.1716, 0.1835, 0.2484, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2166837900876999 tensor([0.1396, 0.1723, 0.2474, 0.2241, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25214508175849915 tensor([0.2521, 0.1604, 0.1517, 0.2642, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20069822669029236 tensor([0.1723, 0.2007, 0.2028, 0.2053, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23347921669483185 tensor([0.1085, 0.1701, 0.2912, 0.1967, 0.2335], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24921724200248718 tensor([0.2492, 0.1601, 0.1500, 0.2727, 0.1681], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21785981953144073 tensor([0.1477, 0.1809, 0.2336, 0.2179, 0.2199], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25791850686073303 tensor([0.3346, 0.1607, 0.1057, 0.2579, 0.1411], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19100891053676605 tensor([0.1843, 0.1910, 0.1936, 0.2278, 0.2033], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2419542521238327 tensor([0.0876, 0.1575, 0.3229, 0.1901, 0.2420], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21522580087184906 tensor([0.2152, 0.1643, 0.1674, 0.2721, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19411087036132812 tensor([0.1580, 0.1941, 0.2186, 0.2167, 0.2126], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27274203300476074 tensor([0.2727, 0.1573, 0.1372, 0.2735, 0.1593], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23811368644237518 tensor([0.1250, 0.1860, 0.2551, 0.1959, 0.2381], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23991161584854126 tensor([0.0994, 0.1623, 0.3065, 0.1919, 0.2399], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2183469831943512 tensor([0.1394, 0.1795, 0.2428, 0.2199, 0.2183], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22605276107788086 tensor([0.2261, 0.1781, 0.1700, 0.2418, 0.1841], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26981139183044434 tensor([0.2915, 0.1577, 0.1286, 0.2698, 0.1523], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21697193384170532 tensor([0.1621, 0.1832, 0.2172, 0.2170, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21026398241519928 tensor([0.1487, 0.1710, 0.2426, 0.2275, 0.2103], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22079817950725555 tensor([0.2208, 0.1624, 0.1726, 0.2603, 0.1839], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26176321506500244 tensor([0.3503, 0.1549, 0.1006, 0.2618, 0.1324], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20180745422840118 tensor([0.1937, 0.2018, 0.1842, 0.2120, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1887960284948349 tensor([0.1224, 0.1888, 0.2636, 0.1939, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24707871675491333 tensor([0.3449, 0.1706, 0.1048, 0.2471, 0.1326], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2148214876651764 tensor([0.1640, 0.1807, 0.2156, 0.2248, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21837328374385834 tensor([0.2184, 0.1732, 0.1802, 0.2429, 0.1853], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23536425828933716 tensor([0.1227, 0.1665, 0.2653, 0.2101, 0.2354], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23955920338630676 tensor([0.0976, 0.1634, 0.3049, 0.1945, 0.2396], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21647758781909943 tensor([0.2165, 0.1833, 0.1766, 0.2394, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21538203954696655 tensor([0.1494, 0.1823, 0.2334, 0.2195, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23990961909294128 tensor([0.3613, 0.1700, 0.0999, 0.2399, 0.1289], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23957791924476624 tensor([0.0979, 0.1705, 0.3048, 0.1873, 0.2396], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2047111839056015 tensor([0.1729, 0.1722, 0.2047, 0.2443, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2271786779165268 tensor([0.1339, 0.1833, 0.2458, 0.2098, 0.2272], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.261121541261673 tensor([0.3244, 0.1541, 0.1161, 0.2611, 0.1443], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2427179515361786 tensor([0.1045, 0.1687, 0.2901, 0.1939, 0.2427], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24890848994255066 tensor([0.0738, 0.1493, 0.3534, 0.1746, 0.2489], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22934037446975708 tensor([0.1238, 0.1766, 0.2639, 0.2063, 0.2293], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2722870409488678 tensor([0.3173, 0.1504, 0.1153, 0.2723, 0.1447], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1864853948354721 tensor([0.1781, 0.1865, 0.2047, 0.2245, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23607537150382996 tensor([0.1207, 0.1780, 0.2627, 0.2025, 0.2361], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21806614100933075 tensor([0.1292, 0.1689, 0.2552, 0.2181, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25982725620269775 tensor([0.3220, 0.1572, 0.1126, 0.2598, 0.1484], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21314840018749237 tensor([0.1531, 0.1791, 0.2388, 0.2159, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23657657206058502 tensor([0.0986, 0.1729, 0.3070, 0.1849, 0.2366], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2083793729543686 tensor([0.1673, 0.1635, 0.2084, 0.2499, 0.2109], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26925191283226013 tensor([0.3406, 0.1505, 0.1045, 0.2693, 0.1352], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24329346418380737 tensor([0.1215, 0.1849, 0.2578, 0.1925, 0.2433], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2164086550474167 tensor([0.1361, 0.1733, 0.2510, 0.2164, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21544668078422546 tensor([0.1534, 0.1686, 0.2263, 0.2361, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21968014538288116 tensor([0.1321, 0.1717, 0.2489, 0.2197, 0.2276], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2527829706668854 tensor([0.2926, 0.1701, 0.1309, 0.2528, 0.1535], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20316475629806519 tensor([0.1918, 0.1836, 0.1953, 0.2261, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24861745536327362 tensor([0.0803, 0.1534, 0.3438, 0.1739, 0.2486], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21741218864917755 tensor([0.1300, 0.1664, 0.2592, 0.2174, 0.2270], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19293411076068878 tensor([0.1493, 0.1929, 0.2306, 0.2080, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25576069951057434 tensor([0.2849, 0.1697, 0.1335, 0.2558, 0.1562], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1900976449251175 tensor([0.1644, 0.1901, 0.2138, 0.2126, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2108534872531891 tensor([0.2109, 0.1789, 0.1896, 0.2346, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2439049482345581 tensor([0.2439, 0.1658, 0.1497, 0.2704, 0.1702], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2166968435049057 tensor([0.2167, 0.1729, 0.1710, 0.2507, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26848462224006653 tensor([0.3658, 0.1449, 0.0943, 0.2685, 0.1265], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19725151360034943 tensor([0.1649, 0.1973, 0.2102, 0.2061, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2350243330001831 tensor([0.1102, 0.1702, 0.2818, 0.2027, 0.2350], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1925758421421051 tensor([0.2940, 0.1926, 0.1262, 0.2319, 0.1554], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18971219658851624 tensor([0.1785, 0.1897, 0.2039, 0.2189, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24997426569461823 tensor([0.4329, 0.1393, 0.0719, 0.2500, 0.1059], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1954660415649414 tensor([0.1731, 0.1955, 0.2027, 0.2158, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2034001648426056 tensor([0.1834, 0.1666, 0.2046, 0.2419, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20525097846984863 tensor([0.1821, 0.1696, 0.2053, 0.2426, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21851655840873718 tensor([0.1399, 0.1725, 0.2493, 0.2197, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25586411356925964 tensor([0.2757, 0.1700, 0.1383, 0.2559, 0.1601], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2020411640405655 tensor([0.2097, 0.2020, 0.1704, 0.2184, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2407558560371399 tensor([0.0958, 0.1650, 0.3048, 0.1936, 0.2408], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20964683592319489 tensor([0.1664, 0.1820, 0.2164, 0.2255, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21914677321910858 tensor([0.1478, 0.1773, 0.2322, 0.2236, 0.2191], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25764498114585876 tensor([0.3076, 0.1628, 0.1213, 0.2576, 0.1507], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23740659654140472 tensor([0.1026, 0.1740, 0.2930, 0.1930, 0.2374], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21891924738883972 tensor([0.1321, 0.1627, 0.2616, 0.2189, 0.2247], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20889072120189667 tensor([0.1543, 0.1633, 0.2237, 0.2498, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18925431370735168 tensor([0.1800, 0.1893, 0.2018, 0.2242, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2559046149253845 tensor([0.2847, 0.1690, 0.1366, 0.2559, 0.1538], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23734889924526215 tensor([0.1282, 0.1829, 0.2534, 0.1982, 0.2373], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20586571097373962 tensor([0.1653, 0.1769, 0.2293, 0.2226, 0.2059], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20651091635227203 tensor([0.2065, 0.1691, 0.1742, 0.2579, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21580620110034943 tensor([0.1513, 0.1731, 0.2291, 0.2307, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25529542565345764 tensor([0.4078, 0.1432, 0.0793, 0.2553, 0.1143], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1988559514284134 tensor([0.2065, 0.1989, 0.1785, 0.2190, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23409287631511688 tensor([0.1316, 0.1763, 0.2519, 0.2061, 0.2341], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20937848091125488 tensor([0.1527, 0.1841, 0.2311, 0.2227, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2108202427625656 tensor([0.1595, 0.1643, 0.2269, 0.2385, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23329037427902222 tensor([0.3667, 0.1731, 0.0987, 0.2333, 0.1282], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2335292547941208 tensor([0.1372, 0.1822, 0.2393, 0.2077, 0.2335], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23980359733104706 tensor([0.0913, 0.1510, 0.3205, 0.1974, 0.2398], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21027307212352753 tensor([0.2103, 0.1538, 0.1739, 0.2748, 0.1873], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21680277585983276 tensor([0.1460, 0.1831, 0.2323, 0.2168, 0.2217], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2615370750427246 tensor([0.2914, 0.1627, 0.1308, 0.2615, 0.1535], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20234692096710205 tensor([0.1896, 0.1787, 0.1931, 0.2364, 0.2023], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20998892188072205 tensor([0.1781, 0.1848, 0.2100, 0.2261, 0.2011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21760207414627075 tensor([0.1487, 0.1723, 0.2336, 0.2278, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21748922765254974 tensor([0.1472, 0.1704, 0.2360, 0.2289, 0.2175], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24326995015144348 tensor([0.3046, 0.1719, 0.1262, 0.2433, 0.1540], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21263086795806885 tensor([0.1504, 0.1845, 0.2316, 0.2126, 0.2209], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2462717741727829 tensor([0.0802, 0.1496, 0.3410, 0.1829, 0.2463], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21225526928901672 tensor([0.1607, 0.1646, 0.2270, 0.2354, 0.2123], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2094072699546814 tensor([0.2094, 0.1830, 0.1784, 0.2399, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2478298544883728 tensor([0.3632, 0.1603, 0.0970, 0.2478, 0.1316], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20965425670146942 tensor([0.2097, 0.1805, 0.1817, 0.2293, 0.1987], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2102002054452896 tensor([0.1726, 0.1747, 0.2136, 0.2289, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20885495841503143 tensor([0.2089, 0.1760, 0.1824, 0.2449, 0.1879], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20251348614692688 tensor([0.1795, 0.1780, 0.2040, 0.2360, 0.2025], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2600969672203064 tensor([0.3534, 0.1548, 0.0990, 0.2601, 0.1327], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21016493439674377 tensor([0.1695, 0.2102, 0.2038, 0.1989, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23901401460170746 tensor([0.1024, 0.1752, 0.2908, 0.1927, 0.2390], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21670274436473846 tensor([0.1474, 0.1669, 0.2295, 0.2395, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26541009545326233 tensor([0.3034, 0.1571, 0.1226, 0.2654, 0.1514], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19663608074188232 tensor([0.1268, 0.1966, 0.2478, 0.1882, 0.2405], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2442869395017624 tensor([0.0907, 0.1591, 0.3187, 0.1872, 0.2443], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23986652493476868 tensor([0.1023, 0.1702, 0.3004, 0.1872, 0.2399], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21458245813846588 tensor([0.1421, 0.1806, 0.2366, 0.2146, 0.2261], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25408002734184265 tensor([0.2656, 0.1724, 0.1448, 0.2541, 0.1630], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22592869400978088 tensor([0.1429, 0.1817, 0.2407, 0.2087, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24626682698726654 tensor([0.0849, 0.1564, 0.3287, 0.1837, 0.2463], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21621915698051453 tensor([0.1591, 0.1762, 0.2194, 0.2292, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2528049945831299 tensor([0.2696, 0.1698, 0.1442, 0.2528, 0.1636], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18709029257297516 tensor([0.1338, 0.1871, 0.2498, 0.1959, 0.2334], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21777726709842682 tensor([0.1469, 0.1728, 0.2411, 0.2178, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20911791920661926 tensor([0.1651, 0.1614, 0.2147, 0.2497, 0.2091], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24191826581954956 tensor([0.3556, 0.1643, 0.1029, 0.2419, 0.1353], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22994795441627502 tensor([0.1302, 0.1806, 0.2556, 0.2037, 0.2299], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23397888243198395 tensor([0.1087, 0.1591, 0.2974, 0.2008, 0.2340], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2450084090232849 tensor([0.2679, 0.1823, 0.1449, 0.2450, 0.1599], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26653483510017395 tensor([0.3534, 0.1485, 0.0991, 0.2665, 0.1324], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21884174644947052 tensor([0.2188, 0.1820, 0.1769, 0.2320, 0.1903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20984672009944916 tensor([0.1714, 0.1783, 0.2134, 0.2271, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22448261082172394 tensor([0.1332, 0.1791, 0.2542, 0.2090, 0.2245], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2551221251487732 tensor([0.4036, 0.1465, 0.0803, 0.2551, 0.1144], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2223961353302002 tensor([0.1484, 0.1740, 0.2304, 0.2247, 0.2224], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2202625870704651 tensor([0.1440, 0.1741, 0.2387, 0.2203, 0.2229], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20170381665229797 tensor([0.1843, 0.1745, 0.1977, 0.2418, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21010352671146393 tensor([0.1616, 0.1762, 0.2206, 0.2315, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18927916884422302 tensor([0.1632, 0.1893, 0.2203, 0.2158, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18704001605510712 tensor([0.1571, 0.1870, 0.2186, 0.2168, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2478204071521759 tensor([0.0870, 0.1704, 0.3194, 0.1753, 0.2478], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2299780696630478 tensor([0.2300, 0.1652, 0.1625, 0.2674, 0.1749], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23521840572357178 tensor([0.2352, 0.1777, 0.1598, 0.2473, 0.1799], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2597595155239105 tensor([0.2850, 0.1619, 0.1323, 0.2598, 0.1610], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1950386017560959 tensor([0.1809, 0.1950, 0.1957, 0.2169, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23947682976722717 tensor([0.0881, 0.1604, 0.3288, 0.1832, 0.2395], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24506503343582153 tensor([0.2451, 0.1628, 0.1548, 0.2669, 0.1704], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21367353200912476 tensor([0.1435, 0.1846, 0.2343, 0.2137, 0.2239], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26348334550857544 tensor([0.3801, 0.1448, 0.0891, 0.2635, 0.1226], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19718950986862183 tensor([0.1688, 0.1972, 0.2066, 0.2096, 0.2178], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23443646728992462 tensor([0.0933, 0.1710, 0.3200, 0.1812, 0.2344], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2259202003479004 tensor([0.2259, 0.1814, 0.1690, 0.2422, 0.1815], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25247690081596375 tensor([0.3204, 0.1655, 0.1143, 0.2525, 0.1474], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19470015168190002 tensor([0.1819, 0.1947, 0.1972, 0.2133, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22906871140003204 tensor([0.1388, 0.1765, 0.2445, 0.2111, 0.2291], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22052116692066193 tensor([0.2205, 0.1632, 0.1699, 0.2662, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2031022310256958 tensor([0.1781, 0.1801, 0.2049, 0.2339, 0.2031], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25811031460762024 tensor([0.3138, 0.1637, 0.1155, 0.2581, 0.1490], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1880117952823639 tensor([0.1467, 0.1880, 0.2312, 0.2098, 0.2243], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23196576535701752 tensor([0.1181, 0.1656, 0.2818, 0.2025, 0.2320], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22818157076835632 tensor([0.1267, 0.1783, 0.2566, 0.2102, 0.2282], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24137449264526367 tensor([0.2491, 0.1739, 0.1594, 0.2414, 0.1763], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20107334852218628 tensor([0.1730, 0.2011, 0.1996, 0.2049, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24034783244132996 tensor([0.1072, 0.1597, 0.2941, 0.1986, 0.2403], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21973776817321777 tensor([0.1353, 0.1730, 0.2477, 0.2197, 0.2242], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25930124521255493 tensor([0.3353, 0.1565, 0.1091, 0.2593, 0.1398], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18794870376586914 tensor([0.1769, 0.1879, 0.2070, 0.2147, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2269504964351654 tensor([0.1207, 0.1753, 0.2775, 0.1995, 0.2270], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2154676616191864 tensor([0.1551, 0.1666, 0.2232, 0.2397, 0.2155], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20386996865272522 tensor([0.2039, 0.1677, 0.1852, 0.2513, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25786682963371277 tensor([0.3885, 0.1452, 0.0872, 0.2579, 0.1213], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1865590363740921 tensor([0.1729, 0.1866, 0.2046, 0.2244, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21645323932170868 tensor([0.1455, 0.1641, 0.2415, 0.2325, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20709705352783203 tensor([0.1628, 0.1569, 0.2236, 0.2495, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21323157846927643 tensor([0.1472, 0.1829, 0.2409, 0.2132, 0.2158], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24938234686851501 tensor([0.2799, 0.1655, 0.1414, 0.2494, 0.1638], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21601925790309906 tensor([0.1468, 0.1853, 0.2294, 0.2160, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2162896692752838 tensor([0.1114, 0.1546, 0.2864, 0.2163, 0.2313], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20814070105552673 tensor([0.1707, 0.1824, 0.2099, 0.2288, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20808154344558716 tensor([0.1684, 0.1745, 0.2193, 0.2297, 0.2081], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2354913055896759 tensor([0.4280, 0.1511, 0.0778, 0.2355, 0.1077], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22671474516391754 tensor([0.2267, 0.1825, 0.1616, 0.2437, 0.1855], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.219990074634552 tensor([0.1392, 0.1814, 0.2501, 0.2094, 0.2200], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2336137592792511 tensor([0.2336, 0.1724, 0.1630, 0.2560, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25944361090660095 tensor([0.2700, 0.1731, 0.1384, 0.2594, 0.1590], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20698174834251404 tensor([0.2070, 0.1703, 0.1797, 0.2509, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24195165932178497 tensor([0.1017, 0.1749, 0.2961, 0.1853, 0.2420], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2168656885623932 tensor([0.1391, 0.1732, 0.2463, 0.2246, 0.2169], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18673880398273468 tensor([0.1606, 0.1867, 0.2232, 0.2202, 0.2093], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 1], [2, 1, 4, 0], [2, 0, 4, 1], [0, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 1, 3], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [2, 4, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 1], [0, 3, 1], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 2, 1], [1, 0], [0, 1, 4], [0, 1], [2, 4, 1], [2, 4], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 1], [2, 1, 4], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 1, 4], [0, 1], [2, 4, 1], [2, 0, 1], [0, 4, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1], [2, 0, 1], [0, 1], [0, 1], [0, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 3], [0, 1, 3], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [0, 1], [2, 1, 4], [2, 4, 1], [0, 2, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1, 4], [0, 1], [0, 1]]\n",
      "[[0, 3], [3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [2, 3, 4], [3], [3], [3, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [1, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [2, 3, 4], [2, 3, 4], [0, 3], [3], [0, 3], [1, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [0, 3], [3, 4], [0, 3], [3, 4], [2, 3, 4], [2, 3], [2, 3, 4], [0, 3], [0, 1, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [3, 4], [0, 3], [2, 4], [2, 3, 4], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 3], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [3, 4], [0, 3], [1, 4], [2, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [2, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [0, 3], [2, 3, 4], [3], [2, 4], [0, 3], [2, 3, 4], [2, 3, 4], [3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 4], [0, 3], [0, 3], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [3, 4], [2, 4], [0, 3], [3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [1, 4], [2, 4], [3], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [3], [2, 3, 4], [2, 3, 4]]\n",
      "NL_pred of 4th iteration [[2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 1], [2, 0, 1], [0, 1], [2, 0, 4, 1], [0, 2, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 0, 1], [2, 0, 1], [0, 1], [0, 3, 1], [0, 2, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [0, 2, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 1], [2, 0, 1], [0, 3, 1], [2, 0, 1], [0, 3, 1], [0, 2, 1], [0, 2, 1], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.03550873862372504  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.03550476166937086  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.03549722499317593  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.03548648291163974  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.035472886429892644  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.03545674019389682  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.03543833229276869  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.035417907767825656  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.03539570503764682  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.03537192940711975  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.035346782869762845  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.03532042768266466  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.035293022791544594  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.035264710585276283  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.03523561358451843  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.03520585430992974  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.03517552547984653  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.03514472312397427  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.035113536649280123  Accuracy on Support set:0.0\n",
      "torch.Size([36, 2048]) torch.Size([36])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.03508203559451633  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.2681787610054016 tensor([0.3510, 0.1417, 0.1029, 0.2682, 0.1362], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23700188100337982 tensor([0.1271, 0.1615, 0.2678, 0.2065, 0.2370], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21564093232154846 tensor([0.1563, 0.1542, 0.2318, 0.2421, 0.2156], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22522501647472382 tensor([0.1408, 0.1575, 0.2506, 0.2252, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2441408783197403 tensor([0.3733, 0.1538, 0.0971, 0.2441, 0.1317], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24083170294761658 tensor([0.1297, 0.1650, 0.2515, 0.2129, 0.2408], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2323446273803711 tensor([0.1064, 0.1479, 0.3085, 0.2049, 0.2323], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24795126914978027 tensor([0.2919, 0.1706, 0.1344, 0.2480, 0.1552], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24381738901138306 tensor([0.2438, 0.1653, 0.1619, 0.2497, 0.1793], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23711997270584106 tensor([0.2496, 0.1755, 0.1603, 0.2371, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2398330122232437 tensor([0.1280, 0.1788, 0.2571, 0.1962, 0.2398], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2283126264810562 tensor([0.1414, 0.1711, 0.2438, 0.2153, 0.2283], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22514154016971588 tensor([0.1331, 0.1633, 0.2533, 0.2251, 0.2251], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2201370894908905 tensor([0.1512, 0.1755, 0.2359, 0.2173, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27090397477149963 tensor([0.3352, 0.1446, 0.1095, 0.2709, 0.1398], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22187136113643646 tensor([0.1598, 0.1725, 0.2229, 0.2230, 0.2219], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24947665631771088 tensor([0.0800, 0.1389, 0.3435, 0.1882, 0.2495], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21972614526748657 tensor([0.1500, 0.1601, 0.2410, 0.2292, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21701931953430176 tensor([0.1658, 0.1764, 0.2174, 0.2233, 0.2170], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2575337588787079 tensor([0.2978, 0.1569, 0.1295, 0.2575, 0.1583], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21097204089164734 tensor([0.1928, 0.1769, 0.1927, 0.2267, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24940049648284912 tensor([0.0950, 0.1520, 0.3100, 0.1936, 0.2494], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20522192120552063 tensor([0.1677, 0.1430, 0.2169, 0.2672, 0.2052], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2125394493341446 tensor([0.1700, 0.1737, 0.2149, 0.2289, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21159638464450836 tensor([0.1765, 0.1809, 0.2068, 0.2242, 0.2116], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27272000908851624 tensor([0.2848, 0.1508, 0.1325, 0.2727, 0.1593], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21962063014507294 tensor([0.1521, 0.1574, 0.2311, 0.2397, 0.2196], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2601271867752075 tensor([0.3928, 0.1380, 0.0871, 0.2601, 0.1219], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22005049884319305 tensor([0.1611, 0.1826, 0.2245, 0.2117, 0.2201], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24482421576976776 tensor([0.0883, 0.1581, 0.3235, 0.1853, 0.2448], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2048410028219223 tensor([0.1796, 0.1573, 0.2044, 0.2539, 0.2048], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2294042408466339 tensor([0.1220, 0.1582, 0.2750, 0.2154, 0.2294], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2637462913990021 tensor([0.4348, 0.1262, 0.0709, 0.2637, 0.1044], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2290448248386383 tensor([0.1450, 0.1718, 0.2391, 0.2150, 0.2290], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24663442373275757 tensor([0.0830, 0.1474, 0.3392, 0.1839, 0.2466], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2120087444782257 tensor([0.1624, 0.1516, 0.2174, 0.2566, 0.2120], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20923468470573425 tensor([0.1699, 0.1787, 0.2153, 0.2268, 0.2092], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2506066560745239 tensor([0.2506, 0.1454, 0.1557, 0.2757, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21050459146499634 tensor([0.1863, 0.1809, 0.1988, 0.2235, 0.2105], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.251207172870636 tensor([0.0772, 0.1416, 0.3510, 0.1790, 0.2512], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21134716272354126 tensor([0.2113, 0.1582, 0.1876, 0.2560, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21953684091567993 tensor([0.1386, 0.1586, 0.2526, 0.2307, 0.2195], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2508056163787842 tensor([0.2508, 0.1479, 0.1552, 0.2721, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18556442856788635 tensor([0.1718, 0.1856, 0.2079, 0.2121, 0.2226], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23634259402751923 tensor([0.1077, 0.1564, 0.2972, 0.2023, 0.2363], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24787500500679016 tensor([0.2479, 0.1473, 0.1534, 0.2810, 0.1705], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22299858927726746 tensor([0.1469, 0.1665, 0.2390, 0.2246, 0.2230], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2662491202354431 tensor([0.3335, 0.1485, 0.1083, 0.2662, 0.1434], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2065456062555313 tensor([0.1836, 0.1762, 0.1984, 0.2353, 0.2065], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24429869651794434 tensor([0.0872, 0.1451, 0.3282, 0.1952, 0.2443], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21398667991161346 tensor([0.2140, 0.1511, 0.1711, 0.2803, 0.1836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21597445011138916 tensor([0.1573, 0.1789, 0.2240, 0.2239, 0.2160], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27115702629089355 tensor([0.2712, 0.1449, 0.1404, 0.2818, 0.1618], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2414984107017517 tensor([0.1243, 0.1713, 0.2610, 0.2019, 0.2415], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24252112209796906 tensor([0.0987, 0.1493, 0.3123, 0.1971, 0.2425], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22132769227027893 tensor([0.1386, 0.1652, 0.2482, 0.2267, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22510287165641785 tensor([0.2251, 0.1642, 0.1741, 0.2496, 0.1870], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.278108686208725 tensor([0.2899, 0.1455, 0.1317, 0.2781, 0.1548], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22227320075035095 tensor([0.1613, 0.1690, 0.2223, 0.2237, 0.2238], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2130349725484848 tensor([0.1477, 0.1573, 0.2478, 0.2342, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.219553142786026 tensor([0.2196, 0.1496, 0.1764, 0.2680, 0.1864], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27007022500038147 tensor([0.3486, 0.1433, 0.1031, 0.2701, 0.1349], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1865350902080536 tensor([0.1933, 0.1865, 0.1890, 0.2193, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23463839292526245 tensor([0.1217, 0.1739, 0.2698, 0.2000, 0.2346], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.255359947681427 tensor([0.3445, 0.1578, 0.1074, 0.2554, 0.1348], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21790198981761932 tensor([0.1632, 0.1666, 0.2206, 0.2318, 0.2179], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2173488587141037 tensor([0.2173, 0.1598, 0.1843, 0.2504, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23815537989139557 tensor([0.1219, 0.1532, 0.2707, 0.2161, 0.2382], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24220430850982666 tensor([0.0969, 0.1503, 0.3107, 0.1998, 0.2422], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21574942767620087 tensor([0.2157, 0.1694, 0.1808, 0.2470, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21848049759864807 tensor([0.1487, 0.1679, 0.2387, 0.2263, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24800145626068115 tensor([0.3609, 0.1575, 0.1024, 0.2480, 0.1312], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24256092309951782 tensor([0.0971, 0.1568, 0.3110, 0.1926, 0.2426], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20869074761867523 tensor([0.1719, 0.1585, 0.2093, 0.2517, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23041699826717377 tensor([0.1332, 0.1687, 0.2514, 0.2163, 0.2304], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26900044083595276 tensor([0.3224, 0.1428, 0.1189, 0.2690, 0.1469], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24563521146774292 tensor([0.1037, 0.1551, 0.2961, 0.1995, 0.2456], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25129809975624084 tensor([0.0738, 0.1388, 0.3572, 0.1788, 0.2513], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23236246407032013 tensor([0.1231, 0.1625, 0.2696, 0.2124, 0.2324], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28017309308052063 tensor([0.3151, 0.1394, 0.1179, 0.2802, 0.1473], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2093595266342163 tensor([0.1773, 0.1720, 0.2096, 0.2317, 0.2094], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23917242884635925 tensor([0.1200, 0.1639, 0.2684, 0.2086, 0.2392], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22443875670433044 tensor([0.1283, 0.1552, 0.2605, 0.2244, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2678399384021759 tensor([0.3205, 0.1455, 0.1155, 0.2678, 0.1507], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21608401834964752 tensor([0.1523, 0.1653, 0.2440, 0.2223, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23946422338485718 tensor([0.0978, 0.1593, 0.3133, 0.1901, 0.2395], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21279177069664001 tensor([0.1662, 0.1504, 0.2128, 0.2571, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2773694396018982 tensor([0.3385, 0.1394, 0.1070, 0.2774, 0.1377], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2468714565038681 tensor([0.1207, 0.1704, 0.2639, 0.1982, 0.2469], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2228325754404068 tensor([0.1352, 0.1594, 0.2564, 0.2228, 0.2261], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21811509132385254 tensor([0.1525, 0.1552, 0.2311, 0.2431, 0.2181], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22611626982688904 tensor([0.1312, 0.1579, 0.2543, 0.2261, 0.2305], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2609950006008148 tensor([0.2916, 0.1572, 0.1342, 0.2610, 0.1560], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20628120005130768 tensor([0.1910, 0.1693, 0.2000, 0.2334, 0.2063], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2506170868873596 tensor([0.0803, 0.1428, 0.3476, 0.1786, 0.2506], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2236328423023224 tensor([0.1290, 0.1530, 0.2645, 0.2236, 0.2298], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22254027426242828 tensor([0.1487, 0.1781, 0.2361, 0.2146, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2640475034713745 tensor([0.2839, 0.1566, 0.1368, 0.2640, 0.1588], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21906998753547668 tensor([0.1636, 0.1752, 0.2191, 0.2195, 0.2226], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21001887321472168 tensor([0.2100, 0.1654, 0.1940, 0.2418, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24282047152519226 tensor([0.2428, 0.1528, 0.1531, 0.2786, 0.1727], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21558308601379395 tensor([0.2156, 0.1594, 0.1750, 0.2586, 0.1914], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27626466751098633 tensor([0.3631, 0.1347, 0.0969, 0.2763, 0.1290], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21550898253917694 tensor([0.1643, 0.1820, 0.2155, 0.2130, 0.2252], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2378624677658081 tensor([0.1094, 0.1565, 0.2877, 0.2085, 0.2379], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23997104167938232 tensor([0.2940, 0.1785, 0.1294, 0.2400, 0.1582], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2121923267841339 tensor([0.1778, 0.1751, 0.2088, 0.2260, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25785303115844727 tensor([0.4296, 0.1300, 0.0743, 0.2579, 0.1082], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21633833646774292 tensor([0.1724, 0.1803, 0.2079, 0.2231, 0.2163], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2060984969139099 tensor([0.1823, 0.1534, 0.2091, 0.2491, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20981170237064362 tensor([0.1810, 0.1561, 0.2098, 0.2499, 0.2032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22134701907634735 tensor([0.1391, 0.1588, 0.2546, 0.2262, 0.2213], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26413771510124207 tensor([0.2747, 0.1569, 0.1417, 0.2641, 0.1626], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18676285445690155 tensor([0.2093, 0.1868, 0.1750, 0.2261, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24349777400493622 tensor([0.0950, 0.1517, 0.3108, 0.1989, 0.2435], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21263135969638824 tensor([0.1657, 0.1679, 0.2214, 0.2324, 0.2126], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22218534350395203 tensor([0.1469, 0.1632, 0.2374, 0.2304, 0.2222], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26583003997802734 tensor([0.3067, 0.1503, 0.1241, 0.2658, 0.1530], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24036267399787903 tensor([0.1019, 0.1600, 0.2991, 0.1986, 0.2404], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.224928617477417 tensor([0.1312, 0.1499, 0.2667, 0.2249, 0.2273], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2114671766757965 tensor([0.1532, 0.1501, 0.2283, 0.2569, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20788098871707916 tensor([0.1792, 0.1745, 0.2068, 0.2316, 0.2079], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2640624940395355 tensor([0.2836, 0.1561, 0.1399, 0.2641, 0.1563], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24069306254386902 tensor([0.1274, 0.1683, 0.2593, 0.2043, 0.2407], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20869264006614685 tensor([0.1645, 0.1634, 0.2344, 0.2291, 0.2087], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20540162920951843 tensor([0.2054, 0.1558, 0.1781, 0.2658, 0.1949], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2187064290046692 tensor([0.1503, 0.1592, 0.2341, 0.2376, 0.2187], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2630937993526459 tensor([0.4051, 0.1334, 0.0818, 0.2631, 0.1166], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2060101479291916 tensor([0.2060, 0.1836, 0.1833, 0.2266, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23711630702018738 tensor([0.1308, 0.1625, 0.2574, 0.2122, 0.2371], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.212398961186409 tensor([0.1520, 0.1696, 0.2365, 0.2296, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21348455548286438 tensor([0.1584, 0.1511, 0.2316, 0.2454, 0.2135], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24106884002685547 tensor([0.3669, 0.1605, 0.1011, 0.2411, 0.1304], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23688139021396637 tensor([0.1364, 0.1678, 0.2449, 0.2141, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24202458560466766 tensor([0.0913, 0.1399, 0.3247, 0.2021, 0.2420], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20882047712802887 tensor([0.2088, 0.1415, 0.1775, 0.2824, 0.1897], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22361446917057037 tensor([0.1452, 0.1685, 0.2377, 0.2236, 0.2249], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26977047324180603 tensor([0.2902, 0.1501, 0.1340, 0.2698, 0.1560], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2053617388010025 tensor([0.1886, 0.1646, 0.1976, 0.2438, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21489503979682922 tensor([0.1773, 0.1704, 0.2149, 0.2332, 0.2041], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2204630821943283 tensor([0.1478, 0.1585, 0.2386, 0.2346, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.220269113779068 tensor([0.1463, 0.1568, 0.2410, 0.2356, 0.2203], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25125980377197266 tensor([0.3038, 0.1590, 0.1293, 0.2513, 0.1566], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21930421888828278 tensor([0.1496, 0.1700, 0.2369, 0.2193, 0.2241], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.248307466506958 tensor([0.0804, 0.1388, 0.3449, 0.1876, 0.2483], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.214987650513649 tensor([0.1597, 0.1515, 0.2317, 0.2422, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20861965417861938 tensor([0.2086, 0.1688, 0.1827, 0.2477, 0.1921], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2560432255268097 tensor([0.3623, 0.1483, 0.0995, 0.2560, 0.1338], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20878590643405914 tensor([0.2088, 0.1667, 0.1861, 0.2367, 0.2017], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21311704814434052 tensor([0.1716, 0.1611, 0.2184, 0.2358, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20787392556667328 tensor([0.2079, 0.1622, 0.1867, 0.2526, 0.1907], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20544461905956268 tensor([0.1785, 0.1639, 0.2087, 0.2434, 0.2054], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2683131992816925 tensor([0.3519, 0.1433, 0.1014, 0.2683, 0.1351], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19444972276687622 tensor([0.1691, 0.1944, 0.2092, 0.2058, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2422628551721573 tensor([0.1014, 0.1611, 0.2970, 0.1982, 0.2423], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21943019330501556 tensor([0.1464, 0.1533, 0.2343, 0.2465, 0.2194], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27349600195884705 tensor([0.3020, 0.1451, 0.1256, 0.2735, 0.1539], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24473173916339874 tensor([0.1259, 0.1815, 0.2540, 0.1939, 0.2447], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24662180244922638 tensor([0.0902, 0.1466, 0.3242, 0.1923, 0.2466], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2427520602941513 tensor([0.1016, 0.1567, 0.3066, 0.1924, 0.2428], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22115683555603027 tensor([0.1413, 0.1664, 0.2419, 0.2212, 0.2292], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2622881829738617 tensor([0.2646, 0.1591, 0.1484, 0.2623, 0.1656], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22911329567432404 tensor([0.1421, 0.1674, 0.2462, 0.2152, 0.2291], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24855099618434906 tensor([0.0847, 0.1447, 0.3337, 0.1884, 0.2486], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2192227840423584 tensor([0.1581, 0.1621, 0.2243, 0.2362, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2609458267688751 tensor([0.2685, 0.1566, 0.1477, 0.2609, 0.1662], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2368858903646469 tensor([0.1331, 0.1725, 0.2556, 0.2020, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22418084740638733 tensor([0.1460, 0.1592, 0.2463, 0.2242, 0.2244], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21168403327465057 tensor([0.1641, 0.1484, 0.2190, 0.2568, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2498159259557724 tensor([0.3551, 0.1522, 0.1054, 0.2498, 0.1375], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23308777809143066 tensor([0.1294, 0.1664, 0.2612, 0.2099, 0.2331], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23652558028697968 tensor([0.1080, 0.1466, 0.3029, 0.2060, 0.2365], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2532705068588257 tensor([0.2671, 0.1684, 0.1486, 0.2533, 0.1627], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2743307054042816 tensor([0.3513, 0.1380, 0.1016, 0.2743, 0.1348], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2180667668581009 tensor([0.2181, 0.1681, 0.1812, 0.2394, 0.1932], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21286368370056152 tensor([0.1704, 0.1644, 0.2182, 0.2341, 0.2129], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22751638293266296 tensor([0.1325, 0.1650, 0.2597, 0.2153, 0.2275], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26277703046798706 tensor([0.4010, 0.1366, 0.0828, 0.2628, 0.1168], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22532857954502106 tensor([0.1476, 0.1601, 0.2355, 0.2315, 0.2253], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22589178383350372 tensor([0.1431, 0.1602, 0.2439, 0.2269, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2045552134513855 tensor([0.1833, 0.1608, 0.2022, 0.2492, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21301323175430298 tensor([0.1608, 0.1622, 0.2255, 0.2385, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21467342972755432 tensor([0.1625, 0.1746, 0.2256, 0.2226, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22380870580673218 tensor([0.1562, 0.1722, 0.2239, 0.2239, 0.2238], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2510620653629303 tensor([0.0862, 0.1573, 0.3256, 0.1798, 0.2511], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22886046767234802 tensor([0.2289, 0.1521, 0.1661, 0.2756, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2342926561832428 tensor([0.2343, 0.1640, 0.1637, 0.2553, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26777568459510803 tensor([0.2837, 0.1496, 0.1355, 0.2678, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21489094197750092 tensor([0.1803, 0.1800, 0.2007, 0.2242, 0.2149], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.241865336894989 tensor([0.0876, 0.1478, 0.3346, 0.1881, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24382783472537994 tensor([0.2438, 0.1500, 0.1583, 0.2750, 0.1728], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22036601603031158 tensor([0.1428, 0.1700, 0.2397, 0.2204, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2710718810558319 tensor([0.3774, 0.1348, 0.0917, 0.2711, 0.1251], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21184179186820984 tensor([0.1681, 0.1820, 0.2118, 0.2166, 0.2214], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2372267097234726 tensor([0.0926, 0.1574, 0.3265, 0.1863, 0.2372], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22508198022842407 tensor([0.2251, 0.1675, 0.1731, 0.2499, 0.1844], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26073193550109863 tensor([0.3195, 0.1530, 0.1171, 0.2607, 0.1497], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21639151871204376 tensor([0.1812, 0.1797, 0.2022, 0.2205, 0.2164], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.232156440615654 tensor([0.1379, 0.1625, 0.2500, 0.2175, 0.2322], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21935464441776276 tensor([0.2194, 0.1503, 0.1736, 0.2741, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20611710846424103 tensor([0.1772, 0.1659, 0.2096, 0.2411, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26640674471855164 tensor([0.3128, 0.1512, 0.1183, 0.2664, 0.1513], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2276557981967926 tensor([0.1459, 0.1732, 0.2367, 0.2165, 0.2277], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2347135990858078 tensor([0.1171, 0.1526, 0.2875, 0.2081, 0.2347], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23128923773765564 tensor([0.1259, 0.1640, 0.2623, 0.2166, 0.2313], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24811097979545593 tensor([0.2481, 0.1607, 0.1632, 0.2490, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18564537167549133 tensor([0.1724, 0.1856, 0.2048, 0.2119, 0.2252], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24295824766159058 tensor([0.1064, 0.1470, 0.2997, 0.2040, 0.2430], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.226268470287323 tensor([0.1344, 0.1592, 0.2531, 0.2263, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26747241616249084 tensor([0.3339, 0.1446, 0.1118, 0.2675, 0.1422], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21677915751934052 tensor([0.1762, 0.1735, 0.2120, 0.2216, 0.2168], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22989921271800995 tensor([0.1199, 0.1613, 0.2835, 0.2054, 0.2299], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21816308796405792 tensor([0.1541, 0.1531, 0.2279, 0.2467, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20273569226264954 tensor([0.2027, 0.1544, 0.1893, 0.2589, 0.1947], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26558053493499756 tensor([0.3858, 0.1352, 0.0898, 0.2656, 0.1236], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21481242775917053 tensor([0.1721, 0.1719, 0.2096, 0.2317, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2190784066915512 tensor([0.1445, 0.1511, 0.2464, 0.2390, 0.2191], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2096114605665207 tensor([0.1617, 0.1445, 0.2280, 0.2562, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21889527142047882 tensor([0.1464, 0.1686, 0.2463, 0.2198, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25722813606262207 tensor([0.2785, 0.1530, 0.1448, 0.2572, 0.1664], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22285804152488708 tensor([0.1460, 0.1706, 0.2347, 0.2229, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22181551158428192 tensor([0.1109, 0.1426, 0.2911, 0.2218, 0.2336], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2112138718366623 tensor([0.1700, 0.1681, 0.2147, 0.2360, 0.2112], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21100373566150665 tensor([0.1674, 0.1607, 0.2243, 0.2367, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2428869754076004 tensor([0.4269, 0.1408, 0.0796, 0.2429, 0.1097], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22593313455581665 tensor([0.2259, 0.1683, 0.1656, 0.2517, 0.1885], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22307589650154114 tensor([0.1384, 0.1670, 0.2557, 0.2158, 0.2231], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23257067799568176 tensor([0.2326, 0.1589, 0.1668, 0.2640, 0.1777], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26783642172813416 tensor([0.2691, 0.1597, 0.1418, 0.2678, 0.1616], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20583397150039673 tensor([0.2058, 0.1568, 0.1839, 0.2587, 0.1948], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.244988352060318 tensor([0.1009, 0.1612, 0.3023, 0.1906, 0.2450], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2196608930826187 tensor([0.1383, 0.1594, 0.2515, 0.2312, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21246084570884705 tensor([0.1598, 0.1720, 0.2285, 0.2272, 0.2125], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 1], [2, 1, 4, 0], [2, 0, 4, 1], [0, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 1, 3], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [2, 4, 1], [2, 0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 1], [0, 3, 1], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 2, 1], [1, 0], [0, 1, 4], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 1], [2, 1, 4], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 1, 4], [0, 1], [2, 4, 1], [2, 0, 1], [0, 4, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1], [2, 0, 1], [0, 1], [0, 1], [0, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 3, 1], [0, 1, 3], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [0, 1], [2, 1, 4], [2, 4, 1], [0, 2, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1, 4], [0, 1], [0, 1]]\n",
      "[[0, 3], [3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [2, 3, 4], [3], [3], [3, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [2, 3, 4], [2, 3, 4], [0, 3], [3], [0, 3], [4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [0, 3], [3, 4], [0, 3], [3, 4], [2, 3, 4], [2, 3], [2, 3, 4], [0, 3], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [3, 4], [0, 3], [2, 4], [2, 3, 4], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 3], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [3, 4], [0, 3], [4], [2, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [2, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [0, 3], [2, 3, 4], [3], [2, 4], [0, 3], [2, 3, 4], [2, 3, 4], [3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 4], [0, 3], [0, 3], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [3, 4], [2, 4], [0, 3], [3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [4], [2, 4], [3], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [3], [2, 3, 4], [2, 3, 4]]\n",
      "NL_pred of 5th iteration [[0, 3, 2, 1], [2, 0, 3, 1], [2, 4, 1], [0, 3, 2, 1], [0, 2, 3, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.2548056125640869  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.2547767639160156  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.2547219514846802  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.2546438932418823  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.25454514026641845  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.2544278621673584  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.25429420471191405  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.2541459560394287  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.2539848804473877  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.2538125038146973  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.253630256652832  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.25343937873840333  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.25324091911315916  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.2530360221862793  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.2528254747390747  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.2526101589202881  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.25239081382751466  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.2521681785583496  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.251942777633667  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.25171504020690916  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 1], [2, 1, 4, 0], [2, 0, 4, 1], [0, 2, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 1, 3], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [2, 4, 1], [2, 0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 1], [0, 3, 1], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 2, 1], [1, 0], [0, 1, 4], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 1], [2, 1, 4], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 1, 4], [0, 1], [2, 4, 1], [2, 0, 1], [0, 4, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1, 3], [0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 1], [0, 1], [2, 0, 4, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 1], [0, 1], [2, 0, 1], [0, 1], [0, 1], [0, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 0, 1], [2, 4, 1], [2, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 2, 3, 1], [0, 1, 3], [1, 0, 2, 4], [0, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [0, 1], [2, 1, 4], [2, 4, 1], [0, 2, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [0, 1], [0, 1], [0, 1], [0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1, 4], [0, 1], [0, 1]]\n",
      "POSITION :  [[0, 3], [3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [2, 3, 4], [3], [3], [3, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [2, 3, 4], [2, 3, 4], [0, 3], [3], [0, 3], [4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [0, 3], [3, 4], [0, 3], [3, 4], [2, 3, 4], [2, 3], [2, 3, 4], [0, 3], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [3, 4], [0, 3], [2, 4], [2, 3, 4], [0, 3], [2, 3, 4], [0, 3], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [3, 4], [2, 3], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [0, 3], [2, 3, 4], [0, 3], [3, 4], [0, 3], [4], [2, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [2, 4], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 3, 4], [0, 3], [2, 4], [2, 3, 4], [2, 3, 4], [3], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [0, 3], [2, 3, 4], [3], [2, 4], [0, 3], [2, 3, 4], [2, 3, 4], [3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 4], [0, 3], [0, 3], [0, 3], [3, 4], [2, 4], [0, 3], [2, 3, 4], [0, 3], [2, 4], [2, 4], [3], [0, 3], [0, 3], [3, 4], [2, 4], [0, 3], [3, 4], [0, 3], [2, 4], [2, 4], [3], [2, 4], [0, 3], [4], [2, 4], [3], [2, 3, 4], [0, 3], [3, 4], [2, 4], [2, 3, 4], [0, 3], [0, 3], [3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [3], [2, 3, 4], [2, 3, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.728\n",
      "tensor([3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 80.95238095238095\n",
      "Epoch: 1  Loss: 80.95238095238095\n",
      "Epoch: 2  Loss: 80.95238095238095\n",
      "Epoch: 3  Loss: 80.95238095238095\n",
      "Epoch: 4  Loss: 80.95238095238095\n",
      "Epoch: 5  Loss: 80.95238095238095\n",
      "Epoch: 6  Loss: 80.95238095238095\n",
      "Epoch: 7  Loss: 80.95238095238095\n",
      "Epoch: 8  Loss: 80.95238095238095\n",
      "Epoch: 9  Loss: 80.95238095238095\n",
      "Start of testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 4/15 [02:53<07:25, 40.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing on Query Set:  20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 2.336250812411308  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 1.8382289361953736  Accuracy on Support set:12.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6679650020599366  Accuracy on Support set:16.0\n",
      "Train_Epoch: 3  Train_Loss: 1.5784055662155152  Accuracy on Support set:16.0\n",
      "Train_Epoch: 4  Train_Loss: 1.5240854740142822  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 5  Train_Loss: 1.4864593839645386  Accuracy on Support set:36.0\n",
      "Train_Epoch: 6  Train_Loss: 1.4564930820465087  Accuracy on Support set:40.0\n",
      "Train_Epoch: 7  Train_Loss: 1.4307232427597045  Accuracy on Support set:44.0\n",
      "Train_Epoch: 8  Train_Loss: 1.4071772193908691  Accuracy on Support set:48.0\n",
      "Train_Epoch: 9  Train_Loss: 1.3849214005470276  Accuracy on Support set:52.0\n",
      "Train_Epoch: 10  Train_Loss: 1.3633856081962585  Accuracy on Support set:76.0\n",
      "Train_Epoch: 11  Train_Loss: 1.3422918319702148  Accuracy on Support set:76.0\n",
      "Train_Epoch: 12  Train_Loss: 1.3212478041648865  Accuracy on Support set:76.0\n",
      "Train_Epoch: 13  Train_Loss: 1.300317006111145  Accuracy on Support set:76.0\n",
      "Train_Epoch: 14  Train_Loss: 1.2793978691101073  Accuracy on Support set:76.0\n",
      "Train_Epoch: 15  Train_Loss: 1.2584550070762635  Accuracy on Support set:76.0\n",
      "Train_Epoch: 16  Train_Loss: 1.2374888777732849  Accuracy on Support set:80.0\n",
      "Train_Epoch: 17  Train_Loss: 1.2165810441970826  Accuracy on Support set:76.0\n",
      "Train_Epoch: 18  Train_Loss: 1.1955960917472839  Accuracy on Support set:76.0\n",
      "Train_Epoch: 19  Train_Loss: 1.1747082901000976  Accuracy on Support set:76.0\n",
      "Train_Epoch: 20  Train_Loss: 1.1538124585151672  Accuracy on Support set:80.0\n",
      "Train_Epoch: 21  Train_Loss: 1.132965624332428  Accuracy on Support set:80.0\n",
      "Train_Epoch: 22  Train_Loss: 1.1122889614105225  Accuracy on Support set:80.0\n",
      "Train_Epoch: 23  Train_Loss: 1.0916953849792481  Accuracy on Support set:80.0\n",
      "Train_Epoch: 24  Train_Loss: 1.0713173389434814  Accuracy on Support set:80.0\n",
      "Train_Epoch: 25  Train_Loss: 1.0509180903434754  Accuracy on Support set:80.0\n",
      "Train_Epoch: 26  Train_Loss: 1.0306345188617707  Accuracy on Support set:80.0\n",
      "Train_Epoch: 27  Train_Loss: 1.0104013359546662  Accuracy on Support set:80.0\n",
      "Train_Epoch: 28  Train_Loss: 0.9902271747589111  Accuracy on Support set:80.0\n",
      "Train_Epoch: 29  Train_Loss: 0.9702244973182679  Accuracy on Support set:84.0\n",
      "Train_Epoch: 30  Train_Loss: 0.9502282083034516  Accuracy on Support set:84.0\n",
      "Train_Epoch: 31  Train_Loss: 0.9302668583393097  Accuracy on Support set:84.0\n",
      "Train_Epoch: 32  Train_Loss: 0.9103534483909607  Accuracy on Support set:84.0\n",
      "Train_Epoch: 33  Train_Loss: 0.8903705298900604  Accuracy on Support set:92.0\n",
      "Train_Epoch: 34  Train_Loss: 0.8703710627555847  Accuracy on Support set:96.0\n",
      "Train_Epoch: 35  Train_Loss: 0.8502780246734619  Accuracy on Support set:96.0\n",
      "Train_Epoch: 36  Train_Loss: 0.8300850653648376  Accuracy on Support set:96.0\n",
      "Train_Epoch: 37  Train_Loss: 0.8098542809486389  Accuracy on Support set:96.0\n",
      "Train_Epoch: 38  Train_Loss: 0.7894314455986023  Accuracy on Support set:96.0\n",
      "Train_Epoch: 39  Train_Loss: 0.7688605189323425  Accuracy on Support set:96.0\n",
      "Train_Epoch: 40  Train_Loss: 0.7483097445964814  Accuracy on Support set:96.0\n",
      "Train_Epoch: 41  Train_Loss: 0.7275759935379028  Accuracy on Support set:96.0\n",
      "Train_Epoch: 42  Train_Loss: 0.7067676365375519  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.6859549582004547  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.6651036548614502  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.6442173039913177  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.6232628953456879  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.6024583280086517  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.5816956716775894  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.5610879755020142  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  66.66666666666666\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.032341428101062775 tensor([0.2138, 0.2963, 0.0323, 0.2831, 0.1745], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0039013985078781843 tensor([0.4136, 0.4191, 0.0039, 0.1213, 0.0421], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012873740866780281 tensor([0.0129, 0.0664, 0.4243, 0.1637, 0.3327], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012122478801757097 tensor([0.5789, 0.2468, 0.0012, 0.1517, 0.0213], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01242381427437067 tensor([0.0124, 0.0592, 0.3159, 0.1400, 0.4724], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011851123534142971 tensor([0.3361, 0.2767, 0.0119, 0.2745, 0.1009], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009367850609123707 tensor([0.3144, 0.4527, 0.0094, 0.1499, 0.0736], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01974973827600479 tensor([0.0197, 0.0986, 0.3208, 0.1641, 0.3967], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01902637630701065 tensor([0.2369, 0.1729, 0.0190, 0.4708, 0.1004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012892298400402069 tensor([0.0129, 0.0616, 0.3406, 0.1429, 0.4420], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002322498243302107 tensor([0.5024, 0.3281, 0.0023, 0.1360, 0.0312], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0033340679947286844 tensor([0.4188, 0.4189, 0.0033, 0.1199, 0.0390], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01747165247797966 tensor([0.0175, 0.0983, 0.3539, 0.1507, 0.3797], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04571458324790001 tensor([0.0457, 0.0931, 0.1707, 0.3744, 0.3161], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017281001433730125 tensor([0.0173, 0.0845, 0.2768, 0.1421, 0.4793], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0027535008266568184 tensor([0.5031, 0.2675, 0.0028, 0.1912, 0.0355], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007990271784365177 tensor([0.3219, 0.4636, 0.0080, 0.1383, 0.0683], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012040910078212619 tensor([0.0012, 0.0115, 0.6788, 0.0529, 0.2556], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01798873394727707 tensor([0.2408, 0.1716, 0.0180, 0.4579, 0.1118], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04641932621598244 tensor([0.0464, 0.1230, 0.1736, 0.2777, 0.3792], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08470627665519714 tensor([0.0847, 0.2095, 0.1148, 0.2664, 0.3246], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04296128824353218 tensor([0.1693, 0.2504, 0.0430, 0.3505, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08565817773342133 tensor([0.0857, 0.2344, 0.1197, 0.2544, 0.3059], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006137709133327007 tensor([0.3749, 0.2020, 0.0061, 0.3619, 0.0551], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.058324556797742844 tensor([0.1404, 0.2415, 0.0583, 0.3185, 0.2413], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03797682002186775 tensor([0.2066, 0.3387, 0.0380, 0.2578, 0.1589], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018986349925398827 tensor([0.2177, 0.5070, 0.0190, 0.1365, 0.1198], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015417871065437794 tensor([0.0154, 0.0585, 0.3456, 0.2166, 0.3639], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007149853743612766 tensor([0.3538, 0.1786, 0.0071, 0.3989, 0.0616], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018574172630906105 tensor([0.2779, 0.3407, 0.0186, 0.2476, 0.1152], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08017287403345108 tensor([0.1201, 0.2965, 0.0802, 0.2302, 0.2730], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01283405814319849 tensor([0.3103, 0.3345, 0.0128, 0.2478, 0.0945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.056674838066101074 tensor([0.0567, 0.1716, 0.1775, 0.2557, 0.3386], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06835101544857025 tensor([0.1161, 0.1509, 0.0684, 0.4497, 0.2149], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09009705483913422 tensor([0.0901, 0.1761, 0.0946, 0.3221, 0.3172], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08911831676959991 tensor([0.1093, 0.2210, 0.0891, 0.3125, 0.2681], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.029110737144947052 tensor([0.2217, 0.3793, 0.0291, 0.2280, 0.1419], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02425198070704937 tensor([0.0243, 0.1086, 0.2679, 0.1852, 0.4140], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015021185390651226 tensor([0.0150, 0.0480, 0.3434, 0.2702, 0.3234], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019453832879662514 tensor([0.0195, 0.0802, 0.2717, 0.1799, 0.4487], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016847318038344383 tensor([0.2867, 0.3382, 0.0168, 0.2429, 0.1153], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013954534195363522 tensor([0.2844, 0.4590, 0.0140, 0.1495, 0.0931], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0062619224190711975 tensor([0.0063, 0.0382, 0.4943, 0.1127, 0.3486], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04775358736515045 tensor([0.1442, 0.1493, 0.0478, 0.4785, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04681431129574776 tensor([0.0468, 0.1107, 0.1552, 0.2926, 0.3947], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02025621570646763 tensor([0.2641, 0.2962, 0.0203, 0.2798, 0.1397], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00486215204000473 tensor([0.3777, 0.4357, 0.0049, 0.1287, 0.0530], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004780888091772795 tensor([0.0048, 0.0324, 0.5511, 0.1039, 0.3078], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0656210333108902 tensor([0.1116, 0.1387, 0.0656, 0.4642, 0.2199], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09829428046941757 tensor([0.1013, 0.2291, 0.0983, 0.3051, 0.2662], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08290693908929825 tensor([0.1214, 0.2179, 0.0829, 0.3198, 0.2580], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013637073338031769 tensor([0.2817, 0.4582, 0.0136, 0.1556, 0.0908], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00442690821364522 tensor([0.0044, 0.0284, 0.5569, 0.1006, 0.3097], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07881581038236618 tensor([0.0788, 0.1183, 0.0986, 0.4491, 0.2552], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09572535753250122 tensor([0.0957, 0.2265, 0.0966, 0.2811, 0.3001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07374528050422668 tensor([0.1239, 0.2499, 0.0737, 0.3115, 0.2410], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07656744122505188 tensor([0.0766, 0.2487, 0.1268, 0.2187, 0.3293], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04278402775526047 tensor([0.0428, 0.1224, 0.2058, 0.3001, 0.3289], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07660035043954849 tensor([0.1198, 0.2603, 0.0766, 0.2879, 0.2554], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0435895174741745 tensor([0.0436, 0.1440, 0.1687, 0.2080, 0.4357], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06453216075897217 tensor([0.0645, 0.1923, 0.1589, 0.2678, 0.3164], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0149303637444973 tensor([0.2585, 0.4665, 0.0149, 0.1603, 0.0998], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0038499098736792803 tensor([0.0038, 0.0303, 0.5582, 0.0896, 0.3181], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020506687462329865 tensor([0.0205, 0.0907, 0.3340, 0.1923, 0.3625], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01641201227903366 tensor([0.2945, 0.3260, 0.0164, 0.2526, 0.1105], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01863371953368187 tensor([0.2595, 0.2185, 0.0186, 0.3934, 0.1100], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0200201328843832 tensor([0.2655, 0.3584, 0.0200, 0.2300, 0.1260], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.029100803658366203 tensor([0.0291, 0.0929, 0.2565, 0.2630, 0.3585], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027201766148209572 tensor([0.2050, 0.1872, 0.0272, 0.4542, 0.1264], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05573742091655731 tensor([0.1631, 0.3218, 0.0557, 0.2757, 0.1837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004226245451718569 tensor([0.4513, 0.2940, 0.0042, 0.2049, 0.0456], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010568732395768166 tensor([0.5504, 0.3332, 0.0011, 0.0971, 0.0183], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006536583416163921 tensor([0.0065, 0.0497, 0.4902, 0.0898, 0.3638], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03206246718764305 tensor([0.1828, 0.1813, 0.0321, 0.4360, 0.1679], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009590486995875835 tensor([0.3504, 0.3310, 0.0096, 0.2287, 0.0803], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06587648391723633 tensor([0.1460, 0.2448, 0.0659, 0.3130, 0.2304], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0067134746350348 tensor([0.3086, 0.5201, 0.0067, 0.1038, 0.0607], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018076780252158642 tensor([0.0018, 0.0139, 0.6345, 0.0692, 0.2806], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.058878958225250244 tensor([0.0589, 0.0867, 0.1165, 0.4749, 0.2629], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012821938842535019 tensor([0.0128, 0.0728, 0.3427, 0.1192, 0.4525], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006753602996468544 tensor([0.3947, 0.3425, 0.0068, 0.1911, 0.0649], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08951293677091599 tensor([0.1081, 0.2854, 0.0895, 0.2603, 0.2567], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005568682681769133 tensor([0.0056, 0.0442, 0.5118, 0.0887, 0.3498], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04176093265414238 tensor([0.1608, 0.1838, 0.0418, 0.4402, 0.1733], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0780469998717308 tensor([0.1190, 0.2280, 0.0780, 0.3081, 0.2668], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09175392985343933 tensor([0.0984, 0.2343, 0.0918, 0.2498, 0.3257], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004415048286318779 tensor([0.4172, 0.3737, 0.0044, 0.1590, 0.0457], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.049940530210733414 tensor([0.0499, 0.1230, 0.1731, 0.3243, 0.3297], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.040500421077013016 tensor([0.1435, 0.1336, 0.0405, 0.5233, 0.1591], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012596056796610355 tensor([0.0126, 0.0696, 0.3447, 0.1386, 0.4346], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01926756463944912 tensor([0.2477, 0.4268, 0.0193, 0.1821, 0.1241], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0442642904818058 tensor([0.1703, 0.3499, 0.0443, 0.2295, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002962779253721237 tensor([0.0030, 0.0300, 0.6233, 0.0622, 0.2816], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.053783245384693146 tensor([0.0538, 0.1182, 0.1638, 0.3451, 0.3192], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027432404458522797 tensor([0.0274, 0.1047, 0.2495, 0.1909, 0.4274], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07116079330444336 tensor([0.1291, 0.2628, 0.0712, 0.2867, 0.2503], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03488611429929733 tensor([0.1883, 0.3996, 0.0349, 0.2021, 0.1751], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016867337981238961 tensor([0.0017, 0.0169, 0.6770, 0.0517, 0.2527], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0644172802567482 tensor([0.0972, 0.0963, 0.0644, 0.5515, 0.1905], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01624409668147564 tensor([0.3001, 0.2822, 0.0162, 0.2946, 0.1069], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07550755143165588 tensor([0.1146, 0.2084, 0.0755, 0.3467, 0.2548], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01421995647251606 tensor([0.2438, 0.5085, 0.0142, 0.1313, 0.1022], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001134861377067864 tensor([0.0011, 0.0122, 0.6922, 0.0431, 0.2513], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07170736789703369 tensor([0.1116, 0.1531, 0.0717, 0.4357, 0.2279], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07915227860212326 tensor([0.1166, 0.2069, 0.0792, 0.3367, 0.2605], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030072584748268127 tensor([0.2225, 0.3213, 0.0301, 0.2567, 0.1694], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.033656045794487 tensor([0.1878, 0.4162, 0.0337, 0.1909, 0.1714], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011571795679628849 tensor([0.0116, 0.0681, 0.3959, 0.1279, 0.3965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026323145255446434 tensor([0.2185, 0.2159, 0.0263, 0.4044, 0.1349], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08346543461084366 tensor([0.1142, 0.3041, 0.0835, 0.2187, 0.2795], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08154274523258209 tensor([0.0815, 0.2164, 0.1195, 0.2696, 0.3129], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010797150433063507 tensor([0.2903, 0.4592, 0.0108, 0.1524, 0.0873], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017058277502655983 tensor([0.0171, 0.0754, 0.3496, 0.1866, 0.3713], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021615859121084213 tensor([0.2085, 0.1369, 0.0216, 0.5204, 0.1126], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06784363836050034 tensor([0.1244, 0.2316, 0.0678, 0.3124, 0.2638], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.029716523364186287 tensor([0.2330, 0.2811, 0.0297, 0.3122, 0.1441], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026800179854035378 tensor([0.2353, 0.3804, 0.0268, 0.2142, 0.1433], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.023989761248230934 tensor([0.0240, 0.0773, 0.2668, 0.2479, 0.3840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03250138461589813 tensor([0.1829, 0.1470, 0.0325, 0.4970, 0.1406], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010816629976034164 tensor([0.0108, 0.0516, 0.3642, 0.1470, 0.4264], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024596255272626877 tensor([0.2457, 0.2759, 0.0246, 0.3022, 0.1517], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013774486258625984 tensor([0.2796, 0.4167, 0.0138, 0.1902, 0.0998], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013583115302026272 tensor([0.0136, 0.0863, 0.4020, 0.1219, 0.3762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03798450529575348 tensor([0.1542, 0.1395, 0.0380, 0.5093, 0.1590], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0708254873752594 tensor([0.1240, 0.2184, 0.0708, 0.3107, 0.2761], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07242052257061005 tensor([0.1359, 0.3119, 0.0724, 0.2297, 0.2500], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008930313400924206 tensor([0.2968, 0.4758, 0.0089, 0.1363, 0.0822], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009343471378087997 tensor([0.0093, 0.0553, 0.4438, 0.1401, 0.3515], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07891087234020233 tensor([0.1217, 0.2247, 0.0789, 0.3303, 0.2444], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08890171349048615 tensor([0.0889, 0.2148, 0.1028, 0.2957, 0.2977], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008915659971535206 tensor([0.3449, 0.3612, 0.0089, 0.2032, 0.0818], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01227753609418869 tensor([0.2969, 0.4370, 0.0123, 0.1656, 0.0882], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015222994843497872 tensor([0.0015, 0.0130, 0.6834, 0.0638, 0.2383], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011725866235792637 tensor([0.2898, 0.1736, 0.0117, 0.4417, 0.0831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0055200825445353985 tensor([0.0055, 0.0323, 0.4439, 0.1054, 0.4128], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05110627040266991 tensor([0.1674, 0.2400, 0.0511, 0.3331, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03159630298614502 tensor([0.1979, 0.3942, 0.0316, 0.2047, 0.1716], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002348497277125716 tensor([0.0023, 0.0199, 0.5968, 0.0642, 0.3168], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04354766756296158 tensor([0.1498, 0.1471, 0.0435, 0.4891, 0.1704], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06862187385559082 tensor([0.1218, 0.1864, 0.0686, 0.3701, 0.2532], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009685131721198559 tensor([0.3622, 0.2662, 0.0097, 0.2902, 0.0717], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03290949761867523 tensor([0.1876, 0.4262, 0.0329, 0.2004, 0.1529], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003944381605833769 tensor([0.0039, 0.0258, 0.5208, 0.0937, 0.3558], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07663316279649734 tensor([0.1060, 0.1415, 0.0766, 0.4544, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09574529528617859 tensor([0.0957, 0.2143, 0.0959, 0.2776, 0.3164], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.054410237818956375 tensor([0.1544, 0.2640, 0.0544, 0.3111, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07166699320077896 tensor([0.1341, 0.2458, 0.0717, 0.3157, 0.2327], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02520274557173252 tensor([0.0252, 0.1134, 0.3116, 0.1898, 0.3600], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07460225373506546 tensor([0.0746, 0.0957, 0.0892, 0.4939, 0.2466], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026815194636583328 tensor([0.0268, 0.0925, 0.2580, 0.2422, 0.3805], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014177079312503338 tensor([0.3127, 0.2792, 0.0142, 0.3004, 0.0935], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0036393250338733196 tensor([0.4253, 0.3859, 0.0036, 0.1417, 0.0435], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02790716104209423 tensor([0.0279, 0.0954, 0.2596, 0.2551, 0.3620], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.059649888426065445 tensor([0.1357, 0.1981, 0.0596, 0.3880, 0.2185], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021638361737132072 tensor([0.0216, 0.0788, 0.2618, 0.2142, 0.4236], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006603281944990158 tensor([0.3989, 0.3105, 0.0066, 0.2226, 0.0614], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.035546377301216125 tensor([0.0355, 0.1859, 0.2149, 0.1586, 0.4050], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005515147000551224 tensor([0.0055, 0.0372, 0.4953, 0.1033, 0.3587], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021156473085284233 tensor([0.2162, 0.1515, 0.0212, 0.4884, 0.1227], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06653090566396713 tensor([0.1287, 0.2645, 0.0665, 0.2587, 0.2817], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018257388845086098 tensor([0.2802, 0.3134, 0.0183, 0.2693, 0.1189], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027327146381139755 tensor([0.2151, 0.4259, 0.0273, 0.1931, 0.1386], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005273660644888878 tensor([0.0053, 0.0364, 0.5469, 0.1022, 0.3092], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03936973959207535 tensor([0.1759, 0.2181, 0.0394, 0.3839, 0.1828], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.038482196629047394 tensor([0.0385, 0.1226, 0.1941, 0.2209, 0.4240], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002472294494509697 tensor([0.5044, 0.3048, 0.0025, 0.1553, 0.0330], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0029737777076661587 tensor([0.4135, 0.4499, 0.0030, 0.0938, 0.0398], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019163556396961212 tensor([0.0192, 0.0636, 0.3064, 0.2594, 0.3515], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03054061345756054 tensor([0.2273, 0.2869, 0.0305, 0.3102, 0.1451], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06076366454362869 tensor([0.0608, 0.1364, 0.1324, 0.3027, 0.3677], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008749748580157757 tensor([0.3711, 0.2774, 0.0087, 0.2677, 0.0750], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004416690673679113 tensor([0.3716, 0.4599, 0.0044, 0.1159, 0.0482], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07223547250032425 tensor([0.0722, 0.1707, 0.1544, 0.3293, 0.2733], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03310762718319893 tensor([0.1902, 0.1762, 0.0331, 0.4529, 0.1476], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018035465851426125 tensor([0.0180, 0.0836, 0.2762, 0.1522, 0.4700], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03685571998357773 tensor([0.2028, 0.3129, 0.0369, 0.2803, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02862435206770897 tensor([0.2340, 0.2983, 0.0286, 0.2952, 0.1439], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010975467739626765 tensor([0.0011, 0.0116, 0.7059, 0.0452, 0.2362], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08314117044210434 tensor([0.1169, 0.1947, 0.0831, 0.3850, 0.2203], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00953187607228756 tensor([0.0095, 0.0611, 0.3903, 0.1107, 0.4283], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026613283902406693 tensor([0.2326, 0.2484, 0.0266, 0.3349, 0.1575], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01849752478301525 tensor([0.2151, 0.5305, 0.0185, 0.1236, 0.1123], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03577246516942978 tensor([0.0358, 0.1228, 0.2512, 0.2637, 0.3265], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03540809452533722 tensor([0.2011, 0.2623, 0.0354, 0.3354, 0.1659], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05804138258099556 tensor([0.1475, 0.2163, 0.0580, 0.3336, 0.2446], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0687018483877182 tensor([0.1340, 0.3391, 0.0687, 0.2343, 0.2240], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009112227708101273 tensor([0.3049, 0.4633, 0.0091, 0.1450, 0.0776], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0035302219912409782 tensor([0.0035, 0.0275, 0.5903, 0.0787, 0.3000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.028651073575019836 tensor([0.1988, 0.1811, 0.0287, 0.4574, 0.1341], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08868156373500824 tensor([0.1057, 0.2765, 0.0887, 0.2328, 0.2964], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08587248623371124 tensor([0.1093, 0.2636, 0.0859, 0.2527, 0.2885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.043192144483327866 tensor([0.1821, 0.3815, 0.0432, 0.2293, 0.1639], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.025680819526314735 tensor([0.0257, 0.0806, 0.2700, 0.2735, 0.3502], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0390184260904789 tensor([0.1672, 0.1723, 0.0390, 0.4442, 0.1773], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08678197860717773 tensor([0.1136, 0.2459, 0.0868, 0.3010, 0.2526], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007877720519900322 tensor([0.3598, 0.3667, 0.0079, 0.1964, 0.0691], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04343874379992485 tensor([0.1809, 0.3194, 0.0434, 0.2761, 0.1802], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012838277965784073 tensor([0.0128, 0.0552, 0.3510, 0.1938, 0.3871], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02561045065522194 tensor([0.2169, 0.2028, 0.0256, 0.4234, 0.1313], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010851631872355938 tensor([0.0109, 0.0531, 0.3316, 0.1406, 0.4638], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03807728737592697 tensor([0.1998, 0.3454, 0.0381, 0.2368, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0067810192704200745 tensor([0.3151, 0.4969, 0.0068, 0.1159, 0.0653], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03389592841267586 tensor([0.0339, 0.1069, 0.2267, 0.2731, 0.3594], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01245157141238451 tensor([0.0125, 0.0699, 0.3533, 0.1368, 0.4276], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07410584390163422 tensor([0.1151, 0.2243, 0.0741, 0.2969, 0.2895], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015795588493347168 tensor([0.3010, 0.3280, 0.0158, 0.2501, 0.1051], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010774694383144379 tensor([0.3207, 0.3520, 0.0108, 0.2272, 0.0894], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021160198375582695 tensor([0.0212, 0.1196, 0.3636, 0.1509, 0.3447], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05743253231048584 tensor([0.1449, 0.1754, 0.0574, 0.4393, 0.1830], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.036705173552036285 tensor([0.0367, 0.1428, 0.1853, 0.1749, 0.4603], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006564991548657417 tensor([0.3876, 0.3121, 0.0066, 0.2383, 0.0554], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019673111382871866 tensor([0.4482, 0.4326, 0.0020, 0.0897, 0.0276], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030872561037540436 tensor([0.0309, 0.1054, 0.2421, 0.2338, 0.3879], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014516335912048817 tensor([0.2445, 0.1372, 0.0145, 0.5140, 0.0898], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.047173891216516495 tensor([0.1753, 0.2433, 0.0472, 0.3381, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09260033816099167 tensor([0.0926, 0.1960, 0.1059, 0.3499, 0.2556], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07117272168397903 tensor([0.1227, 0.3554, 0.0712, 0.2014, 0.2493], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002259038155898452 tensor([0.0023, 0.0227, 0.6109, 0.0567, 0.3076], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007762867491692305 tensor([0.3322, 0.1610, 0.0078, 0.4316, 0.0674], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03750528395175934 tensor([0.0375, 0.1293, 0.2135, 0.2321, 0.3875], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.052794184535741806 tensor([0.1532, 0.2581, 0.0528, 0.3042, 0.2316], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007512648589909077 tensor([0.3654, 0.3782, 0.0075, 0.1851, 0.0637], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016690794378519058 tensor([0.0167, 0.0710, 0.3528, 0.1823, 0.3772], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03601127117872238 tensor([0.1790, 0.1906, 0.0360, 0.4318, 0.1625], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03337440639734268 tensor([0.2078, 0.2376, 0.0334, 0.3518, 0.1694], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.057357098907232285 tensor([0.1602, 0.2539, 0.0574, 0.3320, 0.1965], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07323633879423141 tensor([0.1366, 0.2674, 0.0732, 0.3145, 0.2083], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00797850452363491 tensor([0.0080, 0.0497, 0.4614, 0.1119, 0.3690], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07804732024669647 tensor([0.1084, 0.1621, 0.0780, 0.4099, 0.2415], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05271932855248451 tensor([0.1672, 0.2475, 0.0527, 0.3178, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0018712085438892245 tensor([0.5007, 0.3541, 0.0019, 0.1147, 0.0286], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.020886246114969254 tensor([0.2587, 0.4377, 0.0209, 0.1635, 0.1192], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027643371373414993 tensor([0.0276, 0.1050, 0.3040, 0.2137, 0.3497], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017640752717852592 tensor([0.0176, 0.0587, 0.3065, 0.2486, 0.3686], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09099865704774857 tensor([0.0995, 0.2462, 0.0910, 0.2545, 0.3089], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010785345919430256 tensor([0.3405, 0.3402, 0.0108, 0.2236, 0.0850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010030917823314667 tensor([0.3287, 0.3842, 0.0100, 0.1960, 0.0810], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011636478826403618 tensor([0.0116, 0.0535, 0.3831, 0.1718, 0.3799], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03578609228134155 tensor([0.0358, 0.1150, 0.2435, 0.2489, 0.3568], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03661399334669113 tensor([0.0366, 0.1169, 0.1931, 0.2151, 0.4384], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05116809159517288 tensor([0.1699, 0.2655, 0.0512, 0.3099, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007819087244570255 tensor([0.3583, 0.3749, 0.0078, 0.1856, 0.0733], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008735006675124168 tensor([0.0087, 0.0436, 0.4511, 0.1600, 0.3365], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03684965521097183 tensor([0.0368, 0.1259, 0.2122, 0.2505, 0.3744], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07082965224981308 tensor([0.1125, 0.2021, 0.0708, 0.3370, 0.2776], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03831646591424942 tensor([0.2039, 0.3471, 0.0383, 0.2430, 0.1676], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04514759033918381 tensor([0.1754, 0.3113, 0.0451, 0.2859, 0.1823], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015621831407770514 tensor([0.0016, 0.0140, 0.6370, 0.0555, 0.2920], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009859709069132805 tensor([0.3302, 0.2062, 0.0099, 0.3848, 0.0690], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011658531613647938 tensor([0.0117, 0.0527, 0.3677, 0.1671, 0.4008], grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [0], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [0], [2], [0], [0], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [0], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [0], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [0], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0]]\n",
      "[[0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [0], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [0], [2], [0], [0], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [0], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [0], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [0], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004451061248779297  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0044509778022766115  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004450822353363037  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004450600624084472  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004450322151184082  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004449995040893555  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004449625492095947  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004449220180511475  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00444878625869751  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00444832706451416  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004447849273681641  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004447357654571533  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004446855545043945  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004446346282958985  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004445832729339599  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00444531774520874  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004444804668426514  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004444293975830078  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0044437885284423825  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004443289756774903  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.16349518299102783 tensor([0.2265, 0.3007, 0.0277, 0.2815, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03897496685385704 tensor([0.4256, 0.4136, 0.0033, 0.1185, 0.0390], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07293684780597687 tensor([0.0147, 0.0729, 0.3969, 0.1765, 0.3389], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019426027312874794 tensor([0.5916, 0.2419, 0.0010, 0.1461, 0.0194], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06438902020454407 tensor([0.0140, 0.0644, 0.2935, 0.1494, 0.4786], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09323269873857498 tensor([0.3509, 0.2768, 0.0100, 0.2691, 0.0932], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06785981357097626 tensor([0.3268, 0.4509, 0.0079, 0.1465, 0.0679], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1071038693189621 tensor([0.0223, 0.1071, 0.2965, 0.1749, 0.3992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09377406537532806 tensor([0.2494, 0.1746, 0.0163, 0.4660, 0.0938], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06722098588943481 tensor([0.0146, 0.0672, 0.3171, 0.1530, 0.4480], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02843749150633812 tensor([0.5157, 0.3226, 0.0019, 0.1313, 0.0284], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.035867270082235336 tensor([0.4313, 0.4135, 0.0028, 0.1166, 0.0359], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1071573942899704 tensor([0.0198, 0.1072, 0.3283, 0.1612, 0.3836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09849993884563446 tensor([0.0504, 0.0985, 0.1531, 0.3884, 0.3095], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09118190407752991 tensor([0.0193, 0.0912, 0.2562, 0.1505, 0.4828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03233287110924721 tensor([0.5172, 0.2635, 0.0023, 0.1847, 0.0323], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06305640190839767 tensor([0.3339, 0.4613, 0.0068, 0.1351, 0.0631], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012791045941412449 tensor([0.0014, 0.0128, 0.6581, 0.0583, 0.2693], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.104380302131176 tensor([0.2536, 0.1734, 0.0154, 0.4533, 0.1044], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1304541677236557 tensor([0.0513, 0.1305, 0.1566, 0.2890, 0.3727], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10172048956155777 tensor([0.0924, 0.2190, 0.1017, 0.2731, 0.3138], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17591218650341034 tensor([0.1807, 0.2557, 0.0370, 0.3507, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1061798557639122 tensor([0.0931, 0.2446, 0.1062, 0.2604, 0.2958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05063828453421593 tensor([0.3899, 0.2013, 0.0052, 0.3531, 0.0506], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15068800747394562 tensor([0.1507, 0.2484, 0.0507, 0.3211, 0.2291], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14885342121124268 tensor([0.2189, 0.3435, 0.0325, 0.2562, 0.1489], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1114979088306427 tensor([0.2283, 0.5094, 0.0162, 0.1346, 0.1115], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06372207403182983 tensor([0.0175, 0.0637, 0.3202, 0.2313, 0.3673], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.056772928684949875 tensor([0.3688, 0.1783, 0.0060, 0.3901, 0.0568], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10697832703590393 tensor([0.2913, 0.3421, 0.0158, 0.2438, 0.1070], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1295900195837021 tensor([0.1296, 0.3064, 0.0701, 0.2332, 0.2607], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08728853613138199 tensor([0.3242, 0.3347, 0.0108, 0.2430, 0.0873], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15946601331233978 tensor([0.0624, 0.1814, 0.1595, 0.2652, 0.3316], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12494483590126038 tensor([0.1249, 0.1557, 0.0597, 0.4547, 0.2050], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08365498483181 tensor([0.0978, 0.1835, 0.0837, 0.3291, 0.3059], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11834927648305893 tensor([0.1183, 0.2292, 0.0781, 0.3176, 0.2568], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13233421742916107 tensor([0.2340, 0.3831, 0.0248, 0.2257, 0.1323], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1172274798154831 tensor([0.0273, 0.1172, 0.2457, 0.1961, 0.4138], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.052167825400829315 tensor([0.0170, 0.0522, 0.3171, 0.2881, 0.3256], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08684506267309189 tensor([0.0219, 0.0868, 0.2502, 0.1911, 0.4500], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10685805231332779 tensor([0.3006, 0.3394, 0.0143, 0.2389, 0.1069], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08632636815309525 tensor([0.2963, 0.4588, 0.0119, 0.1466, 0.0863], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04223724454641342 tensor([0.0072, 0.0422, 0.4685, 0.1225, 0.3595], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1530882567167282 tensor([0.1542, 0.1531, 0.0414, 0.4807, 0.1706], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11737119406461716 tensor([0.0517, 0.1174, 0.1394, 0.3043, 0.3872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13004593551158905 tensor([0.2778, 0.2985, 0.0173, 0.2764, 0.1300], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0486161895096302 tensor([0.3907, 0.4314, 0.0041, 0.1252, 0.0486], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0361735001206398 tensor([0.0055, 0.0362, 0.5251, 0.1137, 0.3195], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12023104727268219 tensor([0.1202, 0.1433, 0.0572, 0.4697, 0.2096], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10968879610300064 tensor([0.1097, 0.2379, 0.0864, 0.3106, 0.2555], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13086280226707458 tensor([0.1309, 0.2253, 0.0727, 0.3242, 0.2469], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08413872867822647 tensor([0.2936, 0.4580, 0.0116, 0.1526, 0.0841], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.031564462929964066 tensor([0.0051, 0.0316, 0.5318, 0.1100, 0.3216], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08696919679641724 tensor([0.0855, 0.1232, 0.0870, 0.4585, 0.2459], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08524816483259201 tensor([0.1038, 0.2355, 0.0852, 0.2867, 0.2888], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1332777738571167 tensor([0.1333, 0.2576, 0.0644, 0.3149, 0.2298], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11284631490707397 tensor([0.0835, 0.2601, 0.1128, 0.2244, 0.3192], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13012249767780304 tensor([0.0474, 0.1301, 0.1857, 0.3130, 0.3238], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12903118133544922 tensor([0.1290, 0.2687, 0.0671, 0.2914, 0.2438], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1525353044271469 tensor([0.0483, 0.1532, 0.1525, 0.2170, 0.4289], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14163212478160858 tensor([0.0709, 0.2026, 0.1416, 0.2767, 0.3081], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0924486517906189 tensor([0.2702, 0.4672, 0.0127, 0.1575, 0.0924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0338098481297493 tensor([0.0045, 0.0338, 0.5327, 0.0981, 0.3309], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09832926094532013 tensor([0.0231, 0.0983, 0.3090, 0.2047, 0.3648], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10233250260353088 tensor([0.3084, 0.3269, 0.0139, 0.2484, 0.1023], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10250848531723022 tensor([0.2727, 0.2202, 0.0159, 0.3887, 0.1025], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.117204450070858 tensor([0.2785, 0.3604, 0.0171, 0.2269, 0.1172], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.099732905626297 tensor([0.0326, 0.0997, 0.2340, 0.2771, 0.3566], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11844824254512787 tensor([0.2169, 0.1899, 0.0233, 0.4515, 0.1184], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1735961139202118 tensor([0.1736, 0.3285, 0.0483, 0.2760, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04167879745364189 tensor([0.4654, 0.2906, 0.0035, 0.1987, 0.0417], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016917075961828232 tensor([0.5613, 0.3261, 0.0009, 0.0948, 0.0169], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05496181920170784 tensor([0.0075, 0.0550, 0.4647, 0.0972, 0.3756], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15791364014148712 tensor([0.1944, 0.1848, 0.0276, 0.4353, 0.1579], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07411220669746399 tensor([0.3643, 0.3299, 0.0081, 0.2236, 0.0741], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15673410892486572 tensor([0.1567, 0.2517, 0.0573, 0.3154, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05616641789674759 tensor([0.3202, 0.5161, 0.0057, 0.1018, 0.0562], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015639014542102814 tensor([0.0021, 0.0156, 0.6112, 0.0764, 0.2946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09077835828065872 tensor([0.0642, 0.0908, 0.1033, 0.4872, 0.2546], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07917457073926926 tensor([0.0144, 0.0792, 0.3199, 0.1268, 0.4597], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05957935005426407 tensor([0.4090, 0.3398, 0.0057, 0.1859, 0.0596], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11673189699649811 tensor([0.1167, 0.2953, 0.0783, 0.2641, 0.2455], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.048985566943883896 tensor([0.0064, 0.0490, 0.4863, 0.0965, 0.3618], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1635248214006424 tensor([0.1716, 0.1879, 0.0360, 0.4409, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12841376662254333 tensor([0.1284, 0.2358, 0.0683, 0.3124, 0.2551], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10691437870264053 tensor([0.1069, 0.2440, 0.0809, 0.2549, 0.3133], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04186507686972618 tensor([0.4306, 0.3695, 0.0037, 0.1543, 0.0419], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13021518290042877 tensor([0.0551, 0.1302, 0.1551, 0.3367, 0.3228], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13677483797073364 tensor([0.1533, 0.1368, 0.0350, 0.5247, 0.1503], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07590910792350769 tensor([0.0143, 0.0759, 0.3206, 0.1484, 0.4409], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11528827250003815 tensor([0.2599, 0.4289, 0.0164, 0.1795, 0.1153], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18145734071731567 tensor([0.1815, 0.3569, 0.0382, 0.2294, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.033605482429265976 tensor([0.0034, 0.0336, 0.5998, 0.0684, 0.2947], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12481899559497833 tensor([0.0592, 0.1248, 0.1466, 0.3575, 0.3119], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11254486441612244 tensor([0.0307, 0.1125, 0.2289, 0.2014, 0.4265], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13874197006225586 tensor([0.1387, 0.2708, 0.0622, 0.2897, 0.2386], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16422677040100098 tensor([0.1993, 0.4054, 0.0300, 0.2010, 0.1642], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.018983548507094383 tensor([0.0020, 0.0190, 0.6564, 0.0570, 0.2657], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09943152964115143 tensor([0.1046, 0.0994, 0.0562, 0.5580, 0.1818], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09923580288887024 tensor([0.3140, 0.2831, 0.0138, 0.2899, 0.0992], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12374211102724075 tensor([0.1237, 0.2155, 0.0660, 0.3515, 0.2433], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09460555762052536 tensor([0.2551, 0.5093, 0.0121, 0.1290, 0.0946], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013697256334125996 tensor([0.0013, 0.0137, 0.6722, 0.0475, 0.2653], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12029781937599182 tensor([0.1203, 0.1582, 0.0626, 0.4412, 0.2176], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12605082988739014 tensor([0.1261, 0.2142, 0.0692, 0.3416, 0.2490], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1585509032011032 tensor([0.2352, 0.3255, 0.0258, 0.2549, 0.1586], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.160596564412117 tensor([0.1988, 0.4219, 0.0289, 0.1898, 0.1606], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07459913939237595 tensor([0.0132, 0.0746, 0.3708, 0.1375, 0.4039], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12618230283260345 tensor([0.2311, 0.2188, 0.0225, 0.4014, 0.1262], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12312664836645126 tensor([0.1231, 0.3143, 0.0733, 0.2218, 0.2675], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10585567355155945 tensor([0.0889, 0.2262, 0.1059, 0.2765, 0.3026], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08063516020774841 tensor([0.3025, 0.4585, 0.0091, 0.1492, 0.0806], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08205457031726837 tensor([0.0193, 0.0821, 0.3242, 0.1994, 0.3750], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1053844690322876 tensor([0.2204, 0.1388, 0.0185, 0.5168, 0.1054], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13422876596450806 tensor([0.1342, 0.2392, 0.0591, 0.3162, 0.2513], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1346568763256073 tensor([0.2460, 0.2844, 0.0254, 0.3095, 0.1347], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13402166962623596 tensor([0.2473, 0.3836, 0.0230, 0.2120, 0.1340], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0833539217710495 tensor([0.0269, 0.0834, 0.2443, 0.2621, 0.3833], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13219895958900452 tensor([0.1944, 0.1497, 0.0279, 0.4958, 0.1322], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05634722486138344 tensor([0.0122, 0.0563, 0.3402, 0.1576, 0.4337], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1418265700340271 tensor([0.2589, 0.2788, 0.0211, 0.2994, 0.1418], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09241452068090439 tensor([0.2920, 0.4171, 0.0117, 0.1867, 0.0924], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09467967599630356 tensor([0.0155, 0.0947, 0.3760, 0.1311, 0.3827], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1426384598016739 tensor([0.1644, 0.1426, 0.0328, 0.5101, 0.1501], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13354916870594025 tensor([0.1335, 0.2257, 0.0621, 0.3148, 0.2639], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.145791694521904 tensor([0.1458, 0.3208, 0.0633, 0.2318, 0.2382], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07591859996318817 tensor([0.3087, 0.4744, 0.0076, 0.1334, 0.0759], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.060994938015937805 tensor([0.0107, 0.0610, 0.4171, 0.1516, 0.3596], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13098852336406708 tensor([0.1310, 0.2319, 0.0691, 0.3344, 0.2335], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09072001278400421 tensor([0.0966, 0.2238, 0.0907, 0.3021, 0.2868], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07544098049402237 tensor([0.3585, 0.3599, 0.0075, 0.1986, 0.0754], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08135701715946198 tensor([0.3098, 0.4364, 0.0104, 0.1622, 0.0814], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014600194990634918 tensor([0.0018, 0.0146, 0.6620, 0.0708, 0.2508], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07704368978738785 tensor([0.3039, 0.1745, 0.0099, 0.4347, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03559946268796921 tensor([0.0063, 0.0356, 0.4196, 0.1138, 0.4247], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17888091504573822 tensor([0.1789, 0.2456, 0.0442, 0.3342, 0.1970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16075819730758667 tensor([0.2093, 0.3994, 0.0271, 0.2034, 0.1608], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02223619446158409 tensor([0.0027, 0.0222, 0.5734, 0.0704, 0.3313], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15061113238334656 tensor([0.1599, 0.1506, 0.0377, 0.4906, 0.1612], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13115191459655762 tensor([0.1312, 0.1925, 0.0600, 0.3747, 0.2417], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06610994040966034 tensor([0.3767, 0.2654, 0.0082, 0.2836, 0.0661], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1429692953824997 tensor([0.1984, 0.4315, 0.0282, 0.1989, 0.1430], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028582975268363953 tensor([0.0045, 0.0286, 0.4960, 0.1020, 0.3688], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11425287276506424 tensor([0.1143, 0.1463, 0.0670, 0.4606, 0.2118], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08461546897888184 tensor([0.1040, 0.2232, 0.0846, 0.2835, 0.3047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16512431204319 tensor([0.1651, 0.2705, 0.0472, 0.3125, 0.2047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14413584768772125 tensor([0.1441, 0.2531, 0.0624, 0.3188, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12249279022216797 tensor([0.0283, 0.1225, 0.2872, 0.2012, 0.3608], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07865169644355774 tensor([0.0808, 0.0995, 0.0787, 0.5036, 0.2375], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09961827844381332 tensor([0.0301, 0.0996, 0.2354, 0.2559, 0.3790], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08655604720115662 tensor([0.3269, 0.2796, 0.0120, 0.2949, 0.0866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.039874326437711716 tensor([0.4384, 0.3812, 0.0031, 0.1374, 0.0399], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1024385392665863 tensor([0.0312, 0.1024, 0.2371, 0.2688, 0.3604], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1456790715456009 tensor([0.1457, 0.2038, 0.0518, 0.3911, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08525844663381577 tensor([0.0244, 0.0853, 0.2396, 0.2271, 0.4236], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.056346345692873 tensor([0.4133, 0.3082, 0.0055, 0.2167, 0.0563], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16620947420597076 tensor([0.0396, 0.1986, 0.1949, 0.1662, 0.4006], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04120197892189026 tensor([0.0063, 0.0412, 0.4698, 0.1123, 0.3705], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11484097689390182 tensor([0.2286, 0.1536, 0.0181, 0.4849, 0.1148], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1382555514574051 tensor([0.1383, 0.2727, 0.0584, 0.2617, 0.2690], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11050157994031906 tensor([0.2937, 0.3149, 0.0156, 0.2654, 0.1105], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12933938205242157 tensor([0.2265, 0.4298, 0.0234, 0.1910, 0.1293], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04054407402873039 tensor([0.0061, 0.0405, 0.5213, 0.1117, 0.3204], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17221438884735107 tensor([0.1874, 0.2226, 0.0339, 0.3839, 0.1722], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13076286017894745 tensor([0.0428, 0.1308, 0.1762, 0.2312, 0.4191], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030059853568673134 tensor([0.5180, 0.2999, 0.0021, 0.1499, 0.0301], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03694730997085571 tensor([0.4250, 0.4433, 0.0026, 0.0921, 0.0369], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06886345893144608 tensor([0.0216, 0.0689, 0.2814, 0.2757, 0.3525], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1359514594078064 tensor([0.2397, 0.2903, 0.0262, 0.3079, 0.1360], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11829494684934616 tensor([0.0667, 0.1437, 0.1183, 0.3128, 0.3584], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06893539428710938 tensor([0.3860, 0.2763, 0.0074, 0.2614, 0.0689], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04443640261888504 tensor([0.3838, 0.4549, 0.0037, 0.1131, 0.0444], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13781949877738953 tensor([0.0788, 0.1789, 0.1378, 0.3387, 0.2657], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13861368596553802 tensor([0.2021, 0.1793, 0.0284, 0.4516, 0.1386], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09056764841079712 tensor([0.0203, 0.0906, 0.2549, 0.1618, 0.4724], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15668193995952606 tensor([0.2150, 0.3177, 0.0316, 0.2789, 0.1567], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13449306786060333 tensor([0.2468, 0.3015, 0.0245, 0.2926, 0.1345], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012926565483212471 tensor([0.0013, 0.0129, 0.6869, 0.0498, 0.2492], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1256571263074875 tensor([0.1257, 0.2010, 0.0729, 0.3898, 0.2106], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06707584112882614 tensor([0.0108, 0.0671, 0.3655, 0.1188, 0.4378], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14739316701889038 tensor([0.2458, 0.2516, 0.0228, 0.3324, 0.1474], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10439801961183548 tensor([0.2254, 0.5325, 0.0158, 0.1219, 0.1044], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13140350580215454 tensor([0.0399, 0.1314, 0.2283, 0.2769, 0.3236], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1558758169412613 tensor([0.2131, 0.2665, 0.0305, 0.3341, 0.1559], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15803703665733337 tensor([0.1580, 0.2224, 0.0506, 0.3363, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14353427290916443 tensor([0.1435, 0.3481, 0.0598, 0.2359, 0.2127], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07156576961278915 tensor([0.3172, 0.4617, 0.0077, 0.1418, 0.0716], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030707286670804024 tensor([0.0041, 0.0307, 0.5663, 0.0862, 0.3127], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12567472457885742 tensor([0.2107, 0.1840, 0.0245, 0.4551, 0.1257], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1143646314740181 tensor([0.1144, 0.2866, 0.0780, 0.2367, 0.2842], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11839353293180466 tensor([0.1184, 0.2733, 0.0752, 0.2569, 0.2761], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1539885550737381 tensor([0.1930, 0.3874, 0.0372, 0.2284, 0.1540], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08688066899776459 tensor([0.0289, 0.0869, 0.2462, 0.2890, 0.3490], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16724921762943268 tensor([0.1782, 0.1761, 0.0337, 0.4448, 0.1672], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12246622145175934 tensor([0.1225, 0.2543, 0.0762, 0.3052, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06348221749067307 tensor([0.3738, 0.3646, 0.0066, 0.1915, 0.0635], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16985556483268738 tensor([0.1919, 0.3250, 0.0375, 0.2756, 0.1699], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06041264533996582 tensor([0.0146, 0.0604, 0.3255, 0.2078, 0.3917], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12277279794216156 tensor([0.2295, 0.2055, 0.0219, 0.4203, 0.1228], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05785636231303215 tensor([0.0122, 0.0579, 0.3089, 0.1503, 0.4707], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16893039643764496 tensor([0.2118, 0.3508, 0.0328, 0.2358, 0.1689], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06016526743769646 tensor([0.3270, 0.4939, 0.0057, 0.1132, 0.0602], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11425171047449112 tensor([0.0378, 0.1143, 0.2057, 0.2865, 0.3558], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07627741992473602 tensor([0.0141, 0.0763, 0.3288, 0.1464, 0.4344], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12448182702064514 tensor([0.1245, 0.2323, 0.0649, 0.3014, 0.2769], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0973702073097229 tensor([0.3148, 0.3287, 0.0134, 0.2457, 0.0974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08244013786315918 tensor([0.3345, 0.3515, 0.0091, 0.2225, 0.0824], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1297932267189026 tensor([0.0239, 0.1298, 0.3379, 0.1608, 0.3476], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15510332584381104 tensor([0.1551, 0.1799, 0.0498, 0.4417, 0.1735], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15253670513629913 tensor([0.0408, 0.1525, 0.1683, 0.1831, 0.4552], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05081174150109291 tensor([0.4018, 0.3099, 0.0055, 0.2320, 0.0508], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025760788470506668 tensor([0.4588, 0.4255, 0.0017, 0.0882, 0.0258], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1130223497748375 tensor([0.0345, 0.1130, 0.2209, 0.2460, 0.3855], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08369442075490952 tensor([0.2574, 0.1385, 0.0124, 0.5081, 0.0837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1854289025068283 tensor([0.1866, 0.2485, 0.0409, 0.3386, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09314637631177902 tensor([0.1005, 0.2038, 0.0931, 0.3568, 0.2458], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1318652480840683 tensor([0.1319, 0.3657, 0.0620, 0.2033, 0.2371], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.025443006306886673 tensor([0.0026, 0.0254, 0.5876, 0.0622, 0.3222], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06230933591723442 tensor([0.3468, 0.1612, 0.0066, 0.4232, 0.0623], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13820408284664154 tensor([0.0418, 0.1382, 0.1936, 0.2433, 0.3832], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16424152255058289 tensor([0.1642, 0.2649, 0.0457, 0.3059, 0.2193], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.058484915643930435 tensor([0.3790, 0.3758, 0.0063, 0.1804, 0.0585], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07732221484184265 tensor([0.0189, 0.0773, 0.3275, 0.1950, 0.3812], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1529790610074997 tensor([0.1904, 0.1943, 0.0310, 0.4313, 0.1530], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15911990404129028 tensor([0.2203, 0.2414, 0.0287, 0.3505, 0.1591], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17099563777446747 tensor([0.1710, 0.2598, 0.0498, 0.3333, 0.1861], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1462963968515396 tensor([0.1463, 0.2747, 0.0638, 0.3169, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.055013272911310196 tensor([0.0092, 0.0550, 0.4353, 0.1214, 0.3791], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11718593537807465 tensor([0.1172, 0.1679, 0.0682, 0.4159, 0.2308], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17848551273345947 tensor([0.1785, 0.2533, 0.0458, 0.3190, 0.2034], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026327308267354965 tensor([0.5129, 0.3475, 0.0016, 0.1117, 0.0263], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11167945712804794 tensor([0.2696, 0.4393, 0.0181, 0.1613, 0.1117], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11310355365276337 tensor([0.0310, 0.1131, 0.2800, 0.2260, 0.3499], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06369847059249878 tensor([0.0199, 0.0637, 0.2819, 0.2644, 0.3700], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10786306113004684 tensor([0.1079, 0.2559, 0.0801, 0.2593, 0.2968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07838404923677444 tensor([0.3545, 0.3393, 0.0091, 0.2187, 0.0784], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07469730079174042 tensor([0.3420, 0.3831, 0.0085, 0.1917, 0.0747], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05861736088991165 tensor([0.0133, 0.0586, 0.3574, 0.1847, 0.3860], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12304947525262833 tensor([0.0398, 0.1230, 0.2220, 0.2612, 0.3540], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12467622756958008 tensor([0.0407, 0.1247, 0.1756, 0.2252, 0.4339], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18132764101028442 tensor([0.1813, 0.2715, 0.0443, 0.3106, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06743880361318588 tensor([0.3719, 0.3729, 0.0066, 0.1811, 0.0674], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0480932891368866 tensor([0.0100, 0.0481, 0.4240, 0.1733, 0.3446], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13457949459552765 tensor([0.0411, 0.1346, 0.1920, 0.2625, 0.3699], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12139343470335007 tensor([0.1214, 0.2091, 0.0621, 0.3419, 0.2655], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1576087474822998 tensor([0.2155, 0.3520, 0.0331, 0.2418, 0.1576], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1716892421245575 tensor([0.1866, 0.3172, 0.0389, 0.2856, 0.1717], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015614696778357029 tensor([0.0018, 0.0156, 0.6151, 0.0608, 0.3067], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06369209289550781 tensor([0.3447, 0.2063, 0.0083, 0.3770, 0.0637], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05778864398598671 tensor([0.0133, 0.0578, 0.3423, 0.1797, 0.4068], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [0, 2], [2, 4], [0, 2], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 0], [2, 4], [0, 2], [2, 0], [0, 2], [2, 0], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [2, 0], [2, 0], [2, 4], [0, 1], [0, 2], [0, 2], [2, 0], [0, 2], [0, 1], [2, 0], [0, 2], [0, 2], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 0], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 0], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4], [0, 1], [2, 1], [0, 1], [2, 4], [2, 0], [0, 1], [0, 1], [0, 1], [2, 0], [2, 4], [0, 1], [2, 1], [2, 4], [2, 0], [2, 4], [0, 1], [2, 0], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [2, 0], [0, 2], [2, 4], [0, 1], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 1], [2, 0], [2, 0], [2, 4], [0, 1], [2, 0], [0, 2], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 0], [2, 4], [0, 1], [2, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 0], [0, 2], [2, 0], [2, 0], [0, 1], [0, 2], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 2], [2, 4], [2, 4], [0, 2], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4], [0, 1], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [0, 2], [2, 0], [0, 1], [2, 4], [0, 1], [2, 0], [2, 4], [0, 1], [2, 4], [2, 4], [2, 0], [2, 0], [0, 1], [2, 0], [2, 0], [2, 4], [2, 4], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1], [0, 1], [0, 1], [2, 0], [2, 4], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1]]\n",
      "[[0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [2, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [2, 3, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [1, 3, 4], [2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [0, 2], [2, 4], [0, 2], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 0], [2, 4], [0, 2], [2, 0], [0, 2], [2, 0], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [2, 0], [2, 0], [2, 4], [0, 1], [0, 2], [0, 2], [2, 0], [0, 2], [0, 1], [2, 0], [0, 2], [0, 2], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 0], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 0], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4], [0, 1], [2, 1], [0, 1], [2, 4], [2, 0], [0, 1], [0, 1], [0, 1], [2, 0], [2, 4], [0, 1], [2, 1], [2, 4], [2, 0], [2, 4], [0, 1], [2, 0], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [2, 0], [0, 2], [2, 4], [0, 1], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 1], [2, 0], [2, 0], [2, 4], [0, 1], [2, 0], [0, 2], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 0], [2, 4], [0, 1], [2, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 0], [0, 2], [2, 0], [2, 0], [0, 1], [0, 2], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 2], [2, 4], [2, 4], [0, 2], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4], [0, 1], [2, 4], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [0, 2], [2, 0], [0, 1], [2, 4], [0, 1], [2, 0], [2, 4], [0, 1], [2, 4], [2, 4], [2, 0], [2, 0], [0, 1], [2, 0], [2, 0], [2, 4], [2, 4], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1], [0, 1], [0, 1], [2, 0], [2, 4], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004753547668457031  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00475345230102539  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004753269672393799  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0047530107498168946  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004752681255340576  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004752290725708008  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0047518424987792966  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0047513446807861326  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004750801086425781  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004750217914581299  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.0047495970726013186  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00474894380569458  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004748262405395508  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.0047475543022155765  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004746822834014892  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004746070384979248  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00474530029296875  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004744511604309082  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004743709564208985  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004742895603179932  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.2257152944803238 tensor([0.2257, 0.2923, 0.0276, 0.2965, 0.1579], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12453962117433548 tensor([0.4280, 0.4066, 0.0033, 0.1245, 0.0375], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1854797601699829 tensor([0.0145, 0.0702, 0.3988, 0.1855, 0.3309], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15383030474185944 tensor([0.5906, 0.2360, 0.0010, 0.1538, 0.0186], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15732762217521667 tensor([0.0138, 0.0622, 0.2965, 0.1573, 0.4701], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26864704489707947 tensor([0.3483, 0.2686, 0.0100, 0.2831, 0.0899], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1549593210220337 tensor([0.3293, 0.4424, 0.0078, 0.1550, 0.0655], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18460693955421448 tensor([0.0222, 0.1039, 0.2984, 0.1846, 0.3909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16694679856300354 tensor([0.2445, 0.1669, 0.0159, 0.4837, 0.0890], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1613924503326416 tensor([0.0145, 0.0651, 0.3197, 0.1614, 0.4394], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1386955827474594 tensor([0.5163, 0.3157, 0.0019, 0.1387, 0.0274], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12286988645792007 tensor([0.4343, 0.4057, 0.0028, 0.1229, 0.0344], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1700829118490219 tensor([0.0197, 0.1038, 0.3307, 0.1701, 0.3757], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15199215710163116 tensor([0.0495, 0.0944, 0.1520, 0.4057, 0.2983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15807728469371796 tensor([0.0191, 0.0882, 0.2592, 0.1581, 0.4754], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19440169632434845 tensor([0.5155, 0.2568, 0.0023, 0.1944, 0.0311], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14298401772975922 tensor([0.3363, 0.4530, 0.0067, 0.1430, 0.0609], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06101248040795326 tensor([0.0014, 0.0123, 0.6610, 0.0610, 0.2643], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16582439839839935 tensor([0.2490, 0.1658, 0.0150, 0.4712, 0.0990], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15688087046146393 tensor([0.0507, 0.1260, 0.1569, 0.3039, 0.3625], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2126958966255188 tensor([0.0919, 0.2127, 0.1020, 0.2879, 0.3055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17927242815494537 tensor([0.1793, 0.2471, 0.0366, 0.3681, 0.1690], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2377808541059494 tensor([0.0928, 0.2378, 0.1065, 0.2749, 0.2881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19362176954746246 tensor([0.3844, 0.1936, 0.0051, 0.3687, 0.0481], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2217017263174057 tensor([0.1494, 0.2404, 0.0506, 0.3379, 0.2217], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21848756074905396 tensor([0.2185, 0.3346, 0.0325, 0.2703, 0.1442], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14279547333717346 tensor([0.2309, 0.5019, 0.0161, 0.1428, 0.1082], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2429094910621643 tensor([0.0172, 0.0613, 0.3208, 0.2429, 0.3577], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17100094258785248 tensor([0.3630, 0.1710, 0.0059, 0.4064, 0.0538], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25709840655326843 tensor([0.2905, 0.3332, 0.0158, 0.2571, 0.1035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24660301208496094 tensor([0.1295, 0.2991, 0.0705, 0.2466, 0.2544], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2558883726596832 tensor([0.3236, 0.3258, 0.0107, 0.2559, 0.0839], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17609190940856934 tensor([0.0622, 0.1761, 0.1594, 0.2796, 0.3227], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14916442334651947 tensor([0.1228, 0.1492, 0.0587, 0.4733, 0.1960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1772824227809906 tensor([0.0968, 0.1773, 0.0836, 0.3458, 0.2966], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2216133326292038 tensor([0.1172, 0.2216, 0.0781, 0.3341, 0.2490], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23481270670890808 tensor([0.2348, 0.3742, 0.0247, 0.2385, 0.1278], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20714151859283447 tensor([0.0271, 0.1137, 0.2472, 0.2071, 0.4048], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30159223079681396 tensor([0.0168, 0.0501, 0.3158, 0.3016, 0.3157], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20156490802764893 tensor([0.0218, 0.0841, 0.2521, 0.2016, 0.4406], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2520754039287567 tensor([0.2999, 0.3306, 0.0142, 0.2521, 0.1033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15535645186901093 tensor([0.2979, 0.4507, 0.0119, 0.1554, 0.0840], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12873394787311554 tensor([0.0071, 0.0408, 0.4710, 0.1287, 0.3524], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15114367008209229 tensor([0.1511, 0.1463, 0.0406, 0.4994, 0.1626], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1397494077682495 tensor([0.0511, 0.1131, 0.1397, 0.3198, 0.3762], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27637797594070435 tensor([0.2764, 0.2899, 0.0172, 0.2911, 0.1255], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13252092897891998 tensor([0.3930, 0.4234, 0.0041, 0.1325, 0.0469], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11931749433279037 tensor([0.0054, 0.0347, 0.5280, 0.1193, 0.3125], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13689810037612915 tensor([0.1177, 0.1369, 0.0564, 0.4886, 0.2004], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23023104667663574 tensor([0.1087, 0.2302, 0.0864, 0.3267, 0.2480], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21770204603672028 tensor([0.1291, 0.2177, 0.0730, 0.3404, 0.2398], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16154004633426666 tensor([0.2954, 0.4498, 0.0116, 0.1615, 0.0817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1150868833065033 tensor([0.0050, 0.0303, 0.5349, 0.1151, 0.3147], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11765608936548233 tensor([0.0838, 0.1177, 0.0858, 0.4774, 0.2354], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22867202758789062 tensor([0.1033, 0.2287, 0.0852, 0.3023, 0.2805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22246941924095154 tensor([0.1323, 0.2496, 0.0642, 0.3314, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23756787180900574 tensor([0.0836, 0.2541, 0.1132, 0.2376, 0.3116], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1853257119655609 tensor([0.0468, 0.1254, 0.1853, 0.3285, 0.3139], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23653094470500946 tensor([0.1285, 0.2612, 0.0669, 0.3068, 0.2365], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14888431131839752 tensor([0.0481, 0.1489, 0.1537, 0.2295, 0.4198], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19688548147678375 tensor([0.0708, 0.1969, 0.1413, 0.2918, 0.2993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1669103056192398 tensor([0.2724, 0.4586, 0.0126, 0.1669, 0.0894], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10300736874341965 tensor([0.0044, 0.0326, 0.5355, 0.1030, 0.3245], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2155473381280899 tensor([0.0230, 0.0952, 0.3099, 0.2155, 0.3563], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26181676983833313 tensor([0.3072, 0.3182, 0.0139, 0.2618, 0.0989], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21189069747924805 tensor([0.2686, 0.2119, 0.0157, 0.4058, 0.0980], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23930606245994568 tensor([0.2787, 0.3517, 0.0170, 0.2393, 0.1133], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23445017635822296 tensor([0.0321, 0.0960, 0.2345, 0.2908, 0.3467], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1818636804819107 tensor([0.2132, 0.1819, 0.0229, 0.4695, 0.1126], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17307227849960327 tensor([0.1731, 0.3196, 0.0482, 0.2909, 0.1683], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20946477353572845 tensor([0.4638, 0.2831, 0.0035, 0.2095, 0.0401], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09944335371255875 tensor([0.5640, 0.3196, 0.0009, 0.0994, 0.0161], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10173527151346207 tensor([0.0074, 0.0530, 0.4686, 0.1017, 0.3693], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17727623879909515 tensor([0.1914, 0.1773, 0.0271, 0.4537, 0.1505], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23544462025165558 tensor([0.3635, 0.3216, 0.0081, 0.2354, 0.0714], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2119227647781372 tensor([0.1555, 0.2439, 0.0571, 0.3316, 0.2119], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1074603796005249 tensor([0.3238, 0.5090, 0.0056, 0.1075, 0.0541], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07982953637838364 tensor([0.0021, 0.0150, 0.6145, 0.0798, 0.2886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10158553719520569 tensor([0.0628, 0.0865, 0.1016, 0.5059, 0.2432], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13297921419143677 tensor([0.0142, 0.0765, 0.3235, 0.1330, 0.4527], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1962772160768509 tensor([0.4085, 0.3319, 0.0057, 0.1963, 0.0576], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2384779453277588 tensor([0.1166, 0.2875, 0.0783, 0.2791, 0.2385], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10129016637802124 tensor([0.0063, 0.0473, 0.4897, 0.1013, 0.3554], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16908209025859833 tensor([0.1691, 0.1802, 0.0353, 0.4595, 0.1558], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2281530797481537 tensor([0.1271, 0.2282, 0.0684, 0.3288, 0.2475], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23736540973186493 tensor([0.1066, 0.2374, 0.0812, 0.2694, 0.3054], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16291190683841705 tensor([0.4318, 0.3614, 0.0037, 0.1629, 0.0403], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15461251139640808 tensor([0.0544, 0.1253, 0.1546, 0.3532, 0.3124], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14232927560806274 tensor([0.1499, 0.1302, 0.0341, 0.5434, 0.1423], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1566692292690277 tensor([0.0142, 0.0735, 0.3232, 0.1567, 0.4325], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19009973108768463 tensor([0.2613, 0.4204, 0.0164, 0.1901, 0.1118], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1886989027261734 tensor([0.1817, 0.3485, 0.0382, 0.2429, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07140601426362991 tensor([0.0034, 0.0323, 0.6038, 0.0714, 0.2891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14583039283752441 tensor([0.0584, 0.1200, 0.1458, 0.3743, 0.3014], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21218785643577576 tensor([0.0304, 0.1089, 0.2309, 0.2122, 0.4176], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23170509934425354 tensor([0.1379, 0.2630, 0.0623, 0.3052, 0.2317], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20059926807880402 tensor([0.2006, 0.3972, 0.0299, 0.2130, 0.1593], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05925946682691574 tensor([0.0019, 0.0182, 0.6601, 0.0593, 0.2606], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10187282413244247 tensor([0.1019, 0.0943, 0.0549, 0.5766, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2746284008026123 tensor([0.3115, 0.2746, 0.0137, 0.3045, 0.0956], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20805825293064117 tensor([0.1225, 0.2081, 0.0656, 0.3690, 0.2348], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.136989563703537 tensor([0.2581, 0.5012, 0.0120, 0.1370, 0.0917], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04940108582377434 tensor([0.0013, 0.0131, 0.6759, 0.0494, 0.2603], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15156522393226624 tensor([0.1182, 0.1516, 0.0618, 0.4600, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2067701667547226 tensor([0.1246, 0.2068, 0.0690, 0.3587, 0.2409], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23437488079071045 tensor([0.2344, 0.3169, 0.0258, 0.2691, 0.1538], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20021826028823853 tensor([0.2002, 0.4138, 0.0288, 0.2013, 0.1559], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14485234022140503 tensor([0.0130, 0.0722, 0.3738, 0.1449, 0.3961], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21037964522838593 tensor([0.2281, 0.2104, 0.0221, 0.4190, 0.1204], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23479577898979187 tensor([0.1232, 0.3070, 0.0737, 0.2348, 0.2613], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21978741884231567 tensor([0.0887, 0.2198, 0.1058, 0.2918, 0.2940], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15811796486377716 tensor([0.3050, 0.4500, 0.0091, 0.1581, 0.0779], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2097136527299881 tensor([0.0191, 0.0792, 0.3258, 0.2097, 0.3662], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13219095766544342 tensor([0.2155, 0.1322, 0.0180, 0.5348, 0.0996], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23142096400260925 tensor([0.1329, 0.2314, 0.0592, 0.3329, 0.2436], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24430346488952637 tensor([0.2443, 0.2757, 0.0253, 0.3250, 0.1298], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2238623946905136 tensor([0.2479, 0.3753, 0.0230, 0.2239, 0.1300], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.244893878698349 tensor([0.0266, 0.0802, 0.2449, 0.2753, 0.3730], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14269500970840454 tensor([0.1898, 0.1427, 0.0274, 0.5143, 0.1257], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16585038602352142 tensor([0.0121, 0.0544, 0.3428, 0.1659, 0.4249], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2572977542877197 tensor([0.2573, 0.2706, 0.0209, 0.3145, 0.1367], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19748659431934357 tensor([0.2934, 0.4083, 0.0116, 0.1975, 0.0892], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.138036847114563 tensor([0.0154, 0.0917, 0.3793, 0.1380, 0.3756], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1423155665397644 tensor([0.1608, 0.1359, 0.0320, 0.5289, 0.1423], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21853473782539368 tensor([0.1323, 0.2185, 0.0621, 0.3310, 0.2560], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23262760043144226 tensor([0.1455, 0.3133, 0.0637, 0.2448, 0.2326], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14136835932731628 tensor([0.3116, 0.4662, 0.0075, 0.1414, 0.0733], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15953226387500763 tensor([0.0106, 0.0589, 0.4192, 0.1595, 0.3517], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22432886064052582 tensor([0.1297, 0.2243, 0.0690, 0.3510, 0.2261], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21721535921096802 tensor([0.0963, 0.2172, 0.0903, 0.3185, 0.2777], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20978647470474243 tensor([0.3585, 0.3514, 0.0075, 0.2098, 0.0729], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17171774804592133 tensor([0.3114, 0.4278, 0.0103, 0.1717, 0.0788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07407569885253906 tensor([0.0018, 0.0141, 0.6642, 0.0741, 0.2459], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16696709394454956 tensor([0.2988, 0.1670, 0.0097, 0.4517, 0.0729], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11953961849212646 tensor([0.0062, 0.0343, 0.4229, 0.1195, 0.4170], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1902112513780594 tensor([0.1773, 0.2377, 0.0440, 0.3508, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2104334831237793 tensor([0.2104, 0.3912, 0.0270, 0.2155, 0.1559], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07342981547117233 tensor([0.0027, 0.0214, 0.5773, 0.0734, 0.3253], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15330147743225098 tensor([0.1566, 0.1438, 0.0369, 0.5093, 0.1533], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18532513082027435 tensor([0.1292, 0.1853, 0.0598, 0.3924, 0.2332], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.257268488407135 tensor([0.3734, 0.2573, 0.0081, 0.2977, 0.0636], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19961315393447876 tensor([0.1996, 0.4227, 0.0281, 0.2109, 0.1387], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10682187974452972 tensor([0.0045, 0.0275, 0.4992, 0.1068, 0.3620], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1401095688343048 tensor([0.1122, 0.1401, 0.0660, 0.4791, 0.2026], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2165554314851761 tensor([0.1034, 0.2166, 0.0848, 0.2991, 0.2962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981438249349594 tensor([0.1638, 0.2620, 0.0471, 0.3289, 0.1981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21452206373214722 tensor([0.1428, 0.2450, 0.0624, 0.3353, 0.2145], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21185019612312317 tensor([0.0281, 0.1186, 0.2887, 0.2119, 0.3527], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09478351473808289 tensor([0.0790, 0.0948, 0.0772, 0.5227, 0.2264], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23577770590782166 tensor([0.0298, 0.0962, 0.2358, 0.2693, 0.3689], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27115780115127563 tensor([0.3246, 0.2712, 0.0119, 0.3093, 0.0831], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14526909589767456 tensor([0.4399, 0.3734, 0.0030, 0.1453, 0.0384], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23770415782928467 tensor([0.0308, 0.0986, 0.2377, 0.2822, 0.3507], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1961745023727417 tensor([0.1438, 0.1962, 0.0514, 0.4093, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2392638921737671 tensor([0.0241, 0.0822, 0.2411, 0.2393, 0.4133], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22794194519519806 tensor([0.4122, 0.3002, 0.0055, 0.2279, 0.0542], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1945139765739441 tensor([0.0398, 0.1945, 0.1961, 0.1765, 0.3930], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1180059164762497 tensor([0.0063, 0.0398, 0.4726, 0.1180, 0.3633], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14662331342697144 tensor([0.2241, 0.1466, 0.0177, 0.5029, 0.1087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26247552037239075 tensor([0.1374, 0.2653, 0.0588, 0.2761, 0.2625], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27932730317115784 tensor([0.2924, 0.3062, 0.0155, 0.2793, 0.1066], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2021487057209015 tensor([0.2283, 0.4213, 0.0232, 0.2021, 0.1250], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11734011769294739 tensor([0.0060, 0.0391, 0.5238, 0.1173, 0.3138], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.185319721698761 tensor([0.1853, 0.2146, 0.0335, 0.4016, 0.1650], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17753271758556366 tensor([0.0424, 0.1266, 0.1775, 0.2437, 0.4098], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1581549495458603 tensor([0.5179, 0.2930, 0.0021, 0.1582, 0.0289], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09633317589759827 tensor([0.4285, 0.4372, 0.0025, 0.0963, 0.0354], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.28129419684410095 tensor([0.0213, 0.0661, 0.2813, 0.2891, 0.3422], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23789283633232117 tensor([0.2379, 0.2815, 0.0261, 0.3234, 0.1312], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13866214454174042 tensor([0.0660, 0.1387, 0.1184, 0.3290, 0.3479], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2680458724498749 tensor([0.3830, 0.2680, 0.0073, 0.2752, 0.0664], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11952812969684601 tensor([0.3870, 0.4471, 0.0037, 0.1195, 0.0427], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17258904874324799 tensor([0.0779, 0.1726, 0.1373, 0.3550, 0.2572], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17169108986854553 tensor([0.1985, 0.1717, 0.0279, 0.4699, 0.1320], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1708652824163437 tensor([0.0202, 0.0878, 0.2576, 0.1709, 0.4636], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21446317434310913 tensor([0.2145, 0.3089, 0.0314, 0.2939, 0.1513], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24537526071071625 tensor([0.2454, 0.2926, 0.0244, 0.3079, 0.1298], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05181242898106575 tensor([0.0012, 0.0124, 0.6899, 0.0518, 0.2447], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1935151219367981 tensor([0.1240, 0.1935, 0.0722, 0.4075, 0.2027], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12487322092056274 tensor([0.0107, 0.0648, 0.3694, 0.1249, 0.4303], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24334171414375305 tensor([0.2434, 0.2433, 0.0227, 0.3487, 0.1419], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12865759432315826 tensor([0.2282, 0.5261, 0.0157, 0.1287, 0.1014], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22866304218769073 tensor([0.0394, 0.1266, 0.2287, 0.2908, 0.3146], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2117093950510025 tensor([0.2117, 0.2580, 0.0301, 0.3504, 0.1497], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2148810178041458 tensor([0.1560, 0.2149, 0.0507, 0.3530, 0.2254], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20732015371322632 tensor([0.1434, 0.3398, 0.0600, 0.2495, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15020106732845306 tensor([0.3197, 0.4533, 0.0076, 0.1502, 0.0691], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09002808481454849 tensor([0.0040, 0.0295, 0.5700, 0.0900, 0.3065], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17608287930488586 tensor([0.2071, 0.1761, 0.0240, 0.4734, 0.1194], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.250296026468277 tensor([0.1143, 0.2796, 0.0784, 0.2503, 0.2774], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2658976912498474 tensor([0.1179, 0.2659, 0.0756, 0.2715, 0.2691], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1934032291173935 tensor([0.1934, 0.3785, 0.0371, 0.2415, 0.1495], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24598082900047302 tensor([0.0285, 0.0835, 0.2460, 0.3034, 0.3386], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1686106026172638 tensor([0.1750, 0.1686, 0.0332, 0.4635, 0.1598], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23493830859661102 tensor([0.1214, 0.2463, 0.0763, 0.3211, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20231029391288757 tensor([0.3740, 0.3560, 0.0066, 0.2023, 0.0612], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19189034402370453 tensor([0.1919, 0.3165, 0.0372, 0.2904, 0.1640], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2188161462545395 tensor([0.0145, 0.0583, 0.3264, 0.2188, 0.3820], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19716373085975647 tensor([0.2259, 0.1972, 0.0215, 0.4383, 0.1171], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1582822948694229 tensor([0.0121, 0.0559, 0.3118, 0.1583, 0.4619], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21194063127040863 tensor([0.2119, 0.3424, 0.0327, 0.2490, 0.1639], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11981426179409027 tensor([0.3303, 0.4862, 0.0057, 0.1198, 0.0581], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20588275790214539 tensor([0.0373, 0.1101, 0.2059, 0.3011, 0.3457], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15452520549297333 tensor([0.0140, 0.0739, 0.3314, 0.1545, 0.4261], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2252275049686432 tensor([0.1236, 0.2252, 0.0649, 0.3176, 0.2686], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2587103545665741 tensor([0.3140, 0.3201, 0.0133, 0.2587, 0.0939], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23464956879615784 tensor([0.3346, 0.3425, 0.0090, 0.2346, 0.0792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16946257650852203 tensor([0.0238, 0.1261, 0.3400, 0.1695, 0.3407], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16607566177845 tensor([0.1523, 0.1724, 0.0491, 0.4602, 0.1661], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17016054689884186 tensor([0.0406, 0.1483, 0.1702, 0.1932, 0.4477], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24412785470485687 tensor([0.4003, 0.3013, 0.0055, 0.2441, 0.0488], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0925406664609909 tensor([0.4629, 0.4184, 0.0017, 0.0925, 0.0246], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22132839262485504 tensor([0.0343, 0.1095, 0.2213, 0.2592, 0.3757], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13181260228157043 tensor([0.2512, 0.1318, 0.0121, 0.5258, 0.0791], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18505145609378815 tensor([0.1851, 0.2405, 0.0406, 0.3551, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19680438935756683 tensor([0.0996, 0.1968, 0.0923, 0.3744, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21566666662693024 tensor([0.1325, 0.3582, 0.0622, 0.2157, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06477873772382736 tensor([0.0026, 0.0244, 0.5921, 0.0648, 0.3162], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15433168411254883 tensor([0.3405, 0.1543, 0.0064, 0.4398, 0.0590], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19424134492874146 tensor([0.0415, 0.1339, 0.1942, 0.2566, 0.3737], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21239016950130463 tensor([0.1630, 0.2568, 0.0457, 0.3221, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19026434421539307 tensor([0.3800, 0.3672, 0.0063, 0.1903, 0.0563], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20515429973602295 tensor([0.0188, 0.0747, 0.3290, 0.2052, 0.3724], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18663056194782257 tensor([0.1879, 0.1866, 0.0304, 0.4494, 0.1457], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21761733293533325 tensor([0.2176, 0.2331, 0.0286, 0.3674, 0.1533], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.179801344871521 tensor([0.1695, 0.2515, 0.0495, 0.3497, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19201728701591492 tensor([0.1450, 0.2660, 0.0638, 0.3332, 0.1920], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12765654921531677 tensor([0.0091, 0.0531, 0.4385, 0.1277, 0.3717], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1611686497926712 tensor([0.1154, 0.1612, 0.0674, 0.4345, 0.2216], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19678018987178802 tensor([0.1770, 0.2455, 0.0456, 0.3351, 0.1968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11738154292106628 tensor([0.5149, 0.3410, 0.0016, 0.1174, 0.0252], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17000654339790344 tensor([0.2711, 0.4321, 0.0181, 0.1700, 0.1087], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2375253587961197 tensor([0.0307, 0.1094, 0.2809, 0.2375, 0.3415], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27792224287986755 tensor([0.0198, 0.0614, 0.2814, 0.2779, 0.3595], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24895387887954712 tensor([0.1075, 0.2490, 0.0804, 0.2740, 0.2891], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.230486199259758 tensor([0.3540, 0.3308, 0.0091, 0.2305, 0.0756], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20229306817054749 tensor([0.3432, 0.3743, 0.0084, 0.2023, 0.0718], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19420795142650604 tensor([0.0131, 0.0565, 0.3591, 0.1942, 0.3770], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22254788875579834 tensor([0.0394, 0.1189, 0.2225, 0.2742, 0.3449], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1770360916852951 tensor([0.0403, 0.1207, 0.1770, 0.2375, 0.4245], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18603159487247467 tensor([0.1799, 0.2632, 0.0442, 0.3266, 0.1860], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19122663140296936 tensor([0.3726, 0.3646, 0.0065, 0.1912, 0.0650], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1821301132440567 tensor([0.0099, 0.0463, 0.4253, 0.1821, 0.3364], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19249285757541656 tensor([0.0407, 0.1301, 0.1925, 0.2766, 0.3601], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20184576511383057 tensor([0.1200, 0.2018, 0.0620, 0.3593, 0.2569], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21511149406433105 tensor([0.2151, 0.3434, 0.0331, 0.2552, 0.1532], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18644948303699493 tensor([0.1864, 0.3085, 0.0386, 0.3009, 0.1655], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06351280212402344 tensor([0.0018, 0.0150, 0.6186, 0.0635, 0.3011], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19829638302326202 tensor([0.3397, 0.1983, 0.0082, 0.3932, 0.0606], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18910925090312958 tensor([0.0132, 0.0557, 0.3444, 0.1891, 0.3976], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 2], [0, 2], [2, 4, 0], [0, 2], [2, 4, 1], [2, 0], [2, 4], [2, 4, 3], [0, 1], [2, 4, 1], [2, 4], [2, 0], [2, 4], [0, 2, 1], [2, 0, 1], [0, 2, 1], [2, 0], [2, 4], [0, 1], [0, 1], [0, 1], [2, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0], [0, 1, 2], [2, 4], [2, 4, 3], [0, 1, 3], [2, 0, 1], [2, 0], [2, 0], [2, 4, 3], [0, 1, 3], [0, 2, 1], [0, 2], [2, 0], [0, 2], [0, 1, 2], [2, 0], [0, 2, 1], [0, 2, 1], [2, 4, 3], [0, 1, 3], [0, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4, 1], [2, 4, 0], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4], [2, 0], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 0], [0, 1, 3], [2, 4, 0], [2, 0], [2, 0], [2, 4, 3], [0, 1, 2], [2, 1, 4], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 1, 3], [0, 1, 2], [0, 1], [2, 0], [2, 4], [0, 1, 3], [2, 1, 0], [2, 4], [2, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [2, 0], [2, 4], [2, 4], [0, 1, 3], [2, 4], [2, 0], [0, 2], [2, 4, 3], [0, 1], [2, 4, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 1, 4], [2, 0], [2, 0], [2, 4, 3], [0, 1, 3], [2, 0], [0, 2], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 0, 4], [2, 4], [0, 1, 3], [2, 1, 4], [2, 0, 1], [2, 4], [2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 2], [2, 0, 4], [2, 0], [0, 1], [0, 2, 1], [0, 1], [2, 4], [2, 4, 3], [0, 1], [2, 0, 1], [0, 1], [2, 4], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 0], [2, 4], [2, 4], [0, 1, 3], [2, 4, 0], [0, 1, 2], [2, 4, 3], [2, 4, 3], [0, 1], [2, 4], [0, 2, 1], [2, 4], [2, 4, 3], [0, 2, 1], [2, 4, 1], [0, 1, 3], [2, 4], [2, 4], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 0], [2, 0], [2, 4, 0], [0, 1], [2, 4, 1], [2, 0], [2, 4], [2, 4, 0], [0, 1], [2, 4, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1], [0, 1, 3], [2, 0], [2, 4], [2, 4], [0, 1, 3], [2, 0, 4], [0, 1, 2], [2, 4], [2, 4, 3], [0, 1], [2, 4, 1], [2, 4, 0], [0, 2, 1], [2, 0], [0, 1, 3], [2, 4, 1], [0, 1, 2], [2, 0], [2, 4, 3], [0, 1], [2, 4, 1], [2, 4], [2, 0, 4], [2, 0, 4], [0, 1, 3], [2, 0, 1], [2, 0, 4], [2, 4, 3], [2, 4, 3], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4], [0, 1, 3], [0, 1], [0, 1, 2], [2, 0, 4], [2, 4, 3], [0, 1, 3], [0, 1, 2], [2, 0], [2, 4], [2, 4, 0], [0, 1, 3], [2, 4, 1], [0, 1, 3]]\n",
      "[[0, 1, 3], [0, 1], [2, 4], [0, 1], [2, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 1], [2, 4], [0, 3], [3, 4], [1, 3, 4], [1, 3], [1, 3, 4], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1], [2, 3, 4], [0, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [3, 4], [3, 4], [3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1], [2, 4], [3, 4], [3, 4], [0, 1, 3], [0, 1], [2, 4], [3, 4], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [3, 4], [1, 3, 4], [3, 4], [3, 4], [0, 1], [2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3], [1, 3], [0, 1, 3], [0, 1], [2, 4], [0, 3], [0, 1, 3], [1, 3, 4], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [1, 3, 4], [2, 4], [1, 3], [1, 3, 4], [1, 3, 4], [0, 1], [3, 4], [0, 3], [2, 4], [0, 1], [1, 3], [2, 4], [3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [2, 4], [3, 4], [0, 1, 3], [1, 3, 4], [0, 1], [2, 4], [3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1], [2, 3, 4], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3], [2, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [2, 4], [1, 3], [0, 1, 3], [2, 4], [0, 3], [3, 4], [0, 1, 3], [1, 3], [2, 4], [3, 4], [1, 3, 4], [1, 3], [1, 3, 4], [2, 3, 4], [3, 4], [2, 3, 4], [0, 1, 3], [0, 1], [2, 3, 4], [3, 4], [2, 3, 4], [0, 1, 3], [2, 4], [2, 4], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [1, 3], [3, 4], [0, 1], [0, 1], [2, 3, 4], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1], [3, 4], [0, 3], [2, 4], [0, 1, 3], [0, 1, 3], [2, 4], [3, 4], [2, 4], [0, 1, 3], [0, 1], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [0, 3], [1, 3, 4], [1, 3, 4], [1, 3], [2, 3, 4], [0, 3], [1, 3, 4], [0, 1, 3], [1, 3], [2, 3, 4], [0, 3], [2, 4], [0, 1, 3], [0, 1], [2, 3, 4], [2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [1, 3], [3, 4], [0, 1, 3], [0, 1], [2, 3, 4], [0, 3], [1, 3], [3, 4], [1, 3, 4], [2, 4], [0, 3], [3, 4], [1, 3, 4], [0, 1], [2, 3, 4], [0, 3], [0, 1, 3], [1, 3], [1, 3], [2, 4], [3, 4], [1, 3], [0, 1], [0, 1], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [2, 3, 4], [3, 4], [1, 3], [0, 1], [2, 4], [3, 4], [1, 3, 4], [0, 1, 3], [1, 3], [2, 4], [0, 3], [2, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 2], [2, 4, 0], [2, 4, 1], [2, 4, 3], [2, 4, 1], [0, 2, 1], [2, 0, 1], [0, 2, 1], [2, 4, 3], [0, 1, 3], [2, 1, 0], [0, 1, 2], [2, 4, 3], [0, 1, 3], [2, 0, 1], [2, 4, 3], [0, 1, 3], [0, 2, 1], [0, 1, 2], [0, 2, 1], [0, 2, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4, 0], [2, 4, 3], [0, 1, 2], [2, 1, 4], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 3], [2, 4, 1], [2, 4, 1], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 1, 4], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 0, 4], [0, 1, 3], [2, 1, 4], [2, 0, 1], [2, 4, 0], [0, 1, 3], [2, 0, 1], [2, 0, 4], [0, 2, 1], [2, 4, 3], [2, 0, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 4, 0], [0, 1, 2], [2, 4, 3], [2, 4, 3], [0, 2, 1], [2, 4, 3], [0, 2, 1], [2, 4, 1], [0, 1, 3], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 0], [2, 4, 1], [2, 4, 0], [2, 4, 1], [0, 1, 3], [2, 4, 3], [0, 1, 3], [0, 1, 3], [2, 0, 4], [0, 1, 2], [2, 4, 3], [2, 4, 1], [2, 4, 0], [0, 2, 1], [0, 1, 3], [2, 4, 1], [0, 1, 2], [2, 4, 3], [2, 4, 1], [2, 0, 4], [2, 0, 4], [0, 1, 3], [2, 0, 1], [2, 0, 4], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [2, 0, 4], [2, 4, 3], [0, 1, 3], [0, 1, 2], [2, 4, 0], [0, 1, 3], [2, 4, 1], [0, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.008538341685517193  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.008538119596977756  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.008537695832448462  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.008537094886988809  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.00853633309063846  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.008535431672448981  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.008534407778962017  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.008533273657707318  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.008532047271728516  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00853073923555139  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.008529363429709657  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00852792883572513  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.008526443618617645  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.008524919209414965  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.008523361323630973  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.008521777309783518  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.008520170433880531  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.008518547227937882  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.008516915040473416  Accuracy on Support set:0.0\n",
      "torch.Size([146, 2048]) torch.Size([146])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.008515276320993083  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.21755355596542358 tensor([0.2176, 0.2925, 0.0309, 0.2873, 0.1718], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.41329824924468994 tensor([0.4207, 0.4133, 0.0037, 0.1217, 0.0407], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.33661529421806335 tensor([0.0129, 0.0650, 0.4189, 0.1666, 0.3366], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2407914102077484 tensor([0.5855, 0.2408, 0.0011, 0.1521, 0.0204], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3101191818714142 tensor([0.0124, 0.0581, 0.3101, 0.1424, 0.4770], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2718084156513214 tensor([0.3402, 0.2718, 0.0113, 0.2778, 0.0988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3213507831096649 tensor([0.3214, 0.4478, 0.0088, 0.1508, 0.0713], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31529465317726135 tensor([0.0199, 0.0972, 0.3153, 0.1672, 0.4004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.238766148686409 tensor([0.2388, 0.1683, 0.0180, 0.4766, 0.0982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3344666063785553 tensor([0.0130, 0.0606, 0.3345, 0.1456, 0.4463], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3219417631626129 tensor([0.5092, 0.3219, 0.0022, 0.1366, 0.0301], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.41244611144065857 tensor([0.4269, 0.4124, 0.0031, 0.1200, 0.0375], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.348394513130188 tensor([0.0176, 0.0968, 0.3484, 0.1536, 0.3837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3154044449329376 tensor([0.0462, 0.0911, 0.1651, 0.3822, 0.3154], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2718586325645447 tensor([0.0173, 0.0830, 0.2719, 0.1442, 0.4836], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2616838812828064 tensor([0.5096, 0.2617, 0.0026, 0.1921, 0.0341], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3284244239330292 tensor([0.3284, 0.4585, 0.0075, 0.1392, 0.0664], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2604033350944519 tensor([0.0012, 0.0112, 0.6736, 0.0536, 0.2604], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2428443878889084 tensor([0.2428, 0.1668, 0.0170, 0.4641, 0.1091], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28322967886924744 tensor([0.0469, 0.1211, 0.1688, 0.2832, 0.3799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2071569263935089 tensor([0.0861, 0.2072, 0.1112, 0.2717, 0.3239], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24643604457378387 tensor([0.1721, 0.2464, 0.0410, 0.3563, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23193342983722687 tensor([0.0870, 0.2319, 0.1160, 0.2595, 0.3055], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3658154308795929 tensor([0.3787, 0.1964, 0.0058, 0.3658, 0.0533], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23802126944065094 tensor([0.1422, 0.2380, 0.0562, 0.3240, 0.2395], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2104540318250656 tensor([0.2105, 0.3350, 0.0363, 0.2616, 0.1567], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22335486114025116 tensor([0.2234, 0.5040, 0.0179, 0.1378, 0.1170], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22034214437007904 tensor([0.0154, 0.0571, 0.3400, 0.2203, 0.3672], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3570559620857239 tensor([0.3571, 0.1734, 0.0067, 0.4032, 0.0597], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2509174644947052 tensor([0.2822, 0.3359, 0.0177, 0.2509, 0.1132], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23464475572109222 tensor([0.1224, 0.2940, 0.0773, 0.2346, 0.2716], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2505493462085724 tensor([0.3155, 0.3295, 0.0121, 0.2505, 0.0923], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26079654693603516 tensor([0.0573, 0.1693, 0.1730, 0.2608, 0.3396], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21268849074840546 tensor([0.1170, 0.1473, 0.0656, 0.4574, 0.2127], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.316286563873291 tensor([0.0910, 0.1731, 0.0917, 0.3279, 0.3163], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2178838551044464 tensor([0.1107, 0.2179, 0.0862, 0.3182, 0.2670], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22656835615634918 tensor([0.2266, 0.3753, 0.0276, 0.2313, 0.1393], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18885408341884613 tensor([0.0244, 0.1069, 0.2627, 0.1889, 0.4172], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2764846384525299 tensor([0.0151, 0.0470, 0.3357, 0.2765, 0.3257], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18339113891124725 tensor([0.0196, 0.0790, 0.2660, 0.1834, 0.4520], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24589866399765015 tensor([0.2918, 0.3335, 0.0160, 0.2459, 0.1129], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2892986834049225 tensor([0.2893, 0.4546, 0.0133, 0.1513, 0.0915], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3534049391746521 tensor([0.0063, 0.0374, 0.4882, 0.1148, 0.3534], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1773862987756729 tensor([0.1454, 0.1456, 0.0456, 0.4860, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2981364130973816 tensor([0.0472, 0.1088, 0.1508, 0.2981, 0.3951], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2681787610054016 tensor([0.2682, 0.2918, 0.0193, 0.2835, 0.1372], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.38494741916656494 tensor([0.3849, 0.4296, 0.0046, 0.1296, 0.0513], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3123573362827301 tensor([0.0048, 0.0317, 0.5458, 0.1054, 0.3124], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21728989481925964 tensor([0.1124, 0.1352, 0.0629, 0.4721, 0.2173], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22602298855781555 tensor([0.1026, 0.2260, 0.0951, 0.3108, 0.2654], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21450236439704895 tensor([0.1226, 0.2145, 0.0803, 0.3255, 0.2571], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2870006561279297 tensor([0.2870, 0.4540, 0.0130, 0.1571, 0.0890], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3140544295310974 tensor([0.0044, 0.0276, 0.5521, 0.1018, 0.3141], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2532925009727478 tensor([0.0793, 0.1152, 0.0950, 0.4572, 0.2533], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22388872504234314 tensor([0.0973, 0.2239, 0.0933, 0.2865, 0.2990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2395816594362259 tensor([0.1257, 0.2465, 0.0711, 0.3171, 0.2396], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22336861491203308 tensor([0.0782, 0.2470, 0.1226, 0.2234, 0.3288], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3061099648475647 tensor([0.0432, 0.1204, 0.2004, 0.3061, 0.3299], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25384750962257385 tensor([0.1218, 0.2568, 0.0739, 0.2937, 0.2538], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21201474964618683 tensor([0.0441, 0.1421, 0.1646, 0.2120, 0.4372], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2739700376987457 tensor([0.0658, 0.1903, 0.1537, 0.2740, 0.3162], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2644834518432617 tensor([0.2645, 0.4622, 0.0141, 0.1619, 0.0973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3233734667301178 tensor([0.0039, 0.0297, 0.5518, 0.0913, 0.3234], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19671939313411713 tensor([0.0208, 0.0895, 0.3273, 0.1967, 0.3657], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2561752498149872 tensor([0.2989, 0.3209, 0.0156, 0.2562, 0.1084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2140093296766281 tensor([0.2619, 0.2140, 0.0178, 0.3983, 0.1080], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23293867707252502 tensor([0.2703, 0.3541, 0.0190, 0.2329, 0.1236], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25159960985183716 tensor([0.0291, 0.0908, 0.2516, 0.2678, 0.3607], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20680852234363556 tensor([0.2068, 0.1824, 0.0259, 0.4609, 0.1240], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2805362939834595 tensor([0.1657, 0.3179, 0.0537, 0.2805, 0.1822], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20666281878948212 tensor([0.4567, 0.2883, 0.0040, 0.2067, 0.0443], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32651636004447937 tensor([0.5579, 0.3265, 0.0010, 0.0971, 0.0175], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36906692385673523 tensor([0.0065, 0.0488, 0.4846, 0.0911, 0.3691], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.184869647026062 tensor([0.1849, 0.1770, 0.0305, 0.4430, 0.1647], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23100553452968597 tensor([0.3560, 0.3257, 0.0091, 0.2310, 0.0783], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22828999161720276 tensor([0.1484, 0.2416, 0.0632, 0.3185, 0.2283], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31571003794670105 tensor([0.3157, 0.5152, 0.0063, 0.1042, 0.0587], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2852417230606079 tensor([0.0018, 0.0135, 0.6295, 0.0700, 0.2852], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2608312666416168 tensor([0.0591, 0.0843, 0.1121, 0.4837, 0.2608], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33708539605140686 tensor([0.0128, 0.0717, 0.3371, 0.1208, 0.4576], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3372766673564911 tensor([0.4008, 0.3373, 0.0064, 0.1925, 0.0631], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25559213757514954 tensor([0.1101, 0.2828, 0.0864, 0.2652, 0.2556], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3550429940223694 tensor([0.0056, 0.0433, 0.5059, 0.0902, 0.3550], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17960062623023987 tensor([0.1629, 0.1796, 0.0397, 0.4476, 0.1702], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22478047013282776 tensor([0.1205, 0.2248, 0.0754, 0.3137, 0.2656], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23161837458610535 tensor([0.0998, 0.2316, 0.0890, 0.2545, 0.3251], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36781400442123413 tensor([0.4242, 0.3678, 0.0041, 0.1597, 0.0441], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3298317790031433 tensor([0.0504, 0.1208, 0.1680, 0.3309, 0.3298], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1448822170495987 tensor([0.1449, 0.1300, 0.0384, 0.5309, 0.1558], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33815547823905945 tensor([0.0127, 0.0686, 0.3382, 0.1413, 0.4393], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25281551480293274 tensor([0.2528, 0.4224, 0.0183, 0.1847, 0.1218], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23352758586406708 tensor([0.1739, 0.3466, 0.0424, 0.2335, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28597694635391235 tensor([0.0029, 0.0292, 0.6190, 0.0628, 0.2860], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31862080097198486 tensor([0.0544, 0.1159, 0.1585, 0.3526, 0.3186], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19443826377391815 tensor([0.0276, 0.1031, 0.2445, 0.1944, 0.4304], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24889543652534485 tensor([0.1310, 0.2596, 0.0687, 0.2918, 0.2489], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.192709282040596 tensor([0.1927, 0.3966, 0.0332, 0.2052, 0.1723], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25676608085632324 tensor([0.0017, 0.0164, 0.6729, 0.0522, 0.2568], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.187511146068573 tensor([0.0977, 0.0934, 0.0614, 0.5600, 0.1875], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2774165868759155 tensor([0.3038, 0.2774, 0.0155, 0.2984, 0.1049], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2051367610692978 tensor([0.1163, 0.2051, 0.0727, 0.3531, 0.2528], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24971431493759155 tensor([0.2497, 0.5045, 0.0134, 0.1326, 0.0997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2557866871356964 tensor([0.0011, 0.0119, 0.6876, 0.0436, 0.2558], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22547855973243713 tensor([0.1126, 0.1495, 0.0688, 0.4437, 0.2255], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20379023253917694 tensor([0.1182, 0.2038, 0.0763, 0.3428, 0.2589], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2262394279241562 tensor([0.2262, 0.3175, 0.0288, 0.2605, 0.1670], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19243045151233673 tensor([0.1924, 0.4133, 0.0320, 0.1938, 0.1686], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.39026498794555664 tensor([0.0116, 0.0669, 0.3903, 0.1301, 0.4012], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21081969141960144 tensor([0.2210, 0.2108, 0.0250, 0.4108, 0.1323], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22289235889911652 tensor([0.1162, 0.3015, 0.0807, 0.2229, 0.2786], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21413108706474304 tensor([0.0830, 0.2141, 0.1155, 0.2752, 0.3121], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29668915271759033 tensor([0.2967, 0.4546, 0.0101, 0.1536, 0.0850], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.190168097615242 tensor([0.0171, 0.0739, 0.3438, 0.1902, 0.3749], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20993614196777344 tensor([0.2099, 0.1328, 0.0204, 0.5270, 0.1098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22803202271461487 tensor([0.1260, 0.2280, 0.0655, 0.3182, 0.2623], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2368539720773697 tensor([0.2369, 0.2769, 0.0283, 0.3165, 0.1415], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21694660186767578 tensor([0.2399, 0.3768, 0.0255, 0.2169, 0.1409], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2522694766521454 tensor([0.0240, 0.0756, 0.2616, 0.2523, 0.3865], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1837923377752304 tensor([0.1838, 0.1429, 0.0310, 0.5041, 0.1382], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35779839754104614 tensor([0.0108, 0.0507, 0.3578, 0.1497, 0.4310], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24947556853294373 tensor([0.2495, 0.2719, 0.0234, 0.3061, 0.1491], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28552377223968506 tensor([0.2855, 0.4119, 0.0130, 0.1923, 0.0973], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38064685463905334 tensor([0.0135, 0.0847, 0.3975, 0.1235, 0.3806], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1557185798883438 tensor([0.1557, 0.1358, 0.0360, 0.5169, 0.1557], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2152484655380249 tensor([0.1255, 0.2152, 0.0685, 0.3162, 0.2746], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23374216258525848 tensor([0.1384, 0.3094, 0.0699, 0.2337, 0.2486], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30367642641067505 tensor([0.3037, 0.4712, 0.0083, 0.1371, 0.0797], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35588890314102173 tensor([0.0093, 0.0542, 0.4380, 0.1426, 0.3559], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22095328569412231 tensor([0.1231, 0.2210, 0.0763, 0.3365, 0.2431], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21247854828834534 tensor([0.0907, 0.2125, 0.0988, 0.3020, 0.2960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20570732653141022 tensor([0.3499, 0.3558, 0.0085, 0.2057, 0.0801], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3026520013809204 tensor([0.3027, 0.4321, 0.0116, 0.1674, 0.0862], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24264168739318848 tensor([0.0015, 0.0126, 0.6785, 0.0647, 0.2426], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29289671778678894 tensor([0.2929, 0.1685, 0.0110, 0.4471, 0.0805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4182627201080322 tensor([0.0055, 0.0317, 0.4375, 0.1071, 0.4183], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23674257099628448 tensor([0.1703, 0.2367, 0.0488, 0.3385, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20247679948806763 tensor([0.2025, 0.3912, 0.0300, 0.2076, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3219234347343445 tensor([0.0023, 0.0194, 0.5914, 0.0650, 0.3219], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15120267868041992 tensor([0.1512, 0.1433, 0.0414, 0.4967, 0.1673], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2516584098339081 tensor([0.1228, 0.1829, 0.0664, 0.3763, 0.2517], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2610330581665039 tensor([0.3665, 0.2610, 0.0092, 0.2933, 0.0700], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20356300473213196 tensor([0.1917, 0.4226, 0.0314, 0.2036, 0.1507], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3611253798007965 tensor([0.0039, 0.0252, 0.5145, 0.0952, 0.3611], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21898050606250763 tensor([0.1070, 0.1381, 0.0734, 0.4625, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21160370111465454 tensor([0.0970, 0.2116, 0.0930, 0.2827, 0.3157], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2603571116924286 tensor([0.1569, 0.2604, 0.0522, 0.3165, 0.2140], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23085804283618927 tensor([0.1363, 0.2426, 0.0689, 0.3213, 0.2309], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1932009607553482 tensor([0.0253, 0.1117, 0.3065, 0.1932, 0.3633], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24381966888904572 tensor([0.0751, 0.0931, 0.0854, 0.5026, 0.2438], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24712616205215454 tensor([0.0270, 0.0908, 0.2524, 0.2471, 0.3827], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2743593156337738 tensor([0.3177, 0.2744, 0.0134, 0.3035, 0.0911], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.379640132188797 tensor([0.4323, 0.3796, 0.0034, 0.1426, 0.0421], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2547354996204376 tensor([0.0279, 0.0933, 0.2547, 0.2597, 0.3644], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21645565330982208 tensor([0.1370, 0.1940, 0.0574, 0.3951, 0.2165], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21837933361530304 tensor([0.0218, 0.0774, 0.2560, 0.2184, 0.4264], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2244061827659607 tensor([0.4049, 0.3049, 0.0062, 0.2244, 0.0596], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2092953473329544 tensor([0.0363, 0.1849, 0.2093, 0.1622, 0.4073], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3636638820171356 tensor([0.0055, 0.0364, 0.4896, 0.1049, 0.3637], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21807196736335754 tensor([0.2181, 0.1472, 0.0200, 0.4951, 0.1197], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26144856214523315 tensor([0.1304, 0.2614, 0.0645, 0.2631, 0.2806], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27302855253219604 tensor([0.2842, 0.3085, 0.0174, 0.2730, 0.1168], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19539311528205872 tensor([0.2207, 0.4227, 0.0258, 0.1954, 0.1355], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3140348792076111 tensor([0.0053, 0.0358, 0.5408, 0.1041, 0.3140], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21394596993923187 tensor([0.1783, 0.2139, 0.0376, 0.3902, 0.1800], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22506165504455566 tensor([0.0389, 0.1209, 0.1894, 0.2251, 0.4257], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2988201677799225 tensor([0.5111, 0.2988, 0.0023, 0.1559, 0.0318], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.42112302780151367 tensor([0.4211, 0.4438, 0.0028, 0.0940, 0.0383], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2640184164047241 tensor([0.0192, 0.0622, 0.3005, 0.2640, 0.3541], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23013435304164886 tensor([0.2301, 0.2823, 0.0293, 0.3150, 0.1432], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30840006470680237 tensor([0.0613, 0.1339, 0.1287, 0.3084, 0.3677], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2707158923149109 tensor([0.3755, 0.2722, 0.0083, 0.2707, 0.0733], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3792288601398468 tensor([0.3792, 0.4539, 0.0041, 0.1163, 0.0464], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27374061942100525 tensor([0.0726, 0.1675, 0.1507, 0.3354, 0.2737], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19185832142829895 tensor([0.1919, 0.1719, 0.0316, 0.4597, 0.1451], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2707502841949463 tensor([0.0181, 0.0823, 0.2708, 0.1549, 0.4739], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20663562417030334 tensor([0.2066, 0.3090, 0.0351, 0.2847, 0.1645], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23742152750492096 tensor([0.2374, 0.2939, 0.0274, 0.2995, 0.1418], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24061831831932068 tensor([0.0011, 0.0113, 0.7013, 0.0457, 0.2406], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21898292005062103 tensor([0.1179, 0.1908, 0.0804, 0.3920, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.38451600074768066 tensor([0.0095, 0.0600, 0.3845, 0.1123, 0.4336], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23577113449573517 tensor([0.2358, 0.2444, 0.0254, 0.3395, 0.1549], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22070550918579102 tensor([0.2207, 0.5276, 0.0174, 0.1247, 0.1096], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24630242586135864 tensor([0.0359, 0.1206, 0.2463, 0.2686, 0.3286], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20423100888729095 tensor([0.2042, 0.2579, 0.0338, 0.3408, 0.1633], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21308642625808716 tensor([0.1492, 0.2131, 0.0560, 0.3390, 0.2428], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22281576693058014 tensor([0.1363, 0.3359, 0.0663, 0.2387, 0.2228], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31162145733833313 tensor([0.3116, 0.4585, 0.0085, 0.1460, 0.0753], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3044406771659851 tensor([0.0035, 0.0268, 0.5858, 0.0795, 0.3044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20106182992458344 tensor([0.2011, 0.1764, 0.0271, 0.4644, 0.1311], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23756326735019684 tensor([0.1077, 0.2739, 0.0857, 0.2376, 0.2952], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2573886513710022 tensor([0.1111, 0.2609, 0.0831, 0.2574, 0.2875], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23304322361946106 tensor([0.1855, 0.3778, 0.0415, 0.2330, 0.1622], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26428890228271484 tensor([0.0258, 0.0790, 0.2643, 0.2786, 0.3524], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16844335198402405 tensor([0.1684, 0.1681, 0.0374, 0.4513, 0.1748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24216881394386292 tensor([0.1148, 0.2422, 0.0843, 0.3066, 0.2521], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19846102595329285 tensor([0.3657, 0.3611, 0.0074, 0.1985, 0.0674], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2802506685256958 tensor([0.1844, 0.3160, 0.0415, 0.2803, 0.1779], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1977619230747223 tensor([0.0129, 0.0541, 0.3445, 0.1978, 0.3907], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2189488708972931 tensor([0.2189, 0.1978, 0.0244, 0.4300, 0.1289], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32540586590766907 tensor([0.0109, 0.0522, 0.3254, 0.1431, 0.4685], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20366737246513367 tensor([0.2037, 0.3419, 0.0364, 0.2405, 0.1776], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3223664462566376 tensor([0.3224, 0.4918, 0.0063, 0.1164, 0.0631], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22180625796318054 tensor([0.0340, 0.1048, 0.2218, 0.2781, 0.3613], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3468100130558014 tensor([0.0125, 0.0688, 0.3468, 0.1395, 0.4324], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22160938382148743 tensor([0.1170, 0.2216, 0.0714, 0.3022, 0.2877], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25302043557167053 tensor([0.3062, 0.3231, 0.0150, 0.2530, 0.1027], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2297302484512329 tensor([0.3262, 0.3467, 0.0102, 0.2297, 0.0872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3484558165073395 tensor([0.0213, 0.1179, 0.3587, 0.1536, 0.3485], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1713034212589264 tensor([0.1463, 0.1713, 0.0549, 0.4468, 0.1806], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1782744973897934 tensor([0.0372, 0.1411, 0.1810, 0.1783, 0.4625], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2405029982328415 tensor([0.3933, 0.3062, 0.0062, 0.2405, 0.0538], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.42579442262649536 tensor([0.4561, 0.4258, 0.0018, 0.0898, 0.0265], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23660802841186523 tensor([0.0312, 0.1038, 0.2366, 0.2385, 0.3899], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24560071527957916 tensor([0.2456, 0.1328, 0.0137, 0.5203, 0.0876], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23960541188716888 tensor([0.1776, 0.2396, 0.0453, 0.3434, 0.1940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2541047930717468 tensor([0.0941, 0.1931, 0.1019, 0.3568, 0.2541], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20519597828388214 tensor([0.1256, 0.3533, 0.0684, 0.2052, 0.2475], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.312562495470047 tensor([0.0022, 0.0221, 0.6058, 0.0573, 0.3126], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33489301800727844 tensor([0.3349, 0.1562, 0.0073, 0.4362, 0.0653], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2375432401895523 tensor([0.0382, 0.1279, 0.2075, 0.2375, 0.3889], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22941485047340393 tensor([0.1557, 0.2548, 0.0507, 0.3094, 0.2294], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3724173903465271 tensor([0.3724, 0.3727, 0.0070, 0.1862, 0.0616], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18565888702869415 tensor([0.0167, 0.0696, 0.3471, 0.1857, 0.3808], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1815030574798584 tensor([0.1815, 0.1864, 0.0342, 0.4386, 0.1593], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21036715805530548 tensor([0.2104, 0.2334, 0.0320, 0.3572, 0.1671], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2503734230995178 tensor([0.1627, 0.2504, 0.0550, 0.3375, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2634447515010834 tensor([0.1382, 0.2634, 0.0709, 0.3201, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37382978200912476 tensor([0.0080, 0.0487, 0.4557, 0.1138, 0.3738], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23930002748966217 tensor([0.1095, 0.1585, 0.0750, 0.4178, 0.2393], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2441214770078659 tensor([0.1699, 0.2441, 0.0505, 0.3231, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.34780153632164 tensor([0.5079, 0.3478, 0.0017, 0.1150, 0.0275], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2633640766143799 tensor([0.2634, 0.4344, 0.0199, 0.1651, 0.1172], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21778368949890137 tensor([0.0278, 0.1034, 0.2985, 0.2178, 0.3525], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2546190917491913 tensor([0.0178, 0.0576, 0.2991, 0.2546, 0.3709], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24379195272922516 tensor([0.1013, 0.2438, 0.0879, 0.2594, 0.3076], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22600619494915009 tensor([0.3460, 0.3349, 0.0102, 0.2260, 0.0829], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976730078458786 tensor([0.3354, 0.3789, 0.0094, 0.1977, 0.0786], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.377801775932312 tensor([0.0116, 0.0523, 0.3778, 0.1745, 0.3839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23813793063163757 tensor([0.0361, 0.1131, 0.2381, 0.2541, 0.3586], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21918745338916779 tensor([0.0370, 0.1151, 0.1884, 0.2192, 0.4403], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26211580634117126 tensor([0.1727, 0.2621, 0.0491, 0.3150, 0.2012], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3645990192890167 tensor([0.3646, 0.3697, 0.0074, 0.1871, 0.0713], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34070128202438354 tensor([0.0087, 0.0427, 0.4447, 0.1632, 0.3407], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2559579908847809 tensor([0.0372, 0.1237, 0.2071, 0.2560, 0.3761], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19872841238975525 tensor([0.1137, 0.1987, 0.0685, 0.3430, 0.2761], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20705510675907135 tensor([0.2071, 0.3432, 0.0369, 0.2468, 0.1660], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2903892993927002 tensor([0.1790, 0.3079, 0.0430, 0.2904, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29700303077697754 tensor([0.0015, 0.0136, 0.6316, 0.0562, 0.2970], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3335404396057129 tensor([0.3335, 0.2008, 0.0093, 0.3893, 0.0670], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3615098297595978 tensor([0.0117, 0.0517, 0.3615, 0.1703, 0.4048], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 2], [0, 2], [2, 4, 0], [0, 2], [2, 4, 1], [2, 0], [2, 4], [2, 4, 3], [0, 1], [2, 4, 1], [2, 4], [2, 0], [2, 4], [0, 2, 1], [2, 0, 1], [0, 2, 1], [2, 0], [2, 4], [0, 1, 3], [0, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0, 4], [0, 1, 2], [2, 4], [2, 4, 3], [0, 1, 3], [2, 0, 1], [2, 0], [2, 0], [2, 4, 3], [0, 1, 3], [0, 2, 1], [0, 2], [2, 0], [0, 2], [0, 1, 2], [2, 0], [0, 2, 1], [0, 2, 1], [2, 4, 3], [0, 1, 3], [0, 1, 3], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4, 1], [2, 4, 0], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4], [2, 0], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 0], [0, 1, 3], [2, 4, 0, 1], [2, 0], [2, 0], [2, 4, 3], [0, 1, 2], [2, 1, 4, 0], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 0], [2, 4, 0], [0, 1, 3], [2, 1, 0, 4], [2, 4], [2, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [2, 0], [2, 4], [2, 4, 0], [0, 1, 3], [2, 4], [2, 0], [0, 2], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 0], [2, 4], [2, 4], [0, 1], [2, 4, 1, 0], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 1, 4, 0], [2, 0], [2, 0], [2, 4, 3], [0, 1, 3], [2, 0], [0, 2], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 0, 4], [2, 4], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 4], [2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 2], [2, 0, 4], [2, 0], [0, 1, 3], [0, 2, 1], [0, 1], [2, 4], [2, 4, 3], [0, 1], [2, 0, 1], [0, 1], [2, 4], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 0], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 0], [0, 1, 2], [2, 4, 3], [2, 4, 3], [0, 1], [2, 4], [0, 2, 1], [2, 4], [2, 4, 3], [0, 2, 1], [2, 4, 1, 0], [0, 1, 3], [2, 4], [2, 4], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1], [2, 4], [2, 0], [2, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 0], [2, 0], [2, 4, 0], [0, 1], [2, 4, 1, 0], [2, 0], [2, 4, 3], [2, 4, 0], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1], [0, 1, 3], [2, 0], [2, 4], [2, 4], [0, 1, 3], [2, 0, 4, 1], [0, 1, 2, 3], [2, 4], [2, 4, 3], [0, 1], [2, 4, 1], [2, 4, 0], [0, 2, 1], [2, 0], [0, 1, 3], [2, 4, 1], [0, 1, 2], [2, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4], [2, 0, 4], [2, 0, 4], [0, 1, 3], [2, 0, 1], [2, 0, 4], [2, 4, 3], [2, 4, 3], [0, 1], [0, 1], [2, 0], [2, 4], [2, 4, 3], [0, 1, 3], [0, 1], [0, 1, 2], [2, 0, 4], [2, 4, 3], [0, 1, 3], [0, 1, 2], [2, 0, 1], [2, 4], [2, 4, 0], [0, 1, 3], [2, 4, 1], [0, 1, 3]]\n",
      "[[0, 1, 3], [0, 1], [2, 4], [0, 1], [2, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 1], [2, 4], [0, 3], [3, 4], [1, 3, 4], [1, 3], [1, 3, 4], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1], [2, 3, 4], [0, 3], [0, 1, 3], [1, 3, 4], [0, 1, 3], [3, 4], [3, 4], [3, 4], [1, 3, 4], [0, 1, 3], [2, 4], [2, 3, 4], [2, 4], [0, 1, 3], [0, 1], [2, 4], [3], [3, 4], [0, 1, 3], [0, 1], [2, 4], [3, 4], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [3, 4], [1, 3, 4], [3, 4], [3, 4], [0, 1], [2, 4], [2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3], [1, 3], [0, 1, 3], [0, 1], [2, 4], [3], [0, 1, 3], [1, 3, 4], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [1, 3, 4], [2, 4], [3], [1, 3, 4], [1, 3, 4], [0, 1], [3, 4], [3], [2, 4], [0, 1], [1, 3], [2, 4], [3, 4], [2, 4], [1, 3, 4], [1, 3], [2, 4], [3], [0, 1, 3], [1, 3, 4], [0, 1], [2, 4], [3, 4], [1, 3, 4], [0, 1, 3], [1, 3], [2, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [3], [2, 4], [0, 1, 3], [0, 1], [2, 4], [3], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [2, 4], [1, 3], [0, 1, 3], [2, 4], [3], [3, 4], [0, 1, 3], [1, 3], [2, 4], [3, 4], [1, 3, 4], [1, 3], [1, 3, 4], [2, 4], [3, 4], [2, 3, 4], [0, 1, 3], [0, 1], [2, 3, 4], [3, 4], [2, 3, 4], [0, 1, 3], [2, 4], [2, 4], [0, 3], [1, 3, 4], [0, 1, 3], [0, 1], [2, 4], [1, 3], [3, 4], [0, 1], [0, 1], [2, 3, 4], [0, 1, 3], [3, 4], [0, 1, 3], [0, 1], [3, 4], [3], [2, 4], [0, 1, 3], [0, 1, 3], [2, 4], [3, 4], [2, 4], [0, 1, 3], [0, 1], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [0, 1], [2, 4], [0, 3], [1, 3, 4], [1, 3, 4], [1, 3], [2, 3, 4], [3], [1, 3, 4], [0, 1], [1, 3], [2, 4], [0, 3], [2, 4], [0, 1, 3], [0, 1], [2, 3, 4], [2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [3], [4], [0, 1, 3], [0, 1], [2, 3, 4], [0, 3], [1, 3], [3, 4], [1, 3, 4], [2, 4], [0, 3], [3, 4], [1, 3, 4], [0, 1], [2, 4], [3], [0, 1, 3], [1, 3], [1, 3], [2, 4], [3, 4], [1, 3], [0, 1], [0, 1], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1], [2, 4], [2, 3, 4], [3, 4], [1, 3], [0, 1], [2, 4], [3, 4], [3, 4], [0, 1, 3], [1, 3], [2, 4], [0, 3], [2, 4]]\n",
      "NL_pred of 3th iteration [[0, 1, 3], [0, 1, 3], [2, 1, 0, 4], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0, 1], [2, 1, 4, 0], [0, 1, 3], [2, 4, 0], [2, 1, 0, 4], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [2, 1, 4, 0], [2, 1, 4, 0], [0, 1, 3], [2, 4, 3], [2, 4, 1, 0], [2, 4, 1, 0], [2, 4, 3], [0, 1, 3], [2, 0, 4, 1], [0, 1, 2, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 3], [2, 0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.047418435414632164  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.04740196687203867  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.04737078702008283  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.04732655595850061  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.047270889635439274  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.04720526271396213  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.04713104389331959  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.04704951798474347  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.046961832929540565  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.046869083687111186  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.046772254837883845  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.046672295641016076  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.046569890446133085  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.046465811906037505  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.046360916561550565  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.04625574747721354  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.046150909529791936  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.04604666762881809  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.04594356483883328  Accuracy on Support set:0.0\n",
      "torch.Size([27, 2048]) torch.Size([27])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.04584189256032308  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.14471347630023956 tensor([0.1447, 0.2763, 0.0584, 0.2732, 0.2474], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3333779275417328 tensor([0.3334, 0.4585, 0.0076, 0.1343, 0.0662], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32107916474342346 tensor([0.0057, 0.0406, 0.5285, 0.1041, 0.3211], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28672558069229126 tensor([0.4906, 0.2867, 0.0026, 0.1833, 0.0368], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4003465175628662 tensor([0.0060, 0.0383, 0.4003, 0.0955, 0.4598], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24734559655189514 tensor([0.2473, 0.2816, 0.0236, 0.2907, 0.1568], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23827408254146576 tensor([0.2383, 0.4705, 0.0183, 0.1593, 0.1136], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3990582525730133 tensor([0.0094, 0.0641, 0.4166, 0.1109, 0.3991], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16592402756214142 tensor([0.1659, 0.1669, 0.0365, 0.4805, 0.1501], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4285838305950165 tensor([0.0062, 0.0397, 0.4286, 0.0964, 0.4290], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3705098628997803 tensor([0.4144, 0.3705, 0.0049, 0.1582, 0.0520], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3363225758075714 tensor([0.3363, 0.4606, 0.0067, 0.1338, 0.0626], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3759922981262207 tensor([0.0080, 0.0622, 0.4543, 0.0995, 0.3760], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28864386677742004 tensor([0.0241, 0.0679, 0.2546, 0.2886, 0.3648], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3580760657787323 tensor([0.0087, 0.0566, 0.3581, 0.1004, 0.4762], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29951614141464233 tensor([0.4128, 0.2995, 0.0058, 0.2227, 0.0591], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24514468014240265 tensor([0.2451, 0.4844, 0.0157, 0.1483, 0.1064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2231208235025406 tensor([5.8701e-04, 7.2429e-03, 7.3540e-01, 3.3651e-02, 2.2312e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16597168147563934 tensor([0.1660, 0.1643, 0.0350, 0.4664, 0.1683], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20879746973514557 tensor([0.0242, 0.0889, 0.2510, 0.2088, 0.4271], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16393746435642242 tensor([0.0478, 0.1639, 0.1787, 0.2162, 0.3933], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22438636422157288 tensor([0.1089, 0.2244, 0.0775, 0.3281, 0.2612], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1845618188381195 tensor([0.0487, 0.1846, 0.1870, 0.2074, 0.3724], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28267231583595276 tensor([0.2827, 0.2117, 0.0130, 0.4027, 0.0900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20741795003414154 tensor([0.0866, 0.2074, 0.0999, 0.2844, 0.3216], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13977818191051483 tensor([0.1398, 0.3164, 0.0690, 0.2488, 0.2259], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15566828846931458 tensor([0.1557, 0.4967, 0.0352, 0.1372, 0.1753], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14356625080108643 tensor([0.0071, 0.0372, 0.4479, 0.1436, 0.3642], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2618730366230011 tensor([0.2619, 0.1843, 0.0151, 0.4387, 0.1001], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19931058585643768 tensor([0.1993, 0.3374, 0.0358, 0.2540, 0.1736], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19933100044727325 tensor([0.0734, 0.2488, 0.1303, 0.1993, 0.3482], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22882407903671265 tensor([0.2288, 0.3398, 0.0252, 0.2608, 0.1454], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19542352855205536 tensor([0.0302, 0.1264, 0.2606, 0.1954, 0.3874], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28464993834495544 tensor([0.0707, 0.1276, 0.1168, 0.4002, 0.2846], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2664797902107239 tensor([0.0514, 0.1394, 0.1504, 0.2665, 0.3923], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1812346875667572 tensor([0.0644, 0.1812, 0.1462, 0.2662, 0.3418], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1527949720621109 tensor([0.1528, 0.3619, 0.0542, 0.2247, 0.2064], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36051636934280396 tensor([0.0116, 0.0717, 0.3605, 0.1274, 0.4289], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18387770652770996 tensor([0.0071, 0.0310, 0.4490, 0.1839, 0.3290], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35779139399528503 tensor([0.0095, 0.0532, 0.3578, 0.1235, 0.4561], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20645976066589355 tensor([0.2065, 0.3365, 0.0325, 0.2501, 0.1744], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21041899919509888 tensor([0.2104, 0.4651, 0.0269, 0.1563, 0.1413], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.317751944065094 tensor([0.0028, 0.0227, 0.5862, 0.0705, 0.3178], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2201646864414215 tensor([0.0242, 0.0799, 0.2275, 0.2202, 0.4482], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1848989725112915 tensor([0.1849, 0.2874, 0.0385, 0.2815, 0.2077], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2943941652774811 tensor([0.2944, 0.4682, 0.0099, 0.1424, 0.0851], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.274666428565979 tensor([0.0021, 0.0188, 0.6418, 0.0627, 0.2747], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2912127375602722 tensor([0.0673, 0.1163, 0.1126, 0.4126, 0.2912], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18730555474758148 tensor([0.0598, 0.1873, 0.1588, 0.2584, 0.3357], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1815759390592575 tensor([0.0731, 0.1816, 0.1367, 0.2770, 0.3317], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2085360437631607 tensor([0.2085, 0.4653, 0.0262, 0.1623, 0.1377], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2757256329059601 tensor([0.0019, 0.0165, 0.6449, 0.0610, 0.2757], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3223418891429901 tensor([0.0447, 0.0937, 0.1621, 0.3772, 0.3223], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1829843968153 tensor([0.0560, 0.1830, 0.1534, 0.2350, 0.3727], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21149860322475433 tensor([0.0757, 0.2115, 0.1238, 0.2740, 0.3150], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17634861171245575 tensor([0.0434, 0.1939, 0.1932, 0.1763, 0.3932], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22398100793361664 tensor([0.0221, 0.0877, 0.2972, 0.2240, 0.3690], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21818706393241882 tensor([0.0730, 0.2182, 0.1267, 0.2522, 0.3299], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15309710800647736 tensor([0.0223, 0.1022, 0.2400, 0.1531, 0.4824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21069276332855225 tensor([0.0354, 0.1456, 0.2379, 0.2107, 0.3703], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18811152875423431 tensor([0.1881, 0.4680, 0.0286, 0.1649, 0.1504], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2818479537963867 tensor([0.0017, 0.0174, 0.6448, 0.0543, 0.2818], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36709627509117126 tensor([0.0095, 0.0584, 0.4360, 0.1289, 0.3671], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2126542329788208 tensor([0.2127, 0.3250, 0.0320, 0.2622, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18389175832271576 tensor([0.1839, 0.2140, 0.0357, 0.4015, 0.1649], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18982374668121338 tensor([0.1898, 0.3521, 0.0377, 0.2334, 0.1869], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18513599038124084 tensor([0.0140, 0.0625, 0.3552, 0.1851, 0.3831], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13875289261341095 tensor([0.1388, 0.1756, 0.0512, 0.4498, 0.1847], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2565034329891205 tensor([0.1065, 0.2889, 0.0971, 0.2565, 0.2511], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23398050665855408 tensor([0.3588, 0.3226, 0.0090, 0.2340, 0.0757], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3899289667606354 tensor([0.4698, 0.3899, 0.0021, 0.1091, 0.0291], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3298744857311249 tensor([0.0030, 0.0301, 0.5793, 0.0578, 0.3299], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24613183736801147 tensor([0.2656, 0.3440, 0.0190, 0.2461, 0.1254], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21201330423355103 tensor([0.0919, 0.2120, 0.1109, 0.2808, 0.3044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23562531173229218 tensor([0.2356, 0.5460, 0.0132, 0.1110, 0.0942], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24350237846374512 tensor([0.0008, 0.0084, 0.7054, 0.0419, 0.2435], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3237587809562683 tensor([0.0326, 0.0668, 0.1864, 0.3904, 0.3238], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4279501140117645 tensor([0.0063, 0.0475, 0.4280, 0.0820, 0.4362], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3072876036167145 tensor([0.3073, 0.3657, 0.0136, 0.2101, 0.1034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22292044758796692 tensor([0.0644, 0.2365, 0.1474, 0.2229, 0.3288], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3158595860004425 tensor([0.0025, 0.0265, 0.5986, 0.0565, 0.3159], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1896923929452896 tensor([0.0713, 0.1897, 0.1292, 0.2661, 0.3438], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.187463641166687 tensor([0.0567, 0.1875, 0.1457, 0.2069, 0.4032], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33013060688972473 tensor([0.3301, 0.4077, 0.0092, 0.1785, 0.0745], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24756857752799988 tensor([0.0261, 0.0897, 0.2574, 0.2476, 0.3792], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4210660755634308 tensor([0.0060, 0.0445, 0.4353, 0.0930, 0.4211], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1760397106409073 tensor([0.1760, 0.4185, 0.0366, 0.1844, 0.1846], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21312643587589264 tensor([0.1097, 0.3134, 0.0787, 0.2131, 0.2851], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24480843544006348 tensor([0.0013, 0.0171, 0.6988, 0.0380, 0.2448], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26717546582221985 tensor([0.0284, 0.0867, 0.2466, 0.2672, 0.3711], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3358512222766876 tensor([0.0133, 0.0705, 0.3359, 0.1327, 0.4476], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22374774515628815 tensor([0.0797, 0.2237, 0.1181, 0.2525, 0.3259], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19300764799118042 tensor([0.1260, 0.3705, 0.0631, 0.1930, 0.2474], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.219134122133255 tensor([0.0008, 0.0102, 0.7367, 0.0332, 0.2191], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21728388965129852 tensor([0.2173, 0.2819, 0.0316, 0.3065, 0.1628], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17392946779727936 tensor([0.0691, 0.1739, 0.1255, 0.3016, 0.3298], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17580623924732208 tensor([0.1758, 0.5078, 0.0274, 0.1347, 0.1543], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2239590287208557 tensor([6.1563e-04, 8.2279e-03, 7.3710e-01, 3.0095e-02, 2.2396e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30007749795913696 tensor([0.0665, 0.1270, 0.1228, 0.3837, 0.3001], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17192034423351288 tensor([0.0697, 0.1719, 0.1314, 0.2908, 0.3361], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15175651013851166 tensor([0.1518, 0.3025, 0.0547, 0.2492, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1828247308731079 tensor([0.1261, 0.3870, 0.0610, 0.1828, 0.2432], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3775276243686676 tensor([0.0055, 0.0429, 0.4894, 0.0847, 0.3775], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14691732823848724 tensor([0.1469, 0.2021, 0.0503, 0.4017, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1877785623073578 tensor([0.0686, 0.2523, 0.1359, 0.1878, 0.3555], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16986219584941864 tensor([0.0463, 0.1699, 0.1856, 0.2192, 0.3791], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2149779200553894 tensor([0.2150, 0.4698, 0.0211, 0.1598, 0.1343], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3704761564731598 tensor([0.0078, 0.0477, 0.4515, 0.1226, 0.3705], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14150390028953552 tensor([0.1415, 0.1289, 0.0413, 0.5217, 0.1666], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19352245330810547 tensor([0.0745, 0.1935, 0.1146, 0.2724, 0.3450], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16197146475315094 tensor([0.1620, 0.2678, 0.0543, 0.3084, 0.2075], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16643379628658295 tensor([0.1664, 0.3668, 0.0485, 0.2124, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17071034014225006 tensor([0.0113, 0.0508, 0.3640, 0.1707, 0.4032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.410849928855896 tensor([0.0051, 0.0328, 0.4541, 0.0971, 0.4108], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17158226668834686 tensor([0.1716, 0.2643, 0.0451, 0.2996, 0.2194], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20471937954425812 tensor([0.2047, 0.4197, 0.0266, 0.1975, 0.1515], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36017200350761414 tensor([0.0063, 0.0537, 0.5000, 0.0798, 0.3602], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18274140357971191 tensor([0.0752, 0.1827, 0.1165, 0.2702, 0.3554], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20367006957530975 tensor([0.0858, 0.2687, 0.1185, 0.2037, 0.3233], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22232143580913544 tensor([0.2223, 0.4906, 0.0174, 0.1435, 0.1262], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3304663300514221 tensor([0.0043, 0.0339, 0.5417, 0.0897, 0.3305], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18795321881771088 tensor([0.0739, 0.1880, 0.1314, 0.2899, 0.3169], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17167696356773376 tensor([0.0513, 0.1717, 0.1633, 0.2455, 0.3683], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2189416140317917 tensor([0.2589, 0.3750, 0.0180, 0.2189, 0.1291], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21858441829681396 tensor([0.2186, 0.4459, 0.0244, 0.1745, 0.1367], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20625221729278564 tensor([6.8844e-04, 7.7885e-03, 7.4709e-01, 3.8181e-02, 2.0625e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20701982080936432 tensor([0.2070, 0.1720, 0.0236, 0.4678, 0.1295], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37886983156204224 tensor([0.0026, 0.0201, 0.5296, 0.0688, 0.3789], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2134917974472046 tensor([0.1079, 0.2135, 0.0888, 0.3068, 0.2830], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13407163321971893 tensor([0.1341, 0.3686, 0.0570, 0.1969, 0.2433], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2753634452819824 tensor([0.0011, 0.0118, 0.6717, 0.0401, 0.2754], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3244006037712097 tensor([0.0738, 0.1566, 0.1149, 0.3244, 0.3303], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2745656669139862 tensor([0.2746, 0.2774, 0.0195, 0.3153, 0.1133], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19395272433757782 tensor([0.1270, 0.3997, 0.0604, 0.1940, 0.2189], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31867098808288574 tensor([0.0018, 0.0154, 0.6050, 0.0591, 0.3187], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28848758339881897 tensor([0.0641, 0.1179, 0.1284, 0.4011, 0.2885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17029891908168793 tensor([0.0544, 0.1703, 0.1536, 0.2287, 0.3929], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23183423280715942 tensor([0.0979, 0.2318, 0.0944, 0.2838, 0.2921], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20963729918003082 tensor([0.0823, 0.2096, 0.1216, 0.2794, 0.3071], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37053993344306946 tensor([0.0119, 0.0746, 0.4136, 0.1294, 0.3705], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3130853474140167 tensor([0.0431, 0.0766, 0.1465, 0.4207, 0.3131], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16880178451538086 tensor([0.0128, 0.0617, 0.3539, 0.1688, 0.4028], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2319401055574417 tensor([0.2319, 0.2830, 0.0273, 0.3157, 0.1421], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3383444547653198 tensor([0.3383, 0.4227, 0.0076, 0.1602, 0.0712], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1793367564678192 tensor([0.0135, 0.0642, 0.3576, 0.1793, 0.3854], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2940848767757416 tensor([0.0833, 0.1694, 0.1040, 0.3493, 0.2941], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14643476903438568 tensor([0.0102, 0.0518, 0.3527, 0.1464, 0.4388], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2463577389717102 tensor([0.3103, 0.3315, 0.0134, 0.2464, 0.0984], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2982090413570404 tensor([0.0184, 0.1306, 0.2982, 0.1169, 0.4359], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32525163888931274 tensor([0.0025, 0.0221, 0.5854, 0.0647, 0.3253], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14642305672168732 tensor([0.1464, 0.1423, 0.0405, 0.4891, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22329331934452057 tensor([0.0788, 0.2233, 0.1092, 0.2256, 0.3631], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20082804560661316 tensor([0.2008, 0.3095, 0.0350, 0.2763, 0.1784], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15092888474464417 tensor([0.1509, 0.4101, 0.0497, 0.1899, 0.1994], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27772486209869385 tensor([0.0023, 0.0215, 0.6355, 0.0630, 0.2777], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19613736867904663 tensor([0.1143, 0.1961, 0.0708, 0.3630, 0.2558], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16081728041172028 tensor([0.0196, 0.0863, 0.2711, 0.1608, 0.4622], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3438493609428406 tensor([0.4139, 0.3438, 0.0053, 0.1814, 0.0555], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3343612253665924 tensor([0.3344, 0.4966, 0.0057, 0.1021, 0.0612], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17576858401298523 tensor([0.0089, 0.0413, 0.4106, 0.1758, 0.3634], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15599171817302704 tensor([0.1560, 0.2713, 0.0566, 0.3061, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23525270819664001 tensor([0.0325, 0.1017, 0.1998, 0.2353, 0.4308], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2790874242782593 tensor([0.2791, 0.2905, 0.0181, 0.2918, 0.1206], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2915650010108948 tensor([0.2916, 0.4956, 0.0089, 0.1274, 0.0766], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26429644227027893 tensor([0.0401, 0.1312, 0.2374, 0.2643, 0.3270], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35886162519454956 tensor([0.0090, 0.0558, 0.3589, 0.1061, 0.4703], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1363469809293747 tensor([0.1363, 0.2904, 0.0667, 0.2696, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16098733246326447 tensor([0.1610, 0.2838, 0.0534, 0.2919, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20888246595859528 tensor([5.7559e-04, 7.6707e-03, 7.5256e-01, 3.0316e-02, 2.0888e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28688693046569824 tensor([0.0712, 0.1638, 0.1390, 0.3391, 0.2869], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4032815098762512 tensor([0.0045, 0.0381, 0.4810, 0.0731, 0.4033], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15968896448612213 tensor([0.1597, 0.2353, 0.0490, 0.3287, 0.2273], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1544528603553772 tensor([0.1545, 0.5205, 0.0346, 0.1255, 0.1650], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18963801860809326 tensor([0.0177, 0.0848, 0.3527, 0.1896, 0.3551], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13422249257564545 tensor([0.1342, 0.2417, 0.0645, 0.3237, 0.2359], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1871241331100464 tensor([0.0925, 0.1871, 0.0979, 0.2988, 0.3237], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20985953509807587 tensor([0.0843, 0.2939, 0.1159, 0.2099, 0.2961], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22875267267227173 tensor([0.2288, 0.4794, 0.0179, 0.1537, 0.1203], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26267296075820923 tensor([0.0015, 0.0159, 0.6719, 0.0479, 0.2627], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13243785500526428 tensor([0.1324, 0.1675, 0.0541, 0.4505, 0.1955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1975746750831604 tensor([0.0629, 0.2263, 0.1419, 0.1976, 0.3713], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21412700414657593 tensor([0.0644, 0.2159, 0.1399, 0.2141, 0.3656], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.218466654419899 tensor([0.1222, 0.3525, 0.0772, 0.2185, 0.2296], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18996277451515198 tensor([0.0122, 0.0536, 0.3724, 0.1900, 0.3719], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2040400505065918 tensor([0.0681, 0.2040, 0.1432, 0.2601, 0.3246], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2733834981918335 tensor([0.2734, 0.3865, 0.0161, 0.2135, 0.1105], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26055908203125 tensor([0.1197, 0.2917, 0.0772, 0.2606, 0.2508], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38102537393569946 tensor([0.0059, 0.0348, 0.4510, 0.1273, 0.3810], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14607428014278412 tensor([0.1461, 0.1904, 0.0490, 0.4206, 0.1939], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.416198194026947 tensor([0.0053, 0.0343, 0.4162, 0.0952, 0.4489], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1345948427915573 tensor([0.1346, 0.3198, 0.0678, 0.2260, 0.2518], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24112744629383087 tensor([0.2411, 0.5203, 0.0133, 0.1243, 0.1010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1972915083169937 tensor([0.0169, 0.0741, 0.3192, 0.1973, 0.3925], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4130740463733673 tensor([0.0059, 0.0444, 0.4451, 0.0915, 0.4131], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18532326817512512 tensor([0.0685, 0.1853, 0.1216, 0.2544, 0.3701], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22105029225349426 tensor([0.2211, 0.3296, 0.0302, 0.2604, 0.1587], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23676663637161255 tensor([0.2368, 0.3608, 0.0216, 0.2411, 0.1397], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34535977244377136 tensor([0.0101, 0.0778, 0.4645, 0.1022, 0.3454], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2642572522163391 tensor([0.2997, 0.3330, 0.0136, 0.2643, 0.0895], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.36983656883239746 tensor([0.3698, 0.4871, 0.0037, 0.0971, 0.0422], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.166153222322464 tensor([0.0152, 0.0719, 0.3324, 0.1662, 0.4143], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16976869106292725 tensor([0.1698, 0.1325, 0.0288, 0.5315, 0.1374], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21906401216983795 tensor([0.1150, 0.2191, 0.0820, 0.3157, 0.2683], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29468628764152527 tensor([0.0539, 0.1585, 0.1713, 0.2947, 0.3216], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17706380784511566 tensor([0.0758, 0.3035, 0.1186, 0.1771, 0.3250], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26637810468673706 tensor([0.0010, 0.0133, 0.6839, 0.0354, 0.2664], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24420559406280518 tensor([0.2442, 0.1639, 0.0159, 0.4687, 0.1073], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1688484102487564 tensor([0.0189, 0.0904, 0.2992, 0.1688, 0.4227], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2253783941268921 tensor([0.0964, 0.2254, 0.0911, 0.2753, 0.3118], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.281429260969162 tensor([0.2814, 0.4005, 0.0151, 0.2019, 0.1010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3744509518146515 tensor([0.0076, 0.0448, 0.4537, 0.1195, 0.3745], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13915710151195526 tensor([0.1392, 0.2197, 0.0608, 0.3395, 0.2409], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22518886625766754 tensor([0.1032, 0.2252, 0.0993, 0.3059, 0.2663], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2299637347459793 tensor([0.0848, 0.2300, 0.1255, 0.2821, 0.2776], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34048914909362793 tensor([0.0036, 0.0302, 0.5546, 0.0711, 0.3405], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31446436047554016 tensor([0.0639, 0.1331, 0.1319, 0.3566, 0.3145], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2193673849105835 tensor([0.1080, 0.2194, 0.0905, 0.2925, 0.2896], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4024293124675751 tensor([0.4150, 0.4024, 0.0039, 0.1319, 0.0469], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19279390573501587 tensor([0.1928, 0.4345, 0.0369, 0.1659, 0.1699], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1482921540737152 tensor([0.0134, 0.0704, 0.4046, 0.1483, 0.3633], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16949808597564697 tensor([0.0083, 0.0380, 0.4067, 0.1695, 0.3775], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19925230741500854 tensor([0.0582, 0.1993, 0.1448, 0.2130, 0.3848], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23957933485507965 tensor([0.2555, 0.3510, 0.0214, 0.2396, 0.1326], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24798467755317688 tensor([0.2480, 0.3984, 0.0197, 0.2084, 0.1256], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36771902441978455 tensor([0.0054, 0.0335, 0.4814, 0.1119, 0.3677], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1802297979593277 tensor([0.0179, 0.0794, 0.3374, 0.1802, 0.3850], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15509605407714844 tensor([0.0184, 0.0815, 0.2692, 0.1551, 0.4759], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2364892214536667 tensor([0.1095, 0.2365, 0.0895, 0.2865, 0.2780], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2737444043159485 tensor([0.2737, 0.3941, 0.0156, 0.2010, 0.1155], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3171154260635376 tensor([0.0039, 0.0264, 0.5515, 0.1011, 0.3171], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18143552541732788 tensor([0.0182, 0.0869, 0.3025, 0.1814, 0.4110], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28961265087127686 tensor([0.0660, 0.1657, 0.1189, 0.2896, 0.3598], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1385452300310135 tensor([0.1385, 0.3233, 0.0685, 0.2339, 0.2357], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2684304118156433 tensor([0.1151, 0.2829, 0.0803, 0.2684, 0.2533], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2556857168674469 tensor([0.0008, 0.0088, 0.6987, 0.0360, 0.2557], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24335822463035583 tensor([0.2434, 0.2105, 0.0202, 0.4167, 0.1093], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3893788754940033 tensor([0.0054, 0.0333, 0.4625, 0.1095, 0.3894], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [0, 1, 2], [0, 2, 1], [2, 4, 0], [0, 2, 1], [2, 4, 1], [2, 0], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1], [2, 4, 0], [2, 0, 3], [2, 4], [0, 2, 1, 3], [2, 0, 1], [0, 2, 1], [2, 0, 1], [2, 4, 0], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0, 4], [0, 1, 2], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [2, 0, 1], [2, 0, 1], [2, 4, 3], [0, 1, 3], [0, 2, 1], [0, 2, 1], [2, 0], [0, 2, 3], [0, 1, 2], [2, 0], [0, 2, 1, 3], [0, 2, 1], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 4], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4], [2, 0], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 0], [0, 1, 3], [2, 4, 0, 1], [2, 0, 1], [2, 0, 1], [2, 4, 3], [0, 1, 2], [2, 1, 4, 0], [0, 1, 3], [2, 4, 3, 0], [2, 0, 4], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 0], [2, 4, 0, 3], [0, 1, 3], [2, 1, 0, 4], [2, 4], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 4, 0], [2, 0, 3], [0, 2, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 2, 1], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 1, 3], [2, 0, 4], [2, 4, 0], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 4], [2, 4, 0, 3], [0, 1, 3], [2, 0, 1], [0, 2, 1], [2, 0, 4], [2, 0], [0, 1, 3], [0, 2, 1], [0, 1, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4], [0, 3, 1], [0, 1, 3], [2, 4, 1, 0], [2, 0], [2, 4], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0, 1], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 0], [0, 2, 1], [2, 4], [2, 4, 3], [0, 2, 1], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0], [2, 0, 1], [2, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 0, 3], [2, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [2, 0], [2, 4, 3], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 3], [0, 1, 3], [0, 1, 3], [2, 0, 1], [2, 4], [2, 4], [0, 1, 3], [2, 0, 4, 1], [0, 1, 2, 3], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [0, 2, 1], [2, 0, 3], [0, 1, 3], [2, 4, 1], [0, 1, 2, 3], [2, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 0, 4], [2, 0, 4], [0, 1, 3], [2, 0, 1], [2, 0, 4], [2, 4, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 0, 1], [2, 4], [2, 4, 3], [0, 1, 3], [0, 1, 3], [0, 1, 2, 3], [2, 0, 4], [2, 4, 3], [0, 1, 3], [0, 1, 2, 3], [2, 0, 1], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1], [0, 1, 3]]\n",
      "[[1, 3], [0, 1], [2, 4], [0, 1], [2, 4], [0, 1, 3], [0, 1], [2, 4], [3], [2, 4], [0, 1], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 1], [2, 4], [3], [3, 4], [3, 4], [1, 3], [3, 4], [0, 3], [1, 3, 4], [1, 3], [1], [2, 4], [0, 3], [1, 3], [1, 4], [0, 1, 3], [4], [3, 4], [3, 4], [3, 4], [1, 3], [2, 4], [2, 4], [2, 4], [0, 1, 3], [0, 1], [2, 4], [3], [3, 4], [1, 3], [0, 1], [2, 4], [3, 4], [3, 4], [3, 4], [0, 1], [2, 4], [3, 4], [3, 4], [1, 3, 4], [1, 4], [3, 4], [1, 3, 4], [4], [3, 4], [1], [2, 4], [2, 4], [0, 1, 3], [1, 3], [1, 3], [2, 4], [3], [1, 3], [0, 1, 3], [0, 1], [2, 4], [3], [0, 1, 3], [1, 3, 4], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [1, 3, 4], [2, 4], [3], [3, 4], [3, 4], [0, 1], [3, 4], [3], [2, 4], [1], [1, 3], [2, 4], [3, 4], [2, 4], [1, 3, 4], [1], [2, 4], [3], [0, 1, 3], [3, 4], [1], [2, 4], [3, 4], [3, 4], [1, 3], [1], [2, 4], [1, 3], [1, 4], [3, 4], [0, 1], [2, 4], [3], [3, 4], [1, 3], [1, 3], [2, 4], [3], [2, 4], [1, 3], [0, 1], [2, 4], [3], [3, 4], [1, 3, 4], [0, 1], [2, 4], [3, 4], [3, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [2, 4], [1, 3], [1, 3], [2, 4], [3], [3, 4], [0, 1, 3], [1], [2, 4], [3, 4], [3, 4], [1, 3], [1, 3, 4], [2, 4], [3, 4], [2, 4], [0, 1, 3], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1, 3], [2, 4], [2, 4], [3], [1, 3, 4], [0, 1, 3], [1], [2, 4], [3], [4], [0, 1], [0, 1], [2, 4], [1, 3], [3, 4], [0, 1, 3], [0, 1], [3, 4], [3], [2, 4], [1, 3], [1, 3], [2, 4], [3, 4], [2, 4], [1, 3], [1], [2, 4], [1, 3], [3, 4], [1, 3, 4], [0, 1], [2, 4], [3], [1, 4], [1, 3, 4], [1, 3], [2, 4], [3], [1, 3, 4], [0, 1], [1, 3], [2, 4], [3], [2, 4], [1, 3], [0, 1], [2, 4], [2, 4], [3, 4], [0, 1, 3], [0, 1, 3], [2, 4], [3], [4], [0, 1, 3], [0, 1], [2, 4], [3], [1, 3], [3, 4], [1, 4], [2, 4], [0, 3], [4], [1, 3, 4], [0, 1], [2, 4], [3], [1, 3], [1, 3], [1, 3], [2, 4], [3, 4], [1, 3], [0, 1], [1], [2, 4], [2, 4], [3, 4], [0, 1, 3], [0, 1], [2, 4], [2, 4], [4], [1, 3], [0, 1], [2, 4], [4], [3, 4], [1, 3], [1, 3], [2, 4], [0, 3], [2, 4]]\n",
      "NL_pred of 4th iteration [[2, 4, 0], [2, 4, 1, 0], [2, 4, 1, 0], [0, 2, 1], [0, 2, 1], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0], [2, 0, 3], [0, 2, 1, 3], [2, 0, 1], [2, 4, 0], [0, 1, 3], [2, 4, 0], [2, 0, 1], [2, 0, 1], [0, 2, 1], [0, 2, 3], [0, 2, 1, 3], [2, 4, 3, 0], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 0, 1], [2, 4, 3, 0], [2, 4, 0, 3], [2, 0, 1], [2, 4, 3, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [2, 4, 0], [2, 0, 3], [0, 2, 1], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 0], [2, 0, 1], [2, 0, 1], [0, 2, 1], [2, 4, 0], [2, 4, 0, 3], [0, 2, 1], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 3, 0], [2, 4, 0, 1], [0, 1, 2, 3], [0, 1, 3], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0], [2, 0, 1], [2, 4, 1, 0], [2, 0, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 1, 0], [2, 0, 3], [0, 1, 2, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 0, 1], [0, 1, 3], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.014851714583004222  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.014842841204474953  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.014826052329119514  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014802255350000718  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.014772309976465562  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.014737068905549891  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.01469729788163129  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.014653700940749224  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.014606962484471938  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.014557716425727396  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.014506582652821261  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.014454091296476476  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.014400689742144417  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.014346830985125373  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.01429291051976821  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.014239290181328268  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.014186306560740752  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.014134171429802389  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.014083100767696605  Accuracy on Support set:0.0\n",
      "torch.Size([85, 2048]) torch.Size([85])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.014033279699437759  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.20820602774620056 tensor([0.0725, 0.2082, 0.1225, 0.2368, 0.3600], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23082514107227325 tensor([0.2308, 0.4683, 0.0202, 0.1559, 0.1247], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2730790078639984 tensor([0.0019, 0.0190, 0.6507, 0.0554, 0.2731], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3182574510574341 tensor([0.3632, 0.3183, 0.0079, 0.2334, 0.0772], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4061831533908844 tensor([0.0022, 0.0195, 0.5156, 0.0565, 0.4062], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14350184798240662 tensor([0.1435, 0.2454, 0.0568, 0.2916, 0.2627], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14814327657222748 tensor([0.1481, 0.4369, 0.0458, 0.1693, 0.1998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3569316565990448 tensor([0.0033, 0.0319, 0.5437, 0.0641, 0.3569], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37527477741241455 tensor([0.0022, 0.0199, 0.5462, 0.0564, 0.3753], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.293347030878067 tensor([0.2933, 0.3940, 0.0142, 0.1930, 0.1054], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23104527592658997 tensor([0.2310, 0.4724, 0.0184, 0.1570, 0.1211], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3300257623195648 tensor([0.0028, 0.0302, 0.5809, 0.0561, 0.3300], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18158367276191711 tensor([0.0089, 0.0375, 0.3872, 0.1816, 0.3849], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.43289658427238464 tensor([0.0033, 0.0294, 0.4732, 0.0612, 0.4329], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28770866990089417 tensor([0.2877, 0.3120, 0.0164, 0.2669, 0.1170], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15489840507507324 tensor([0.1549, 0.4566, 0.0398, 0.1598, 0.1889], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967812180519104 tensor([2.9895e-04, 4.6411e-03, 7.7483e-01, 2.3451e-02, 1.9678e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1298803687095642 tensor([0.0090, 0.0486, 0.3727, 0.1299, 0.4399], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14609961211681366 tensor([0.0188, 0.0968, 0.2926, 0.1461, 0.4458], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16012278199195862 tensor([0.0513, 0.1601, 0.1565, 0.2691, 0.3630], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14104215800762177 tensor([0.0193, 0.1097, 0.3063, 0.1410, 0.4236], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1739800125360489 tensor([0.1740, 0.1973, 0.0338, 0.4323, 0.1626], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13867567479610443 tensor([0.0384, 0.1387, 0.1877, 0.2183, 0.4169], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21607448160648346 tensor([0.0703, 0.2395, 0.1453, 0.2161, 0.3289], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32400277256965637 tensor([0.0024, 0.0182, 0.5764, 0.0791, 0.3240], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1569976955652237 tensor([0.1570, 0.1675, 0.0385, 0.4600, 0.1771], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24438539147377014 tensor([0.1111, 0.2827, 0.0829, 0.2444, 0.2790], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1610436737537384 tensor([0.0318, 0.1610, 0.2309, 0.1474, 0.4289], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13378578424453735 tensor([0.1338, 0.2981, 0.0607, 0.2629, 0.2445], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3053283989429474 tensor([0.0316, 0.0853, 0.2144, 0.3053, 0.3634], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18562640249729156 tensor([0.0210, 0.0850, 0.2515, 0.1856, 0.4569], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1905318796634674 tensor([0.0266, 0.1131, 0.2565, 0.1905, 0.4133], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20280328392982483 tensor([0.0795, 0.2846, 0.1195, 0.2028, 0.3136], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3967606723308563 tensor([0.0042, 0.0364, 0.4877, 0.0749, 0.3968], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29857736825942993 tensor([0.0025, 0.0157, 0.5808, 0.1024, 0.2986], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41737619042396545 tensor([0.0034, 0.0271, 0.4789, 0.0732, 0.4174], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11508912593126297 tensor([0.1151, 0.2829, 0.0764, 0.2421, 0.2835], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12697932124137878 tensor([0.1270, 0.4151, 0.0634, 0.1596, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2593500316143036 tensor([0.0009, 0.0107, 0.6908, 0.0381, 0.2594], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13830412924289703 tensor([0.0090, 0.0440, 0.3429, 0.1383, 0.4657], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23068925738334656 tensor([0.0982, 0.2307, 0.0868, 0.2604, 0.3239], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19280655682086945 tensor([0.1928, 0.4607, 0.0266, 0.1605, 0.1593], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.219893679022789 tensor([6.8181e-04, 8.6759e-03, 7.3783e-01, 3.2916e-02, 2.1989e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31327182054519653 tensor([0.0298, 0.0773, 0.2076, 0.3133, 0.3720], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1843985617160797 tensor([0.0249, 0.1169, 0.2729, 0.1844, 0.4009], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20420390367507935 tensor([0.0316, 0.1171, 0.2404, 0.2042, 0.4067], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12510107457637787 tensor([0.1251, 0.4155, 0.0626, 0.1655, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2241717129945755 tensor([6.9397e-04, 8.1718e-03, 7.3326e-01, 3.3707e-02, 2.2417e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2651385962963104 tensor([0.0183, 0.0576, 0.2771, 0.2651, 0.3818], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1651880145072937 tensor([0.0230, 0.1124, 0.2605, 0.1652, 0.4390], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13966815173625946 tensor([0.0334, 0.1397, 0.2238, 0.2066, 0.3965], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11435240507125854 tensor([0.0173, 0.1144, 0.3103, 0.1192, 0.4389], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13555073738098145 tensor([0.0079, 0.0466, 0.4353, 0.1356, 0.3746], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14340268075466156 tensor([0.0323, 0.1434, 0.2252, 0.1891, 0.4100], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13520368933677673 tensor([0.0133, 0.0817, 0.3707, 0.1352, 0.3991], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.230491042137146 tensor([6.1780e-04, 8.6880e-03, 7.2946e-01, 3.0740e-02, 2.3049e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32993489503860474 tensor([0.0034, 0.0292, 0.5639, 0.0736, 0.3299], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1204710453748703 tensor([0.1205, 0.2761, 0.0746, 0.2556, 0.2731], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1768733561038971 tensor([0.1011, 0.1769, 0.0812, 0.3799, 0.2608], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2218429148197174 tensor([0.1055, 0.2925, 0.0851, 0.2218, 0.2951], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3658486008644104 tensor([0.0049, 0.0319, 0.4905, 0.1069, 0.3658], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2075614333152771 tensor([0.0514, 0.2076, 0.1890, 0.2104, 0.3416], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23919638991355896 tensor([0.2392, 0.3227, 0.0245, 0.2691, 0.1445], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.35291406512260437 tensor([0.3529, 0.4414, 0.0061, 0.1388, 0.0607], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2738753855228424 tensor([0.0011, 0.0147, 0.6768, 0.0336, 0.2739], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16290730237960815 tensor([0.1629, 0.3148, 0.0470, 0.2581, 0.2172], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14286494255065918 tensor([0.0413, 0.1429, 0.2062, 0.2168, 0.3928], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15055152773857117 tensor([0.1506, 0.5217, 0.0343, 0.1223, 0.1711], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20938538014888763 tensor([3.8276e-04, 5.1510e-03, 7.5795e-01, 2.7131e-02, 2.0939e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26681989431381226 tensor([0.0131, 0.0402, 0.3081, 0.2668, 0.3719], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3821743428707123 tensor([0.0023, 0.0241, 0.5424, 0.0489, 0.3822], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19565817713737488 tensor([0.1957, 0.3493, 0.0357, 0.2307, 0.1887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1496874988079071 tensor([0.0270, 0.1497, 0.2606, 0.1615, 0.4012], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2607261538505554 tensor([0.0009, 0.0129, 0.6926, 0.0328, 0.2607], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19460558891296387 tensor([0.0302, 0.1209, 0.2306, 0.1946, 0.4237], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14383535087108612 tensor([0.0228, 0.1135, 0.2477, 0.1438, 0.4721], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22012335062026978 tensor([0.2201, 0.4079, 0.0250, 0.2049, 0.1420], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.153486430644989 tensor([0.0094, 0.0486, 0.3913, 0.1535, 0.3971], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36640438437461853 tensor([0.0021, 0.0215, 0.5571, 0.0529, 0.3664], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17464977502822876 tensor([0.0520, 0.2239, 0.1569, 0.1746, 0.3925], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21064986288547516 tensor([5.7922e-04, 9.3565e-03, 7.5375e-01, 2.5662e-02, 2.1065e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16860690712928772 tensor([0.0105, 0.0480, 0.3785, 0.1686, 0.3944], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4202379584312439 tensor([0.0050, 0.0372, 0.4565, 0.0810, 0.4202], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14857299625873566 tensor([0.0355, 0.1486, 0.2135, 0.1914, 0.4110], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1967393010854721 tensor([4.3142e-04, 6.5998e-03, 7.7163e-01, 2.4601e-02, 1.9674e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1236448585987091 tensor([0.1236, 0.2401, 0.0734, 0.2989, 0.2639], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22199594974517822 tensor([0.0293, 0.1114, 0.2268, 0.2220, 0.4105], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20131143927574158 tensor([3.3445e-04, 5.3941e-03, 7.7040e-01, 2.2557e-02, 2.0131e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28701290488243103 tensor([0.0290, 0.0832, 0.2232, 0.2870, 0.3777], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21221862733364105 tensor([0.0294, 0.1092, 0.2350, 0.2122, 0.4142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2189984768629074 tensor([0.0773, 0.2317, 0.1160, 0.2190, 0.3560], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3227538466453552 tensor([0.0019, 0.0208, 0.6066, 0.0479, 0.3228], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15738330781459808 tensor([0.0759, 0.1574, 0.1091, 0.3590, 0.2987], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16218754649162292 tensor([0.0296, 0.1622, 0.2374, 0.1380, 0.4327], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14724910259246826 tensor([0.0180, 0.0996, 0.3054, 0.1472, 0.4297], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1283256709575653 tensor([0.1283, 0.4223, 0.0525, 0.1649, 0.2320], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3275775909423828 tensor([0.0027, 0.0232, 0.5782, 0.0683, 0.3276], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20226632058620453 tensor([0.0320, 0.1252, 0.2079, 0.2023, 0.4326], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20980393886566162 tensor([0.0846, 0.2098, 0.1174, 0.2773, 0.3110], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19604767858982086 tensor([0.0908, 0.2966, 0.1045, 0.1960, 0.3120], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3775326609611511 tensor([0.0039, 0.0257, 0.4949, 0.0979, 0.3775], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3557063043117523 tensor([0.0018, 0.0164, 0.5700, 0.0561, 0.3557], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20893427729606628 tensor([0.0907, 0.2089, 0.0976, 0.2717, 0.3311], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1195390373468399 tensor([0.1195, 0.3670, 0.0633, 0.1978, 0.2523], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3074896037578583 tensor([0.0022, 0.0260, 0.6189, 0.0454, 0.3075], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.200521782040596 tensor([0.0325, 0.1183, 0.2080, 0.2005, 0.4407], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15683786571025848 tensor([0.0392, 0.1813, 0.2136, 0.1568, 0.4091], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13591589033603668 tensor([0.1359, 0.4495, 0.0434, 0.1510, 0.2202], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2769109606742859 tensor([0.0014, 0.0158, 0.6579, 0.0480, 0.2769], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21698081493377686 tensor([0.0326, 0.1233, 0.2335, 0.2170, 0.3937], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1696140170097351 tensor([0.0205, 0.1034, 0.2767, 0.1696, 0.4297], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1575208157300949 tensor([0.1575, 0.3425, 0.0451, 0.2296, 0.2254], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12944670021533966 tensor([0.1294, 0.3978, 0.0601, 0.1786, 0.2339], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17999687790870667 tensor([3.3746e-04, 4.8541e-03, 7.8961e-01, 2.5205e-02, 1.8000e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11784633994102478 tensor([0.1178, 0.1477, 0.0566, 0.4625, 0.2153], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3182428181171417 tensor([0.0010, 0.0103, 0.6306, 0.0399, 0.3182], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1497424691915512 tensor([0.0504, 0.1497, 0.1724, 0.2466, 0.3808], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17259618639945984 tensor([0.0683, 0.2818, 0.1203, 0.1726, 0.3570], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23884478211402893 tensor([5.1453e-04, 7.2311e-03, 7.2580e-01, 2.7609e-02, 2.3884e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24287422001361847 tensor([0.0322, 0.1024, 0.2077, 0.2429, 0.4148], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16820374131202698 tensor([0.1682, 0.2541, 0.0486, 0.3315, 0.1976], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2662636637687683 tensor([0.0007, 0.0083, 0.6898, 0.0350, 0.2663], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30264830589294434 tensor([0.0286, 0.0780, 0.2296, 0.3026, 0.3612], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1577247977256775 tensor([0.0217, 0.1022, 0.2602, 0.1577, 0.4582], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16031135618686676 tensor([0.0450, 0.1603, 0.1814, 0.2248, 0.3884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13849107921123505 tensor([0.0362, 0.1385, 0.2235, 0.2113, 0.3906], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.338964581489563 tensor([0.0042, 0.0376, 0.5444, 0.0748, 0.3390], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.302814781665802 tensor([0.0183, 0.0486, 0.2532, 0.3028, 0.3772], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38033080101013184 tensor([0.0044, 0.0312, 0.4869, 0.0971, 0.3803], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13586260378360748 tensor([0.1359, 0.2475, 0.0647, 0.3164, 0.2355], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2286561280488968 tensor([0.2287, 0.4279, 0.0207, 0.1858, 0.1368], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3664318025112152 tensor([0.0048, 0.0329, 0.4917, 0.1042, 0.3664], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2701725959777832 tensor([0.0374, 0.1146, 0.1953, 0.2702, 0.3825], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4063512086868286 tensor([0.0036, 0.0260, 0.4799, 0.0842, 0.4064], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1987161785364151 tensor([0.1987, 0.3175, 0.0349, 0.2703, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4239712655544281 tensor([0.0070, 0.0700, 0.4252, 0.0739, 0.4240], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2670486867427826 tensor([0.0009, 0.0107, 0.6856, 0.0357, 0.2670], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14756424725055695 tensor([0.0351, 0.1476, 0.1941, 0.1702, 0.4529], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11222127825021744 tensor([0.1122, 0.2588, 0.0800, 0.2648, 0.2841], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22326712310314178 tensor([0.0008, 0.0099, 0.7325, 0.0335, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29063940048217773 tensor([0.2906, 0.3624, 0.0152, 0.2201, 0.1117], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23313575983047485 tensor([0.2331, 0.5152, 0.0154, 0.1194, 0.1169], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3327569365501404 tensor([0.0030, 0.0203, 0.5465, 0.0975, 0.3328], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21273726224899292 tensor([0.0821, 0.2127, 0.1199, 0.2741, 0.3111], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15119482576847076 tensor([0.0122, 0.0572, 0.3134, 0.1512, 0.4660], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16785727441310883 tensor([0.1679, 0.2649, 0.0465, 0.3068, 0.2139], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19308090209960938 tensor([0.1931, 0.4925, 0.0240, 0.1452, 0.1452], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17476961016654968 tensor([0.0157, 0.0762, 0.3735, 0.1748, 0.3598], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4272245764732361 tensor([0.0033, 0.0287, 0.4769, 0.0639, 0.4272], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21771222352981567 tensor([0.0678, 0.2177, 0.1396, 0.2321, 0.3428], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2222364842891693 tensor([0.0838, 0.2222, 0.1161, 0.2622, 0.3157], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18868309259414673 tensor([3.1688e-04, 5.0907e-03, 7.8332e-01, 2.2588e-02, 1.8868e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.254705548286438 tensor([0.0315, 0.1080, 0.2480, 0.2547, 0.3578], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3415582478046417 tensor([0.0015, 0.0183, 0.5973, 0.0413, 0.3416], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18189969658851624 tensor([0.0820, 0.1819, 0.1053, 0.2918, 0.3389], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3455756604671478 tensor([0.0061, 0.0434, 0.4948, 0.1101, 0.3456], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18218952417373657 tensor([0.0677, 0.1822, 0.1327, 0.2791, 0.3384], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23179644346237183 tensor([0.0419, 0.1267, 0.1814, 0.2318, 0.4182], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16335716843605042 tensor([0.0385, 0.2003, 0.2149, 0.1634, 0.3829], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1400892734527588 tensor([0.1401, 0.4411, 0.0452, 0.1621, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21827740967273712 tensor([6.2889e-04, 8.4822e-03, 7.4338e-01, 2.9230e-02, 2.1828e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14247305691242218 tensor([0.0266, 0.1425, 0.2443, 0.1424, 0.4443], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1349872648715973 tensor([0.0267, 0.1350, 0.2442, 0.1533, 0.4408], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18783825635910034 tensor([0.0617, 0.2654, 0.1577, 0.1878, 0.3274], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3508903682231903 tensor([0.0041, 0.0267, 0.5110, 0.1073, 0.3509], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13158631324768066 tensor([0.0295, 0.1316, 0.2510, 0.1912, 0.3968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16850492358207703 tensor([0.1685, 0.3607, 0.0420, 0.2293, 0.1995], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21575288474559784 tensor([0.0594, 0.2158, 0.1542, 0.2198, 0.3508], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3341231048107147 tensor([0.0020, 0.0167, 0.5777, 0.0695, 0.3341], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3936627507209778 tensor([0.0019, 0.0175, 0.5308, 0.0562, 0.3937], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19356153905391693 tensor([0.0670, 0.2389, 0.1400, 0.1936, 0.3604], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15366660058498383 tensor([0.1537, 0.4944, 0.0342, 0.1359, 0.1819], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38577377796173096 tensor([0.0059, 0.0382, 0.4542, 0.1159, 0.3858], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3580571115016937 tensor([0.0021, 0.0215, 0.5663, 0.0521, 0.3581], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18464221060276031 tensor([0.0287, 0.1170, 0.2164, 0.1846, 0.4533], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12744414806365967 tensor([0.1274, 0.2841, 0.0710, 0.2572, 0.2602], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13930624723434448 tensor([0.1393, 0.3208, 0.0535, 0.2464, 0.2399], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30753979086875916 tensor([0.0036, 0.0386, 0.5916, 0.0587, 0.3075], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1899876892566681 tensor([0.1900, 0.3185, 0.0360, 0.2907, 0.1649], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2669697403907776 tensor([0.2670, 0.5270, 0.0099, 0.1149, 0.0813], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39748865365982056 tensor([0.0054, 0.0373, 0.4617, 0.0981, 0.3975], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15810924768447876 tensor([0.0558, 0.1581, 0.1598, 0.2603, 0.3660], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20706620812416077 tensor([0.0221, 0.0975, 0.2929, 0.2071, 0.3805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20152001082897186 tensor([0.0336, 0.2015, 0.2176, 0.1349, 0.4124], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2338608354330063 tensor([5.0064e-04, 7.9775e-03, 7.3253e-01, 2.5132e-02, 2.3386e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14558182656764984 tensor([0.1456, 0.1469, 0.0393, 0.4832, 0.1850], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15397745370864868 tensor([0.0437, 0.1540, 0.1743, 0.2158, 0.4123], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1780375987291336 tensor([0.1780, 0.3799, 0.0392, 0.2200, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3292713761329651 tensor([0.0026, 0.0218, 0.5797, 0.0666, 0.3293], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1647053360939026 tensor([0.0695, 0.1647, 0.1261, 0.2925, 0.3472], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15871082246303558 tensor([0.0487, 0.1587, 0.1904, 0.2461, 0.3561], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15593740344047546 tensor([0.0385, 0.1559, 0.2309, 0.2179, 0.3567], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2802707254886627 tensor([0.0012, 0.0141, 0.6658, 0.0387, 0.2803], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2617802917957306 tensor([0.0272, 0.0853, 0.2364, 0.2618, 0.3893], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1537519246339798 tensor([0.0507, 0.1538, 0.1738, 0.2348, 0.3870], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.297023743391037 tensor([0.2970, 0.4332, 0.0113, 0.1627, 0.0958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3370055556297302 tensor([0.0047, 0.0355, 0.5375, 0.0852, 0.3370], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3446994721889496 tensor([0.0029, 0.0190, 0.5381, 0.0954, 0.3447], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15023955702781677 tensor([0.0239, 0.1225, 0.2477, 0.1502, 0.4557], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15376392006874084 tensor([0.1538, 0.3168, 0.0529, 0.2482, 0.2283], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15033292770385742 tensor([0.1503, 0.3628, 0.0494, 0.2181, 0.2194], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3178648352622986 tensor([0.0018, 0.0161, 0.6027, 0.0614, 0.3179], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3752135932445526 tensor([0.0065, 0.0419, 0.4687, 0.1077, 0.3752], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16675134003162384 tensor([0.0515, 0.1668, 0.1744, 0.2317, 0.3757], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17061452567577362 tensor([0.1706, 0.3681, 0.0399, 0.2156, 0.2057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26487085223197937 tensor([0.0013, 0.0121, 0.6689, 0.0528, 0.2649], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21151134371757507 tensor([0.0279, 0.1056, 0.2114, 0.2115, 0.4436], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2036236673593521 tensor([0.0712, 0.2469, 0.1401, 0.2036, 0.3382], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20540033280849457 tensor([0.0555, 0.2054, 0.1622, 0.2233, 0.3536], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22525808215141296 tensor([3.9325e-04, 5.7178e-03, 7.4305e-01, 2.5582e-02, 2.2526e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14518898725509644 tensor([0.1452, 0.1887, 0.0496, 0.4289, 0.1876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3379075229167938 tensor([0.0018, 0.0161, 0.5836, 0.0605, 0.3379], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2, 3], [0, 1, 3], [2, 4, 3], [2, 4, 3, 0], [0, 1, 3, 4], [2, 4, 1, 0], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 0, 3, 1], [2, 4, 0], [0, 2, 1, 3], [2, 0, 1], [0, 2, 1, 3], [2, 0, 1, 3], [2, 4, 0], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [2, 0, 1, 3], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [0, 2, 1], [0, 2, 1, 3], [2, 0, 1], [0, 2, 3, 1], [0, 1, 2, 3], [2, 0, 1], [0, 2, 1, 3], [0, 2, 1, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 4, 0], [2, 4, 0, 1], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3, 0], [2, 0, 1], [0, 1, 3], [2, 4, 0, 1], [2, 0, 1, 3], [2, 0, 1, 3], [2, 4, 3], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 3], [2, 4, 3, 0], [2, 0, 4, 3], [0, 1, 3], [0, 1, 2, 3], [0, 1, 3], [2, 0, 1], [2, 4, 0, 3], [0, 1, 3, 4], [2, 1, 0, 4], [2, 4, 0], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 4, 0, 1], [2, 0, 3, 1], [0, 2, 1, 3], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 0, 3], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [0, 2, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3, 4], [2, 4, 1, 0], [0, 1, 3], [2, 0, 4, 1], [2, 4, 0, 3], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 0, 1], [0, 2, 1, 3], [2, 0, 4, 1], [2, 0, 1], [0, 1, 3], [0, 2, 1], [0, 1, 3], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 0], [0, 3, 1], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0, 1], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 0], [0, 2, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 2, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 0], [0, 1, 3, 4], [2, 0, 1], [0, 1, 3], [2, 4, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0, 1], [2, 0, 1], [2, 0, 3], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 0, 3, 1], [2, 0, 1], [2, 4, 0, 3], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 3, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 0, 1, 3], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 0, 4, 1], [0, 1, 2, 3], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0, 1], [0, 2, 1], [2, 0, 3], [0, 1, 3], [2, 4, 1, 0], [0, 1, 2, 3], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0, 1], [2, 0, 4, 1], [2, 0, 4, 1], [0, 1, 3], [2, 0, 1], [2, 0, 4, 1], [2, 4, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [0, 1, 2, 3], [2, 0, 4, 1], [2, 4, 3, 0], [0, 1, 3], [0, 1, 2, 3], [2, 0, 1], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3]]\n",
      "[[1, 3], [0, 1], [2, 4], [0, 1], [2, 4], [1, 3], [1], [2, 4], [3], [2, 4], [0, 1], [0, 1], [2, 4], [4], [2, 4], [0, 1], [1], [2], [3], [4], [4], [3], [4], [3], [3, 4], [1, 3], [1], [2, 4], [3], [1, 3], [4], [1, 3], [4], [3, 4], [4], [4], [1, 3], [2, 4], [2, 4], [2, 4], [1, 3], [1], [2, 4], [3], [4], [1, 3], [1], [2, 4], [3, 4], [4], [3, 4], [1], [2, 4], [3, 4], [4], [3, 4], [4], [4], [3, 4], [4], [4], [1], [2, 4], [2, 4], [1, 3], [3], [1, 3], [2, 4], [3], [1, 3], [0, 1, 3], [0, 1], [2, 4], [3], [1, 3], [3, 4], [1], [2, 4], [3, 4], [2, 4], [1], [3, 4], [2, 4], [3], [4], [4], [0, 1], [4], [3], [2, 4], [1], [1], [2, 4], [4], [2, 4], [3, 4], [1], [2], [3], [1, 3], [3, 4], [1], [2, 4], [3, 4], [3, 4], [1, 3], [1], [2, 4], [3], [4], [4], [1], [2, 4], [3], [3, 4], [1, 3], [1], [2, 4], [3], [2, 4], [1, 3], [1], [2, 4], [3], [3, 4], [1, 4], [1], [2, 4], [3, 4], [4], [1, 3], [1], [2], [3], [2, 4], [3], [1], [2, 4], [3], [3, 4], [1, 3], [1], [2, 4], [3, 4], [4], [3], [3, 4], [2, 4], [3, 4], [2, 4], [1, 3], [0, 1], [2, 4], [3, 4], [2, 4], [1, 3], [2, 4], [2, 4], [3], [3, 4], [1, 3], [1], [2, 4], [3], [4], [0, 1], [0, 1], [2, 4], [1, 3], [4], [1, 3], [1], [4], [3], [2, 4], [1, 3], [1, 3], [2], [3, 4], [2, 4], [3], [1], [2, 4], [3], [3, 4], [1, 4], [1], [2, 4], [3], [4], [3, 4], [1], [2, 4], [3], [3, 4], [1], [1, 3], [2, 4], [3], [2, 4], [1], [1], [2, 4], [2, 4], [4], [1, 3], [1, 3], [2, 4], [3], [4], [1, 3], [0, 1], [2, 4], [3], [3], [3, 4], [1, 4], [2, 4], [3], [4], [3, 4], [1], [2, 4], [3], [3], [3], [3], [2, 4], [3, 4], [3], [0, 1], [1], [2, 4], [2, 4], [4], [1, 3], [1], [2, 4], [2, 4], [4], [3], [1], [2, 4], [4], [3, 4], [1, 3], [1, 3], [2, 4], [3], [2, 4]]\n",
      "NL_pred of 5th iteration [[2, 4, 0], [2, 4, 3, 0], [0, 1, 2, 3], [2, 4, 3, 0], [0, 1, 3, 4], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 1, 0], [2, 0, 3, 1], [2, 4, 0], [0, 2, 1, 3], [2, 0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 2, 3], [2, 4, 3, 0], [2, 0, 1, 3], [2, 4, 3, 0], [0, 2, 1, 3], [2, 0, 1], [0, 2, 3, 1], [0, 1, 2, 3], [2, 0, 1], [0, 2, 1, 3], [2, 4, 0], [2, 4, 0, 1], [2, 4, 0], [2, 0, 1], [2, 4, 3, 0], [2, 4, 3, 0], [2, 0, 1], [2, 0, 1, 3], [2, 0, 1, 3], [0, 1, 2, 3], [2, 0, 4, 3], [0, 1, 2, 3], [2, 0, 1], [0, 1, 3, 4], [2, 4, 0], [2, 4, 0, 1], [2, 0, 3, 1], [0, 2, 1, 3], [2, 4, 3, 0], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 3], [2, 4, 3, 0], [0, 2, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3, 4], [2, 4, 1, 0], [2, 0, 4, 1], [2, 4, 0, 3], [2, 4, 0], [0, 2, 1, 3], [2, 0, 4, 1], [2, 0, 1], [2, 4, 0], [2, 4, 0], [2, 0, 1], [2, 4, 0], [0, 2, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 2, 1, 3], [0, 1, 3, 4], [2, 4, 0, 1], [2, 4, 0, 1], [2, 0, 3], [2, 4, 3, 0], [2, 0, 3, 1], [2, 0, 1], [2, 4, 0, 3], [2, 0, 1], [2, 4, 3, 0], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 1, 3], [2, 4, 0], [2, 4, 0], [2, 4, 0], [2, 4, 0, 1], [2, 4, 1, 0], [2, 0, 1], [2, 4, 3, 0], [2, 4, 0, 1], [2, 0, 4, 1], [2, 0, 4, 1], [2, 0, 4, 1], [2, 0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [2, 0, 4, 1], [2, 4, 3, 0], [2, 4, 1, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.012489635944366455  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.012483053207397461  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0124705970287323  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.012452948093414306  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.012430753707885742  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.01240462899208069  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.012375160455703735  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.012342901229858398  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.01230835795402527  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.012271984815597534  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.01223424196243286  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.012195498943328857  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.012156155109405518  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.012116520404815674  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.012076900005340577  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.012037540674209595  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.01199867606163025  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.011960452795028687  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.011923058032989502  Accuracy on Support set:0.0\n",
      "torch.Size([100, 2048]) torch.Size([100])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.011886621713638306  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2, 3], [0, 1, 3], [2, 4, 3], [2, 4, 3, 0], [0, 1, 3, 4], [2, 4, 1, 0], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 0, 3, 1], [2, 4, 0], [0, 2, 1, 3], [2, 0, 1], [0, 2, 1, 3], [2, 0, 1, 3], [2, 4, 0], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [2, 0, 1, 3], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [0, 2, 1], [0, 2, 1, 3], [2, 0, 1], [0, 2, 3, 1], [0, 1, 2, 3], [2, 0, 1], [0, 2, 1, 3], [0, 2, 1, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 4, 0], [2, 4, 0, 1], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3, 0], [2, 0, 1], [0, 1, 3], [2, 4, 0, 1], [2, 0, 1, 3], [2, 0, 1, 3], [2, 4, 3], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 3], [2, 4, 3, 0], [2, 0, 4, 3], [0, 1, 3], [0, 1, 2, 3], [0, 1, 3], [2, 0, 1], [2, 4, 0, 3], [0, 1, 3, 4], [2, 1, 0, 4], [2, 4, 0], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 4, 0, 1], [2, 0, 3, 1], [0, 2, 1, 3], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 0, 3], [2, 4, 3, 0], [0, 1, 3], [2, 0, 1], [0, 2, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3, 4], [2, 4, 1, 0], [0, 1, 3], [2, 0, 4, 1], [2, 4, 0, 3], [0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [2, 4, 0], [2, 4, 0, 3], [0, 1, 3], [2, 0, 1], [0, 2, 1, 3], [2, 0, 4, 1], [2, 0, 1], [0, 1, 3], [0, 2, 1], [0, 1, 3], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 0], [0, 3, 1], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0, 1], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 0], [0, 2, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 2, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0], [2, 4, 0], [0, 1, 3, 4], [2, 0, 1], [0, 1, 3], [2, 4, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 4, 0, 1], [2, 0, 1], [2, 0, 3], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 0, 3, 1], [2, 0, 1], [2, 4, 0, 3], [0, 1, 3], [2, 4, 1, 0], [2, 0, 1], [2, 4, 3, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3], [2, 4, 0, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 0, 1, 3], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 0, 4, 1], [0, 1, 2, 3], [2, 4, 0], [2, 4, 3], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0, 1], [0, 2, 1], [2, 0, 3], [0, 1, 3], [2, 4, 1, 0], [0, 1, 2, 3], [2, 0, 1], [2, 4, 3, 0], [0, 1, 3], [2, 4, 1, 0], [2, 4, 0, 1], [2, 0, 4, 1], [2, 0, 4, 1], [0, 1, 3], [2, 0, 1], [2, 0, 4, 1], [2, 4, 3], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [2, 0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [0, 1, 3], [0, 1, 3], [0, 1, 2, 3], [2, 0, 4, 1], [2, 4, 3, 0], [0, 1, 3], [0, 1, 2, 3], [2, 0, 1], [2, 4, 0], [2, 4, 0], [0, 1, 3], [2, 4, 1, 0], [0, 1, 3]]\n",
      "POSITION :  [[1, 3], [0, 1], [2, 4], [0, 1], [2, 4], [1, 3], [1], [2, 4], [3], [2, 4], [0, 1], [0, 1], [2, 4], [4], [2, 4], [0, 1], [1], [2], [3], [4], [4], [3], [4], [3], [3, 4], [1, 3], [1], [2, 4], [3], [1, 3], [4], [1, 3], [4], [3, 4], [4], [4], [1, 3], [2, 4], [2, 4], [2, 4], [1, 3], [1], [2, 4], [3], [4], [1, 3], [1], [2, 4], [3, 4], [4], [3, 4], [1], [2, 4], [3, 4], [4], [3, 4], [4], [4], [3, 4], [4], [4], [1], [2, 4], [2, 4], [1, 3], [3], [1, 3], [2, 4], [3], [1, 3], [0, 1, 3], [0, 1], [2, 4], [3], [1, 3], [3, 4], [1], [2, 4], [3, 4], [2, 4], [1], [3, 4], [2, 4], [3], [4], [4], [0, 1], [4], [3], [2, 4], [1], [1], [2, 4], [4], [2, 4], [3, 4], [1], [2], [3], [1, 3], [3, 4], [1], [2, 4], [3, 4], [3, 4], [1, 3], [1], [2, 4], [3], [4], [4], [1], [2, 4], [3], [3, 4], [1, 3], [1], [2, 4], [3], [2, 4], [1, 3], [1], [2, 4], [3], [3, 4], [1, 4], [1], [2, 4], [3, 4], [4], [1, 3], [1], [2], [3], [2, 4], [3], [1], [2, 4], [3], [3, 4], [1, 3], [1], [2, 4], [3, 4], [4], [3], [3, 4], [2, 4], [3, 4], [2, 4], [1, 3], [0, 1], [2, 4], [3, 4], [2, 4], [1, 3], [2, 4], [2, 4], [3], [3, 4], [1, 3], [1], [2, 4], [3], [4], [0, 1], [0, 1], [2, 4], [1, 3], [4], [1, 3], [1], [4], [3], [2, 4], [1, 3], [1, 3], [2], [3, 4], [2, 4], [3], [1], [2, 4], [3], [3, 4], [1, 4], [1], [2, 4], [3], [4], [3, 4], [1], [2, 4], [3], [3, 4], [1], [1, 3], [2, 4], [3], [2, 4], [1], [1], [2, 4], [2, 4], [4], [1, 3], [1, 3], [2, 4], [3], [4], [1, 3], [0, 1], [2, 4], [3], [3], [3, 4], [1, 4], [2, 4], [3], [4], [3, 4], [1], [2, 4], [3], [3], [3], [3], [2, 4], [3, 4], [3], [0, 1], [1], [2, 4], [2, 4], [4], [1, 3], [1], [2, 4], [2, 4], [4], [3], [1], [2, 4], [4], [3, 4], [1, 3], [1, 3], [2, 4], [3], [2, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.708\n",
      "tensor([1, 3, 4, 1, 2, 3, 4, 4, 3, 4, 3, 1, 3, 4, 4, 4, 4, 1, 3, 4, 1, 4, 1, 4,\n",
      "        4, 4, 4, 4, 1, 3, 3, 3, 1, 1, 3, 4, 4, 4, 3, 1, 1, 4, 1, 2, 3, 1, 1, 3,\n",
      "        4, 4, 1, 3, 1, 3, 1, 3, 1, 4, 1, 2, 3, 3, 1, 3, 1, 4, 3, 3, 1, 3, 4, 4,\n",
      "        1, 4, 3, 2, 3, 1, 3, 1, 3, 4, 1, 3, 1, 3, 1, 1, 4, 3, 4, 3, 3, 3, 4, 1,\n",
      "        3, 3, 3, 3, 3, 1, 4, 1, 4, 3, 1, 4, 3])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 80.73394495412845\n",
      "Epoch: 1  Loss: 81.65137614678899\n",
      "Epoch: 2  Loss: 85.3211009174312\n",
      "Epoch: 3  Loss: 86.23853211009175\n",
      "Epoch: 4  Loss: 88.9908256880734\n",
      "Epoch: 5  Loss: 88.9908256880734\n",
      "Epoch: 6  Loss: 90.82568807339449\n",
      "Epoch: 7  Loss: 90.82568807339449\n",
      "Epoch: 8  Loss: 90.82568807339449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 5/15 [03:37<06:58, 41.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 90.82568807339449\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 2.877951059341431  Accuracy on Support set:20.0\n",
      "Train_Epoch: 1  Train_Loss: 2.2085810005664825  Accuracy on Support set:24.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6949939692020417  Accuracy on Support set:36.0\n",
      "Train_Epoch: 3  Train_Loss: 1.3853734290599824  Accuracy on Support set:40.0\n",
      "Train_Epoch: 4  Train_Loss: 1.1916523134708406  Accuracy on Support set:60.0\n",
      "Train_Epoch: 5  Train_Loss: 1.0591265457868575  Accuracy on Support set:60.0\n",
      "Train_Epoch: 6  Train_Loss: 0.9605029946565629  Accuracy on Support set:60.0\n",
      "Train_Epoch: 7  Train_Loss: 0.8825041937828064  Accuracy on Support set:76.0\n",
      "Train_Epoch: 8  Train_Loss: 0.8179124593734741  Accuracy on Support set:76.0\n",
      "Train_Epoch: 9  Train_Loss: 0.7617778801918029  Accuracy on Support set:80.0\n",
      "Train_Epoch: 10  Train_Loss: 0.7121407377719879  Accuracy on Support set:84.0\n",
      "Train_Epoch: 11  Train_Loss: 0.6671361899375916  Accuracy on Support set:92.0\n",
      "Train_Epoch: 12  Train_Loss: 0.625370798110962  Accuracy on Support set:92.0\n",
      "Train_Epoch: 13  Train_Loss: 0.5861483871936798  Accuracy on Support set:92.0\n",
      "Train_Epoch: 14  Train_Loss: 0.5497928839921952  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.5158439838886261  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.4841644638776779  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.4546649706363678  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.42711827099323274  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.4014294198155403  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.37752270996570586  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.3551996627449989  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.3344786736369133  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.3151571950316429  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.2973218238353729  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.28067813456058505  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.2653359228372574  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.2509267687797546  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.23767994731664657  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.22536158412694932  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.213887996673584  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.20331495374441147  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.19334835678339005  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.18415755152702332  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.1755455169081688  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.16752179712057114  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.16003026872873305  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.15302748665213584  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.14648587837815286  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.14036484137177468  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.13460342317819596  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.1292233009636402  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.12416644483804702  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.119409269541502  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.11492818847298622  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.11070171609520912  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.1067229813337326  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.10296868443489075  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.09940002262592315  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.09604107618331909  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  62.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 6.85779559717048e-06 tensor([6.5220e-01, 1.8732e-01, 6.8578e-06, 1.5557e-01, 4.9122e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005568633321672678 tensor([0.0395, 0.4610, 0.0056, 0.2849, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.661429779342143e-06 tensor([6.6614e-06, 9.3345e-03, 6.2061e-01, 1.4068e-03, 3.6865e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013209133176133037 tensor([0.0935, 0.2761, 0.0013, 0.4857, 0.1433], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011611423455178738 tensor([0.0204, 0.4018, 0.0116, 0.1445, 0.4216], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00041067044367082417 tensor([1.9715e-01, 5.8479e-01, 4.1067e-04, 1.5586e-01, 6.1781e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00023239028814714402 tensor([2.0829e-01, 4.5785e-01, 2.3239e-04, 2.8756e-01, 4.6072e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.4799482313683257e-05 tensor([5.4799e-05, 3.0443e-02, 3.2388e-01, 5.1545e-03, 6.4047e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003166353271808475 tensor([2.0784e-01, 3.7034e-01, 3.1664e-04, 3.6626e-01, 5.5240e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.168862455524504e-05 tensor([6.1689e-05, 1.6011e-02, 1.8618e-01, 1.1606e-02, 7.8615e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.813733645889442e-06 tensor([6.3407e-01, 2.0582e-01, 7.8137e-06, 1.5516e-01, 4.9440e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006912151351571083 tensor([0.0069, 0.3181, 0.0351, 0.1238, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9997875117260264e-06 tensor([2.9998e-06, 3.1097e-03, 6.0717e-01, 1.6001e-03, 3.8811e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000980391982011497 tensor([0.0103, 0.0142, 0.0010, 0.9033, 0.0713], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013014773838222027 tensor([0.0130, 0.1635, 0.0140, 0.2478, 0.5617], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.6138965192076284e-06 tensor([6.1181e-01, 2.7122e-01, 5.6139e-06, 1.1300e-01, 3.9662e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006303070113062859 tensor([0.0006, 0.2565, 0.2416, 0.0128, 0.4885], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.519189810205717e-06 tensor([6.5192e-06, 1.0937e-02, 5.0253e-01, 1.1577e-03, 4.8537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.1374416216276586e-05 tensor([8.2561e-02, 2.4376e-02, 5.1374e-05, 8.7766e-01, 1.5357e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3076152046996867e-06 tensor([2.3076e-06, 3.0073e-03, 7.3513e-01, 1.3025e-03, 2.6056e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006176340393722057 tensor([0.1991, 0.4854, 0.0006, 0.2555, 0.0594], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0035466470289975405 tensor([0.0429, 0.2586, 0.0035, 0.4835, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.570515557134058e-06 tensor([4.5705e-06, 4.5613e-03, 6.4117e-01, 2.1200e-03, 3.5214e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005281887133605778 tensor([1.3596e-01, 1.5336e-01, 5.2819e-04, 6.3600e-01, 7.4151e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011979109840467572 tensor([0.0012, 0.0492, 0.0453, 0.0908, 0.8134], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.14554817748558e-08 tensor([8.0313e-01, 9.7233e-03, 3.1455e-08, 1.8705e-01, 9.4461e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010522342287003994 tensor([0.0258, 0.5457, 0.0105, 0.1793, 0.2386], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009246857953257859 tensor([0.0009, 0.1748, 0.1674, 0.0195, 0.6374], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003986093506682664 tensor([1.8126e-01, 4.2182e-01, 3.9861e-04, 3.1873e-01, 7.7784e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.4296001533439266e-07 tensor([4.4296e-07, 1.2770e-03, 8.0161e-01, 3.3084e-04, 1.9678e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.061287001671076e-09 tensor([9.5288e-01, 1.3546e-02, 3.0613e-09, 3.3546e-02, 2.4466e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013964815065264702 tensor([0.0140, 0.5916, 0.0145, 0.0704, 0.3095], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.326186283127754e-07 tensor([5.3262e-07, 1.9338e-03, 8.5490e-01, 2.7116e-04, 1.4290e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016577722504734993 tensor([0.0166, 0.2535, 0.0187, 0.2470, 0.4642], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0038389868568629026 tensor([0.0038, 0.0893, 0.0230, 0.1212, 0.7626], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00371998967602849 tensor([0.0450, 0.1869, 0.0037, 0.5365, 0.2279], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005655839340761304 tensor([4.5752e-02, 8.8193e-01, 5.6558e-04, 1.7008e-02, 5.4749e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.198258491465822e-05 tensor([3.1983e-05, 2.9750e-02, 5.1090e-01, 2.6465e-03, 4.5668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019021430052816868 tensor([0.0705, 0.2870, 0.0019, 0.4524, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.843385638901964e-05 tensor([3.8434e-05, 1.0470e-02, 2.1929e-01, 1.0215e-02, 7.5999e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.691356885658024e-07 tensor([8.5477e-01, 6.6126e-02, 1.6914e-07, 7.8647e-02, 4.6192e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017682280158624053 tensor([0.0735, 0.6381, 0.0018, 0.1594, 0.1273], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008434555493295193 tensor([0.0084, 0.3973, 0.0167, 0.0556, 0.5220], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015666967956349254 tensor([0.0581, 0.1488, 0.0016, 0.6844, 0.1072], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.733345809247112e-06 tensor([2.7333e-06, 5.6311e-03, 7.0535e-01, 7.6014e-04, 2.8825e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.512994445984077e-07 tensor([7.9718e-01, 1.5333e-01, 6.5130e-07, 4.8767e-02, 7.1883e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010285096941515803 tensor([0.0414, 0.8528, 0.0010, 0.0376, 0.0672], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00013290125934872776 tensor([1.3290e-04, 2.1568e-02, 1.8022e-01, 2.5521e-02, 7.7255e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00623641861602664 tensor([0.0120, 0.0649, 0.0062, 0.6067, 0.3102], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002877031685784459 tensor([0.0029, 0.1779, 0.0448, 0.0574, 0.7171], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001208301866427064 tensor([0.1322, 0.4059, 0.0012, 0.3620, 0.0986], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013855688273906708 tensor([0.0139, 0.3925, 0.0271, 0.1393, 0.4273], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013868745416402817 tensor([0.0139, 0.3883, 0.0199, 0.1345, 0.4435], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008131446316838264 tensor([0.0272, 0.2411, 0.0081, 0.3620, 0.3616], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8442455257172696e-05 tensor([1.8442e-05, 9.3356e-03, 3.8813e-01, 5.6208e-03, 5.9689e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009301012614741921 tensor([0.1433, 0.5972, 0.0009, 0.1897, 0.0688], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003567039093468338 tensor([1.5873e-01, 6.3716e-01, 3.5670e-04, 1.5004e-01, 5.3715e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011458005756139755 tensor([0.0142, 0.1454, 0.0115, 0.3663, 0.4627], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.703897891682573e-05 tensor([3.7039e-05, 9.4853e-03, 4.6530e-01, 2.0034e-02, 5.0514e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004754333756864071 tensor([0.0325, 0.1826, 0.0048, 0.4033, 0.3769], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006414384581148624 tensor([0.0433, 0.4667, 0.0064, 0.2113, 0.2724], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010610718745738268 tensor([0.0760, 0.2533, 0.0011, 0.5630, 0.1066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010309802019037306 tensor([1.0310e-04, 3.5654e-02, 3.5541e-01, 1.1856e-02, 5.9698e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9975545001216233e-05 tensor([1.9929e-01, 4.4666e-02, 1.9976e-05, 7.4510e-01, 1.0927e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0073521011509001255 tensor([0.0074, 0.1019, 0.0139, 0.2112, 0.6657], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.344736091368759e-07 tensor([8.8202e-01, 7.5287e-02, 1.3447e-07, 4.2286e-02, 4.0731e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007542746607214212 tensor([0.0268, 0.6286, 0.0075, 0.1259, 0.2112], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008264129282906651 tensor([0.0008, 0.1778, 0.1354, 0.0151, 0.6709], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00032690170337446034 tensor([1.3859e-01, 1.7088e-01, 3.2690e-04, 6.1212e-01, 7.8081e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009527637623250484 tensor([0.0095, 0.1924, 0.0173, 0.1531, 0.6276], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.354112312896177e-05 tensor([7.3541e-05, 3.8021e-02, 4.9717e-01, 6.6249e-03, 4.5811e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013366035418584943 tensor([0.0013, 0.2356, 0.1227, 0.0347, 0.6056], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.437632701912662e-06 tensor([2.4376e-06, 3.7142e-03, 7.1392e-01, 1.0786e-03, 2.8128e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007172161131165922 tensor([0.0007, 0.0450, 0.1311, 0.1293, 0.6939], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.316230842960067e-05 tensor([4.3162e-05, 1.7100e-02, 2.0873e-01, 6.5369e-03, 7.6759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7409050744608976e-06 tensor([6.8403e-01, 1.2848e-01, 2.7409e-06, 1.8443e-01, 3.0661e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009229102171957493 tensor([0.0110, 0.7685, 0.0092, 0.0209, 0.1904], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.1040646970504895e-05 tensor([2.1041e-05, 7.0038e-03, 4.4749e-01, 1.2007e-02, 5.3348e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002212498802691698 tensor([0.0107, 0.0222, 0.0022, 0.8616, 0.1033], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012852380750700831 tensor([0.0013, 0.0765, 0.0511, 0.0483, 0.8228], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.497226934243372e-08 tensor([9.1136e-01, 4.9804e-02, 6.4972e-08, 3.8673e-02, 1.6542e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.8594753025099635e-05 tensor([2.9101e-01, 5.8185e-01, 4.8595e-05, 1.1198e-01, 1.5109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010894615203142166 tensor([0.0218, 0.2302, 0.0109, 0.3344, 0.4027], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016590923769399524 tensor([0.0017, 0.0232, 0.0416, 0.4562, 0.4773], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005146992043592036 tensor([5.1470e-04, 3.5888e-02, 6.1910e-02, 4.4752e-02, 8.5694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0390285751782358e-05 tensor([5.3298e-01, 3.6554e-01, 1.0390e-05, 9.4664e-02, 6.8076e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007007410749793053 tensor([0.1176, 0.2998, 0.0007, 0.5028, 0.0791], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014163627056404948 tensor([0.0014, 0.0866, 0.1324, 0.1210, 0.6586], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005011699628084898 tensor([0.0059, 0.0192, 0.0050, 0.7960, 0.1739], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006821014685556293 tensor([6.8210e-04, 4.0222e-02, 7.4959e-02, 5.7020e-02, 8.2712e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.347804734716192e-06 tensor([5.5921e-01, 1.1416e-01, 9.3478e-06, 3.2249e-01, 4.1332e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002116894582286477 tensor([0.0760, 0.3146, 0.0021, 0.4531, 0.1541], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0609146304195747e-05 tensor([2.0609e-05, 2.4044e-02, 5.1040e-01, 2.4178e-03, 4.6311e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0019012725679203868 tensor([0.0019, 0.2530, 0.1098, 0.0280, 0.6073], grad_fn=<SoftmaxBackward0>)\n",
      "0 9.974782733479515e-05 tensor([9.9748e-05, 4.0035e-02, 2.5685e-01, 7.5709e-03, 6.9545e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.037373582832515e-05 tensor([4.1633e-01, 3.2880e-01, 4.0374e-05, 2.3982e-01, 1.5005e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010021122870966792 tensor([0.1028, 0.5574, 0.0010, 0.2230, 0.1158], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0289580387734532e-07 tensor([1.0290e-07, 7.2347e-04, 8.6039e-01, 1.0159e-04, 1.3879e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0047459788620471954 tensor([0.0047, 0.0592, 0.0255, 0.4930, 0.4175], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002165905199944973 tensor([0.0598, 0.6639, 0.0022, 0.1175, 0.1566], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00018793586059473455 tensor([2.2955e-01, 2.1807e-01, 1.8794e-04, 5.1357e-01, 3.8623e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.990951856598258e-05 tensor([3.3082e-01, 5.3263e-01, 4.9910e-05, 1.2192e-01, 1.4574e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.554650843147101e-07 tensor([4.5547e-07, 2.2795e-03, 8.7857e-01, 1.6677e-04, 1.1898e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006665387772955 tensor([0.1634, 0.4183, 0.0007, 0.3300, 0.0876], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005048914346843958 tensor([0.0050, 0.3631, 0.0266, 0.0370, 0.5683], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.309988786919348e-07 tensor([8.5440e-01, 9.2788e-02, 2.3100e-07, 5.2331e-02, 4.8421e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014514021575450897 tensor([0.0145, 0.6295, 0.0156, 0.0567, 0.2837], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.986754831293183e-08 tensor([4.9868e-08, 1.9226e-04, 8.5049e-01, 2.0240e-04, 1.4912e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005108424928039312 tensor([0.0051, 0.0305, 0.0106, 0.6958, 0.2581], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012434187810868025 tensor([0.0571, 0.1344, 0.0012, 0.5670, 0.2402], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.160197413696551e-07 tensor([8.6542e-01, 5.8896e-02, 1.1602e-07, 7.5320e-02, 3.6865e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001589777646586299 tensor([0.0841, 0.5224, 0.0016, 0.2847, 0.1072], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0108469616909588e-08 tensor([1.0108e-08, 1.0209e-04, 9.5253e-01, 3.8845e-05, 4.7324e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00100295792799443 tensor([0.1325, 0.4677, 0.0010, 0.3013, 0.0975], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.525238470407203e-05 tensor([7.5252e-05, 3.6780e-02, 2.6242e-01, 5.2292e-03, 6.9550e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4281923768066918e-06 tensor([7.6556e-01, 1.3547e-01, 1.4282e-06, 9.7618e-02, 1.3521e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013499802444130182 tensor([0.0013, 0.1144, 0.1206, 0.0934, 0.6703], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1732807251974009e-05 tensor([1.1733e-05, 2.3568e-02, 6.0232e-01, 1.0015e-03, 3.7310e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.994394203094998e-06 tensor([6.1090e-01, 2.2000e-01, 4.9944e-06, 1.6387e-01, 5.2233e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6679904319971683e-06 tensor([1.6680e-06, 1.4937e-03, 5.5588e-01, 1.9093e-03, 4.4071e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.608093826798722e-06 tensor([5.0600e-01, 1.0652e-01, 6.6081e-06, 3.8218e-01, 5.2971e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.145795537624508e-05 tensor([3.3244e-01, 4.9195e-01, 4.1458e-05, 1.6131e-01, 1.4258e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.631459993717726e-06 tensor([7.6315e-06, 7.2792e-03, 6.4396e-01, 2.4748e-03, 3.4628e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007238209946081042 tensor([5.2764e-02, 6.9473e-02, 7.2382e-04, 7.9300e-01, 8.4041e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015981913020368665 tensor([1.5982e-04, 1.6676e-02, 1.1096e-01, 2.9742e-02, 8.4246e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007317920681089163 tensor([0.1497, 0.5523, 0.0007, 0.2014, 0.0958], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007082611322402954 tensor([0.0071, 0.3327, 0.0349, 0.0916, 0.5337], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1097937497625026e-08 tensor([4.1098e-08, 2.5730e-04, 9.3312e-01, 7.6901e-05, 6.6545e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012985733337700367 tensor([0.0551, 0.1227, 0.0013, 0.6885, 0.1324], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.0819442549254745e-05 tensor([5.0819e-05, 9.3663e-03, 1.7083e-01, 1.4310e-02, 8.0544e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3814209271222353e-05 tensor([3.4361e-01, 1.1134e-01, 2.3814e-05, 5.3141e-01, 1.3618e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0053715454414486885 tensor([0.0316, 0.5391, 0.0054, 0.1459, 0.2779], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.756871471225168e-07 tensor([6.7569e-07, 2.5869e-03, 8.2140e-01, 2.9462e-04, 1.7571e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004437434545252472 tensor([4.4374e-04, 3.8459e-02, 1.4882e-01, 7.0503e-02, 7.4177e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001103583024814725 tensor([0.0011, 0.0399, 0.0543, 0.1083, 0.7964], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.472679600236006e-05 tensor([3.9645e-01, 4.5659e-01, 3.4727e-05, 1.3057e-01, 1.6357e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01198037900030613 tensor([0.0245, 0.4139, 0.0120, 0.2553, 0.2943], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.567948615388559e-08 tensor([1.5679e-08, 2.1572e-04, 9.5051e-01, 2.6874e-05, 4.9243e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007230936316773295 tensor([5.5261e-02, 7.6776e-02, 7.2309e-04, 7.8900e-01, 7.8239e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007675853557884693 tensor([0.0077, 0.1495, 0.0170, 0.1500, 0.6758], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.697913670246521e-09 tensor([9.6523e-01, 1.5015e-02, 2.6979e-09, 1.9742e-02, 1.6367e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017818420892581344 tensor([0.0822, 0.4432, 0.0018, 0.3276, 0.1453], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014609834179282188 tensor([0.0015, 0.3636, 0.0889, 0.0122, 0.5337], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.884829468210228e-05 tensor([3.2333e-01, 1.9375e-01, 4.8848e-05, 4.6624e-01, 1.6641e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.104938539559953e-05 tensor([2.1049e-05, 2.3806e-02, 5.0838e-01, 2.3812e-03, 4.6541e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8289787476533093e-05 tensor([3.2711e-01, 7.9194e-02, 1.8290e-05, 5.8213e-01, 1.1551e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.5089288101298735e-05 tensor([2.5089e-05, 4.4903e-02, 5.8556e-01, 1.4606e-03, 3.6806e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4137967557180673e-05 tensor([2.4138e-05, 1.5953e-02, 5.5310e-01, 4.4354e-03, 4.2649e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0021015184465795755 tensor([0.0146, 0.0363, 0.0021, 0.8357, 0.1112], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008327343966811895 tensor([0.0690, 0.0856, 0.0008, 0.7201, 0.1245], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013805681373924017 tensor([0.1144, 0.5771, 0.0014, 0.1996, 0.1075], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014757935423403978 tensor([0.0015, 0.1050, 0.0968, 0.1051, 0.6916], grad_fn=<SoftmaxBackward0>)\n",
      "0 9.902842066367157e-07 tensor([9.9028e-07, 6.9919e-03, 8.2950e-01, 1.5673e-04, 1.6335e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.320401346078143e-05 tensor([2.7903e-01, 1.7470e-01, 7.3204e-05, 5.2158e-01, 2.4613e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.70294934429694e-06 tensor([8.7029e-06, 4.3950e-03, 3.5131e-01, 4.8000e-03, 6.3949e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017244641203433275 tensor([0.0543, 0.7585, 0.0017, 0.0511, 0.1344], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006019304972141981 tensor([0.0277, 0.1876, 0.0060, 0.5119, 0.2667], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014329495606943965 tensor([0.0014, 0.1267, 0.0898, 0.0569, 0.7253], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004025191534310579 tensor([0.0043, 0.0121, 0.0040, 0.8294, 0.1502], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004404354258440435 tensor([1.5896e-01, 2.6791e-01, 4.4044e-04, 4.9177e-01, 8.0921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.16462527634576e-05 tensor([3.4652e-01, 1.4684e-01, 7.1646e-05, 4.8935e-01, 1.7221e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0028885253705084324 tensor([0.0671, 0.3688, 0.0029, 0.3807, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.780539453960955e-05 tensor([7.7805e-05, 2.1366e-02, 2.8770e-01, 1.5708e-02, 6.7515e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2373045542044565e-06 tensor([6.3665e-01, 1.3972e-01, 3.2373e-06, 2.2082e-01, 2.8058e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.652475374517962e-05 tensor([5.6525e-05, 3.0912e-02, 4.0178e-01, 5.1229e-03, 5.6212e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.62084613495972e-05 tensor([4.6099e-01, 2.8628e-01, 2.6208e-05, 2.3956e-01, 1.3141e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00029999666730873287 tensor([1.5708e-01, 6.4367e-01, 3.0000e-04, 1.5538e-01, 4.3571e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.076000254509381e-08 tensor([3.0760e-08, 3.0031e-04, 9.3229e-01, 4.5588e-05, 6.7367e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0064842309802770615 tensor([0.0410, 0.3211, 0.0065, 0.3732, 0.2582], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014093701727688313 tensor([0.0168, 0.2502, 0.0141, 0.1997, 0.5192], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00021517666755244136 tensor([3.1045e-01, 4.1242e-01, 2.1518e-04, 2.4798e-01, 2.8930e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006528723519295454 tensor([0.0303, 0.6444, 0.0065, 0.1040, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00016520604549441487 tensor([1.6521e-04, 7.4046e-02, 2.5838e-01, 7.3969e-03, 6.6002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012251492589712143 tensor([0.0123, 0.1602, 0.0178, 0.3779, 0.4319], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003786699671763927 tensor([7.8341e-02, 7.0454e-02, 3.7867e-04, 7.6898e-01, 8.1846e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.661447323111133e-08 tensor([9.0702e-01, 3.5991e-02, 3.6614e-08, 5.6782e-02, 2.0441e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014484571292996407 tensor([0.0014, 0.2154, 0.1181, 0.0462, 0.6189], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.906029062112793e-06 tensor([4.9060e-06, 7.3566e-03, 6.2735e-01, 1.1151e-03, 3.6417e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003218498546630144 tensor([0.0601, 0.3228, 0.0032, 0.3893, 0.2246], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004922640509903431 tensor([4.9226e-04, 3.0407e-02, 6.2013e-02, 6.6050e-02, 8.4104e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.978650743694743e-06 tensor([7.4116e-01, 1.8169e-01, 1.9787e-06, 7.4279e-02, 2.8617e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0033938405103981495 tensor([0.0457, 0.5291, 0.0034, 0.1878, 0.2340], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.820542668923736e-05 tensor([7.8205e-05, 2.3325e-02, 3.1837e-01, 1.2300e-02, 6.4593e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.921662472654134e-05 tensor([6.9659e-02, 2.9032e-02, 7.9217e-05, 8.7503e-01, 2.6196e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00027907497133128345 tensor([2.7907e-04, 3.5731e-02, 1.2336e-01, 3.2949e-02, 8.0768e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.3973167268850375e-06 tensor([5.7200e-01, 8.3700e-02, 5.3973e-06, 3.4015e-01, 4.1402e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006106653599999845 tensor([0.1482, 0.5051, 0.0006, 0.2884, 0.0577], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00011037847434636205 tensor([1.1038e-04, 2.3953e-02, 2.1254e-01, 1.6596e-02, 7.4680e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006568541284650564 tensor([8.4723e-02, 7.9458e-01, 6.5685e-04, 5.8344e-02, 6.1700e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001044483738951385 tensor([0.0010, 0.0558, 0.0512, 0.0690, 0.8230], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.3269326965855726e-07 tensor([7.2238e-01, 2.6064e-01, 4.3269e-07, 1.6334e-02, 6.4375e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00115075777284801 tensor([0.0776, 0.7270, 0.0012, 0.1178, 0.0764], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2945841731948349e-08 tensor([1.2946e-08, 1.4294e-04, 9.3529e-01, 3.7933e-05, 6.4529e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0027377870865166187 tensor([0.0027, 0.0535, 0.0469, 0.3119, 0.5850], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.7158247323532123e-07 tensor([3.7158e-07, 5.5914e-04, 7.2789e-01, 8.7047e-04, 2.7068e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.890281957159459e-07 tensor([8.0832e-01, 1.2023e-01, 4.8903e-07, 7.0419e-02, 1.0312e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003856194205582142 tensor([0.0473, 0.4452, 0.0039, 0.2945, 0.2091], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.9431798136793077e-05 tensor([1.9432e-05, 2.1006e-02, 5.0221e-01, 2.6746e-03, 4.7409e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006236955989152193 tensor([0.0062, 0.0792, 0.0271, 0.4699, 0.4177], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0033248267136514187 tensor([0.0582, 0.3807, 0.0033, 0.3106, 0.2472], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.660950253310148e-06 tensor([6.1215e-01, 2.9788e-01, 4.6610e-06, 8.5959e-02, 4.0035e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008926603943109512 tensor([0.0144, 0.7039, 0.0089, 0.0467, 0.2260], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.797949143016012e-06 tensor([5.7979e-06, 7.3937e-03, 5.7105e-01, 1.4027e-03, 4.2015e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005674626678228378 tensor([0.0310, 0.2606, 0.0057, 0.3740, 0.3287], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00188878842163831 tensor([0.0019, 0.0466, 0.0244, 0.1202, 0.8069], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8987511793966405e-05 tensor([4.8056e-01, 1.9412e-01, 1.8988e-05, 3.1604e-01, 9.2667e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.025598118711059e-07 tensor([7.6605e-01, 1.9677e-01, 3.0256e-07, 3.6599e-02, 5.7586e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000993529916740954 tensor([0.0010, 0.0985, 0.0809, 0.0410, 0.7787], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.250449561979622e-05 tensor([2.3979e-01, 1.2162e-01, 7.2504e-05, 6.0753e-01, 3.0989e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0064367009326815605 tensor([0.0064, 0.0824, 0.0155, 0.2184, 0.6773], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.967392731534346e-07 tensor([7.8844e-01, 5.1359e-02, 2.9674e-07, 1.5952e-01, 6.8919e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008521018549799919 tensor([0.0221, 0.6752, 0.0085, 0.0868, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008226121426559985 tensor([0.0008, 0.2340, 0.1476, 0.0097, 0.6079], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00020920550741720945 tensor([2.0727e-01, 2.2433e-01, 2.0921e-04, 5.2380e-01, 4.4389e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002267577510792762 tensor([2.2676e-04, 1.3920e-02, 9.8640e-02, 6.4970e-02, 8.2224e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.09152744396124e-06 tensor([6.9498e-01, 1.3015e-01, 4.0915e-06, 1.7236e-01, 2.5084e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010824382305145264 tensor([0.0108, 0.7482, 0.0149, 0.0268, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.033949608128751e-06 tensor([4.0339e-06, 5.6772e-03, 7.1738e-01, 1.3209e-03, 2.7562e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007469136267900467 tensor([0.0162, 0.0883, 0.0075, 0.6278, 0.2603], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.771975454787025e-07 tensor([4.7720e-07, 9.5072e-04, 6.7806e-01, 6.0834e-04, 3.2038e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.41304097068496e-05 tensor([3.5974e-01, 3.5903e-01, 9.4130e-05, 2.6467e-01, 1.6464e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007256646058522165 tensor([0.0007, 0.3470, 0.1608, 0.0078, 0.4837], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.539803871419281e-05 tensor([6.5398e-05, 3.6888e-02, 4.5757e-01, 6.3786e-03, 4.9910e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.139392108801985e-06 tensor([6.5393e-01, 2.6671e-01, 2.1394e-06, 7.6648e-02, 2.7076e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012419496662914753 tensor([0.0124, 0.2061, 0.0279, 0.2699, 0.4837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012175136944279075 tensor([0.1198, 0.3512, 0.0012, 0.4337, 0.0942], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005874174647033215 tensor([0.0338, 0.5606, 0.0059, 0.1865, 0.2133], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.389733920921572e-06 tensor([8.3897e-06, 7.9371e-03, 5.3347e-01, 2.3194e-03, 4.5627e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8298165741725825e-05 tensor([1.5735e-01, 2.9496e-02, 1.8298e-05, 8.0492e-01, 8.2208e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014256346912588924 tensor([3.0996e-01, 3.7843e-01, 1.4256e-04, 2.7047e-01, 4.1004e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.9189609424283844e-07 tensor([8.0061e-01, 5.6617e-02, 3.9190e-07, 1.4208e-01, 6.9504e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011435619089752436 tensor([0.1063, 0.5032, 0.0011, 0.3087, 0.0807], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.337915379437618e-05 tensor([4.3379e-05, 2.2281e-02, 4.8418e-01, 6.8164e-03, 4.8668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007070436142385006 tensor([0.0071, 0.0946, 0.0171, 0.3612, 0.5201], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007764262612909079 tensor([0.0078, 0.1263, 0.0204, 0.2592, 0.5863], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00047799316234886646 tensor([0.2493, 0.3869, 0.0005, 0.3087, 0.0547], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0026239026337862015 tensor([0.0026, 0.2172, 0.0687, 0.0491, 0.6623], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.2297142752213404e-05 tensor([3.2297e-05, 1.7884e-02, 4.2145e-01, 5.2508e-03, 5.5538e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014671471435576677 tensor([0.0624, 0.1629, 0.0015, 0.6259, 0.1473], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0677818611147813e-05 tensor([2.0678e-05, 6.0990e-03, 3.4859e-01, 1.1228e-02, 6.3407e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.828897428727942e-06 tensor([6.3620e-01, 2.1979e-01, 5.8289e-06, 1.3919e-01, 4.8196e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012228372506797314 tensor([0.0944, 0.5351, 0.0012, 0.2480, 0.1213], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3039155419392046e-05 tensor([1.3039e-05, 1.8578e-02, 5.9645e-01, 1.5854e-03, 3.8337e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009414103697054088 tensor([0.0171, 0.0252, 0.0009, 0.8646, 0.0923], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015858588740229607 tensor([0.0016, 0.1178, 0.0727, 0.0514, 0.7565], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.383732132851037e-08 tensor([8.6502e-01, 1.2444e-01, 4.3837e-08, 1.0400e-02, 1.4543e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008841617964208126 tensor([0.0630, 0.8067, 0.0009, 0.0701, 0.0594], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.3409944083805385e-08 tensor([4.3410e-08, 3.1718e-04, 9.3274e-01, 6.6896e-05, 6.6878e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010111573152244091 tensor([0.0101, 0.1602, 0.0230, 0.3055, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.837793752201833e-05 tensor([1.8378e-05, 5.9763e-03, 2.8682e-01, 8.0863e-03, 6.9910e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [2], [0], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [0], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [2], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0], [2], [0], [0], [0], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [0], [2], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0]]\n",
      "[[0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [2], [0], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [0], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [2], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0], [2], [0], [0], [0], [2], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [0], [2], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [2], [2], [2], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004393850803375244  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004393849372863769  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004393847465515137  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0043938450813293456  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.00439384126663208  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004393837928771973  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004393834114074707  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0043938283920288085  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004393823623657226  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00439381742477417  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.00439381217956543  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0043938055038452144  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004393798828125  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004393792629241943  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0043937845230102536  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004393777370452881  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004393770694732666  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004393762111663819  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004393754959106445  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004393746376037597  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.0049573276191949844 tensor([6.4984e-01, 1.8882e-01, 6.9406e-06, 1.5637e-01, 4.9573e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.039295222610235214 tensor([0.0393, 0.4614, 0.0056, 0.2849, 0.2088], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014115730300545692 tensor([6.6695e-06, 9.3846e-03, 6.2031e-01, 1.4116e-03, 3.6888e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09288941323757172 tensor([0.0929, 0.2769, 0.0013, 0.4854, 0.1435], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020280182361602783 tensor([0.0203, 0.4026, 0.0116, 0.1444, 0.4211], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06194877624511719 tensor([1.9558e-01, 5.8620e-01, 4.1274e-04, 1.5585e-01, 6.1949e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04610852524638176 tensor([2.0705e-01, 4.5876e-01, 2.3264e-04, 2.8784e-01, 4.6109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00516321649774909 tensor([5.4721e-05, 3.0566e-02, 3.2384e-01, 5.1632e-03, 6.4038e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05539186671376228 tensor([2.0641e-01, 3.7171e-01, 3.1804e-04, 3.6618e-01, 5.5392e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011628563515841961 tensor([6.1634e-05, 1.6088e-02, 1.8618e-01, 1.1629e-02, 7.8604e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004987397231161594 tensor([6.3169e-01, 2.0736e-01, 7.9033e-06, 1.5595e-01, 4.9874e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.035069022327661514 tensor([0.0069, 0.3186, 0.0351, 0.1239, 0.5156], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001604966470040381 tensor([3.0045e-06, 3.1285e-03, 6.0693e-01, 1.6050e-03, 3.8834e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010241671465337276 tensor([0.0102, 0.0142, 0.0010, 0.9031, 0.0714], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013969358056783676 tensor([0.0130, 0.1640, 0.0140, 0.2477, 0.5613], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003999137785285711 tensor([6.0927e-01, 2.7319e-01, 5.6742e-06, 1.1354e-01, 3.9991e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012789299711585045 tensor([0.0006, 0.2570, 0.2413, 0.0128, 0.4883], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011611210647970438 tensor([6.5249e-06, 1.0997e-02, 5.0231e-01, 1.1611e-03, 4.8553e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01541992649435997 tensor([8.2113e-02, 2.4526e-02, 5.1682e-05, 8.7789e-01, 1.5420e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013068544212728739 tensor([2.3103e-06, 3.0235e-03, 7.3494e-01, 1.3069e-03, 2.6072e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.059517741203308105 tensor([0.1977, 0.4867, 0.0006, 0.2554, 0.0595], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04263324290513992 tensor([0.0426, 0.2591, 0.0035, 0.4832, 0.2114], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002126709558069706 tensor([4.5771e-06, 4.5880e-03, 6.4091e-01, 2.1267e-03, 3.5237e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07431388646364212 tensor([1.3521e-01, 1.5403e-01, 5.3005e-04, 6.3591e-01, 7.4314e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.045350465923547745 tensor([0.0012, 0.0494, 0.0454, 0.0909, 0.8132], grad_fn=<SoftmaxBackward0>)\n",
      "4 9.5592942670919e-05 tensor([8.0165e-01, 9.8252e-03, 3.1907e-08, 1.8843e-01, 9.5593e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.025702882558107376 tensor([0.0257, 0.5461, 0.0105, 0.1794, 0.2384], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01950656808912754 tensor([0.0009, 0.1752, 0.1674, 0.0195, 0.6370], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07793067395687103 tensor([1.7998e-01, 4.2311e-01, 4.0002e-04, 3.1858e-01, 7.7931e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003322666452731937 tensor([4.4433e-07, 1.2855e-03, 8.0138e-01, 3.3227e-04, 1.9701e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.474753091519233e-05 tensor([9.5249e-01, 1.3689e-02, 3.0992e-09, 3.3801e-02, 2.4748e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01448319386690855 tensor([0.0139, 0.5920, 0.0145, 0.0704, 0.3092], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002721261989790946 tensor([5.3338e-07, 1.9442e-03, 8.5476e-01, 2.7213e-04, 1.4302e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018693089485168457 tensor([0.0165, 0.2541, 0.0187, 0.2467, 0.4640], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.022977473214268684 tensor([0.0038, 0.0896, 0.0230, 0.1213, 0.7623], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04468486085534096 tensor([0.0447, 0.1875, 0.0037, 0.5359, 0.2282], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017006652429699898 tensor([4.5426e-02, 8.8226e-01, 5.6584e-04, 1.7007e-02, 5.4739e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0026533787604421377 tensor([3.1977e-05, 2.9881e-02, 5.1065e-01, 2.6534e-03, 4.5678e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07005593180656433 tensor([0.0701, 0.2878, 0.0019, 0.4519, 0.1883], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010234127752482891 tensor([3.8396e-05, 1.0519e-02, 2.1929e-01, 1.0234e-02, 7.5992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004678147961385548 tensor([8.5337e-01, 6.6858e-02, 1.7181e-07, 7.9300e-02, 4.6781e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07294218987226486 tensor([0.0729, 0.6387, 0.0018, 0.1593, 0.1273], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016697995364665985 tensor([0.0084, 0.3980, 0.0167, 0.0556, 0.5214], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05771508440375328 tensor([0.0577, 0.1495, 0.0016, 0.6838, 0.1074], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007624546997249126 tensor([2.7350e-06, 5.6592e-03, 7.0519e-01, 7.6245e-04, 2.8838e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007270650821737945 tensor([7.9533e-01, 1.5482e-01, 6.6055e-07, 4.9117e-02, 7.2707e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03762625530362129 tensor([0.0411, 0.8531, 0.0010, 0.0376, 0.0671], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021666469052433968 tensor([1.3270e-04, 2.1666e-02, 1.8023e-01, 2.5556e-02, 7.7242e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011942760087549686 tensor([0.0119, 0.0652, 0.0063, 0.6061, 0.3105], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04476899281144142 tensor([0.0029, 0.1783, 0.0448, 0.0574, 0.7166], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09880311787128448 tensor([0.1312, 0.4069, 0.0012, 0.3619, 0.0988], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027162102982401848 tensor([0.0138, 0.3928, 0.0272, 0.1391, 0.4271], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01986050046980381 tensor([0.0138, 0.3889, 0.0199, 0.1344, 0.4431], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026985792443156242 tensor([0.0270, 0.2417, 0.0081, 0.3615, 0.3617], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005634276662021875 tensor([1.8443e-05, 9.3840e-03, 3.8803e-01, 5.6343e-03, 5.9693e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06888729333877563 tensor([0.1422, 0.5983, 0.0009, 0.1896, 0.0689], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05376720428466797 tensor([1.5762e-01, 6.3817e-01, 3.5736e-04, 1.5009e-01, 5.3767e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014166089706122875 tensor([0.0142, 0.1458, 0.0114, 0.3663, 0.4623], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009529272094368935 tensor([3.6986e-05, 9.5293e-03, 4.6525e-01, 2.0054e-02, 5.0513e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03229522705078125 tensor([0.0323, 0.1832, 0.0048, 0.4030, 0.3767], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04298066720366478 tensor([0.0430, 0.4673, 0.0064, 0.2110, 0.2722], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07553619146347046 tensor([0.0755, 0.2538, 0.0011, 0.5629, 0.1066], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01187182404100895 tensor([1.0286e-04, 3.5776e-02, 3.5540e-01, 1.1872e-02, 5.9685e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01097786147147417 tensor([1.9820e-01, 4.4945e-02, 2.0109e-05, 7.4586e-01, 1.0978e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013917846605181694 tensor([0.0073, 0.1022, 0.0139, 0.2112, 0.6653], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00041263268212787807 tensor([8.8081e-01, 7.6131e-02, 1.3666e-07, 4.2648e-02, 4.1263e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026623232290148735 tensor([0.0266, 0.6289, 0.0075, 0.1259, 0.2110], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0151210343465209 tensor([0.0008, 0.1782, 0.1353, 0.0151, 0.6705], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07826516777276993 tensor([1.3777e-01, 1.7165e-01, 3.2816e-04, 6.1199e-01, 7.8265e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017335498705506325 tensor([0.0095, 0.1929, 0.0173, 0.1531, 0.6272], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0066380188800394535 tensor([7.3441e-05, 3.8157e-02, 4.9703e-01, 6.6380e-03, 4.5810e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03477523475885391 tensor([0.0013, 0.2361, 0.1226, 0.0348, 0.6052], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010823605116456747 tensor([2.4421e-06, 3.7363e-03, 7.1367e-01, 1.0824e-03, 2.8151e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.045184094458818436 tensor([0.0007, 0.0452, 0.1312, 0.1292, 0.6938], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006546701304614544 tensor([4.3103e-05, 1.7176e-02, 2.0875e-01, 6.5467e-03, 7.6748e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0030946137849241495 tensor([6.8189e-01, 1.2962e-01, 2.7735e-06, 1.8540e-01, 3.0946e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01096387766301632 tensor([0.0110, 0.7688, 0.0092, 0.0209, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007042801473289728 tensor([2.1054e-05, 7.0428e-03, 4.4727e-01, 1.2041e-02, 5.3363e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010628186166286469 tensor([0.0106, 0.0223, 0.0022, 0.8613, 0.1035], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.048356782644987106 tensor([0.0013, 0.0768, 0.0511, 0.0484, 0.8225], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00016772221715655178 tensor([9.1042e-01, 5.0403e-02, 6.6090e-08, 3.9011e-02, 1.6772e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015146064572036266 tensor([2.8931e-01, 5.8325e-01, 4.8744e-05, 1.1225e-01, 1.5146e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021673614159226418 tensor([0.0217, 0.2307, 0.0109, 0.3342, 0.4025], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.023345090448856354 tensor([0.0017, 0.0233, 0.0417, 0.4558, 0.4775], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03604618459939957 tensor([5.1356e-04, 3.6046e-02, 6.1904e-02, 4.4809e-02, 8.5673e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006854415405541658 tensor([5.3035e-01, 3.6777e-01, 1.0486e-05, 9.5017e-02, 6.8544e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07919126749038696 tensor([0.1168, 0.3006, 0.0007, 0.5026, 0.0792], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08684861660003662 tensor([0.0014, 0.0868, 0.1323, 0.1211, 0.6583], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005881639197468758 tensor([0.0059, 0.0193, 0.0050, 0.7956, 0.1742], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04037462919950485 tensor([6.7962e-04, 4.0375e-02, 7.5004e-02, 5.7024e-02, 8.2692e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004171197302639484 tensor([5.5685e-01, 1.1508e-01, 9.4612e-06, 3.2389e-01, 4.1712e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07549185305833817 tensor([0.0755, 0.3154, 0.0021, 0.4528, 0.1542], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002424814272671938 tensor([2.0606e-05, 2.4153e-02, 5.1018e-01, 2.4248e-03, 4.6323e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02800878696143627 tensor([0.0019, 0.2535, 0.1098, 0.0280, 0.6068], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007584359496831894 tensor([9.9615e-05, 4.0203e-02, 2.5680e-01, 7.5844e-03, 6.9531e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015087457373738289 tensor([4.1396e-01, 3.3053e-01, 4.0681e-05, 2.4039e-01, 1.5087e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10205961763858795 tensor([0.1021, 0.5582, 0.0010, 0.2229, 0.1159], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010214636131422594 tensor([1.0335e-07, 7.2883e-04, 8.6015e-01, 1.0215e-04, 1.3902e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.025564197450876236 tensor([0.0047, 0.0595, 0.0256, 0.4924, 0.4179], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.059384312480688095 tensor([0.0594, 0.6645, 0.0022, 0.1174, 0.1565], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03875100612640381 tensor([2.2814e-01, 2.1897e-01, 1.8887e-04, 5.1395e-01, 3.8751e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014618431217968464 tensor([3.2895e-01, 5.3416e-01, 5.0109e-05, 1.2223e-01, 1.4618e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001674408558756113 tensor([4.5645e-07, 2.2924e-03, 8.7842e-01, 1.6744e-04, 1.1912e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08778209984302521 tensor([0.1622, 0.4195, 0.0007, 0.3298, 0.0878], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02655736543238163 tensor([0.0050, 0.3637, 0.0266, 0.0369, 0.5677], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004898458137176931 tensor([8.5303e-01, 9.3753e-02, 2.3428e-07, 5.2732e-02, 4.8985e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015558510087430477 tensor([0.0144, 0.6298, 0.0156, 0.0567, 0.2835], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00019356237316969782 tensor([5.0004e-08, 1.9356e-04, 8.5036e-01, 2.0319e-04, 1.4925e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010576315224170685 tensor([0.0051, 0.0306, 0.0106, 0.6952, 0.2585], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05680917575955391 tensor([0.0568, 0.1349, 0.0012, 0.5668, 0.2403], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003737493243534118 tensor([8.6406e-01, 5.9591e-02, 1.1803e-07, 7.5971e-02, 3.7375e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0835643783211708 tensor([0.0836, 0.5230, 0.0016, 0.2847, 0.1072], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.899339571944438e-05 tensor([1.0137e-08, 1.0274e-04, 9.5249e-01, 3.8993e-05, 4.7367e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09757030010223389 tensor([0.1316, 0.4688, 0.0010, 0.3011, 0.0976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005238961894065142 tensor([7.5153e-05, 3.6930e-02, 2.6239e-01, 5.2390e-03, 6.9536e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001368428347632289 tensor([7.6352e-01, 1.3678e-01, 1.4502e-06, 9.8332e-02, 1.3684e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09344547241926193 tensor([0.0013, 0.1147, 0.1205, 0.0934, 0.6700], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010048578260466456 tensor([1.1743e-05, 2.3685e-02, 6.0199e-01, 1.0049e-03, 3.7331e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0052602957002818584 tensor([6.0861e-01, 2.2160e-01, 5.0393e-06, 1.6453e-01, 5.2603e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015031180810183287 tensor([1.6709e-06, 1.5031e-03, 5.5568e-01, 1.9151e-03, 4.4090e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0053329551592469215 tensor([5.0393e-01, 1.0731e-01, 6.6659e-06, 3.8343e-01, 5.3330e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014301721006631851 tensor([3.3055e-01, 4.9338e-01, 4.1612e-05, 1.6173e-01, 1.4302e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002483043819665909 tensor([7.6420e-06, 7.3202e-03, 6.4366e-01, 2.4830e-03, 3.4653e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.052485376596450806 tensor([5.2485e-02, 6.9840e-02, 7.2668e-04, 7.9269e-01, 8.4256e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01676235906779766 tensor([1.5971e-04, 1.6762e-02, 1.1093e-01, 2.9793e-02, 8.4235e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09593694657087326 tensor([0.1487, 0.5533, 0.0007, 0.2013, 0.0959], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03488193824887276 tensor([0.0070, 0.3332, 0.0349, 0.0916, 0.5332], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.722334703430533e-05 tensor([4.1224e-08, 2.5899e-04, 9.3304e-01, 7.7223e-05, 6.6627e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.054733142256736755 tensor([0.0547, 0.1233, 0.0013, 0.6880, 0.1327], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009410267695784569 tensor([5.0761e-05, 9.4103e-03, 1.7085e-01, 1.4333e-02, 8.0536e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013700238429009914 tensor([3.4177e-01, 1.1207e-01, 2.4017e-05, 5.3243e-01, 1.3700e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.031431734561920166 tensor([0.0314, 0.5396, 0.0054, 0.1459, 0.2777], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00029584881849586964 tensor([6.7719e-07, 2.6022e-03, 8.2119e-01, 2.9585e-04, 1.7591e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038592446595430374 tensor([4.4209e-04, 3.8592e-02, 1.4885e-01, 7.0514e-02, 7.4160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.040051329880952835 tensor([0.0011, 0.0401, 0.0542, 0.1084, 0.7962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016420841217041016 tensor([3.9418e-01, 4.5854e-01, 3.4916e-05, 1.3082e-01, 1.6421e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.024339433759450912 tensor([0.0243, 0.4144, 0.0120, 0.2553, 0.2940], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.6985104341292754e-05 tensor([1.5729e-08, 2.1709e-04, 9.5046e-01, 2.6985e-05, 4.9298e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.054942019283771515 tensor([5.4942e-02, 7.7152e-02, 7.2609e-04, 7.8873e-01, 7.8447e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017025083303451538 tensor([0.0076, 0.1500, 0.0170, 0.1500, 0.6754], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.6536594557692297e-05 tensor([9.6494e-01, 1.5160e-02, 2.7285e-09, 1.9882e-02, 1.6537e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08167126774787903 tensor([0.0817, 0.4438, 0.0018, 0.3274, 0.1453], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012252344749867916 tensor([0.0015, 0.3642, 0.0889, 0.0123, 0.5333], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01671675406396389 tensor([3.2152e-01, 1.9480e-01, 4.9158e-05, 4.6691e-01, 1.6717e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0023884568363428116 tensor([2.1055e-05, 2.3918e-02, 5.0813e-01, 2.3885e-03, 4.6554e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011614035815000534 tensor([3.2549e-01, 7.9719e-02, 1.8426e-05, 5.8316e-01, 1.1614e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014654035912826657 tensor([2.5085e-05, 4.5078e-02, 5.8523e-01, 1.4654e-03, 3.6820e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00444480124861002 tensor([2.4116e-05, 1.6019e-02, 5.5299e-01, 4.4448e-03, 4.2652e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014550876803696156 tensor([0.0146, 0.0365, 0.0021, 0.8353, 0.1115], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0685894563794136 tensor([0.0686, 0.0860, 0.0008, 0.7199, 0.1247], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10767180472612381 tensor([0.1135, 0.5780, 0.0014, 0.1995, 0.1077], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09668082743883133 tensor([0.0015, 0.1053, 0.0967, 0.1053, 0.6913], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00015740304661449045 tensor([9.9213e-07, 7.0289e-03, 8.2928e-01, 1.5740e-04, 1.6353e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024723820388317108 tensor([2.7741e-01, 1.7565e-01, 7.3681e-05, 5.2215e-01, 2.4724e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004419715143740177 tensor([8.7104e-06, 4.4197e-03, 3.5113e-01, 4.8138e-03, 6.3963e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05105939507484436 tensor([0.0538, 0.7589, 0.0017, 0.0511, 0.1345], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027567539364099503 tensor([0.0276, 0.1881, 0.0060, 0.5116, 0.2668], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05687103420495987 tensor([0.0014, 0.1270, 0.0898, 0.0569, 0.7249], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004314254503697157 tensor([0.0043, 0.0121, 0.0040, 0.8290, 0.1505], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08102071285247803 tensor([1.5800e-01, 2.6873e-01, 4.4143e-04, 4.9181e-01, 8.1021e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017321312800049782 tensor([3.4464e-01, 1.4770e-01, 7.2244e-05, 4.9026e-01, 1.7321e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06672918796539307 tensor([0.0667, 0.3694, 0.0029, 0.3805, 0.1804], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015739435330033302 tensor([7.7712e-05, 2.1459e-02, 2.8764e-01, 1.5739e-02, 6.7508e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002830492565408349 tensor([6.3441e-01, 1.4092e-01, 3.2732e-06, 2.2184e-01, 2.8305e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00513211265206337 tensor([5.6471e-05, 3.1044e-02, 4.0171e-01, 5.1321e-03, 5.6206e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013222814537584782 tensor([4.5854e-01, 2.8794e-01, 2.6433e-05, 2.4027e-01, 1.3223e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04362521693110466 tensor([1.5599e-01, 6.4455e-01, 3.0064e-04, 1.5554e-01, 4.3625e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.5778800995321944e-05 tensor([3.0853e-08, 3.0228e-04, 9.3220e-01, 4.5779e-05, 6.7450e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0407099574804306 tensor([0.0407, 0.3219, 0.0065, 0.3727, 0.2582], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016728617250919342 tensor([0.0167, 0.2508, 0.0141, 0.1996, 0.5188], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.029057884588837624 tensor([3.0839e-01, 4.1404e-01, 2.1660e-04, 2.4830e-01, 2.9058e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030093099921941757 tensor([0.0301, 0.6448, 0.0065, 0.1040, 0.2145], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00741036469116807 tensor([1.6496e-04, 7.4328e-02, 2.5825e-01, 7.4104e-03, 6.5985e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01777234673500061 tensor([0.0122, 0.1608, 0.0178, 0.3774, 0.4319], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07078646123409271 tensor([7.7946e-02, 7.0786e-02, 3.7972e-04, 7.6891e-01, 8.1982e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00020718258747365326 tensor([9.0610e-01, 3.6416e-02, 3.7213e-08, 5.7278e-02, 2.0718e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04623154550790787 tensor([0.0014, 0.2159, 0.1180, 0.0462, 0.6185], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011185311013832688 tensor([4.9108e-06, 7.3954e-03, 6.2712e-01, 1.1185e-03, 3.6436e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05963505432009697 tensor([0.0596, 0.3237, 0.0032, 0.3886, 0.2248], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030542545020580292 tensor([4.9110e-04, 3.0543e-02, 6.2015e-02, 6.6119e-02, 8.4083e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002888955408707261 tensor([7.3912e-01, 1.8328e-01, 2.0028e-06, 7.4709e-02, 2.8890e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04543616995215416 tensor([0.0454, 0.5296, 0.0034, 0.1878, 0.2337], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012324408628046513 tensor([7.8149e-05, 2.3434e-02, 3.1826e-01, 1.2324e-02, 6.4591e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026282381266355515 tensor([6.9298e-02, 2.9206e-02, 7.9601e-05, 8.7513e-01, 2.6282e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0330030620098114 tensor([2.7857e-04, 3.5883e-02, 1.2333e-01, 3.3003e-02, 8.0750e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0041764043271541595 tensor([5.6975e-01, 8.4370e-02, 5.4596e-06, 3.4170e-01, 4.1764e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05772083252668381 tensor([0.1473, 0.5058, 0.0006, 0.2886, 0.0577], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016616854816675186 tensor([1.1016e-04, 2.4054e-02, 2.1259e-01, 1.6617e-02, 7.4663e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05828748643398285 tensor([8.4057e-02, 7.9529e-01, 6.5764e-04, 5.8287e-02, 6.1707e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.051159054040908813 tensor([0.0010, 0.0561, 0.0512, 0.0690, 0.8227], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006493074470199645 tensor([7.2021e-01, 2.6272e-01, 4.3735e-07, 1.6423e-02, 6.4931e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07634789496660233 tensor([0.0771, 0.7275, 0.0012, 0.1179, 0.0763], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.806691165664233e-05 tensor([1.2978e-08, 1.4379e-04, 9.3523e-01, 3.8067e-05, 6.4586e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04692002758383751 tensor([0.0027, 0.0537, 0.0469, 0.3116, 0.5850], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005625844933092594 tensor([3.7220e-07, 5.6258e-04, 7.2774e-01, 8.7320e-04, 2.7083e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010424599749967456 tensor([8.0662e-01, 1.2142e-01, 4.9570e-07, 7.0915e-02, 1.0425e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.047011639922857285 tensor([0.0470, 0.4457, 0.0039, 0.2945, 0.2089], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00268365116789937 tensor([1.9447e-05, 2.1114e-02, 5.0192e-01, 2.6837e-03, 4.7426e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027095071971416473 tensor([0.0062, 0.0795, 0.0271, 0.4693, 0.4179], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05784041807055473 tensor([0.0578, 0.3814, 0.0033, 0.3103, 0.2471], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004034107085317373 tensor([6.0968e-01, 2.9991e-01, 4.7066e-06, 8.6377e-02, 4.0341e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014303202740848064 tensor([0.0143, 0.7043, 0.0089, 0.0467, 0.2258], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014067833544686437 tensor([5.8025e-06, 7.4328e-03, 5.7084e-01, 1.4068e-03, 4.2032e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03083747997879982 tensor([0.0308, 0.2613, 0.0057, 0.3735, 0.3287], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024438515305519104 tensor([0.0019, 0.0468, 0.0244, 0.1202, 0.8066], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009328410029411316 tensor([4.7822e-01, 1.9536e-01, 1.9157e-05, 3.1707e-01, 9.3284e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005810839356854558 tensor([7.6422e-01, 1.9834e-01, 3.0578e-07, 3.6860e-02, 5.8108e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04101025313138962 tensor([0.0010, 0.0988, 0.0808, 0.0410, 0.7784], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.031090937554836273 tensor([2.3858e-01, 1.2229e-01, 7.2855e-05, 6.0797e-01, 3.1091e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015477295033633709 tensor([0.0064, 0.0827, 0.0155, 0.2185, 0.6769], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006971029215492308 tensor([7.8676e-01, 5.1898e-02, 3.0098e-07, 1.6064e-01, 6.9710e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021987223997712135 tensor([0.0220, 0.6755, 0.0085, 0.0868, 0.2071], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00972001999616623 tensor([0.0008, 0.2345, 0.1474, 0.0097, 0.6075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.044533271342515945 tensor([2.0597e-01, 2.2532e-01, 2.1024e-04, 5.2398e-01, 4.4533e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013985286466777325 tensor([2.2620e-04, 1.3985e-02, 9.8676e-02, 6.5004e-02, 8.2211e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00253524212166667 tensor([6.9276e-01, 1.3131e-01, 4.1482e-06, 1.7339e-01, 2.5352e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014920441433787346 tensor([0.0107, 0.7483, 0.0149, 0.0267, 0.1993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013251033378764987 tensor([4.0398e-06, 5.7098e-03, 7.1713e-01, 1.3251e-03, 2.7583e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016069285571575165 tensor([0.0161, 0.0886, 0.0075, 0.6273, 0.2606], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006105579086579382 tensor([4.7836e-07, 9.5694e-04, 6.7785e-01, 6.1056e-04, 3.2058e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016537433490157127 tensor([3.5763e-01, 3.6055e-01, 9.4713e-05, 2.6519e-01, 1.6537e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007842655293643475 tensor([0.0007, 0.3475, 0.1606, 0.0078, 0.4833], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006392949260771275 tensor([6.5362e-05, 3.7050e-02, 4.5733e-01, 6.3929e-03, 4.9916e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002729434287175536 tensor([6.5150e-01, 2.6874e-01, 2.1613e-06, 7.7021e-02, 2.7294e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027927707880735397 tensor([0.0123, 0.2065, 0.0279, 0.2697, 0.4836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09432406723499298 tensor([0.1189, 0.3520, 0.0012, 0.4335, 0.0943], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.033612318336963654 tensor([0.0336, 0.5609, 0.0059, 0.1866, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002326472196727991 tensor([8.3999e-06, 7.9818e-03, 5.3320e-01, 2.3265e-03, 4.5648e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008262665010988712 tensor([1.5652e-01, 2.9698e-02, 1.8427e-05, 8.0550e-01, 8.2627e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.041144825518131256 tensor([3.0802e-01, 3.7990e-01, 1.4332e-04, 2.7079e-01, 4.1145e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007036734605208039 tensor([7.9893e-01, 5.7232e-02, 3.9803e-07, 1.4313e-01, 7.0367e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08065222948789597 tensor([0.1056, 0.5039, 0.0011, 0.3087, 0.0807], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006830691359937191 tensor([4.3362e-05, 2.2386e-02, 4.8400e-01, 6.8307e-03, 4.8674e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017106730490922928 tensor([0.0070, 0.0949, 0.0171, 0.3609, 0.5201], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02040036767721176 tensor([0.0077, 0.1267, 0.0204, 0.2592, 0.5860], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05485537275671959 tensor([0.2476, 0.3882, 0.0005, 0.3088, 0.0549], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04914503172039986 tensor([0.0026, 0.2176, 0.0687, 0.0491, 0.6619], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005262629594653845 tensor([3.2288e-05, 1.7970e-02, 4.2131e-01, 5.2626e-03, 5.5543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06205790117383003 tensor([0.0621, 0.1636, 0.0015, 0.6254, 0.1475], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006133283954113722 tensor([2.0682e-05, 6.1333e-03, 3.4851e-01, 1.1251e-02, 6.3408e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004860703833401203 tensor([6.3379e-01, 2.2148e-01, 5.8935e-06, 1.3986e-01, 4.8607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09377599507570267 tensor([0.0938, 0.5358, 0.0012, 0.2480, 0.1213], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015899036079645157 tensor([1.3040e-05, 1.8662e-02, 5.9622e-01, 1.5899e-03, 3.8352e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016973696649074554 tensor([0.0170, 0.0253, 0.0009, 0.8643, 0.0925], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.051470283418893814 tensor([0.0016, 0.1182, 0.0726, 0.0515, 0.7562], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00014696404105052352 tensor([8.6370e-01, 1.2569e-01, 4.4393e-08, 1.0469e-02, 1.4696e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.059347428381443024 tensor([0.0626, 0.8071, 0.0009, 0.0701, 0.0593], grad_fn=<SoftmaxBackward0>)\n",
      "3 6.720328383380547e-05 tensor([4.3567e-08, 3.1942e-04, 9.3264e-01, 6.7203e-05, 6.6975e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.022993380203843117 tensor([0.0101, 0.1607, 0.0230, 0.3051, 0.5012], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006006812676787376 tensor([1.8374e-05, 6.0068e-03, 2.8677e-01, 8.1037e-03, 6.9910e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [2, 0], [0, 3], [2, 0], [2, 0], [2, 4], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [0, 2], [0, 3], [2, 0], [0, 2], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 4], [2, 0], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [0, 2], [0, 3], [0, 2], [0, 2], [2, 0], [2, 3], [0, 3], [2, 0], [0, 3], [2, 4], [2, 0], [0, 2], [2, 0], [0, 3], [2, 4], [2, 3], [0, 1], [2, 0], [0, 2], [2, 4], [0, 2], [0, 2], [2, 0], [0, 3], [2, 4], [2, 4], [2, 0], [0, 1], [2, 0], [2, 0], [2, 0], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 2], [0, 3], [0, 3], [0, 3], [0, 1], [0, 3], [2, 4], [2, 0], [0, 1], [2, 0], [0, 3], [2, 4], [2, 4], [2, 0], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [0, 3], [0, 3], [2, 4], [2, 0], [0, 3], [0, 2], [2, 0], [2, 4], [2, 4], [0, 3], [2, 4], [0, 2], [2, 4], [0, 2], [0, 1], [0, 2], [2, 0], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 1], [2, 4], [2, 4], [0, 3], [2, 0], [0, 1], [2, 4], [0, 2], [0, 3], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [0, 1], [0, 1], [2, 4], [2, 0], [0, 3], [2, 0], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [0, 3], [2, 0], [2, 0], [2, 4], [0, 2], [0, 3], [2, 4], [0, 1], [2, 3], [2, 0], [0, 3], [2, 0], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 0], [2, 0], [2, 4], [2, 0], [0, 3], [0, 2], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3], [0, 2], [2, 4], [2, 4], [0, 3], [0, 2], [0, 1], [2, 4], [2, 0], [0, 3], [0, 2], [2, 0], [2, 4], [2, 0], [0, 3], [2, 0], [0, 2], [2, 4], [2, 4], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 1], [2, 4], [0, 2], [0, 3], [2, 0], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 2], [0, 2], [2, 4], [0, 3], [0, 3], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [2, 0], [0, 3], [2, 4], [2, 4], [0, 3], [0, 2], [0, 1]]\n",
      "[[0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 4], [2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [2, 3, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 4], [1, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 3, 4], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [2, 0], [0, 3], [2, 0], [2, 0], [2, 4], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [0, 2], [0, 3], [2, 0], [0, 2], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 4], [2, 0], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [0, 2], [0, 3], [0, 2], [0, 2], [2, 0], [2, 3], [0, 3], [2, 0], [0, 3], [2, 4], [2, 0], [0, 2], [2, 0], [0, 3], [2, 4], [2, 3], [0, 1], [2, 0], [0, 2], [2, 4], [0, 2], [0, 2], [2, 0], [0, 3], [2, 4], [2, 4], [2, 0], [0, 1], [2, 0], [2, 0], [2, 0], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 2], [0, 3], [0, 3], [0, 3], [0, 1], [0, 3], [2, 4], [2, 0], [0, 1], [2, 0], [0, 3], [2, 4], [2, 4], [2, 0], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [0, 3], [0, 3], [2, 4], [2, 0], [0, 3], [0, 2], [2, 0], [2, 4], [2, 4], [0, 3], [2, 4], [0, 2], [2, 4], [0, 2], [0, 1], [0, 2], [2, 0], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 1], [2, 4], [2, 4], [0, 3], [2, 0], [0, 1], [2, 4], [0, 2], [0, 3], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [0, 1], [0, 1], [2, 4], [2, 0], [0, 3], [2, 0], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [0, 3], [2, 0], [2, 0], [2, 4], [0, 2], [0, 3], [2, 4], [0, 1], [2, 3], [2, 0], [0, 3], [2, 0], [2, 4], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 0], [2, 0], [2, 4], [2, 0], [0, 3], [0, 2], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 3], [0, 2], [2, 4], [2, 4], [0, 3], [0, 2], [0, 1], [2, 4], [2, 0], [0, 3], [0, 2], [2, 0], [2, 4], [2, 0], [0, 3], [2, 0], [0, 2], [2, 4], [2, 4], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [0, 1], [2, 4], [0, 2], [0, 3], [2, 0], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [2, 4], [2, 0], [0, 3], [2, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 2], [0, 2], [2, 4], [0, 3], [0, 3], [2, 0], [0, 1], [2, 4], [2, 0], [0, 3], [2, 0], [0, 3], [2, 4], [2, 4], [0, 3], [0, 2], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004501797199249267  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0045017824172973635  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004501752853393555  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004501711845397949  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004501660346984863  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004501598834991455  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004501527786254883  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004501449584960938  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004501364231109619  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004501273155212402  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004501175880432129  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004501073837280273  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004500967502593994  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004500856399536133  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004500741958618164  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004500625610351562  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004500505924224854  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004500383853912353  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004500260353088379  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004500134944915772  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.1593008190393448 tensor([6.4074e-01, 1.9497e-01, 7.0599e-06, 1.5930e-01, 4.9762e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20698848366737366 tensor([0.0377, 0.4664, 0.0056, 0.2833, 0.2070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009489656426012516 tensor([6.4797e-06, 9.4897e-03, 6.2173e-01, 1.4048e-03, 3.6737e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14239709079265594 tensor([0.0897, 0.2812, 0.0013, 0.4854, 0.1424], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1439192146062851 tensor([0.0196, 0.4076, 0.0116, 0.1439, 0.4173], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1554642617702484 tensor([1.8941e-01, 5.9369e-01, 4.1120e-04, 1.5546e-01, 6.1026e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19987916946411133 tensor([1.9988e-01, 4.6621e-01, 2.3420e-04, 2.8796e-01, 4.5722e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03106648474931717 tensor([5.3434e-05, 3.1066e-02, 3.2503e-01, 5.1626e-03, 6.3869e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20005834102630615 tensor([2.0006e-01, 3.7796e-01, 3.1900e-04, 3.6681e-01, 5.4848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01636345311999321 tensor([6.0089e-05, 1.6363e-02, 1.8727e-01, 1.1642e-02, 7.8466e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1586364507675171 tensor([6.2228e-01, 2.1407e-01, 8.0421e-06, 1.5864e-01, 5.0060e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12340256571769714 tensor([0.0066, 0.3227, 0.0353, 0.1234, 0.5120], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003167322836816311 tensor([2.9255e-06, 3.1673e-03, 6.0803e-01, 1.5991e-03, 3.8720e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014467213302850723 tensor([0.0099, 0.0145, 0.0010, 0.9038, 0.0709], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16676631569862366 tensor([0.0126, 0.1668, 0.0141, 0.2482, 0.5585], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11538992077112198 tensor([5.9894e-01, 2.8166e-01, 5.7555e-06, 1.1539e-01, 4.0023e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2420153021812439 tensor([0.0006, 0.2598, 0.2420, 0.0127, 0.4848], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011142267845571041 tensor([6.3488e-06, 1.1142e-02, 5.0371e-01, 1.1575e-03, 4.8398e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024967726320028305 tensor([7.9435e-02, 2.4968e-02, 5.1995e-05, 8.8025e-01, 1.5297e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0030625544022768736 tensor([2.2528e-06, 3.0626e-03, 7.3582e-01, 1.3034e-03, 2.5981e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19232864677906036 tensor([0.1923, 0.4931, 0.0006, 0.2554, 0.0586], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21009695529937744 tensor([0.0410, 0.2629, 0.0036, 0.4824, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0046434104442596436 tensor([4.4520e-06, 4.6434e-03, 6.4225e-01, 2.1186e-03, 3.5098e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1316627562046051 tensor([1.3166e-01, 1.5660e-01, 5.2893e-04, 6.3775e-01, 7.3460e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05039814114570618 tensor([0.0012, 0.0504, 0.0456, 0.0913, 0.8115], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010217256844043732 tensor([7.9574e-01, 1.0217e-02, 3.2595e-08, 1.9395e-01, 9.6422e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17822210490703583 tensor([0.0247, 0.5510, 0.0105, 0.1782, 0.2356], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.167984738945961 tensor([0.0009, 0.1777, 0.1680, 0.0195, 0.6339], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17428770661354065 tensor([1.7429e-01, 4.2954e-01, 4.0049e-04, 3.1871e-01, 7.7063e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012996892910450697 tensor([4.3248e-07, 1.2997e-03, 8.0200e-01, 3.3068e-04, 1.9637e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014258389361202717 tensor([9.5087e-01, 1.4258e-02, 3.1487e-09, 3.4845e-02, 2.4928e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06992560625076294 tensor([0.0133, 0.5971, 0.0145, 0.0699, 0.3052], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001966172596439719 tensor([5.1929e-07, 1.9662e-03, 8.5532e-01, 2.7081e-04, 1.4244e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24690784513950348 tensor([0.0160, 0.2580, 0.0187, 0.2469, 0.4604], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09134263545274734 tensor([0.0037, 0.0913, 0.0231, 0.1218, 0.7600], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1904575675725937 tensor([0.0433, 0.1905, 0.0037, 0.5361, 0.2264], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04350348934531212 tensor([4.3503e-02, 8.8552e-01, 5.6136e-04, 1.6784e-02, 5.3632e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030293621122837067 tensor([3.1166e-05, 3.0294e-02, 5.1198e-01, 2.6469e-03, 4.5505e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1868337243795395 tensor([0.0676, 0.2921, 0.0019, 0.4515, 0.1868], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010664708912372589 tensor([3.7295e-05, 1.0665e-02, 2.2064e-01, 1.0219e-02, 7.5844e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0698700100183487 tensor([8.4798e-01, 6.9870e-02, 1.7674e-07, 8.1673e-02, 4.7389e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12546448409557343 tensor([0.0700, 0.6445, 0.0018, 0.1583, 0.1255], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05539320781826973 tensor([0.0081, 0.4031, 0.0167, 0.0554, 0.5166], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10661067813634872 tensor([0.0558, 0.1519, 0.0016, 0.6842, 0.1066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005716309417039156 tensor([2.6543e-06, 5.7163e-03, 7.0645e-01, 7.5806e-04, 2.8707e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05033709108829498 tensor([7.8796e-01, 1.6097e-01, 6.7645e-07, 5.0337e-02, 7.3377e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03920437768101692 tensor([0.0392, 0.8567, 0.0010, 0.0372, 0.0659], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025596823543310165 tensor([1.2912e-04, 2.2014e-02, 1.8157e-01, 2.5597e-02, 7.7069e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06633905321359634 tensor([0.0115, 0.0663, 0.0063, 0.6067, 0.3091], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.057552993297576904 tensor([0.0028, 0.1815, 0.0449, 0.0576, 0.7132], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12744037806987762 tensor([0.1274, 0.4125, 0.0012, 0.3615, 0.0973], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1388661414384842 tensor([0.0134, 0.3980, 0.0271, 0.1389, 0.4227], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1339314877986908 tensor([0.0133, 0.3938, 0.0199, 0.1339, 0.4391], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24547472596168518 tensor([0.0261, 0.2455, 0.0082, 0.3614, 0.3588], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00951642170548439 tensor([1.7948e-05, 9.5164e-03, 3.8964e-01, 5.6275e-03, 5.9520e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13789719343185425 tensor([0.1379, 0.6046, 0.0009, 0.1890, 0.0676], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1494719684123993 tensor([1.5184e-01, 6.4535e-01, 3.5680e-04, 1.4947e-01, 5.2980e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1482214331626892 tensor([0.0137, 0.1482, 0.0115, 0.3667, 0.4599], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0200758408755064 tensor([3.6071e-05, 9.6758e-03, 4.6684e-01, 2.0076e-02, 5.0338e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18610678613185883 tensor([0.0312, 0.1861, 0.0048, 0.4032, 0.3746], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2101420909166336 tensor([0.0415, 0.4727, 0.0064, 0.2101, 0.2693], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1058921292424202 tensor([0.0727, 0.2578, 0.0011, 0.5626, 0.1059], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.036353643983602524 tensor([1.0028e-04, 3.6354e-02, 3.5691e-01, 1.1864e-02, 5.9477e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.045951347798109055 tensor([1.9237e-01, 4.5951e-02, 2.0302e-05, 7.5074e-01, 1.0921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10417378693819046 tensor([0.0071, 0.1042, 0.0140, 0.2122, 0.6625], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.043922532349824905 tensor([8.7611e-01, 7.9553e-02, 1.4066e-07, 4.3923e-02, 4.1815e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12491188198328018 tensor([0.0255, 0.6339, 0.0075, 0.1249, 0.2082], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1359649896621704 tensor([0.0008, 0.1807, 0.1360, 0.0151, 0.6675], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13370093703269958 tensor([1.3370e-01, 1.7474e-01, 3.2894e-04, 6.1371e-01, 7.7523e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1534738838672638 tensor([0.0092, 0.1962, 0.0174, 0.1535, 0.6237], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038792211562395096 tensor([7.1940e-05, 3.8792e-02, 4.9786e-01, 6.6455e-03, 4.5663e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12322958558797836 tensor([0.0013, 0.2392, 0.1232, 0.0347, 0.6016], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0037847221828997135 tensor([2.3822e-06, 3.7847e-03, 7.1453e-01, 1.0796e-03, 2.8060e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12920527160167694 tensor([0.0007, 0.0459, 0.1322, 0.1292, 0.6920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01740751415491104 tensor([4.1820e-05, 1.7408e-02, 2.1012e-01, 6.5274e-03, 7.6590e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13433894515037537 tensor([6.7323e-01, 1.3434e-01, 2.8287e-06, 1.8931e-01, 3.1125e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02063920721411705 tensor([0.0105, 0.7726, 0.0092, 0.0206, 0.1871], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012046540156006813 tensor([2.0514e-05, 7.1504e-03, 4.4897e-01, 1.2047e-02, 5.3181e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022708533331751823 tensor([0.0103, 0.0227, 0.0022, 0.8620, 0.1028], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05153557285666466 tensor([0.0012, 0.0780, 0.0515, 0.0484, 0.8208], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.040229376405477524 tensor([9.0687e-01, 5.2728e-02, 6.8082e-08, 4.0229e-02, 1.7012e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11237110942602158 tensor([2.7929e-01, 5.9327e-01, 4.9123e-05, 1.1237e-01, 1.5022e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2343311309814453 tensor([0.0210, 0.2343, 0.0109, 0.3344, 0.3993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.042059652507305145 tensor([0.0016, 0.0237, 0.0421, 0.4563, 0.4763], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04495181515812874 tensor([4.9981e-04, 3.6708e-02, 6.2403e-02, 4.4952e-02, 8.5544e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09618578851222992 tensor([5.1932e-01, 3.7764e-01, 1.0601e-05, 9.6186e-02, 6.8379e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11298844963312149 tensor([0.1130, 0.3054, 0.0007, 0.5026, 0.0783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12097794562578201 tensor([0.0014, 0.0881, 0.1334, 0.1210, 0.6562], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01963980682194233 tensor([0.0057, 0.0196, 0.0051, 0.7964, 0.1732], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.057245075702667236 tensor([6.6302e-04, 4.1106e-02, 7.5512e-02, 5.7245e-02, 8.2547e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11853333562612534 tensor([5.4840e-01, 1.1853e-01, 9.5572e-06, 3.2889e-01, 4.1650e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15297693014144897 tensor([0.0728, 0.3199, 0.0021, 0.4522, 0.1530], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024478338658809662 tensor([2.0041e-05, 2.4478e-02, 5.1181e-01, 2.4185e-03, 4.6127e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10995039343833923 tensor([0.0018, 0.2573, 0.1100, 0.0280, 0.6029], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04085792973637581 tensor([9.7128e-05, 4.0858e-02, 2.5803e-01, 7.5813e-03, 6.9344e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24251000583171844 tensor([4.0397e-01, 3.3846e-01, 4.1035e-05, 2.4251e-01, 1.5017e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11455098539590836 tensor([0.0980, 0.5645, 0.0010, 0.2219, 0.1146], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007347562932409346 tensor([1.0010e-07, 7.3476e-04, 8.6078e-01, 1.0131e-04, 1.3838e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06052679568529129 tensor([0.0046, 0.0605, 0.0257, 0.4933, 0.4159], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11657759547233582 tensor([0.0571, 0.6701, 0.0022, 0.1166, 0.1541], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22196482121944427 tensor([2.2196e-01, 2.2330e-01, 1.8963e-04, 5.1612e-01, 3.8433e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1225844994187355 tensor([3.1889e-01, 5.4397e-01, 5.0437e-05, 1.2258e-01, 1.4502e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023131368216127157 tensor([4.4402e-07, 2.3131e-03, 8.7881e-01, 1.6660e-04, 1.1871e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15751883387565613 tensor([0.1575, 0.4254, 0.0007, 0.3299, 0.0866], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.036839187145233154 tensor([0.0049, 0.3685, 0.0266, 0.0368, 0.5632], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05425294488668442 tensor([8.4735e-01, 9.7896e-02, 2.4104e-07, 5.4253e-02, 4.9623e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05627043917775154 tensor([0.0139, 0.6346, 0.0155, 0.0563, 0.2797], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002019061503233388 tensor([4.8513e-08, 1.9543e-04, 8.5089e-01, 2.0191e-04, 1.4871e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03112448751926422 tensor([0.0049, 0.0311, 0.0107, 0.6960, 0.2573], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13720735907554626 tensor([0.0550, 0.1372, 0.0013, 0.5679, 0.2386], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06237500533461571 tensor([8.5889e-01, 6.2375e-02, 1.2179e-07, 7.8352e-02, 3.7940e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10615561902523041 tensor([0.0801, 0.5289, 0.0016, 0.2832, 0.1062], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010372979159001261 tensor([9.8601e-09, 1.0373e-04, 9.5266e-01, 3.8773e-05, 4.7197e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12722748517990112 tensor([0.1272, 0.4749, 0.0010, 0.3005, 0.0963], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.037536513060331345 tensor([7.3384e-05, 3.7537e-02, 2.6347e-01, 5.2399e-03, 6.9368e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10064816474914551 tensor([7.5592e-01, 1.4204e-01, 1.4851e-06, 1.0065e-01, 1.3811e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1163361519575119 tensor([0.0013, 0.1163, 0.1215, 0.0932, 0.6677], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.023977380245923996 tensor([1.1435e-05, 2.3977e-02, 6.0313e-01, 1.0014e-03, 3.7188e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16760148108005524 tensor([5.9803e-01, 2.2907e-01, 5.1483e-06, 1.6760e-01, 5.2918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001907076919451356 tensor([1.6229e-06, 1.5200e-03, 5.5703e-01, 1.9071e-03, 4.3954e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11080151796340942 tensor([4.9391e-01, 1.1080e-01, 6.8074e-06, 3.8992e-01, 5.3605e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16248776018619537 tensor([3.1949e-01, 5.0372e-01, 4.2241e-05, 1.6249e-01, 1.4262e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0074167107231915 tensor([7.4485e-06, 7.4167e-03, 6.4482e-01, 2.4775e-03, 3.4528e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07098992168903351 tensor([5.0752e-02, 7.0990e-02, 7.3143e-04, 7.9386e-01, 8.3667e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02988301031291485 tensor([1.5586e-04, 1.7064e-02, 1.1166e-01, 2.9883e-02, 8.4124e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14367009699344635 tensor([0.1437, 0.5602, 0.0007, 0.2008, 0.0946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09141971170902252 tensor([0.0068, 0.3377, 0.0350, 0.0914, 0.5290], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002620558370836079 tensor([4.0245e-08, 2.6206e-04, 9.3320e-01, 7.6962e-05, 6.6456e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12529070675373077 tensor([0.0529, 0.1253, 0.0013, 0.6888, 0.1317], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014321000315248966 tensor([4.9339e-05, 9.5486e-03, 1.7196e-01, 1.4321e-02, 8.0412e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11488436907529831 tensor([3.3376e-01, 1.1488e-01, 2.4226e-05, 5.3769e-01, 1.3641e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14494732022285461 tensor([0.0302, 0.5448, 0.0054, 0.1449, 0.2747], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002636600285768509 tensor([6.6091e-07, 2.6366e-03, 8.2168e-01, 2.9497e-04, 1.7538e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07068853080272675 tensor([4.3030e-04, 3.9257e-02, 1.4980e-01, 7.0689e-02, 7.3983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.054652873426675797 tensor([0.0011, 0.0408, 0.0547, 0.1088, 0.7946], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13167697191238403 tensor([3.8348e-01, 4.6848e-01, 3.5203e-05, 1.3168e-01, 1.6326e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2541097402572632 tensor([0.0234, 0.4191, 0.0120, 0.2541, 0.2913], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021967625070828944 tensor([1.5375e-08, 2.1968e-04, 9.5051e-01, 2.6924e-05, 4.9246e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07787731289863586 tensor([5.3120e-02, 7.8457e-02, 7.3074e-04, 7.8982e-01, 7.7877e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15041932463645935 tensor([0.0074, 0.1526, 0.0171, 0.1504, 0.6725], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01572762057185173 tensor([9.6380e-01, 1.5728e-02, 2.7551e-09, 2.0452e-02, 1.6581e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14393436908721924 tensor([0.0786, 0.4495, 0.0018, 0.3262, 0.1439], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08890677243471146 tensor([0.0014, 0.3686, 0.0889, 0.0122, 0.5289], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1995374858379364 tensor([3.1296e-01, 1.9954e-01, 4.9725e-05, 4.7079e-01, 1.6663e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024231258779764175 tensor([2.0480e-05, 2.4231e-02, 5.0966e-01, 2.3812e-03, 4.6371e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08180175721645355 tensor([3.1740e-01, 8.1802e-02, 1.8671e-05, 5.8918e-01, 1.1597e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04564448818564415 tensor([2.4398e-05, 4.5644e-02, 5.8644e-01, 1.4596e-03, 3.6643e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016254669055342674 tensor([2.3526e-05, 1.6255e-02, 5.5435e-01, 4.4395e-03, 4.2493e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03711603209376335 tensor([0.0140, 0.0371, 0.0021, 0.8358, 0.1109], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08734066039323807 tensor([0.0665, 0.0873, 0.0008, 0.7215, 0.1238], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.109734445810318 tensor([0.1097, 0.5843, 0.0014, 0.1987, 0.1059], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10523399710655212 tensor([0.0014, 0.1069, 0.0974, 0.1052, 0.6890], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007105138618499041 tensor([9.6644e-07, 7.1051e-03, 8.2969e-01, 1.5688e-04, 1.6304e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17957210540771484 tensor([2.6995e-01, 1.7957e-01, 7.4305e-05, 5.2580e-01, 2.4599e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004799934104084969 tensor([8.4577e-06, 4.4768e-03, 3.5272e-01, 4.7999e-03, 6.3800e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.051738396286964417 tensor([0.0517, 0.7639, 0.0017, 0.0506, 0.1321], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19098597764968872 tensor([0.0267, 0.1910, 0.0061, 0.5115, 0.2648], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09036023169755936 tensor([0.0014, 0.1290, 0.0904, 0.0569, 0.7223], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012327813543379307 tensor([0.0042, 0.0123, 0.0041, 0.8297, 0.1497], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1534057855606079 tensor([1.5341e-01, 2.7320e-01, 4.4156e-04, 4.9282e-01, 8.0137e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15111899375915527 tensor([3.3717e-01, 1.5112e-01, 7.2714e-05, 4.9440e-01, 1.7230e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1789926290512085 tensor([0.0643, 0.3743, 0.0029, 0.3795, 0.1790], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021819233894348145 tensor([7.5759e-05, 2.1819e-02, 2.8921e-01, 1.5761e-02, 6.7313e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14610308408737183 tensor([6.2458e-01, 1.4610e-01, 3.3502e-06, 2.2646e-01, 2.8520e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03149271011352539 tensor([5.5078e-05, 3.1493e-02, 4.0305e-01, 5.1244e-03, 5.6028e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24298904836177826 tensor([4.4828e-01, 2.9550e-01, 2.6749e-05, 2.4299e-01, 1.3199e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15003038942813873 tensor([1.5003e-01, 6.5174e-01, 3.0086e-04, 1.5489e-01, 4.3038e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00030582575709559023 tensor([3.0074e-08, 3.0583e-04, 9.3239e-01, 4.5547e-05, 6.7256e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2559097409248352 tensor([0.0393, 0.3262, 0.0065, 0.3720, 0.2559], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19973036646842957 tensor([0.0162, 0.2548, 0.0141, 0.1997, 0.5151], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24913834035396576 tensor([3.0044e-01, 4.2146e-01, 2.1662e-04, 2.4914e-01, 2.8745e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.103224016726017 tensor([0.0289, 0.6499, 0.0065, 0.1032, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07546354830265045 tensor([1.6069e-04, 7.5464e-02, 2.5936e-01, 7.4028e-03, 6.5761e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16339698433876038 tensor([0.0118, 0.1634, 0.0178, 0.3778, 0.4291], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07540544122457504 tensor([7.5405e-02, 7.1921e-02, 3.8209e-04, 7.7089e-01, 8.1406e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03808361291885376 tensor([9.0252e-01, 3.8084e-02, 3.8281e-08, 5.9186e-02, 2.1015e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11864308267831802 tensor([0.0014, 0.2189, 0.1186, 0.0461, 0.6150], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0074846381321549416 tensor([4.7809e-06, 7.4846e-03, 6.2837e-01, 1.1142e-03, 3.6302e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22268791496753693 tensor([0.0577, 0.3282, 0.0032, 0.3882, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0625821128487587 tensor([4.7669e-04, 3.1067e-02, 6.2582e-02, 6.6246e-02, 8.3963e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07627074420452118 tensor([7.3106e-01, 1.8976e-01, 2.0390e-06, 7.6271e-02, 2.9034e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18667511641979218 tensor([0.0436, 0.5351, 0.0034, 0.1867, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02381179854273796 tensor([7.6217e-05, 2.3812e-02, 3.1972e-01, 1.2333e-02, 6.4406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02972336858510971 tensor([6.7034e-02, 2.9723e-02, 7.9947e-05, 8.7712e-01, 2.6046e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0365036241710186 tensor([2.7096e-04, 3.6504e-02, 1.2429e-01, 3.3048e-02, 8.0589e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08709876239299774 tensor([5.6106e-01, 8.7099e-02, 5.5443e-06, 3.4765e-01, 4.1861e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1420002579689026 tensor([0.1420, 0.5125, 0.0006, 0.2878, 0.0571], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02446143329143524 tensor([1.0735e-04, 2.4461e-02, 2.1389e-01, 1.6631e-02, 7.4491e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.060600291937589645 tensor([8.0624e-02, 8.0041e-01, 6.5439e-04, 5.7708e-02, 6.0600e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05715903267264366 tensor([0.0010, 0.0572, 0.0515, 0.0693, 0.8210], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016737869009375572 tensor([7.1067e-01, 2.7194e-01, 4.4572e-07, 1.6738e-02, 6.5249e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0740000531077385 tensor([0.0740, 0.7329, 0.0011, 0.1169, 0.0751], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001449171541025862 tensor([1.2602e-08, 1.4492e-04, 9.3546e-01, 3.7772e-05, 6.4361e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05465109646320343 tensor([0.0026, 0.0547, 0.0472, 0.3125, 0.5830], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008673297706991434 tensor([3.6019e-07, 5.6721e-04, 7.2901e-01, 8.6733e-04, 2.6956e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07288829982280731 tensor([7.9937e-01, 1.2668e-01, 5.1096e-07, 7.2888e-02, 1.0571e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2068869024515152 tensor([0.0452, 0.4509, 0.0039, 0.2932, 0.2069], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02139834500849247 tensor([1.8914e-05, 2.1398e-02, 5.0358e-01, 2.6773e-03, 4.7233e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08080700784921646 tensor([0.0060, 0.0808, 0.0273, 0.4700, 0.4159], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2449227124452591 tensor([0.0558, 0.3864, 0.0033, 0.3096, 0.2449], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08778664469718933 tensor([5.9875e-01, 3.0942e-01, 4.7880e-06, 8.7787e-02, 4.0443e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04622336104512215 tensor([0.0137, 0.7085, 0.0089, 0.0462, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007530860137194395 tensor([5.6541e-06, 7.5309e-03, 5.7208e-01, 1.4024e-03, 4.1898e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2652515172958374 tensor([0.0298, 0.2653, 0.0057, 0.3733, 0.3260], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04770025983452797 tensor([0.0018, 0.0477, 0.0246, 0.1207, 0.8052], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20068757236003876 tensor([4.6886e-01, 2.0069e-01, 1.9333e-05, 3.2114e-01, 9.3004e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03785236179828644 tensor([7.5462e-01, 2.0693e-01, 3.1633e-07, 3.7852e-02, 5.9004e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08151508867740631 tensor([0.0010, 0.1004, 0.0815, 0.0410, 0.7762], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1248483955860138 tensor([2.3206e-01, 1.2485e-01, 7.3412e-05, 6.1209e-01, 3.0919e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08416242897510529 tensor([0.0062, 0.0842, 0.0156, 0.2192, 0.6748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05408519506454468 tensor([7.8018e-01, 5.4085e-02, 3.0832e-07, 1.6503e-01, 7.0386e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08600001782178879 tensor([0.0210, 0.6798, 0.0085, 0.0860, 0.2046], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14750850200653076 tensor([0.0008, 0.2380, 0.1475, 0.0097, 0.6040], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20005442202091217 tensor([2.0005e-01, 2.2963e-01, 2.1104e-04, 5.2596e-01, 4.4145e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0651906356215477 tensor([2.2024e-04, 1.4235e-02, 9.9431e-02, 6.5191e-02, 8.2092e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1359080672264099 tensor([6.8454e-01, 1.3591e-01, 4.2331e-06, 1.7699e-01, 2.5519e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026466991752386093 tensor([0.0103, 0.7525, 0.0148, 0.0265, 0.1959], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005777942482382059 tensor([3.9330e-06, 5.7779e-03, 7.1818e-01, 1.3197e-03, 2.7472e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09008027613162994 tensor([0.0156, 0.0901, 0.0075, 0.6278, 0.2590], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009668573038652539 tensor([4.6460e-07, 9.6686e-04, 6.7887e-01, 6.0744e-04, 3.1955e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26677626371383667 tensor([3.4890e-01, 3.6784e-01, 9.4910e-05, 2.6678e-01, 1.6387e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16078577935695648 tensor([0.0007, 0.3513, 0.1608, 0.0078, 0.4794], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.037587568163871765 tensor([6.3694e-05, 3.7588e-02, 4.5878e-01, 6.3824e-03, 4.9718e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0784933865070343 tensor([6.4066e-01, 2.7810e-01, 2.2073e-06, 7.8493e-02, 2.7446e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20979036390781403 tensor([0.0120, 0.2098, 0.0280, 0.2700, 0.4802], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11527252942323685 tensor([0.1153, 0.3572, 0.0012, 0.4333, 0.0931], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18516777455806732 tensor([0.0322, 0.5660, 0.0059, 0.1852, 0.2108], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008090410381555557 tensor([8.1853e-06, 8.0904e-03, 5.3449e-01, 2.3202e-03, 4.5509e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030310040339827538 tensor([1.5174e-01, 3.0310e-02, 1.8568e-05, 8.0973e-01, 8.2074e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27201738953590393 tensor([2.9973e-01, 3.8729e-01, 1.4391e-04, 2.7202e-01, 4.0810e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05960702896118164 tensor([7.9269e-01, 5.9607e-02, 4.0764e-07, 1.4700e-01, 7.1049e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10171883553266525 tensor([0.1017, 0.5098, 0.0011, 0.3077, 0.0796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02268804796040058 tensor([4.2191e-05, 2.2688e-02, 4.8570e-01, 6.8159e-03, 4.8476e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09656871110200882 tensor([0.0068, 0.0966, 0.0172, 0.3616, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12886568903923035 tensor([0.0075, 0.1289, 0.0205, 0.2597, 0.5834], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24143067002296448 tensor([0.2414, 0.3945, 0.0005, 0.3094, 0.0542], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06894948333501816 tensor([0.0025, 0.2212, 0.0689, 0.0491, 0.6582], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01821497082710266 tensor([3.1400e-05, 1.8215e-02, 4.2309e-01, 5.2486e-03, 5.5342e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1463673859834671 tensor([0.0600, 0.1663, 0.0015, 0.6259, 0.1464], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0112381586804986 tensor([2.0110e-05, 6.2183e-03, 3.5023e-01, 1.1238e-02, 6.3229e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1423405259847641 tensor([6.2410e-01, 2.2868e-01, 5.9899e-06, 1.4234e-01, 4.8740e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11993756145238876 tensor([0.0901, 0.5419, 0.0012, 0.2469, 0.1199], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.018923932686448097 tensor([1.2723e-05, 1.8924e-02, 5.9724e-01, 1.5863e-03, 3.8224e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02571944333612919 tensor([0.0164, 0.0257, 0.0010, 0.8651, 0.0919], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07316206395626068 tensor([0.0015, 0.1201, 0.0732, 0.0515, 0.7537], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01072619017213583 tensor([8.5819e-01, 1.3093e-01, 4.5299e-08, 1.0726e-02, 1.4811e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05977761000394821 tensor([0.0598, 0.8116, 0.0009, 0.0694, 0.0584], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003229467256460339 tensor([4.2443e-08, 3.2295e-04, 9.3286e-01, 6.6869e-05, 6.6751e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16322222352027893 tensor([0.0097, 0.1632, 0.0231, 0.3053, 0.4986], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008079923689365387 tensor([1.7821e-05, 6.0809e-03, 2.8843e-01, 8.0799e-03, 6.9739e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3], [2, 0], [0, 3, 1], [2, 0, 4], [2, 0, 3], [2, 4, 3], [2, 4, 0], [0, 3, 1], [2, 4], [0, 3, 1], [2, 4, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 2, 1], [2, 4, 3], [0, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 0], [2, 0], [0, 3, 1], [2, 4, 0], [0, 2, 1], [2, 4, 1], [2, 0, 3], [0, 3, 2], [2, 4, 0], [0, 3, 1], [2, 4, 1], [0, 2, 3], [0, 3, 1], [0, 2], [0, 2, 1], [2, 0, 1], [2, 3, 0], [0, 3, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [2, 0, 4], [0, 2, 3], [2, 0, 4], [0, 3, 1], [2, 4, 3], [2, 3, 0], [0, 1, 3], [2, 0, 1], [0, 2, 3], [2, 4, 0], [0, 2, 3], [0, 2, 3], [2, 0], [0, 3, 1], [2, 4, 0], [2, 4, 3], [2, 0, 1], [0, 1, 3], [2, 0, 1], [2, 0], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 3], [2, 0, 3], [0, 3, 2], [2, 4, 0], [0, 2, 3], [0, 3, 1], [0, 3, 2], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [2, 0, 3], [0, 1, 3], [2, 0, 1], [0, 3, 2], [2, 4, 3], [2, 4, 3], [2, 0], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 1], [2, 0, 4], [0, 3, 1], [0, 3, 2], [0, 3, 1], [2, 4], [2, 0, 4], [0, 3, 1], [0, 2, 1], [2, 0, 3], [2, 4], [2, 4, 3], [0, 3, 1], [2, 4, 0], [0, 2, 3], [2, 4, 3], [0, 2, 3], [0, 1, 3], [0, 2, 1], [2, 0, 1], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 0], [0, 3, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 0, 1], [0, 1, 3], [2, 4, 0], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 1, 3], [2, 4, 1], [2, 0, 3], [0, 3, 1], [0, 1, 3], [0, 1, 2], [2, 4, 3], [2, 0], [0, 3, 1], [2, 0, 4], [0, 2, 3], [2, 4, 1], [2, 0, 4], [0, 3, 2], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 4, 0], [0, 2, 3], [0, 3, 1], [2, 4, 1], [0, 1, 3], [2, 3, 0], [2, 0, 1], [0, 3, 2], [2, 0, 1], [2, 4, 0], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4], [2, 4, 0], [0, 3, 1], [2, 0], [2, 0, 3], [2, 4], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 1, 0], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 0], [0, 1, 2], [2, 4, 3], [2, 0, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 0], [0, 3, 1], [2, 3, 4], [0, 2, 1], [2, 4, 3], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 3], [2, 0], [0, 3, 1], [0, 2, 1], [2, 0], [2, 4, 3], [2, 0, 3], [0, 3, 1], [2, 0], [0, 2, 1], [2, 4], [2, 4, 3], [0, 3, 2], [2, 4, 1], [0, 2, 1], [2, 4, 1], [2, 0, 3], [0, 3, 2], [2, 4], [0, 1, 3], [2, 4, 1], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4], [0, 3, 2], [0, 3, 1], [2, 4, 3], [0, 2], [2, 4, 0], [2, 0, 3], [0, 3, 1], [2, 4, 1], [2, 4], [2, 4, 1], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 2, 1], [2, 4], [0, 3, 2], [0, 3, 1], [2, 0, 4], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 3, 1], [2, 0, 1], [0, 3, 2], [2, 4, 3], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 1, 3]]\n",
      "[[0, 1], [1, 3, 4], [2, 4], [1, 3], [1, 4], [0, 1], [1, 3], [2, 4], [0, 1, 3], [2, 4], [0, 1], [1, 4], [2, 4], [3, 4], [3, 4], [0, 1], [1, 2, 4], [2, 4], [0, 3], [2, 4], [1, 3], [1, 3, 4], [2, 4], [1, 3], [3, 4], [0, 3], [1, 4], [1, 4], [1, 3], [2, 4], [0, 3], [1, 4], [2, 4], [1, 3, 4], [3, 4], [3, 4], [1, 4], [2, 4], [1, 3], [2, 4], [0, 3], [1, 3], [1, 4], [1, 3], [2, 4], [0, 1], [1, 4], [2, 4], [3, 4], [1, 4], [1, 3], [1, 4], [1, 4], [1, 3, 4], [2, 4], [1, 3], [0, 1], [3, 4], [2, 4], [3, 4], [1, 3, 4], [1, 3], [2, 4], [0, 3], [3, 4], [0, 1], [1, 4], [1, 4], [1, 3], [1, 4], [2, 4], [1, 4], [2, 4], [2, 4], [2, 4], [0, 3], [1, 4], [2, 4], [3, 4], [1, 4], [0, 1], [0, 1], [1, 3, 4], [3, 4], [2, 4], [0, 1], [1, 3], [2, 4], [3, 4], [2, 4], [0, 3], [1, 3], [2, 4], [1, 4], [2, 4], [0, 1, 3], [1, 3], [2, 4], [3, 4], [1, 4], [0, 1, 3], [0, 1], [2, 4], [1, 3], [1, 4], [0, 1], [1, 4], [2, 4], [3, 4], [3, 4], [0, 3], [1, 3], [2, 4], [1, 3], [2, 4], [0, 1], [2, 4], [2, 4], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [3, 4], [2, 4], [1, 3], [1, 4], [2, 4], [3, 4], [2, 4], [0, 3], [1, 4], [2, 4], [2, 4], [3, 4], [0, 1], [1, 3, 4], [2, 4], [1, 3], [1, 4], [0, 3], [1, 3], [1, 4], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [3, 4], [3, 4], [1, 3], [1, 4], [2, 4], [0, 3], [2, 4], [1, 4], [3, 4], [1, 4], [3, 4], [1, 3], [0, 3], [1, 3], [2, 4], [0, 3], [2, 4], [0, 1, 3], [1, 3], [2, 4], [1, 3, 4], [1, 4], [0, 1, 3], [1, 4], [2, 4], [3, 4], [3, 4], [0, 3], [1, 4], [2, 4], [1, 3, 4], [3, 4], [0, 1], [1, 4], [2, 4], [0, 3], [2, 4], [0, 3], [1, 3], [2, 4], [0, 1], [3, 4], [0, 1], [1, 3], [2, 4], [3, 4], [2, 4], [0, 1], [1, 3, 4], [2, 4], [3, 4], [1, 3, 4], [0, 1], [1, 4], [2, 4], [1, 3, 4], [3, 4], [0, 1, 3], [0, 1], [1, 4], [0, 3], [3, 4], [0, 3], [1, 4], [1, 4], [0, 1, 3], [2, 4], [0, 3], [1, 4], [2, 4], [3, 4], [2, 4], [0, 1, 3], [1, 4], [2, 4], [0, 1], [1, 3, 4], [1, 3], [1, 4], [2, 4], [0, 3], [0, 1, 3], [0, 3], [1, 3], [2, 4], [3, 4], [3, 4], [0, 1, 3], [1, 4], [2, 4], [1, 3], [2, 4], [0, 1], [1, 3], [2, 4], [3, 4], [1, 4], [0, 1], [1, 3], [2, 4], [3, 4], [2, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 3], [0, 3, 1], [2, 0, 4], [2, 0, 3], [2, 4, 3], [2, 4, 0], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 2, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 0], [0, 3, 1], [2, 4, 0], [0, 2, 1], [2, 4, 1], [2, 0, 3], [0, 3, 2], [2, 4, 0], [0, 3, 1], [2, 4, 1], [0, 2, 3], [0, 3, 1], [0, 2, 1], [2, 0, 1], [2, 3, 0], [0, 3, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [2, 0, 4], [0, 2, 3], [2, 0, 4], [0, 3, 1], [2, 4, 3], [2, 3, 0], [0, 1, 3], [2, 0, 1], [0, 2, 3], [2, 4, 0], [0, 2, 3], [0, 2, 3], [0, 3, 1], [2, 4, 0], [2, 4, 3], [2, 0, 1], [0, 1, 3], [2, 0, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [2, 4, 3], [2, 0, 3], [0, 3, 2], [2, 4, 0], [0, 2, 3], [0, 3, 1], [0, 3, 2], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [2, 0, 3], [0, 1, 3], [2, 0, 1], [0, 3, 2], [2, 4, 3], [2, 4, 3], [0, 1, 2], [0, 1, 3], [2, 4, 3], [2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 1, 3], [2, 4, 1], [2, 0, 4], [0, 3, 1], [0, 3, 2], [0, 3, 1], [2, 0, 4], [0, 3, 1], [0, 2, 1], [2, 0, 3], [2, 4, 3], [0, 3, 1], [2, 4, 0], [0, 2, 3], [2, 4, 3], [0, 2, 3], [0, 1, 3], [0, 2, 1], [2, 0, 1], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 0], [0, 3, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 0, 1], [0, 1, 3], [2, 4, 0], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 1, 3], [2, 4, 1], [2, 0, 3], [0, 3, 1], [0, 1, 3], [0, 1, 2], [2, 4, 3], [0, 3, 1], [2, 0, 4], [0, 2, 3], [2, 4, 1], [2, 0, 4], [0, 3, 2], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 4, 0], [0, 2, 3], [0, 3, 1], [2, 4, 1], [0, 1, 3], [2, 3, 0], [2, 0, 1], [0, 3, 2], [2, 0, 1], [2, 4, 0], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 0], [0, 3, 1], [2, 0, 3], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 1, 0], [2, 4, 1], [0, 3, 2], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 0, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 0], [0, 3, 1], [2, 3, 4], [0, 2, 1], [2, 4, 3], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 1, 3], [2, 4, 3], [0, 3, 1], [0, 2, 1], [2, 4, 3], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 4, 3], [0, 3, 2], [2, 4, 1], [0, 2, 1], [2, 4, 1], [2, 0, 3], [0, 3, 2], [0, 1, 3], [2, 4, 1], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 3, 1], [0, 3, 2], [0, 3, 1], [2, 4, 3], [2, 4, 0], [2, 0, 3], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 2, 1], [0, 3, 2], [0, 3, 1], [2, 0, 4], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 3, 1], [2, 0, 1], [0, 3, 2], [2, 4, 3], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.005240030520785172  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.005239968278766733  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0052398517068508454  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0052396855523101  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.005239477199790752  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.00523923034161593  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.005238949725058227  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.005238640097390234  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00523830515093508  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.005237949105490625  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.005237573016006335  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.005237183739653731  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.005236780221483349  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.005236367208767781  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.005235944701507028  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.005235517446973682  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.005235084390218279  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.005234647113665015  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0052342098371117515  Accuracy on Support set:0.0\n",
      "torch.Size([226, 2048]) torch.Size([226])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0052337699231848256  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.2032705843448639 tensor([6.3030e-01, 2.0327e-01, 8.0777e-06, 1.6072e-01, 5.6985e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2285865992307663 tensor([0.0346, 0.4617, 0.0062, 0.2689, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36747923493385315 tensor([5.5951e-06, 8.6330e-03, 6.2262e-01, 1.2639e-03, 3.6748e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28194472193717957 tensor([0.0847, 0.2819, 0.0015, 0.4732, 0.1587], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3894314765930176 tensor([0.0174, 0.3894, 0.0125, 0.1333, 0.4473], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18006648123264313 tensor([1.8007e-01, 5.9945e-01, 4.5637e-04, 1.5202e-01, 6.8016e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.282072514295578 tensor([1.8919e-01, 4.7633e-01, 2.6870e-04, 2.8207e-01, 5.2135e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3276720941066742 tensor([4.5576e-05, 2.8026e-02, 3.2767e-01, 4.6078e-03, 6.3965e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1912134438753128 tensor([1.9121e-01, 3.8374e-01, 3.5949e-04, 3.6272e-01, 6.1960e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18921443819999695 tensor([5.0930e-05, 1.4699e-02, 1.8921e-01, 1.0334e-02, 7.8570e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22220559418201447 tensor([6.1291e-01, 2.2221e-01, 9.0459e-06, 1.5921e-01, 5.6644e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3053743839263916 tensor([0.0058, 0.3054, 0.0370, 0.1122, 0.5396], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3863433599472046 tensor([2.5030e-06, 2.8632e-03, 6.0936e-01, 1.4323e-03, 3.8634e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08072631806135178 tensor([0.0094, 0.0146, 0.0011, 0.8941, 0.0807], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22703871130943298 tensor([0.0110, 0.1562, 0.0149, 0.2270, 0.5909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2931896150112152 tensor([5.8612e-01, 2.9319e-01, 6.6097e-06, 1.1609e-01, 4.5987e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24103733897209167 tensor([0.0005, 0.2410, 0.2489, 0.0115, 0.4981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48360809683799744 tensor([5.4281e-06, 1.0059e-02, 5.0529e-01, 1.0370e-03, 4.8361e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07642947882413864 tensor([7.6429e-02, 2.5453e-02, 5.9293e-05, 8.8059e-01, 1.7467e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2597016990184784 tensor([1.9354e-06, 2.7795e-03, 7.3635e-01, 1.1654e-03, 2.5970e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2512776553630829 tensor([0.1823, 0.4997, 0.0007, 0.2513, 0.0661], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.233423113822937 tensor([0.0381, 0.2617, 0.0040, 0.4628, 0.2334], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3509954810142517 tensor([3.8228e-06, 4.2051e-03, 6.4290e-01, 1.8989e-03, 3.5100e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15882384777069092 tensor([1.2517e-01, 1.5882e-01, 6.0277e-04, 6.3199e-01, 8.3411e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08025602251291275 tensor([0.0010, 0.0454, 0.0465, 0.0803, 0.8269], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19835439324378967 tensor([7.9087e-01, 1.0660e-02, 3.7439e-08, 1.9835e-01, 1.1099e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25772783160209656 tensor([0.0224, 0.5410, 0.0115, 0.1672, 0.2577], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16500818729400635 tensor([0.0008, 0.1650, 0.1698, 0.0178, 0.6466], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31356146931648254 tensor([0.1651, 0.4340, 0.0005, 0.3136, 0.0870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19566771388053894 tensor([3.6892e-07, 1.1724e-03, 8.0286e-01, 2.9569e-04, 1.9567e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03482227772474289 tensor([9.5038e-01, 1.4766e-02, 3.4564e-09, 3.4822e-02, 2.7814e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32948964834213257 tensor([0.0120, 0.5779, 0.0156, 0.0650, 0.3295], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14232933521270752 tensor([4.4544e-07, 1.7824e-03, 8.5565e-01, 2.4228e-04, 1.4233e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22917139530181885 tensor([0.0143, 0.2463, 0.0199, 0.2292, 0.4904], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1084124892950058 tensor([0.0032, 0.0833, 0.0238, 0.1084, 0.7813], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2497105598449707 tensor([0.0403, 0.1882, 0.0041, 0.5177, 0.2497], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05909207463264465 tensor([4.0572e-02, 8.8353e-01, 6.1961e-04, 1.6184e-02, 5.9092e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4555909037590027 tensor([2.6759e-05, 2.7415e-02, 5.1459e-01, 2.3739e-03, 4.5559e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29045915603637695 tensor([0.0631, 0.2905, 0.0021, 0.4370, 0.2073], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22242537140846252 tensor([3.1916e-05, 9.6591e-03, 2.2243e-01, 9.1438e-03, 7.5874e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08349091559648514 tensor([8.4205e-01, 7.3909e-02, 2.0700e-07, 8.3491e-02, 5.5399e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15212967991828918 tensor([0.0653, 0.6419, 0.0019, 0.1521, 0.1388], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3797681927680969 tensor([0.0071, 0.3798, 0.0177, 0.0507, 0.5448], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1525237113237381 tensor([0.0527, 0.1525, 0.0018, 0.6730, 0.1201], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28625038266181946 tensor([2.2661e-06, 5.1617e-03, 7.0791e-01, 6.7725e-04, 2.8625e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16873294115066528 tensor([7.7944e-01, 1.6873e-01, 7.7562e-07, 5.0980e-02, 8.4244e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07292889058589935 tensor([0.0363, 0.8543, 0.0011, 0.0354, 0.0729], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1829940229654312 tensor([1.0952e-04, 1.9821e-02, 1.8299e-01, 2.2547e-02, 7.7453e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3392844498157501 tensor([0.0105, 0.0646, 0.0069, 0.5786, 0.3393], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16606566309928894 tensor([0.0024, 0.1661, 0.0463, 0.0511, 0.7342], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35354703664779663 tensor([0.1206, 0.4156, 0.0013, 0.3535, 0.1089], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.38165438175201416 tensor([0.0120, 0.3817, 0.0287, 0.1285, 0.4491], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3755217492580414 tensor([0.0118, 0.3755, 0.0212, 0.1236, 0.4678], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23751616477966309 tensor([0.0237, 0.2375, 0.0088, 0.3410, 0.3889], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3916409909725189 tensor([1.5256e-05, 8.5686e-03, 3.9164e-01, 5.0068e-03, 5.9477e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18445798754692078 tensor([0.1294, 0.6090, 0.0010, 0.1845, 0.0761], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14312738180160522 tensor([1.4313e-01, 6.5217e-01, 3.9961e-04, 1.4496e-01, 5.9343e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3397616147994995 tensor([0.0121, 0.1411, 0.0124, 0.3398, 0.4945], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4689822196960449 tensor([3.0480e-05, 8.6744e-03, 4.6898e-01, 1.7668e-02, 5.0464e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.37994685769081116 tensor([0.0281, 0.1793, 0.0052, 0.3799, 0.4074], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19951070845127106 tensor([0.0381, 0.4628, 0.0070, 0.1995, 0.2927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2610854208469391 tensor([0.0687, 0.2611, 0.0012, 0.5493, 0.1198], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3585798740386963 tensor([8.6777e-05, 3.3120e-02, 3.5858e-01, 1.0633e-02, 5.9758e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18631552159786224 tensor([1.8632e-01, 4.7231e-02, 2.3299e-05, 7.5388e-01, 1.2547e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19113677740097046 tensor([0.0061, 0.0962, 0.0147, 0.1911, 0.6918], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0839196965098381 tensor([8.7085e-01, 8.3920e-02, 1.6286e-07, 4.4742e-02, 4.8417e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22772707045078278 tensor([0.0232, 0.6236, 0.0083, 0.1171, 0.2277], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16722676157951355 tensor([0.0007, 0.1672, 0.1381, 0.0137, 0.6802], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17726576328277588 tensor([1.2705e-01, 1.7727e-01, 3.7501e-04, 6.0708e-01, 8.8233e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18233445286750793 tensor([0.0080, 0.1823, 0.0183, 0.1385, 0.6529], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4573805034160614 tensor([6.1645e-05, 3.5177e-02, 5.0145e-01, 5.9279e-03, 4.5738e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.221797376871109 tensor([0.0011, 0.2218, 0.1264, 0.0310, 0.6197], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28028208017349243 tensor([2.0426e-06, 3.4267e-03, 7.1532e-01, 9.6644e-04, 2.8028e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13488292694091797 tensor([5.8774e-04, 4.1672e-02, 1.3488e-01, 1.1468e-01, 7.0818e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21172089874744415 tensor([3.5842e-05, 1.5773e-02, 2.1172e-01, 5.8572e-03, 7.6661e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19266313314437866 tensor([6.6341e-01, 1.4033e-01, 3.2720e-06, 1.9266e-01, 3.6023e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20374824106693268 tensor([0.0096, 0.7571, 0.0100, 0.0195, 0.2037], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.45040956139564514 tensor([1.7329e-05, 6.4117e-03, 4.5041e-01, 1.0581e-02, 5.3258e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11595139652490616 tensor([0.0097, 0.0228, 0.0025, 0.8491, 0.1160], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07046245783567429 tensor([0.0010, 0.0705, 0.0524, 0.0427, 0.8333], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.055368926376104355 tensor([9.0362e-01, 5.5369e-02, 7.7459e-08, 4.0812e-02, 1.9437e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26576903462409973 tensor([2.6577e-01, 6.0770e-01, 5.5610e-05, 1.0949e-01, 1.6981e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2259354144334793 tensor([0.0190, 0.2259, 0.0117, 0.3131, 0.4302], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4231988787651062 tensor([0.0014, 0.0224, 0.0447, 0.4232, 0.5082], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06312654912471771 tensor([4.1476e-04, 3.2777e-02, 6.3127e-02, 3.9103e-02, 8.6458e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.39133358001708984 tensor([5.0456e-01, 3.9133e-01, 1.2166e-05, 9.6255e-02, 7.8356e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3094835877418518 tensor([0.1070, 0.3095, 0.0008, 0.4939, 0.0888], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13714104890823364 tensor([0.0012, 0.0805, 0.1371, 0.1071, 0.6740], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19407840073108673 tensor([0.0053, 0.0195, 0.0057, 0.7755, 0.1941], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07639186829328537 tensor([5.5760e-04, 3.7002e-02, 7.6392e-02, 5.0311e-02, 8.3574e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.33240893483161926 tensor([5.3994e-01, 1.2290e-01, 1.0842e-05, 3.3241e-01, 4.7465e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.31990647315979004 tensor([0.0685, 0.3199, 0.0024, 0.4391, 0.1701], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.46163639426231384 tensor([1.7104e-05, 2.2093e-02, 5.1410e-01, 2.1527e-03, 4.6164e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23858341574668884 tensor([0.0016, 0.2386, 0.1133, 0.0255, 0.6210], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26042062044143677 tensor([8.3141e-05, 3.6984e-02, 2.6042e-01, 6.7751e-03, 6.9574e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24243131279945374 tensor([3.9162e-01, 3.4874e-01, 4.6805e-05, 2.4243e-01, 1.7156e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21397733688354492 tensor([0.0918, 0.5657, 0.0011, 0.2140, 0.1274], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13868241012096405 tensor([8.6710e-08, 6.6978e-04, 8.6056e-01, 9.1745e-05, 1.3868e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4477362334728241 tensor([0.0041, 0.0579, 0.0276, 0.4627, 0.4477], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1707780510187149 tensor([0.0524, 0.6631, 0.0024, 0.1113, 0.1708], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2145155370235443 tensor([2.1452e-01, 2.2864e-01, 2.1358e-04, 5.1314e-01, 4.3486e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3051842451095581 tensor([3.0518e-01, 5.5817e-01, 5.7073e-05, 1.2020e-01, 1.6385e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11917626112699509 tensor([3.8738e-07, 2.1171e-03, 8.7855e-01, 1.5155e-04, 1.1918e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32383817434310913 tensor([0.1495, 0.4290, 0.0007, 0.3238, 0.0969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3447949290275574 tensor([0.0042, 0.3448, 0.0280, 0.0336, 0.5894], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1032087653875351 tensor([8.4079e-01, 1.0321e-01, 2.8068e-07, 5.5425e-02, 5.7706e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30184391140937805 tensor([0.0125, 0.6164, 0.0168, 0.0525, 0.3018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14884258806705475 tensor([4.1769e-08, 1.7793e-04, 8.5080e-01, 1.8086e-04, 1.4884e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28470292687416077 tensor([0.0045, 0.0305, 0.0118, 0.6685, 0.2847], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2659642696380615 tensor([0.0506, 0.1354, 0.0014, 0.5466, 0.2660], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07982583343982697 tensor([8.5412e-01, 6.5617e-02, 1.4039e-07, 7.9826e-02, 4.3865e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27316710352897644 tensor([0.0752, 0.5317, 0.0018, 0.2732, 0.1182], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04849039390683174 tensor([8.9892e-09, 9.8319e-05, 9.5137e-01, 3.6434e-05, 4.8490e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29380738735198975 tensor([0.1195, 0.4772, 0.0011, 0.2938, 0.1083], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2664836645126343 tensor([6.2342e-05, 3.3827e-02, 2.6648e-01, 4.6650e-03, 6.9496e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14832592010498047 tensor([7.4869e-01, 1.4833e-01, 1.6766e-06, 1.0141e-01, 1.5674e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12478446215391159 tensor([0.0011, 0.1064, 0.1248, 0.0823, 0.6854], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3718739449977875 tensor([9.8426e-06, 2.1777e-02, 6.0544e-01, 9.0036e-04, 3.7187e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23905277252197266 tensor([5.8576e-01, 2.3905e-01, 5.9399e-06, 1.6907e-01, 6.1043e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4390864074230194 tensor([1.3940e-06, 1.3770e-03, 5.5782e-01, 1.7108e-03, 4.3909e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.39519384503364563 tensor([4.8352e-01, 1.1508e-01, 7.8714e-06, 3.9519e-01, 6.1972e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30593499541282654 tensor([3.0593e-01, 5.1863e-01, 4.8004e-05, 1.5919e-01, 1.6195e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3448289632797241 tensor([6.3746e-06, 6.7092e-03, 6.4624e-01, 2.2133e-03, 3.4483e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09460010379552841 tensor([0.0481, 0.0715, 0.0008, 0.7850, 0.0946], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11299252510070801 tensor([1.3227e-04, 1.5365e-02, 1.1299e-01, 2.6478e-02, 8.4503e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19530057907104492 tensor([0.1349, 0.5630, 0.0008, 0.1953, 0.1060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.31743425130844116 tensor([0.0059, 0.3174, 0.0369, 0.0827, 0.5570], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06716634333133698 tensor([3.5575e-08, 2.4233e-04, 9.3252e-01, 7.0532e-05, 6.7166e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1481449007987976 tensor([0.0497, 0.1253, 0.0015, 0.6754, 0.1481], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17347954213619232 tensor([4.2292e-05, 8.6591e-03, 1.7348e-01, 1.2828e-02, 8.0499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3248593807220459 tensor([3.2486e-01, 1.1823e-01, 2.7647e-05, 5.4127e-01, 1.5612e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2989862859249115 tensor([0.0273, 0.5324, 0.0059, 0.1354, 0.2990], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17518526315689087 tensor([5.6732e-07, 2.3898e-03, 8.2216e-01, 2.6432e-04, 1.7519e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15188714861869812 tensor([3.6092e-04, 3.5362e-02, 1.5189e-01, 6.1929e-02, 7.5046e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09603943675756454 tensor([0.0009, 0.0368, 0.0557, 0.0960, 0.8105], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.36879199743270874 tensor([3.6879e-01, 4.8199e-01, 4.0238e-05, 1.3054e-01, 1.8636e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23876437544822693 tensor([0.0213, 0.4095, 0.0131, 0.2388, 0.3174], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.050746966153383255 tensor([1.4099e-08, 2.0845e-04, 9.4902e-01, 2.5619e-05, 5.0747e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07915610820055008 tensor([0.0505, 0.0792, 0.0008, 0.7814, 0.0881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14053158462047577 tensor([0.0064, 0.1405, 0.0179, 0.1348, 0.7004], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02019265852868557 tensor([9.6367e-01, 1.6115e-02, 2.9384e-09, 2.0193e-02, 1.7954e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3146626651287079 tensor([0.0734, 0.4497, 0.0020, 0.3147, 0.1602], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3459099531173706 tensor([0.0013, 0.3459, 0.0924, 0.0113, 0.5492], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30333372950553894 tensor([3.0333e-01, 2.0536e-01, 5.6865e-05, 4.7215e-01, 1.9097e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4633151590824127 tensor([1.7414e-05, 2.1824e-02, 5.1273e-01, 2.1177e-03, 4.6332e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30866625905036926 tensor([3.0867e-01, 8.4393e-02, 2.1454e-05, 5.9359e-01, 1.3333e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.366802453994751 tensor([2.0883e-05, 4.1426e-02, 5.9045e-01, 1.2994e-03, 3.6680e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42532217502593994 tensor([2.0171e-05, 1.4710e-02, 5.5599e-01, 3.9586e-03, 4.2532e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12508219480514526 tensor([0.0133, 0.0373, 0.0024, 0.8220, 0.1251], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1395062655210495 tensor([0.0628, 0.0879, 0.0009, 0.7088, 0.1395], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19242043793201447 tensor([0.1028, 0.5853, 0.0015, 0.1924, 0.1180], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09822920709848404 tensor([0.0012, 0.0982, 0.1001, 0.0928, 0.7077], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16419149935245514 tensor([8.4633e-07, 6.5147e-03, 8.2915e-01, 1.4347e-04, 1.6419e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26104027032852173 tensor([2.6104e-01, 1.8406e-01, 8.4685e-05, 5.2672e-01, 2.8091e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35449573397636414 tensor([7.2159e-06, 4.0432e-03, 3.5450e-01, 4.2824e-03, 6.3717e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14470650255680084 tensor([0.0480, 0.7570, 0.0019, 0.0484, 0.1447], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2922993302345276 tensor([0.0245, 0.1880, 0.0067, 0.4886, 0.2923], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11766914278268814 tensor([0.0012, 0.1177, 0.0922, 0.0503, 0.7386], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16865573823451996 tensor([0.0039, 0.0123, 0.0046, 0.8106, 0.1687], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27758514881134033 tensor([0.1451, 0.2776, 0.0005, 0.4855, 0.0913], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3295203447341919 tensor([3.2952e-01, 1.5496e-01, 8.1271e-05, 4.9603e-01, 1.9407e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3653058409690857 tensor([0.0600, 0.3735, 0.0032, 0.3653, 0.1980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29098889231681824 tensor([6.4537e-05, 1.9681e-02, 2.9099e-01, 1.3956e-02, 6.7531e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22976310551166534 tensor([6.1467e-01, 1.5227e-01, 3.8522e-06, 2.2976e-01, 3.2890e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.405060350894928 tensor([4.7567e-05, 2.8624e-02, 4.0506e-01, 4.6064e-03, 5.6166e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24335747957229614 tensor([4.3587e-01, 3.0563e-01, 3.0595e-05, 2.4336e-01, 1.5105e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1499509960412979 tensor([1.4099e-01, 6.6030e-01, 3.3967e-04, 1.4995e-01, 4.8424e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0681864321231842 tensor([2.6695e-08, 2.8325e-04, 9.3149e-01, 4.2202e-05, 6.8186e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2805255055427551 tensor([0.0362, 0.3202, 0.0071, 0.3559, 0.2805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24097031354904175 tensor([0.0143, 0.2410, 0.0150, 0.1833, 0.5464], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2471567690372467 tensor([2.8948e-01, 4.3069e-01, 2.4372e-04, 2.4716e-01, 3.2435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23147167265415192 tensor([0.0263, 0.6378, 0.0071, 0.0972, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26332807540893555 tensor([1.3729e-04, 6.8273e-02, 2.6333e-01, 6.6292e-03, 6.6163e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3532460927963257 tensor([0.0106, 0.1559, 0.0192, 0.3532, 0.4611], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09283237904310226 tensor([7.1381e-02, 7.2830e-02, 4.3763e-04, 7.6252e-01, 9.2832e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0606064610183239 tensor([8.9913e-01, 4.0016e-02, 4.4212e-08, 6.0606e-02, 2.4357e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2016386240720749 tensor([0.0012, 0.2016, 0.1220, 0.0409, 0.6343], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3626355528831482 tensor([4.1128e-06, 6.7897e-03, 6.2957e-01, 1.0008e-03, 3.6264e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.245803102850914 tensor([0.0533, 0.3233, 0.0036, 0.3740, 0.2458], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0579519160091877 tensor([3.9964e-04, 2.7936e-02, 6.3288e-02, 5.7952e-02, 8.5042e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19847166538238525 tensor([7.2089e-01, 1.9847e-01, 2.3472e-06, 7.7292e-02, 3.3404e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25390705466270447 tensor([0.0397, 0.5269, 0.0037, 0.1757, 0.2539], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32181984186172485 tensor([6.5114e-05, 2.1480e-02, 3.2182e-01, 1.0990e-02, 6.4564e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06418333947658539 tensor([6.4183e-02, 3.0259e-02, 9.1569e-05, 8.7563e-01, 2.9834e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12576469779014587 tensor([2.2653e-04, 3.2697e-02, 1.2576e-01, 2.8823e-02, 8.1249e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3515397012233734 tensor([5.5324e-01, 9.0442e-02, 6.2956e-06, 3.5154e-01, 4.7756e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28024470806121826 tensor([0.1340, 0.5207, 0.0007, 0.2802, 0.0644], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21529404819011688 tensor([9.1991e-05, 2.2103e-02, 2.1529e-01, 1.4847e-02, 7.4766e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0755854919552803 tensor([7.5585e-02, 8.0067e-01, 7.2397e-04, 5.5829e-02, 6.7188e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06070251390337944 tensor([0.0008, 0.0514, 0.0524, 0.0607, 0.8346], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2843466103076935 tensor([6.9826e-01, 2.8435e-01, 5.0492e-07, 1.6659e-02, 7.3726e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11182846128940582 tensor([0.0686, 0.7345, 0.0013, 0.1118, 0.0837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06599237024784088 tensor([1.1522e-08, 1.3779e-04, 9.3383e-01, 3.5662e-05, 6.5992e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28455594182014465 tensor([0.0023, 0.0509, 0.0496, 0.2846, 0.6126], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2701694369316101 tensor([3.1209e-07, 5.1792e-04, 7.2853e-01, 7.7919e-04, 2.7017e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1332574486732483 tensor([7.9139e-01, 1.3326e-01, 5.9195e-07, 7.4125e-02, 1.2250e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2286417931318283 tensor([0.0416, 0.4467, 0.0043, 0.2788, 0.2286], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4723690450191498 tensor([1.6062e-05, 1.9239e-02, 5.0600e-01, 2.3760e-03, 4.7237e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.44079336524009705 tensor([0.0054, 0.0773, 0.0292, 0.4408, 0.4472], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26889610290527344 tensor([0.0514, 0.3805, 0.0037, 0.2956, 0.2689], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32295653223991394 tensor([5.8408e-01, 3.2296e-01, 5.5554e-06, 8.8280e-02, 4.6758e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2420165240764618 tensor([0.0124, 0.6926, 0.0097, 0.0433, 0.2420], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41933673620224 tensor([4.8954e-06, 6.8578e-03, 5.7254e-01, 1.2646e-03, 4.1934e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25786951184272766 tensor([0.0271, 0.2579, 0.0062, 0.3533, 0.3555], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10675779730081558 tensor([0.0016, 0.0433, 0.0252, 0.1068, 0.8231], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20877504348754883 tensor([4.5679e-01, 2.0878e-01, 2.2429e-05, 3.2365e-01, 1.0757e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2182365357875824 tensor([7.4331e-01, 2.1824e-01, 3.6287e-07, 3.7781e-02, 6.7697e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09087838977575302 tensor([0.0008, 0.0909, 0.0829, 0.0361, 0.7893], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22339767217636108 tensor([2.2340e-01, 1.2805e-01, 8.4512e-05, 6.1297e-01, 3.5497e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1976885348558426 tensor([0.0054, 0.0778, 0.0163, 0.1977, 0.7029], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1685589700937271 tensor([7.7373e-01, 5.6895e-02, 3.5845e-07, 1.6856e-01, 8.1852e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22325317561626434 tensor([0.0192, 0.6674, 0.0093, 0.0808, 0.2233], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2213268280029297 tensor([0.0007, 0.2213, 0.1506, 0.0089, 0.6185], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1918233335018158 tensor([1.9182e-01, 2.3467e-01, 2.4081e-04, 5.2289e-01, 5.0374e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10058581084012985 tensor([1.8591e-04, 1.2812e-02, 1.0059e-01, 5.7317e-02, 8.2910e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17908161878585815 tensor([6.7639e-01, 1.4162e-01, 4.8078e-06, 1.7908e-01, 2.9067e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21307088434696198 tensor([0.0094, 0.7364, 0.0161, 0.0250, 0.2131], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2750256061553955 tensor([3.4007e-06, 5.2653e-03, 7.1852e-01, 1.1862e-03, 2.7503e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2861956059932709 tensor([0.0143, 0.0884, 0.0083, 0.6028, 0.2862], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31867140531539917 tensor([3.9728e-07, 8.7357e-04, 6.7991e-01, 5.4383e-04, 3.1867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26561546325683594 tensor([3.3650e-01, 3.7906e-01, 1.0866e-04, 2.6562e-01, 1.8715e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32993683218955994 tensor([0.0006, 0.3299, 0.1661, 0.0071, 0.4962], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.46162402629852295 tensor([5.4624e-05, 3.4045e-02, 4.6162e-01, 5.7025e-03, 4.9857e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2906592786312103 tensor([6.2701e-01, 2.9066e-01, 2.5562e-06, 7.9152e-02, 3.1745e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19960007071495056 tensor([0.0107, 0.1996, 0.0298, 0.2494, 0.5105], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.360872358083725 tensor([0.1089, 0.3609, 0.0014, 0.4240, 0.1048], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23104459047317505 tensor([0.0295, 0.5589, 0.0065, 0.1741, 0.2310], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4551536440849304 tensor([7.0583e-06, 7.3570e-03, 5.3540e-01, 2.0841e-03, 4.5515e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14676131308078766 tensor([1.4676e-01, 3.1023e-02, 2.1189e-05, 8.1281e-01, 9.3813e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26947513222694397 tensor([2.8793e-01, 3.9617e-01, 1.6308e-04, 2.6948e-01, 4.6262e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15010571479797363 tensor([7.8646e-01, 6.2607e-02, 4.7198e-07, 1.5011e-01, 8.2282e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2988058924674988 tensor([0.0956, 0.5148, 0.0013, 0.2988, 0.0896], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48505887389183044 tensor([3.5958e-05, 2.0436e-02, 4.8840e-01, 6.0677e-03, 4.8506e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.33327916264533997 tensor([0.0060, 0.0912, 0.0183, 0.3333, 0.5512], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23630185425281525 tensor([0.0065, 0.1206, 0.0217, 0.2363, 0.6149], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23282955586910248 tensor([0.2328, 0.4003, 0.0005, 0.3061, 0.0603], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20486745238304138 tensor([0.0022, 0.2049, 0.0707, 0.0440, 0.6782], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4252576529979706 tensor([2.6704e-05, 1.6385e-02, 4.2526e-01, 4.6687e-03, 5.5366e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16617095470428467 tensor([0.0565, 0.1662, 0.0016, 0.6121, 0.1636], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3517065942287445 tensor([1.7152e-05, 5.6046e-03, 3.5171e-01, 1.0017e-02, 6.3265e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2381259799003601 tensor([6.1291e-01, 2.3813e-01, 6.8484e-06, 1.4338e-01, 5.5818e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2377682328224182 tensor([0.0842, 0.5431, 0.0014, 0.2378, 0.1336], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38168495893478394 tensor([1.0894e-05, 1.7138e-02, 5.9975e-01, 1.4202e-03, 3.8168e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10417106747627258 tensor([0.0155, 0.0259, 0.0011, 0.8533, 0.1042], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1091441959142685 tensor([0.0013, 0.1091, 0.0746, 0.0456, 0.7694], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13672715425491333 tensor([8.5246e-01, 1.3673e-01, 5.0245e-08, 1.0650e-02, 1.6517e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06622008979320526 tensor([0.0554, 0.8125, 0.0010, 0.0662, 0.0649], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0671304315328598 tensor([3.7011e-08, 2.9509e-04, 9.3251e-01, 6.0939e-05, 6.7130e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28224387764930725 tensor([0.0086, 0.1542, 0.0246, 0.2822, 0.5304], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2906140685081482 tensor([1.5182e-05, 5.4906e-03, 2.9061e-01, 7.2072e-03, 6.9667e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3], [2, 0], [0, 3, 1], [2, 0, 4], [2, 0, 3], [2, 4, 3, 0], [2, 4, 0], [0, 3, 1], [2, 4, 0], [0, 3, 1, 2], [2, 4, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1, 4], [0, 2, 1], [2, 4, 3], [0, 3], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1], [2, 4, 0], [2, 0], [0, 3, 1], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1, 4], [0, 2], [0, 2, 1, 3], [2, 0, 1], [2, 3, 0, 4], [0, 3, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1, 3], [2, 0, 4, 3], [0, 2, 3], [2, 0, 4, 1], [0, 3, 1], [2, 4, 3, 1], [2, 3, 0, 4], [0, 1, 3, 2], [2, 0, 1], [0, 2, 3, 1], [2, 4, 0], [0, 2, 3], [0, 2, 3], [2, 0], [0, 3, 1], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 1], [0, 1, 3], [2, 0, 1], [2, 0, 3], [2, 0, 4], [0, 3, 1], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 3, 1], [0, 3, 2], [0, 3, 1], [0, 1, 3, 2], [0, 3, 1], [2, 4, 1, 3], [2, 0, 3], [0, 1, 3], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 3], [2, 0], [0, 1, 2], [0, 1, 3, 2], [2, 4, 3], [2, 4, 0], [0, 1, 3, 2], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 4], [0, 3, 1], [0, 3, 2], [0, 3, 1], [2, 4], [2, 0, 4], [0, 3, 1, 4], [0, 2, 1], [2, 0, 3, 4], [2, 4], [2, 4, 3], [0, 3, 1, 4], [2, 4, 0], [0, 2, 3], [2, 4, 3, 1], [0, 2, 3], [0, 1, 3, 4], [0, 2, 1], [2, 0, 1], [2, 4, 1, 3], [2, 0, 4], [0, 3, 1, 4], [2, 4, 0], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 0, 3], [0, 2, 3], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 3], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3], [2, 0], [0, 3, 1, 4], [2, 0, 4, 1], [0, 2, 3, 1], [2, 4, 1, 3], [2, 0, 4], [0, 3, 2], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1, 4], [2, 0, 1, 4], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 1, 3], [2, 3, 0, 4], [2, 0, 1], [0, 3, 2, 1], [2, 0, 1, 4], [2, 4, 0], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4], [2, 4, 0, 3], [0, 3, 1, 4], [2, 0], [2, 0, 3], [2, 4], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 2], [0, 3, 1], [2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 4, 1], [2, 4, 0], [0, 3, 1], [2, 3, 4, 0], [0, 2, 1, 3], [2, 4, 3], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3], [2, 4, 3, 1], [2, 0], [0, 3, 1], [0, 2, 1], [2, 0], [2, 4, 3], [2, 0, 3], [0, 3, 1], [2, 0], [0, 2, 1, 3], [2, 4], [2, 4, 3], [0, 3, 2, 1], [2, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2], [2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4], [0, 3, 2], [0, 3, 1], [2, 4, 3], [0, 2, 1], [2, 4, 0], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [2, 4], [2, 4, 1, 3], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 2, 1], [2, 4], [0, 3, 2], [0, 3, 1], [2, 0, 4, 1], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 3, 1], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3]]\n",
      "[[0, 1], [1, 3, 4], [2, 4], [1, 3], [1, 4], [1], [1, 3], [2, 4], [1, 3], [4], [0, 1], [1, 4], [2, 4], [3], [3, 4], [0, 1], [1, 2, 4], [2, 4], [3], [2, 4], [1, 3], [1, 3, 4], [2, 4], [3], [4], [0], [1, 4], [4], [1, 3], [2], [0], [1, 4], [2], [1, 3, 4], [4], [3, 4], [1], [2, 4], [1, 3], [2, 4], [0], [1], [1, 4], [3], [2, 4], [0], [1], [4], [3, 4], [4], [1, 3], [1, 4], [1, 4], [1, 3, 4], [2, 4], [1], [1], [3, 4], [2, 4], [3, 4], [1, 4], [1, 3], [2, 4], [3], [4], [0], [1, 4], [4], [3], [4], [2, 4], [1, 4], [2, 4], [4], [2, 4], [0], [1, 4], [2, 4], [3], [4], [0], [0, 1], [1, 3, 4], [3, 4], [4], [0, 1], [1, 3], [4], [3], [4], [0, 3], [1, 3], [2, 4], [1, 4], [2, 4], [0, 1, 3], [1, 3], [2], [3, 4], [1], [0, 1, 3], [0, 1], [2], [1, 3], [1, 4], [0], [1, 4], [2], [3, 4], [3, 4], [0], [1, 3], [2], [1, 3], [2, 4], [0], [4], [2, 4], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [3], [4], [1], [1, 4], [2], [3], [4], [0, 3], [1, 4], [2], [4], [4], [0, 1], [1, 3, 4], [2], [3], [4], [0], [1, 3], [1, 4], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [3], [3], [1], [4], [2], [0, 3], [2, 4], [1], [3, 4], [4], [3], [1, 3], [0, 3], [1, 3], [2, 4], [0, 3], [2, 4], [0, 1, 3], [1], [2], [1, 3, 4], [1, 4], [0, 1, 3], [1, 4], [2, 4], [3, 4], [3], [0], [1, 4], [2, 4], [1, 3, 4], [4], [0], [1, 4], [2, 4], [3], [4], [0, 3], [1, 3], [2, 4], [1], [4], [0, 1], [1], [2], [3, 4], [2, 4], [0], [1, 3, 4], [2, 4], [3, 4], [1, 3, 4], [0, 1], [1, 4], [2, 4], [1, 3, 4], [4], [0, 1, 3], [0, 1], [4], [0, 3], [4], [0], [1, 4], [1, 4], [1, 3], [4], [0], [1, 4], [2, 4], [3, 4], [2, 4], [0, 1, 3], [1, 4], [2, 4], [0, 1], [3, 4], [1, 3], [1, 4], [2, 4], [3], [0, 1, 3], [0], [1, 3], [2, 4], [3, 4], [3, 4], [0, 1, 3], [1, 4], [2, 4], [3], [2, 4], [0, 1], [1, 3], [2, 4], [3], [4], [0], [1], [2], [3, 4], [2, 4]]\n",
      "NL_pred of 3th iteration [[2, 4, 3, 0], [2, 4, 0], [0, 3, 1, 2], [2, 0, 1, 4], [2, 4, 1, 0], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 3], [0, 3, 2, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 2, 1, 3], [2, 3, 0, 4], [2, 4, 1, 3], [2, 0, 4, 3], [2, 0, 4, 1], [2, 4, 3, 1], [2, 3, 0, 4], [0, 1, 3, 2], [0, 2, 3, 1], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 3], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 3, 1], [0, 3, 2, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [0, 1, 3, 2], [0, 1, 3, 2], [2, 0, 1, 4], [0, 1, 3, 2], [0, 3, 1, 4], [2, 0, 3, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 3, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 2], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 0, 3], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 2, 3], [0, 3, 1, 4], [2, 0, 4, 1], [0, 2, 3, 1], [2, 4, 1, 3], [2, 0, 1, 4], [2, 0, 1, 4], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 3, 0, 4], [0, 3, 2, 1], [2, 0, 1, 4], [2, 4, 0, 3], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 3, 4, 0], [0, 2, 1, 3], [2, 4, 0, 3], [0, 3, 1, 4], [2, 4, 3, 1], [0, 2, 1, 3], [0, 3, 2, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 1], [2, 4, 1, 0], [2, 4, 1, 3], [2, 0, 4, 1], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.012747977461133684  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.01274778769940746  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.012747425205853521  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.012746908226791693  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.012746256224963129  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.012745483797423693  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.012744601891965282  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.012743627538486403  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.012742569251936309  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.012741439196528221  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.012740247103632713  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.012738997838935074  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.012737704783069844  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.012736370368879668  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0127350031113138  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.012733606659636205  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.012732185879532171  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.012730745636686986  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.012729288363943294  Accuracy on Support set:0.0\n",
      "torch.Size([98, 2048]) torch.Size([98])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.012727820143407705  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.18862798810005188 tensor([6.5027e-01, 1.8863e-01, 7.1455e-06, 1.5575e-01, 5.3468e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2280086725950241 tensor([0.0372, 0.4522, 0.0059, 0.2767, 0.2280], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3720828592777252 tensor([5.8529e-06, 8.4985e-03, 6.1811e-01, 1.3076e-03, 3.7208e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27160531282424927 tensor([0.0902, 0.2716, 0.0014, 0.4811, 0.1557], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.38101887702941895 tensor([0.0186, 0.3810, 0.0121, 0.1378, 0.4505], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28605979681015015 tensor([2.0146e-01, 4.6071e-01, 2.5284e-04, 2.8606e-01, 5.1516e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3219573199748993 tensor([4.7967e-05, 2.7573e-02, 3.2196e-01, 4.7608e-03, 6.4566e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36754411458969116 tensor([2.0251e-01, 3.6754e-01, 3.3637e-04, 3.6875e-01, 6.0865e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20656171441078186 tensor([6.3307e-01, 2.0656e-01, 8.0462e-06, 1.5503e-01, 5.3347e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3007393479347229 tensor([0.0063, 0.3007, 0.0355, 0.1166, 0.5408], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39097079634666443 tensor([2.6164e-06, 2.8162e-03, 6.0473e-01, 1.4845e-03, 3.9097e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23204728960990906 tensor([0.0115, 0.1513, 0.0144, 0.2320, 0.5908], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27463498711586 tensor([6.0818e-01, 2.7463e-01, 5.9204e-06, 1.1282e-01, 4.3561e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2384379506111145 tensor([0.0006, 0.2384, 0.2447, 0.0119, 0.5044], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4893966615200043 tensor([5.6932e-06, 9.8881e-03, 4.9964e-01, 1.0744e-03, 4.8940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2643759846687317 tensor([2.0439e-06, 2.7576e-03, 7.3165e-01, 1.2150e-03, 2.6438e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25662317872047424 tensor([0.1951, 0.4827, 0.0006, 0.2566, 0.0649], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22993351519107819 tensor([0.0407, 0.2535, 0.0037, 0.4722, 0.2299], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3558695614337921 tensor([4.0102e-06, 4.1487e-03, 6.3801e-01, 1.9685e-03, 3.5587e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25872179865837097 tensor([0.0242, 0.5333, 0.0111, 0.1727, 0.2587], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31966108083724976 tensor([0.1769, 0.4179, 0.0004, 0.3197, 0.0851], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3322162330150604 tensor([0.0130, 0.5722, 0.0151, 0.0675, 0.3322], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23665884137153625 tensor([0.0153, 0.2402, 0.0190, 0.2367, 0.4888], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2449234277009964 tensor([0.0429, 0.1813, 0.0038, 0.5271, 0.2449], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.46119225025177 tensor([2.8123e-05, 2.7082e-02, 5.0925e-01, 2.4528e-03, 4.6119e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2791593670845032 tensor([0.0677, 0.2792, 0.0020, 0.4483, 0.2029], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21735510230064392 tensor([3.3480e-05, 9.4669e-03, 2.1736e-01, 9.4134e-03, 7.6373e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3729385435581207 tensor([0.0076, 0.3729, 0.0172, 0.0526, 0.5496], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2912214994430542 tensor([2.3975e-06, 5.1302e-03, 7.0294e-01, 7.0523e-04, 2.9122e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3289005756378174 tensor([0.0112, 0.0614, 0.0063, 0.5921, 0.3289], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35967496037483215 tensor([0.1290, 0.4030, 0.0013, 0.3597, 0.1071], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3762245178222656 tensor([0.0129, 0.3762, 0.0277, 0.1330, 0.4503], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36749541759490967 tensor([0.0126, 0.3675, 0.0206, 0.1280, 0.4713], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23005633056163788 tensor([0.0254, 0.2301, 0.0083, 0.3520, 0.3842], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3864573538303375 tensor([1.5802e-05, 8.3514e-03, 3.8646e-01, 5.1342e-03, 6.0004e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34667348861694336 tensor([0.0128, 0.1367, 0.0119, 0.3467, 0.4919], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4587836265563965 tensor([3.3038e-05, 8.6614e-03, 4.5878e-01, 1.8739e-02, 5.1378e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38789016008377075 tensor([0.0297, 0.1730, 0.0050, 0.3879, 0.4045], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2937135398387909 tensor([0.0407, 0.4538, 0.0067, 0.2051, 0.2937], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25106167793273926 tensor([0.0730, 0.2511, 0.0011, 0.5573, 0.1176], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35113775730133057 tensor([9.2397e-05, 3.2807e-02, 3.5114e-01, 1.1052e-02, 6.0491e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22965341806411743 tensor([0.0251, 0.6161, 0.0080, 0.1212, 0.2297], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.46432703733444214 tensor([6.5353e-05, 3.4878e-02, 4.9457e-01, 6.1602e-03, 4.6433e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21911869943141937 tensor([0.0012, 0.2191, 0.1229, 0.0321, 0.6247], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28507351875305176 tensor([2.1578e-06, 3.3954e-03, 7.1052e-01, 1.0095e-03, 2.8507e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20677655935287476 tensor([3.7682e-05, 1.5446e-02, 2.0678e-01, 6.0482e-03, 7.7169e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2062273472547531 tensor([0.0104, 0.7534, 0.0097, 0.0202, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4434645175933838 tensor([1.8261e-05, 6.3127e-03, 4.4346e-01, 1.0974e-02, 5.3923e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28377214074134827 tensor([2.8377e-01, 5.8852e-01, 5.2297e-05, 1.1086e-01, 1.6787e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21950744092464447 tensor([0.0201, 0.2195, 0.0112, 0.3204, 0.4288], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.43767574429512024 tensor([0.0015, 0.0215, 0.0417, 0.4377, 0.4976], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3690491020679474 tensor([5.2868e-01, 3.6905e-01, 1.0972e-05, 9.4783e-02, 7.4809e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29784339666366577 tensor([0.1140, 0.2978, 0.0007, 0.5004, 0.0870], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.323807030916214 tensor([5.5775e-01, 1.1397e-01, 9.6143e-06, 3.2381e-01, 4.4605e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30809539556503296 tensor([0.0726, 0.3081, 0.0022, 0.4489, 0.1682], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.46855756640434265 tensor([1.8189e-05, 2.1960e-02, 5.0723e-01, 2.2393e-03, 4.6856e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23466810584068298 tensor([0.0017, 0.2347, 0.1104, 0.0264, 0.6268], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25507014989852905 tensor([8.7348e-05, 3.6243e-02, 2.5507e-01, 6.9941e-03, 7.0161e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24061013758182526 tensor([4.1192e-01, 3.3091e-01, 4.2668e-05, 2.4061e-01, 1.6518e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22022873163223267 tensor([0.0990, 0.5526, 0.0011, 0.2202, 0.1271], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.43774762749671936 tensor([0.0044, 0.0553, 0.0257, 0.4769, 0.4377], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21769049763679504 tensor([2.2559e-01, 2.1769e-01, 1.9719e-04, 5.1428e-01, 4.2237e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32460933923721313 tensor([3.2461e-01, 5.3837e-01, 5.3135e-05, 1.2090e-01, 1.6065e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3300769329071045 tensor([0.1594, 0.4144, 0.0007, 0.3301, 0.0954], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3395436108112335 tensor([0.0045, 0.3395, 0.0271, 0.0348, 0.5940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30502384901046753 tensor([0.0135, 0.6107, 0.0163, 0.0545, 0.3050], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2743232846260071 tensor([0.0048, 0.0288, 0.0107, 0.6813, 0.2743], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26121073961257935 tensor([0.0535, 0.1300, 0.0013, 0.5540, 0.2612], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28064030408859253 tensor([0.0808, 0.5191, 0.0017, 0.2806, 0.1177], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3010830283164978 tensor([0.1285, 0.4626, 0.0011, 0.3011, 0.1068], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26096537709236145 tensor([6.5526e-05, 3.3187e-02, 2.6097e-01, 4.8150e-03, 7.0097e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3770138621330261 tensor([1.0371e-05, 2.1571e-02, 6.0047e-01, 9.3265e-04, 3.7701e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22234676778316498 tensor([6.0632e-01, 2.2235e-01, 5.3296e-06, 1.6554e-01, 5.7922e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4430869221687317 tensor([1.4388e-06, 1.3421e-03, 5.5381e-01, 1.7619e-03, 4.4309e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38668394088745117 tensor([5.0100e-01, 1.0647e-01, 6.9686e-06, 3.8668e-01, 5.8305e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3253382742404938 tensor([3.2534e-01, 4.9865e-01, 4.4597e-05, 1.6010e-01, 1.5864e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3494900166988373 tensor([6.6719e-06, 6.6071e-03, 6.4160e-01, 2.2913e-03, 3.4949e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3119254410266876 tensor([0.0063, 0.3119, 0.0358, 0.0855, 0.5605], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3400808870792389 tensor([3.4008e-01, 1.1047e-01, 2.4612e-05, 5.3465e-01, 1.4779e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30045679211616516 tensor([0.0295, 0.5241, 0.0056, 0.1403, 0.3005], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3899742364883423 tensor([3.8997e-01, 4.6099e-01, 3.7248e-05, 1.3083e-01, 1.8168e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2461957484483719 tensor([0.0229, 0.4014, 0.0125, 0.2462, 0.3171], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3229215741157532 tensor([0.0786, 0.4372, 0.0019, 0.3229, 0.1594], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.34149712324142456 tensor([0.0013, 0.3415, 0.0905, 0.0116, 0.5550], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3175877034664154 tensor([3.1759e-01, 1.9308e-01, 5.1787e-05, 4.7090e-01, 1.8378e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.46865832805633545 tensor([1.8138e-05, 2.1376e-02, 5.0777e-01, 2.1820e-03, 4.6866e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32169288396835327 tensor([3.2169e-01, 7.8641e-02, 1.9212e-05, 5.8699e-01, 1.2661e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3722502589225769 tensor([2.2028e-05, 4.1022e-02, 5.8536e-01, 1.3462e-03, 3.7225e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4326010048389435 tensor([2.1487e-05, 1.4609e-02, 5.4863e-01, 4.1393e-03, 4.3260e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2746904492378235 tensor([2.7469e-01, 1.7306e-01, 7.6180e-05, 5.2535e-01, 2.6828e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.34991830587387085 tensor([7.4737e-06, 3.9385e-03, 3.4992e-01, 4.3992e-03, 6.4174e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28779900074005127 tensor([0.0260, 0.1810, 0.0063, 0.4989, 0.2878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26653754711151123 tensor([1.5322e-01, 2.6654e-01, 4.7354e-04, 4.9001e-01, 8.9757e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.34398749470710754 tensor([3.4399e-01, 1.4612e-01, 7.3676e-05, 4.9124e-01, 1.8578e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36270207166671753 tensor([0.0639, 0.3627, 0.0031, 0.3737, 0.1966], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.284909725189209 tensor([6.7864e-05, 1.9304e-02, 2.8491e-01, 1.4397e-02, 6.8132e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22383147478103638 tensor([6.3316e-01, 1.3992e-01, 3.4019e-06, 2.2383e-01, 3.0816e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3997012674808502 tensor([4.9670e-05, 2.8026e-02, 3.9970e-01, 4.7556e-03, 5.6747e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24017271399497986 tensor([4.5692e-01, 2.8845e-01, 2.7627e-05, 2.4017e-01, 1.4437e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27701595425605774 tensor([0.0389, 0.3102, 0.0067, 0.3672, 0.2770], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23479728400707245 tensor([0.0152, 0.2348, 0.0144, 0.1887, 0.5469], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24797150492668152 tensor([3.0649e-01, 4.1372e-01, 2.2566e-04, 2.4797e-01, 3.1593e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23450198769569397 tensor([0.0283, 0.6294, 0.0070, 0.1009, 0.2345], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2577240765094757 tensor([1.4523e-04, 6.7287e-02, 2.5772e-01, 6.8527e-03, 6.6799e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3645608425140381 tensor([0.0113, 0.1504, 0.0181, 0.3646, 0.4557], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1990528106689453 tensor([0.0013, 0.1991, 0.1184, 0.0424, 0.6389], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3675798773765564 tensor([4.3172e-06, 6.7049e-03, 6.2467e-01, 1.0378e-03, 3.6758e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24164487421512604 tensor([0.0573, 0.3122, 0.0033, 0.3855, 0.2416], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25483107566833496 tensor([0.0428, 0.5175, 0.0036, 0.1813, 0.2548], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3158092796802521 tensor([6.8356e-05, 2.1056e-02, 3.1581e-01, 1.1347e-02, 6.5172e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34172800183296204 tensor([5.6993e-01, 8.3837e-02, 5.5986e-06, 3.4173e-01, 4.4959e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28555840253829956 tensor([0.1426, 0.5069, 0.0007, 0.2856, 0.0643], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21041661500930786 tensor([9.6368e-05, 2.1562e-02, 2.1042e-01, 1.5305e-02, 7.5262e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26386576890945435 tensor([7.1928e-01, 2.6387e-01, 4.5111e-07, 1.6154e-02, 6.9664e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2968449294567108 tensor([0.0025, 0.0495, 0.0464, 0.2968, 0.6047], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27496424317359924 tensor([3.2955e-07, 5.1287e-04, 7.2371e-01, 8.1596e-04, 2.7496e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22870580852031708 tensor([0.0445, 0.4365, 0.0041, 0.2862, 0.2287], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.47870495915412903 tensor([1.6879e-05, 1.8951e-02, 4.9987e-01, 2.4582e-03, 4.7870e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.43917953968048096 tensor([0.0057, 0.0741, 0.0274, 0.4535, 0.4392], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2671020030975342 tensor([0.0551, 0.3706, 0.0035, 0.3037, 0.2671], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3030315637588501 tensor([6.0658e-01, 3.0303e-01, 4.9949e-06, 8.5944e-02, 4.4433e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24449004232883453 tensor([0.0135, 0.6875, 0.0094, 0.0452, 0.2445], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4235924780368805 tensor([5.0800e-06, 6.7097e-03, 5.6839e-01, 1.3033e-03, 4.2359e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24931146204471588 tensor([0.0290, 0.2493, 0.0059, 0.3640, 0.3519], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1959494650363922 tensor([4.7660e-01, 1.9595e-01, 2.0087e-05, 3.1723e-01, 1.0207e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20089030265808105 tensor([7.6220e-01, 2.0089e-01, 3.2052e-07, 3.6278e-02, 6.3415e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23391805589199066 tensor([2.3392e-01, 1.2018e-01, 7.6476e-05, 6.1181e-01, 3.4012e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2252713441848755 tensor([0.0208, 0.6610, 0.0090, 0.0840, 0.2253], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21659846603870392 tensor([0.0007, 0.2166, 0.1489, 0.0091, 0.6247], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22298751771450043 tensor([2.0161e-01, 2.2299e-01, 2.2305e-04, 5.2614e-01, 4.9035e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21499304473400116 tensor([0.0102, 0.7332, 0.0156, 0.0260, 0.2150], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27861452102661133 tensor([3.5404e-06, 5.1601e-03, 7.1499e-01, 1.2299e-03, 2.7861e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2781832814216614 tensor([0.0151, 0.0841, 0.0077, 0.6149, 0.2782], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3222149610519409 tensor([4.1178e-07, 8.5649e-04, 6.7637e-01, 5.6104e-04, 3.2221e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2645324170589447 tensor([3.5563e-01, 3.6168e-01, 9.9101e-05, 2.6453e-01, 1.8053e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32736316323280334 tensor([0.0007, 0.3274, 0.1627, 0.0074, 0.5019], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.45553725957870483 tensor([5.7515e-05, 3.3562e-02, 4.5554e-01, 5.9028e-03, 5.0494e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2699703276157379 tensor([6.4972e-01, 2.6997e-01, 2.2835e-06, 7.7303e-02, 3.0053e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25714656710624695 tensor([0.0113, 0.1938, 0.0285, 0.2571, 0.5092], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.34911060333251953 tensor([0.1163, 0.3491, 0.0013, 0.4303, 0.1029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23145446181297302 tensor([0.0318, 0.5508, 0.0062, 0.1797, 0.2315], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.45963984727859497 tensor([7.3224e-06, 7.1832e-03, 5.3102e-01, 2.1502e-03, 4.5964e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2708839178085327 tensor([3.0510e-01, 3.7889e-01, 1.5019e-04, 2.7088e-01, 4.4979e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30535760521888733 tensor([0.1021, 0.5020, 0.0012, 0.3054, 0.0893], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4811982810497284 tensor([3.8126e-05, 2.0198e-02, 4.8120e-01, 6.3266e-03, 4.9224e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3437013626098633 tensor([0.0064, 0.0882, 0.0172, 0.3437, 0.5445], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24256984889507294 tensor([0.0069, 0.1171, 0.0207, 0.2426, 0.6127], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24655845761299133 tensor([0.2466, 0.3851, 0.0005, 0.3089, 0.0589], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20093941688537598 tensor([0.0023, 0.2009, 0.0688, 0.0455, 0.6824], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4188807010650635 tensor([2.8093e-05, 1.6116e-02, 4.1888e-01, 4.8371e-03, 5.6014e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3453369140625 tensor([1.7950e-05, 5.4801e-03, 3.4534e-01, 1.0359e-02, 6.3881e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22102998197078705 tensor([6.3413e-01, 2.2103e-01, 6.0749e-06, 1.3958e-01, 5.2518e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24394245445728302 tensor([0.0903, 0.5309, 0.0013, 0.2439, 0.1336], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3873578608036041 tensor([1.1508e-05, 1.6992e-02, 5.9416e-01, 1.4745e-03, 3.8736e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2928166091442108 tensor([0.0093, 0.1497, 0.0232, 0.2928, 0.5250], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.28467729687690735 tensor([1.5920e-05, 5.3801e-03, 2.8468e-01, 7.4471e-03, 7.0248e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3, 1], [2, 0], [0, 3, 1], [2, 0, 4], [2, 0, 3], [2, 4, 3, 0], [2, 4, 0], [0, 3, 1], [2, 4, 0], [0, 3, 1, 2], [2, 4, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1, 4], [0, 2, 1], [2, 4, 3], [0, 3], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1], [2, 4, 0], [2, 0], [0, 3, 1], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1, 4], [0, 2], [0, 2, 1, 3], [2, 0, 1], [2, 3, 0, 4], [0, 3, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1, 3], [2, 0, 4, 3], [0, 2, 3], [2, 0, 4, 1], [0, 3, 1], [2, 4, 3, 1], [2, 3, 0, 4], [0, 1, 3, 2], [2, 0, 1], [0, 2, 3, 1], [2, 4, 0], [0, 2, 3], [0, 2, 3], [2, 0], [0, 3, 1], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 1], [0, 1, 3], [2, 0, 1], [2, 0, 3], [2, 0, 4], [0, 3, 1], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 3, 1], [0, 3, 2], [0, 3, 1], [0, 1, 3, 2], [0, 3, 1], [2, 4, 1, 3], [2, 0, 3], [0, 1, 3], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 3], [2, 0], [0, 1, 2], [0, 1, 3, 2], [2, 4, 3], [2, 4, 0], [0, 1, 3, 2], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 4], [0, 3, 1], [0, 3, 2], [0, 3, 1], [2, 4], [2, 0, 4], [0, 3, 1, 4], [0, 2, 1], [2, 0, 3, 4], [2, 4], [2, 4, 3], [0, 3, 1, 4], [2, 4, 0], [0, 2, 3], [2, 4, 3, 1], [0, 2, 3], [0, 1, 3, 4], [0, 2, 1], [2, 0, 1], [2, 4, 1, 3], [2, 0, 4], [0, 3, 1, 4], [2, 4, 0], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 0, 3], [0, 2, 3], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 3], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3], [2, 0], [0, 3, 1, 4], [2, 0, 4, 1], [0, 2, 3, 1], [2, 4, 1, 3], [2, 0, 4], [0, 3, 2], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1, 4], [2, 0, 1, 4], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 1, 3], [2, 3, 0, 4], [2, 0, 1], [0, 3, 2, 1], [2, 0, 1, 4], [2, 4, 0], [2, 4, 1], [2, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4], [2, 4, 0, 3], [0, 3, 1, 4], [2, 0], [2, 0, 3], [2, 4], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 2, 1], [0, 3, 1], [2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 4, 1], [2, 4, 0], [0, 3, 1], [2, 3, 4, 0], [0, 2, 1, 3], [2, 4, 3], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3], [2, 4, 3, 1], [2, 0], [0, 3, 1], [0, 2, 1], [2, 0], [2, 4, 3], [2, 0, 3], [0, 3, 1], [2, 0], [0, 2, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 2, 1], [2, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2], [2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4], [0, 3, 2], [0, 3, 1], [2, 4, 3], [0, 2, 1], [2, 4, 0], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [2, 4], [2, 4, 1, 3], [2, 4, 0], [0, 3, 1], [0, 2, 1], [0, 2, 1], [2, 4], [0, 3, 2], [0, 3, 1], [2, 0, 4, 1], [0, 1, 3], [2, 4, 3], [2, 0, 4], [0, 3, 1], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3]]\n",
      "[[0], [1, 3, 4], [2, 4], [1, 3], [1, 4], [1], [1, 3], [2, 4], [1, 3], [4], [0, 1], [1, 4], [2, 4], [3], [3, 4], [0, 1], [1, 2, 4], [2, 4], [3], [2, 4], [1, 3], [1, 3, 4], [2, 4], [3], [4], [0], [1, 4], [4], [1, 3], [2], [0], [1, 4], [2], [1, 3, 4], [4], [3, 4], [1], [2, 4], [1, 3], [2, 4], [0], [1], [1, 4], [3], [2, 4], [0], [1], [4], [3, 4], [4], [1, 3], [1, 4], [1, 4], [1, 3, 4], [2, 4], [1], [1], [3, 4], [2, 4], [3, 4], [1, 4], [1, 3], [2, 4], [3], [4], [0], [1, 4], [4], [3], [4], [2, 4], [1, 4], [2, 4], [4], [2, 4], [0], [1, 4], [2, 4], [3], [4], [0], [0, 1], [1, 3, 4], [3, 4], [4], [0, 1], [1, 3], [4], [3], [4], [0, 3], [1, 3], [2, 4], [1, 4], [2, 4], [0, 1, 3], [1, 3], [2], [3, 4], [1], [0, 1, 3], [0, 1], [2], [1, 3], [1, 4], [0], [1, 4], [2], [3, 4], [3, 4], [0], [1, 3], [2], [1, 3], [2, 4], [0], [4], [2, 4], [0, 1], [2, 4], [0, 3], [0, 1], [2, 4], [3], [4], [1], [1, 4], [2], [3], [4], [0, 3], [1, 4], [2], [4], [4], [0, 1], [1, 3, 4], [2], [3], [4], [0], [1, 3], [1, 4], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [3], [3], [1], [4], [2], [0, 3], [2, 4], [1], [3, 4], [4], [3], [1, 3], [0, 3], [1, 3], [2, 4], [0, 3], [2, 4], [0, 1, 3], [1], [2], [1, 3, 4], [1, 4], [0, 1, 3], [1, 4], [2, 4], [3, 4], [3], [0], [4], [2, 4], [1, 3, 4], [4], [0], [1, 4], [2, 4], [3], [4], [0, 3], [1, 3], [2, 4], [1], [4], [0, 1], [1], [2], [3, 4], [2, 4], [0], [1, 3, 4], [2, 4], [3, 4], [1, 3, 4], [0, 1], [1, 4], [2, 4], [1, 3, 4], [4], [0, 3], [0, 1], [4], [0, 3], [4], [0], [1, 4], [1, 4], [1, 3], [4], [0], [1, 4], [2, 4], [3, 4], [2, 4], [0, 1, 3], [1, 4], [2, 4], [0, 1], [3, 4], [1, 3], [1, 4], [2, 4], [3], [0, 1, 3], [0], [1, 3], [2, 4], [3, 4], [3, 4], [0, 1, 3], [1, 4], [2, 4], [3], [2, 4], [0, 1], [1, 3], [2, 4], [3], [4], [0], [1], [2], [3, 4], [2, 4]]\n",
      "NL_pred of 4th iteration [[2, 4, 3, 1], [0, 3, 2, 1], [2, 4, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.43537604808807373  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.4345027208328247  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.43287607034047443  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.4306288957595825  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.42789848645528156  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.42481815814971924  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.42151184876759845  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.41808950901031494  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.41464436054229736  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.4112520217895508  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.40797150135040283  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.4048465092976888  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.40190283457438153  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.39915653069814044  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.396614670753479  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.3942771355311076  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.39213836193084717  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.39018909136454266  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.3884180784225464  Accuracy on Support set:0.0\n",
      "torch.Size([3, 2048]) torch.Size([3])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.38681256771087646  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.15830525755882263 tensor([0.0365, 0.1583, 0.0050, 0.5708, 0.2293], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4258613586425781 tensor([5.9144e-06, 3.6483e-03, 5.6792e-01, 2.5678e-03, 4.2586e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07181763648986816 tensor([7.4428e-02, 7.1818e-02, 7.2583e-04, 7.4839e-01, 1.0464e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1578918993473053 tensor([0.0228, 0.1579, 0.0103, 0.3275, 0.4815], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16091497242450714 tensor([2.0483e-01, 1.6091e-01, 2.0491e-04, 5.8400e-01, 5.0052e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.266794890165329 tensor([5.2628e-05, 1.1857e-02, 2.6679e-01, 9.5294e-03, 7.1177e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11041209101676941 tensor([1.9143e-01, 1.1041e-01, 2.0096e-04, 6.5152e-01, 4.6444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06792332977056503 tensor([6.4615e-01, 6.7923e-02, 5.0143e-06, 2.8170e-01, 4.2212e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1156996339559555 tensor([0.0066, 0.1157, 0.0330, 0.2554, 0.5893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.45475730299949646 tensor([2.9666e-06, 1.3340e-03, 5.4087e-01, 3.0329e-03, 4.5476e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4346820116043091 tensor([0.0114, 0.0501, 0.0095, 0.4347, 0.4943], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0914488211274147 tensor([6.6586e-01, 9.1449e-02, 4.1087e-06, 2.3885e-01, 3.8361e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1090850904583931 tensor([0.0006, 0.1091, 0.2434, 0.0266, 0.6202], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4441579580307007 tensor([5.8327e-06, 4.3419e-03, 4.4416e-01, 2.0250e-03, 5.4947e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3138338029384613 tensor([2.2451e-06, 1.2936e-03, 6.8236e-01, 2.5136e-03, 3.1383e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1827930063009262 tensor([2.2363e-01, 1.8279e-01, 4.6581e-04, 5.3366e-01, 5.9457e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06492039561271667 tensor([0.0312, 0.0649, 0.0022, 0.7374, 0.1644], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4133976101875305 tensor([4.3769e-06, 1.8954e-03, 5.8067e-01, 4.0304e-03, 4.1340e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23116733133792877 tensor([0.0290, 0.2312, 0.0111, 0.4207, 0.3081], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13839726150035858 tensor([1.7992e-01, 1.3840e-01, 2.8057e-04, 6.0979e-01, 7.1609e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3012191951274872 tensor([0.0193, 0.3012, 0.0173, 0.1986, 0.4636], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08415333926677704 tensor([0.0154, 0.0842, 0.0136, 0.4535, 0.4335], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1594982147216797 tensor([0.0313, 0.0443, 0.0020, 0.7628, 0.1595], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4523327946662903 tensor([3.0783e-05, 1.2277e-02, 4.5233e-01, 4.9100e-03, 5.3045e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07660102099180222 tensor([0.0565, 0.0766, 0.0011, 0.7200, 0.1458], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18017056584358215 tensor([3.3266e-05, 3.8267e-03, 1.8017e-01, 1.7824e-02, 7.9815e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17443707585334778 tensor([0.0100, 0.1744, 0.0169, 0.1337, 0.6650], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3499244451522827 tensor([2.7562e-06, 2.4203e-03, 6.4615e-01, 1.4998e-03, 3.4992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20276449620723724 tensor([0.0076, 0.0143, 0.0032, 0.7722, 0.2028], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1276715099811554 tensor([0.1275, 0.1277, 0.0008, 0.6598, 0.0843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1630188673734665 tensor([0.0161, 0.1630, 0.0236, 0.3140, 0.4833], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15546225011348724 tensor([0.0149, 0.1555, 0.0185, 0.2942, 0.5169], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06992693245410919 tensor([0.0224, 0.0699, 0.0053, 0.5991, 0.3032], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33029448986053467 tensor([1.6493e-05, 3.5167e-03, 3.3029e-01, 1.0224e-02, 6.5595e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38713338971138 tensor([0.0102, 0.0394, 0.0079, 0.5554, 0.3871], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3993092179298401 tensor([3.6940e-05, 3.8384e-03, 3.9931e-01, 3.9252e-02, 5.5756e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29818978905677795 tensor([0.0247, 0.0490, 0.0030, 0.6252, 0.2982], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18420684337615967 tensor([0.0480, 0.1842, 0.0055, 0.4601, 0.3021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05966941639780998 tensor([5.2312e-02, 5.9669e-02, 6.1757e-04, 8.0898e-01, 7.8418e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3043188154697418 tensor([9.5312e-05, 1.3554e-02, 3.0432e-01, 2.2370e-02, 6.5966e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29973480105400085 tensor([0.0342, 0.2997, 0.0092, 0.3429, 0.3139], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4409494698047638 tensor([7.2556e-05, 1.6062e-02, 4.4095e-01, 1.2476e-02, 5.3044e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08914154022932053 tensor([0.0013, 0.0891, 0.1209, 0.0715, 0.7171], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3374502956867218 tensor([2.4545e-06, 1.6369e-03, 6.5883e-01, 2.0844e-03, 3.3745e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1696241796016693 tensor([3.7855e-05, 6.2206e-03, 1.6962e-01, 1.1476e-02, 8.1264e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3768603801727295 tensor([0.0194, 0.5138, 0.0148, 0.0752, 0.3769], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3827853798866272 tensor([2.0043e-05, 2.7825e-03, 3.8279e-01, 2.2733e-02, 5.9168e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2774616777896881 tensor([3.9012e-01, 2.7746e-01, 5.9000e-05, 3.0989e-01, 2.2467e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06730012595653534 tensor([0.0192, 0.0673, 0.0067, 0.5763, 0.3305], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3477868139743805 tensor([0.0012, 0.0060, 0.0232, 0.6219, 0.3478], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13851918280124664 tensor([6.3934e-01, 1.3852e-01, 8.2978e-06, 2.1494e-01, 7.1914e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07356638461351395 tensor([8.6525e-02, 7.3566e-02, 4.2177e-04, 7.7874e-01, 6.0746e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4702030420303345 tensor([4.7020e-01, 2.9290e-02, 4.7112e-06, 4.9767e-01, 2.8340e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08567918837070465 tensor([0.0620, 0.0857, 0.0013, 0.7307, 0.1203], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4536309242248535 tensor([1.8960e-05, 9.3506e-03, 4.5363e-01, 4.4716e-03, 5.3253e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10974550992250443 tensor([0.0022, 0.1097, 0.1001, 0.0634, 0.7246], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21202288568019867 tensor([9.1037e-05, 1.5017e-02, 2.1202e-01, 1.3544e-02, 7.5933e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10128100961446762 tensor([4.1872e-01, 1.0128e-01, 2.6322e-05, 4.6684e-01, 1.3128e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2178548127412796 tensor([0.1190, 0.2179, 0.0009, 0.5262, 0.1361], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2968757748603821 tensor([0.0033, 0.0146, 0.0139, 0.6714, 0.2969], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.054433226585388184 tensor([1.7283e-01, 5.4433e-02, 1.0148e-04, 7.4551e-01, 2.7124e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2381076216697693 tensor([4.3620e-01, 2.3811e-01, 4.8856e-05, 3.0723e-01, 1.8415e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.139719620347023 tensor([1.6424e-01, 1.3972e-01, 4.4713e-04, 6.1737e-01, 7.8229e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15961959958076477 tensor([0.0061, 0.1596, 0.0258, 0.0895, 0.7190], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3422500491142273 tensor([0.0213, 0.3423, 0.0196, 0.1696, 0.4473], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16085104644298553 tensor([0.0031, 0.0067, 0.0050, 0.8244, 0.1609], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1631937474012375 tensor([3.8515e-02, 3.1423e-02, 6.6757e-04, 7.6620e-01, 1.6319e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18870039284229279 tensor([0.0846, 0.1887, 0.0015, 0.6044, 0.1209], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15966291725635529 tensor([0.1349, 0.1597, 0.0008, 0.6087, 0.0960], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21819697320461273 tensor([6.8200e-05, 1.3992e-02, 2.1820e-01, 9.2302e-03, 7.5851e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4379839599132538 tensor([1.0904e-05, 9.8967e-03, 5.5032e-01, 1.7872e-03, 4.3798e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0687817707657814 tensor([6.1136e-01, 6.8782e-02, 3.3784e-06, 3.1520e-01, 4.6526e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4830697476863861 tensor([1.6533e-06, 6.4539e-04, 4.8307e-01, 3.5734e-03, 5.1271e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.38911572098731995 tensor([3.8912e-01, 2.6698e-02, 3.8493e-06, 5.8025e-01, 3.9359e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19942310452461243 tensor([3.9330e-01, 1.9942e-01, 4.1384e-05, 3.8943e-01, 1.7813e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.40926408767700195 tensor([7.5673e-06, 3.1000e-03, 5.8283e-01, 4.7939e-03, 4.0926e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12667036056518555 tensor([0.0070, 0.1267, 0.0343, 0.1974, 0.6346], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24198535084724426 tensor([2.4199e-01, 2.4823e-02, 1.1613e-05, 7.2447e-01, 8.7123e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23034410178661346 tensor([0.0376, 0.2303, 0.0056, 0.3632, 0.3632], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17863115668296814 tensor([4.8630e-01, 1.7863e-01, 3.0244e-05, 3.1643e-01, 1.8607e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1423838585615158 tensor([0.0235, 0.1424, 0.0100, 0.5145, 0.3095], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1474882811307907 tensor([0.0771, 0.1475, 0.0014, 0.6284, 0.1457], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16599784791469574 tensor([0.0016, 0.1660, 0.0908, 0.0262, 0.7153], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24503083527088165 tensor([2.4503e-01, 4.6015e-02, 2.5811e-05, 6.9734e-01, 1.1588e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.45420724153518677 tensor([1.9149e-05, 9.4389e-03, 4.5421e-01, 4.2678e-03, 5.3207e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2160995900630951 tensor([2.1610e-01, 1.6674e-02, 8.7056e-06, 7.6008e-01, 7.1372e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4323311448097229 tensor([2.2157e-05, 1.8087e-02, 5.4702e-01, 2.5379e-03, 4.3233e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4892541170120239 tensor([2.3537e-05, 6.5560e-03, 4.8925e-01, 8.5101e-03, 4.9566e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20260298252105713 tensor([2.0260e-01, 4.1187e-02, 3.8350e-05, 7.3941e-01, 1.6765e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29616519808769226 tensor([7.7436e-06, 1.7088e-03, 2.9617e-01, 8.3483e-03, 6.9377e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1944909691810608 tensor([0.0193, 0.0453, 0.0034, 0.7374, 0.1945], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06918758898973465 tensor([1.2360e-01, 6.9188e-02, 2.4911e-04, 7.4730e-01, 5.9659e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2652369439601898 tensor([2.6524e-01, 3.6570e-02, 3.5298e-05, 6.8684e-01, 1.1317e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1133124977350235 tensor([0.0580, 0.1133, 0.0021, 0.6639, 0.1628], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23717248439788818 tensor([7.5858e-05, 8.0966e-03, 2.3717e-01, 3.1028e-02, 7.2363e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38789239525794983 tensor([5.7091e-01, 3.8904e-02, 2.0201e-06, 3.8789e-01, 2.2911e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3477613627910614 tensor([5.2493e-05, 1.2451e-02, 3.4776e-01, 9.0639e-03, 6.3067e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08627467602491379 tensor([4.5846e-01, 8.6275e-02, 1.5860e-05, 4.4442e-01, 1.0834e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09560871124267578 tensor([0.0359, 0.0956, 0.0042, 0.6460, 0.2183], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0861239805817604 tensor([0.0162, 0.0861, 0.0107, 0.3810, 0.5060], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1370290368795395 tensor([3.3329e-01, 1.3703e-01, 1.4141e-04, 5.0347e-01, 2.6066e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3265465795993805 tensor([0.0416, 0.3314, 0.0080, 0.2924, 0.3265], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2244860976934433 tensor([1.4621e-04, 2.7923e-02, 2.2449e-01, 1.2985e-02, 7.3446e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3485559821128845 tensor([0.0092, 0.0434, 0.0113, 0.5874, 0.3486], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42660486698150635 tensor([4.7220e-06, 3.1527e-03, 5.6819e-01, 2.0480e-03, 4.2660e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09309839457273483 tensor([0.0519, 0.0931, 0.0020, 0.6676, 0.1854], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2120099663734436 tensor([0.0508, 0.2120, 0.0034, 0.4424, 0.2914], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2658524513244629 tensor([7.2277e-05, 8.8310e-03, 2.6585e-01, 2.2812e-02, 7.0243e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.46853214502334595 tensor([4.6853e-01, 2.1358e-02, 2.7362e-06, 5.0729e-01, 2.8197e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18383696675300598 tensor([1.4828e-01, 1.8384e-01, 5.6186e-04, 6.0238e-01, 6.4945e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17367395758628845 tensor([1.0116e-04, 8.7903e-03, 1.7367e-01, 3.0715e-02, 7.8672e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09785806387662888 tensor([8.7103e-01, 9.7858e-02, 2.7652e-07, 3.0564e-02, 5.5254e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4682713449001312 tensor([0.0022, 0.0155, 0.0283, 0.4857, 0.4683], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3349880278110504 tensor([4.1810e-07, 2.8182e-04, 6.6302e-01, 1.7112e-03, 3.3499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15205127000808716 tensor([0.0452, 0.1521, 0.0032, 0.5818, 0.2178], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4526701271533966 tensor([1.6693e-05, 7.9356e-03, 4.5267e-01, 4.7166e-03, 5.3466e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30380845069885254 tensor([0.0044, 0.0201, 0.0152, 0.6565, 0.3038], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12506814301013947 tensor([0.0542, 0.1251, 0.0025, 0.5815, 0.2367], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10617130249738693 tensor([6.9972e-01, 1.0617e-01, 3.6084e-06, 1.9002e-01, 4.0852e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40202751755714417 tensor([0.0211, 0.4020, 0.0137, 0.1504, 0.4127], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4894174039363861 tensor([5.6895e-06, 3.1057e-03, 5.0481e-01, 2.6579e-03, 4.8942e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07373686134815216 tensor([0.0256, 0.0737, 0.0037, 0.6228, 0.2743], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4191162884235382 tensor([4.1912e-01, 4.4953e-02, 8.4705e-06, 5.2994e-01, 5.9786e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07081235200166702 tensor([8.5178e-01, 7.0812e-02, 2.4390e-07, 7.6810e-02, 5.9533e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.162991464138031 tensor([1.6299e-01, 2.6275e-02, 3.3228e-05, 7.9194e-01, 1.8764e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3363206088542938 tensor([0.0317, 0.3607, 0.0112, 0.2601, 0.3363], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10307012498378754 tensor([0.0008, 0.1031, 0.1376, 0.0183, 0.7402], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05389607697725296 tensor([1.5250e-01, 5.3896e-02, 1.1244e-04, 7.6246e-01, 3.1038e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36985233426094055 tensor([0.0198, 0.4909, 0.0215, 0.0979, 0.3699], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3299671709537506 tensor([4.0135e-06, 2.4763e-03, 6.6499e-01, 2.5577e-03, 3.2997e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16936539113521576 tensor([0.0103, 0.0198, 0.0038, 0.7967, 0.1694], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38971883058547974 tensor([5.1215e-07, 4.4758e-04, 6.0865e-01, 1.1792e-03, 3.8972e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11650799214839935 tensor([3.5830e-01, 1.1651e-01, 6.5628e-05, 5.1002e-01, 1.5105e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15670670568943024 tensor([0.0007, 0.1567, 0.1746, 0.0157, 0.6523], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4079222083091736 tensor([5.8944e-05, 1.4334e-02, 4.0792e-01, 1.1567e-02, 5.6612e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09445729106664658 tensor([7.3657e-01, 9.4457e-02, 1.6687e-06, 1.6621e-01, 2.7631e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42818689346313477 tensor([0.0112, 0.0660, 0.0190, 0.4756, 0.4282], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09688384085893631 tensor([0.0994, 0.0969, 0.0008, 0.7264, 0.0765], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23364731669425964 tensor([0.0373, 0.2336, 0.0064, 0.4411, 0.2815], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.46745800971984863 tensor([8.0316e-06, 3.2378e-03, 4.6746e-01, 4.3366e-03, 5.2496e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12117722630500793 tensor([3.1911e-01, 1.2118e-01, 9.1636e-05, 5.2392e-01, 3.5703e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1762177050113678 tensor([0.1066, 0.1762, 0.0010, 0.6312, 0.0850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.42656663060188293 tensor([4.0251e-05, 8.7964e-03, 4.2657e-01, 1.2650e-02, 5.5195e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.40707626938819885 tensor([0.0052, 0.0245, 0.0105, 0.5527, 0.4071], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4240611493587494 tensor([0.0061, 0.0375, 0.0145, 0.4241, 0.5178], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13239096105098724 tensor([2.5272e-01, 1.3239e-01, 3.1393e-04, 5.6663e-01, 4.7949e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.086733877658844 tensor([0.0027, 0.0867, 0.0616, 0.1033, 0.7456], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36317554116249084 tensor([2.9597e-05, 6.9139e-03, 3.6318e-01, 9.6131e-03, 6.2027e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2919973134994507 tensor([1.9143e-05, 2.4270e-03, 2.9200e-01, 2.0131e-02, 6.8543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06878459453582764 tensor([6.5928e-01, 6.8785e-02, 3.6452e-06, 2.6784e-01, 4.0959e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20254884660243988 tensor([0.1040, 0.2025, 0.0011, 0.5550, 0.1373], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44502967596054077 tensor([1.1919e-05, 7.4910e-03, 5.4457e-01, 2.8958e-03, 4.4503e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4211198687553406 tensor([0.0083, 0.0463, 0.0150, 0.5093, 0.4211], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23775942623615265 tensor([1.6213e-05, 2.2906e-03, 2.3776e-01, 1.3944e-02, 7.4599e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3, 1], [2, 0, 1], [0, 3, 1], [2, 0, 4, 1], [2, 0, 3, 1], [2, 4, 3, 0], [2, 4, 0, 1], [0, 3, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 2, 3, 1], [0, 3, 1], [2, 0, 1, 4], [0, 2, 1], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1], [2, 4, 0, 1], [2, 0, 1], [0, 3, 1], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1, 4], [0, 2, 1], [0, 2, 1, 3], [2, 0, 1, 4], [2, 3, 0, 4], [0, 3, 1], [2, 0, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [2, 0, 4, 3], [0, 2, 3, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 3, 1], [2, 3, 0, 4], [0, 1, 3, 2], [2, 0, 1], [0, 2, 3, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 2, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 1], [0, 1, 3], [2, 0, 1], [2, 0, 3, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 3, 1], [0, 3, 2, 1], [0, 3, 1], [0, 1, 3, 2], [0, 3, 1, 2], [2, 4, 1, 3], [2, 0, 3], [0, 1, 3], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 3], [2, 0, 1], [0, 1, 2], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 0, 1], [0, 1, 3, 2], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 1], [2, 0, 4], [0, 3, 1, 4], [0, 2, 1], [2, 0, 3, 4], [2, 4, 1], [2, 4, 3], [0, 3, 1, 4], [2, 4, 0, 1], [0, 2, 3, 1], [2, 4, 3, 1], [0, 2, 3], [0, 1, 3, 4], [0, 2, 1, 4], [2, 0, 1, 4], [2, 4, 1, 3], [2, 0, 4, 1], [0, 3, 1, 4], [2, 4, 0, 1], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [2, 4, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 3], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 0, 1], [0, 3, 1, 4], [2, 0, 4, 1], [0, 2, 3, 1], [2, 4, 1, 3], [2, 0, 4, 1], [0, 3, 2, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1, 4], [2, 0, 1, 4], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 1, 3], [2, 3, 0, 4], [2, 0, 1, 4], [0, 3, 2, 1], [2, 0, 1, 4], [2, 4, 0, 1], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 0, 3], [0, 3, 1, 4], [2, 0, 1], [2, 0, 3, 1], [2, 4, 1], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 2, 1], [0, 3, 1], [2, 0, 1], [0, 1, 2, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 4, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 3, 4, 0], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3], [2, 4, 3, 1], [2, 0, 1], [0, 3, 1], [0, 2, 1], [2, 0, 1], [2, 4, 3, 1], [2, 0, 3], [0, 3, 1], [2, 0, 1], [0, 2, 1, 3], [2, 4, 1], [2, 4, 3, 1], [0, 3, 2, 1], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 3, 1], [0, 2, 1], [2, 4, 0, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [2, 4, 1], [2, 4, 1, 3], [2, 4, 0, 1], [0, 3, 1], [0, 2, 1], [0, 2, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1, 3], [2, 4, 3, 1], [2, 0, 4], [0, 3, 1], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3]]\n",
      "[[0], [3, 4], [2, 4], [3], [4], [1], [3], [2, 4], [3], [4], [0], [4], [2, 4], [3], [3, 4], [0], [2, 4], [2, 4], [3], [2, 4], [3], [3, 4], [2, 4], [3], [4], [0], [1, 4], [4], [3], [2], [0], [1, 4], [2], [3, 4], [4], [3], [1], [2, 4], [3], [4], [0], [1], [4], [3], [2, 4], [0], [1], [4], [3, 4], [4], [3], [4], [4], [3, 4], [2, 4], [1], [1], [3, 4], [2, 4], [3, 4], [4], [3], [2, 4], [3], [4], [0], [1, 4], [4], [3], [4], [2, 4], [4], [2, 4], [4], [4], [0], [1, 4], [2, 4], [3], [4], [0], [0, 1], [3, 4], [3, 4], [4], [0], [3], [4], [3], [4], [0, 3], [3], [2, 4], [4], [2, 4], [0, 3], [1, 3], [2], [3, 4], [1], [0, 3], [0, 1], [2], [3], [4], [0], [1, 4], [2], [3], [3], [0], [3], [2], [3], [2, 4], [0], [4], [2, 4], [0], [2, 4], [0, 3], [0], [2, 4], [3], [4], [1], [4], [2], [3], [4], [0, 3], [1, 4], [2], [4], [4], [0], [3, 4], [2], [3], [4], [0], [3], [4], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [3], [3], [1], [4], [2], [0, 3], [2, 4], [1], [3], [4], [3], [3], [0, 3], [3], [2, 4], [0, 3], [2, 4], [0, 3], [1], [2], [3, 4], [4], [0, 3], [1, 4], [2, 4], [3, 4], [3], [0], [4], [2, 4], [3, 4], [4], [0], [1, 4], [2, 4], [3], [4], [0, 3], [3], [4], [1], [4], [0], [1], [2], [3, 4], [2, 4], [0], [3, 4], [2, 4], [3, 4], [3, 4], [0], [1, 4], [2, 4], [3, 4], [4], [0, 3], [0], [4], [3], [4], [0], [1, 4], [4], [3], [4], [0], [1, 4], [2, 4], [3], [2, 4], [0, 3], [4], [2, 4], [0], [3, 4], [3], [1, 4], [2, 4], [3], [0, 3], [0], [3], [2, 4], [3, 4], [3, 4], [0, 3], [4], [2, 4], [3], [2, 4], [0], [1, 3], [2, 4], [3], [4], [0], [1], [2], [3, 4], [2, 4]]\n",
      "NL_pred of 5th iteration [[2, 0, 1], [2, 0, 4, 1], [2, 0, 3, 1], [2, 4, 0, 1], [2, 4, 0, 1], [2, 4, 3, 1], [0, 2, 3, 1], [2, 4, 3, 1], [0, 3, 1], [2, 4, 0, 1], [2, 0, 1], [2, 4, 0, 1], [0, 2, 1], [2, 0, 1, 4], [2, 0, 4, 1], [0, 3, 1, 2], [0, 2, 3, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 2, 3, 1], [2, 0, 1], [2, 0, 3, 1], [2, 0, 4, 1], [0, 3, 2, 1], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3, 1], [2, 4, 0, 1], [2, 0, 4, 1], [0, 3, 2, 1], [2, 4, 1], [2, 4, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 2, 1, 4], [2, 0, 1, 4], [2, 0, 4, 1], [2, 4, 0, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 2, 3, 1], [2, 4, 3, 1], [2, 0, 1], [2, 0, 4, 1], [0, 3, 2, 1], [2, 0, 1, 4], [2, 4, 0, 1], [2, 0, 4, 1], [2, 4, 1], [2, 0, 1], [2, 0, 3, 1], [2, 4, 1], [2, 0, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 4, 3, 1], [2, 0, 1], [2, 0, 1], [2, 4, 3, 1], [2, 0, 1], [2, 4, 3, 1], [2, 4, 1, 0], [0, 3, 2, 1], [2, 4, 0, 1], [2, 0, 1, 4], [2, 4, 1], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 0, 1], [2, 4, 1], [2, 4, 0, 1], [2, 4, 1], [0, 3, 2, 1], [2, 4, 3, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01665381483129553  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.016639922116253827  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.016613929658322722  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.016577744806135022  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.01653331840360487  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.016482546522810653  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.016427207637477566  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.01636890462926916  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.016309029347187764  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.016248802880983095  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.016189219178380194  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.016131080485679006  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01607497801651826  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.01602135961120193  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.01597050396171776  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.015922591492936417  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.015877696307929786  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.01583581357388883  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.015796893351786846  Accuracy on Support set:0.0\n",
      "torch.Size([74, 2048]) torch.Size([74])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.015760830930761388  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 3, 1], [2, 0, 1], [0, 3, 1], [2, 0, 4, 1], [2, 0, 3, 1], [2, 4, 3, 0], [2, 4, 0, 1], [0, 3, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 2, 3, 1], [0, 3, 1], [2, 0, 1, 4], [0, 2, 1], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1], [2, 4, 0, 1], [2, 0, 1], [0, 3, 1], [2, 4, 0, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1, 4], [0, 2, 1], [0, 2, 1, 3], [2, 0, 1, 4], [2, 3, 0, 4], [0, 3, 1], [2, 0, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [2, 0, 4, 3], [0, 2, 3, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 3, 1], [2, 3, 0, 4], [0, 1, 3, 2], [2, 0, 1], [0, 2, 3, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 2, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4, 0, 3], [2, 4, 3, 0], [2, 0, 1], [0, 1, 3], [2, 0, 1], [2, 0, 3, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 2, 3, 1], [0, 3, 1], [0, 3, 2, 1], [0, 3, 1], [0, 1, 3, 2], [0, 3, 1, 2], [2, 4, 1, 3], [2, 0, 3], [0, 1, 3], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 3], [2, 0, 1], [0, 1, 2], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 0, 1], [0, 1, 3, 2], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 1], [2, 0, 4], [0, 3, 1, 4], [0, 2, 1], [2, 0, 3, 4], [2, 4, 1], [2, 4, 3], [0, 3, 1, 4], [2, 4, 0, 1], [0, 2, 3, 1], [2, 4, 3, 1], [0, 2, 3], [0, 1, 3, 4], [0, 2, 1, 4], [2, 0, 1, 4], [2, 4, 1, 3], [2, 0, 4, 1], [0, 3, 1, 4], [2, 4, 0, 1], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [2, 4, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 0, 3], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 0, 1], [0, 3, 1, 4], [2, 0, 4, 1], [0, 2, 3, 1], [2, 4, 1, 3], [2, 0, 4, 1], [0, 3, 2, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1, 4], [2, 0, 1, 4], [2, 4, 0, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 1, 3], [2, 3, 0, 4], [2, 0, 1, 4], [0, 3, 2, 1], [2, 0, 1, 4], [2, 4, 0, 1], [2, 4, 1], [2, 0, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 4, 0, 3], [0, 3, 1, 4], [2, 0, 1], [2, 0, 3, 1], [2, 4, 1], [2, 0, 3], [0, 3, 1], [0, 2, 1], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 2, 1], [0, 3, 1], [2, 0, 1], [0, 1, 2, 3], [2, 4, 3, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 4, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 3, 4, 0], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3], [2, 4, 3, 1], [2, 0, 1], [0, 3, 1], [0, 2, 1], [2, 0, 1], [2, 4, 3, 1], [2, 0, 3], [0, 3, 1], [2, 0, 1], [0, 2, 1, 3], [2, 4, 1], [2, 4, 3, 1], [0, 3, 2, 1], [2, 4, 1, 0], [0, 2, 1, 3], [2, 4, 1, 3], [2, 0, 3], [0, 3, 2, 1], [2, 4, 0, 1], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1], [2, 0, 1, 4], [0, 3, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 3, 1], [0, 2, 1], [2, 4, 0, 1], [2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [2, 4, 1], [2, 4, 1, 3], [2, 4, 0, 1], [0, 3, 1], [0, 2, 1], [0, 2, 1], [2, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 0, 4, 1], [0, 1, 3], [2, 4, 3, 1], [2, 0, 4], [0, 3, 1], [2, 0, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [2, 4, 0, 3], [0, 3, 1, 4], [0, 2, 1], [0, 1, 3]]\n",
      "POSITION :  [[0], [3, 4], [2, 4], [3], [4], [1], [3], [2, 4], [3], [4], [0], [4], [2, 4], [3], [3, 4], [0], [2, 4], [2, 4], [3], [2, 4], [3], [3, 4], [2, 4], [3], [4], [0], [1, 4], [4], [3], [2], [0], [1, 4], [2], [3, 4], [4], [3], [1], [2, 4], [3], [4], [0], [1], [4], [3], [2, 4], [0], [1], [4], [3, 4], [4], [3], [4], [4], [3, 4], [2, 4], [1], [1], [3, 4], [2, 4], [3, 4], [4], [3], [2, 4], [3], [4], [0], [1, 4], [4], [3], [4], [2, 4], [4], [2, 4], [4], [4], [0], [1, 4], [2, 4], [3], [4], [0], [0, 1], [3, 4], [3, 4], [4], [0], [3], [4], [3], [4], [0, 3], [3], [2, 4], [4], [2, 4], [0, 3], [1, 3], [2], [3, 4], [1], [0, 3], [0, 1], [2], [3], [4], [0], [1, 4], [2], [3], [3], [0], [3], [2], [3], [2, 4], [0], [4], [2, 4], [0], [2, 4], [0, 3], [0], [2, 4], [3], [4], [1], [4], [2], [3], [4], [0, 3], [1, 4], [2], [4], [4], [0], [3, 4], [2], [3], [4], [0], [3], [4], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [3], [3], [1], [4], [2], [0, 3], [2, 4], [1], [3], [4], [3], [3], [0, 3], [3], [2, 4], [0, 3], [2, 4], [0, 3], [1], [2], [3, 4], [4], [0, 3], [1, 4], [2, 4], [3, 4], [3], [0], [4], [2, 4], [3, 4], [4], [0], [1, 4], [2, 4], [3], [4], [0, 3], [3], [4], [1], [4], [0], [1], [2], [3, 4], [2, 4], [0], [3, 4], [2, 4], [3, 4], [3, 4], [0], [1, 4], [2, 4], [3, 4], [4], [0, 3], [0], [4], [3], [4], [0], [1, 4], [4], [3], [4], [0], [1, 4], [2, 4], [3], [2, 4], [0, 3], [4], [2, 4], [0], [3, 4], [3], [1, 4], [2, 4], [3], [0, 3], [0], [3], [2, 4], [3, 4], [3, 4], [0, 3], [4], [2, 4], [3], [2, 4], [0], [1, 3], [2, 4], [3], [4], [0], [1], [2], [3, 4], [2, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.74\n",
      "tensor([0, 3, 4, 1, 3, 3, 4, 0, 4, 3, 0, 3, 3, 3, 4, 0, 4, 3, 2, 0, 2, 4, 3, 1,\n",
      "        3, 4, 0, 1, 4, 3, 0, 1, 4, 4, 3, 4, 4, 1, 1, 4, 3, 3, 4, 0, 4, 3, 4, 4,\n",
      "        4, 4, 0, 3, 4, 0, 4, 0, 3, 4, 3, 4, 3, 4, 2, 1, 2, 3, 4, 0, 2, 3, 3, 0,\n",
      "        3, 2, 3, 0, 4, 0, 0, 3, 4, 1, 4, 2, 3, 4, 2, 4, 4, 0, 2, 3, 4, 0, 3, 4,\n",
      "        3, 3, 1, 4, 2, 1, 3, 4, 3, 3, 3, 1, 2, 4, 3, 0, 4, 4, 0, 3, 4, 3, 4, 1,\n",
      "        4, 0, 1, 2, 0, 0, 4, 0, 4, 3, 4, 0, 4, 3, 4, 0, 3, 4, 0, 3, 3, 0, 3, 4,\n",
      "        3, 0, 3, 4, 0, 1, 2])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 93.37748344370861\n",
      "Epoch: 1  Loss: 94.03973509933775\n",
      "Epoch: 2  Loss: 96.68874172185431\n",
      "Epoch: 3  Loss: 97.35099337748345\n",
      "Epoch: 4  Loss: 98.67549668874173\n",
      "Epoch: 5  Loss: 99.33774834437085\n",
      "Epoch: 6  Loss: 99.33774834437085\n",
      "Epoch: 7  Loss: 99.33774834437085\n",
      "Epoch: 8  Loss: 99.33774834437085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 6/15 [04:26<06:38, 44.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  58.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 5.135318286418915  Accuracy on Support set:4.0\n",
      "Train_Epoch: 1  Train_Loss: 3.5202585729956626  Accuracy on Support set:4.0\n",
      "Train_Epoch: 2  Train_Loss: 2.4623681139945983  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 3  Train_Loss: 1.7951578888297082  Accuracy on Support set:36.0\n",
      "Train_Epoch: 4  Train_Loss: 1.336740072965622  Accuracy on Support set:52.0\n",
      "Train_Epoch: 5  Train_Loss: 1.0262812966108321  Accuracy on Support set:68.0\n",
      "Train_Epoch: 6  Train_Loss: 0.821474352478981  Accuracy on Support set:68.0\n",
      "Train_Epoch: 7  Train_Loss: 0.6757063210010529  Accuracy on Support set:84.0\n",
      "Train_Epoch: 8  Train_Loss: 0.5686709636449814  Accuracy on Support set:84.0\n",
      "Train_Epoch: 9  Train_Loss: 0.4856910318136215  Accuracy on Support set:88.0\n",
      "Train_Epoch: 10  Train_Loss: 0.4193109482526779  Accuracy on Support set:92.0\n",
      "Train_Epoch: 11  Train_Loss: 0.36525679036974906  Accuracy on Support set:96.0\n",
      "Train_Epoch: 12  Train_Loss: 0.32296874433755873  Accuracy on Support set:96.0\n",
      "Train_Epoch: 13  Train_Loss: 0.2884242308139801  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.2601671801507473  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.23636958837509156  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.21604656517505647  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.19867400020360948  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.18344012774527074  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.17012308955192565  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.15823195271193982  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.14768829375505446  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.13806523695588113  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.12960797667503357  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.12199844494462013  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.11513524793088437  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.10891068391501904  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.10321958191692829  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.09801026344299317  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.09309673555195332  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.08859782479703426  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.08442812893539667  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.08064646184444428  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.07711142390966415  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.07388806141912937  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.07091946829110383  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.06825378440320491  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.06570432480424643  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.06336661294102669  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.06113864831626415  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.059069631956517696  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.057136517800390724  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.05530388262122869  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.05358811967074871  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.05195818066596985  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.05042106315493584  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.04897540021687746  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.04758805513381958  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.04628726936876774  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.045031994432210926  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  52.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 1.2237948112669983e-06 tensor([8.7629e-01, 5.8964e-02, 1.2238e-06, 5.1536e-02, 1.3209e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.714604967010928e-08 tensor([5.7146e-08, 1.8848e-02, 9.7236e-01, 1.2626e-06, 8.7860e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001817864103941247 tensor([1.8179e-04, 1.6123e-01, 2.3876e-01, 4.0753e-03, 5.9575e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.0356450224644504e-05 tensor([6.1954e-02, 8.3172e-03, 3.0356e-05, 8.7188e-01, 5.7822e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015337760851252824 tensor([1.5338e-04, 3.9643e-02, 7.8852e-02, 9.9661e-03, 8.7139e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016648333985358477 tensor([0.0017, 0.1590, 0.0462, 0.0248, 0.7684], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002280046697705984 tensor([0.0148, 0.9227, 0.0023, 0.0025, 0.0577], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005854791030287743 tensor([0.0127, 0.2049, 0.0059, 0.0406, 0.7359], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.62746562436223e-05 tensor([3.0163e-02, 8.0708e-03, 9.6275e-05, 8.4210e-01, 1.1957e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.832764822524041e-05 tensor([7.8328e-05, 3.1495e-02, 1.3634e-01, 1.0330e-02, 8.2176e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019928435795009136 tensor([0.0347, 0.8784, 0.0020, 0.0083, 0.0767], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006238747737370431 tensor([1.5296e-03, 8.9781e-01, 2.2558e-02, 6.2387e-04, 7.7483e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.337012444148968e-08 tensor([3.3370e-08, 1.2920e-03, 9.6309e-01, 2.8927e-05, 3.5588e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.627501089591533e-06 tensor([2.5905e-01, 6.9474e-03, 2.6275e-06, 7.1276e-01, 2.1237e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.748037435968854e-10 tensor([5.1110e-01, 9.4152e-05, 3.7480e-10, 4.8865e-01, 1.5485e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002199343405663967 tensor([0.0703, 0.5460, 0.0022, 0.0606, 0.3209], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009772893972694874 tensor([0.1302, 0.2991, 0.0010, 0.2198, 0.3498], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.102298928250093e-06 tensor([5.1023e-06, 2.5534e-02, 6.7878e-01, 6.8938e-04, 2.9499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0245279327136814e-06 tensor([1.4707e-01, 2.7842e-03, 1.0245e-06, 8.4077e-01, 9.3703e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0020359032787382603 tensor([0.0020, 0.0952, 0.0273, 0.0647, 0.8108], grad_fn=<SoftmaxBackward0>)\n",
      "0 9.793717617867514e-05 tensor([9.7937e-05, 1.1279e-01, 2.7673e-01, 2.5976e-03, 6.0778e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00044003070797771215 tensor([6.7224e-03, 9.7865e-01, 1.3328e-03, 4.4003e-04, 1.2851e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004098219331353903 tensor([0.0041, 0.6414, 0.0235, 0.0061, 0.3249], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019098569173365831 tensor([0.0786, 0.2333, 0.0019, 0.3314, 0.3548], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.21818903204985e-05 tensor([1.0321e-02, 2.9870e-03, 8.2182e-05, 7.7867e-01, 2.0794e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.393777650577249e-06 tensor([7.5518e-01, 1.6414e-01, 7.3938e-06, 5.7965e-02, 2.2707e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00285809556953609 tensor([0.0029, 0.4418, 0.0333, 0.0095, 0.5126], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.398594673053594e-07 tensor([8.3986e-07, 1.8333e-02, 8.5646e-01, 7.0858e-05, 1.2513e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4488442445781402e-08 tensor([1.2769e-01, 1.4821e-04, 1.4488e-08, 8.7114e-01, 1.0190e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00010762031160993502 tensor([5.1732e-03, 1.8905e-03, 1.0762e-04, 6.8156e-01, 3.1127e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.0246563255786896e-05 tensor([5.7594e-01, 3.3982e-01, 3.0247e-05, 4.7443e-02, 3.6771e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.697306136833504e-05 tensor([7.6973e-05, 7.0391e-01, 2.4357e-01, 8.4729e-05, 5.2355e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010834898421308026 tensor([1.0835e-04, 4.9837e-01, 3.1629e-01, 3.1420e-04, 1.8492e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016359497385565192 tensor([3.4178e-01, 2.2288e-01, 1.6359e-04, 2.9333e-01, 1.4185e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009956102585420012 tensor([0.0010, 0.0087, 0.0032, 0.1940, 0.7931], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003796585078816861 tensor([2.0643e-01, 5.2833e-01, 3.7966e-04, 7.1151e-02, 1.9371e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005237291334196925 tensor([5.2373e-04, 1.5634e-01, 1.2740e-01, 1.0419e-02, 7.0531e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.4914660084741627e-07 tensor([1.4915e-07, 5.6117e-03, 9.3083e-01, 2.7948e-05, 6.3535e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.068366869589227e-08 tensor([4.0491e-01, 1.4253e-03, 7.0684e-08, 5.9185e-01, 1.8078e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014890378806740046 tensor([0.0016, 0.0046, 0.0015, 0.4867, 0.5057], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0022575901821255684 tensor([0.0046, 0.8522, 0.0130, 0.0023, 0.1279], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001593523658812046 tensor([0.0625, 0.7729, 0.0016, 0.0315, 0.1315], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.200148933681703e-08 tensor([5.2001e-08, 4.0141e-03, 9.6359e-01, 9.6607e-06, 3.2389e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.714354761541472e-06 tensor([4.0554e-01, 1.4023e-02, 3.7144e-06, 5.4391e-01, 3.6525e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0020060520619153976 tensor([0.0297, 0.1385, 0.0020, 0.2280, 0.6018], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0037926926743239164 tensor([0.0038, 0.4388, 0.0245, 0.0093, 0.5235], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00036802105023525655 tensor([3.6802e-04, 4.4818e-01, 1.7459e-01, 1.5594e-03, 3.7530e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2192962195456403e-08 tensor([1.2193e-08, 3.2487e-03, 9.8590e-01, 1.3073e-06, 1.0847e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00019572724704630673 tensor([1.8432e-01, 8.1097e-02, 1.9573e-04, 5.9987e-01, 1.3452e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.24953107337933e-05 tensor([5.8951e-02, 8.8752e-03, 3.2495e-05, 8.2844e-01, 1.0370e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.616559638430772e-07 tensor([8.9753e-01, 8.9615e-02, 3.6166e-07, 1.1172e-02, 1.6854e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0045381998643279076 tensor([0.0065, 0.7978, 0.0145, 0.0045, 0.1767], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8899925180448918e-06 tensor([1.8900e-06, 2.3968e-02, 7.8365e-01, 1.8212e-04, 1.9220e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002607405884191394 tensor([0.0642, 0.4226, 0.0026, 0.1206, 0.3899], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.39588953915893e-11 tensor([2.7317e-01, 1.9283e-05, 8.3959e-11, 7.2675e-01, 5.5894e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1548168004082981e-06 tensor([8.8120e-01, 8.9531e-02, 1.1548e-06, 2.4658e-02, 4.6137e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00032838209881447256 tensor([3.2838e-04, 3.8691e-01, 2.4945e-01, 2.3965e-03, 3.6091e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6392547195209772e-06 tensor([1.6393e-06, 1.9310e-02, 7.6160e-01, 1.6534e-04, 2.1892e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0016591917956247926 tensor([0.0704, 0.1840, 0.0017, 0.3846, 0.3593], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008632898679934442 tensor([0.0062, 0.0101, 0.0009, 0.4507, 0.5321], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000371631293091923 tensor([1.7338e-01, 7.3006e-01, 3.7163e-04, 2.4628e-02, 7.1566e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.5948533160781153e-08 tensor([1.5949e-08, 5.7153e-03, 9.8710e-01, 1.1617e-06, 7.1881e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0577107190101742e-08 tensor([1.0577e-08, 5.1810e-03, 9.8878e-01, 5.7691e-07, 6.0346e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016748395282775164 tensor([0.0017, 0.0508, 0.0338, 0.1511, 0.7625], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1841620423401764e-07 tensor([9.4240e-02, 5.6828e-04, 2.1842e-07, 9.0065e-01, 4.5441e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.738805495842826e-06 tensor([6.7388e-06, 1.2662e-01, 8.0261e-01, 7.0385e-05, 7.0692e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005838542128913105 tensor([0.1973, 0.4733, 0.0006, 0.1170, 0.2118], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.76113855256699e-05 tensor([4.7611e-05, 1.2456e-01, 4.8415e-01, 1.2356e-03, 3.9001e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.463621389698801e-09 tensor([3.8369e-01, 2.1289e-04, 3.4636e-09, 6.1573e-01, 3.6452e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014634515391662717 tensor([0.0015, 0.0719, 0.0179, 0.0440, 0.8647], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00141854805406183 tensor([0.0328, 0.8271, 0.0014, 0.0059, 0.1328], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.954270214189819e-09 tensor([8.9543e-09, 7.2327e-03, 9.8763e-01, 3.1379e-07, 5.1331e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.7143581468844786e-05 tensor([4.7144e-05, 1.7581e-01, 4.4347e-01, 6.6008e-04, 3.8001e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7882691736303968e-06 tensor([1.0361e-01, 1.7448e-03, 1.7883e-06, 8.7906e-01, 1.5578e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004831889644265175 tensor([0.0048, 0.1286, 0.0216, 0.1181, 0.7269], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.3574625781038776e-05 tensor([5.0452e-01, 2.0383e-01, 5.3575e-05, 2.1196e-01, 7.9633e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.83409356395714e-05 tensor([1.8341e-05, 1.0519e-01, 5.9103e-01, 4.3640e-04, 3.0333e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009001446887850761 tensor([0.0090, 0.4177, 0.0129, 0.0140, 0.5463], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.895694137805549e-07 tensor([3.2607e-01, 2.0601e-03, 2.8957e-07, 6.6785e-01, 4.0144e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009576283046044409 tensor([0.0010, 0.1253, 0.0423, 0.0230, 0.8084], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.9103500714409165e-06 tensor([7.7873e-01, 1.7403e-01, 4.9104e-06, 3.6879e-02, 1.0352e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.6154167901258916e-05 tensor([3.6154e-05, 1.3208e-01, 5.4422e-01, 8.2363e-04, 3.2284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009023090242408216 tensor([0.0009, 0.2709, 0.0988, 0.0107, 0.6186], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003136821906082332 tensor([4.0506e-02, 2.4629e-02, 3.1368e-04, 7.4506e-01, 1.8949e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009635126334615052 tensor([0.0367, 0.0653, 0.0010, 0.4378, 0.4592], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0016114546451717615 tensor([0.0953, 0.5925, 0.0016, 0.0596, 0.2509], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.644506361259573e-09 tensor([7.6445e-09, 1.4253e-03, 9.8609e-01, 2.8317e-06, 1.2483e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8569033272797242e-05 tensor([1.8569e-05, 1.1917e-01, 6.0481e-01, 4.2970e-04, 2.7557e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.351687708658574e-07 tensor([2.3342e-01, 3.7332e-03, 9.3517e-07, 7.5076e-01, 1.2085e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.6491491704946384e-05 tensor([3.7718e-02, 1.0431e-02, 5.6491e-05, 7.3199e-01, 2.1981e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003826255560852587 tensor([1.7942e-01, 6.8026e-01, 3.8263e-04, 2.9025e-02, 1.1092e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.083137587760575e-05 tensor([2.0831e-05, 1.6282e-02, 3.2995e-01, 6.0573e-03, 6.4769e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.390179704685579e-07 tensor([1.3902e-07, 3.8635e-03, 9.4002e-01, 6.3647e-05, 5.6054e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.743697642581537e-05 tensor([7.7437e-05, 9.9614e-02, 3.5495e-01, 3.2067e-03, 5.4216e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.780416591325775e-05 tensor([3.6971e-03, 7.3274e-04, 3.7804e-05, 8.5862e-01, 1.3691e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00036769243888556957 tensor([0.2457, 0.3606, 0.0004, 0.1125, 0.2809], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003001179138664156 tensor([3.0012e-04, 3.9446e-01, 2.4843e-01, 1.8765e-03, 3.5494e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1700886926035992e-09 tensor([1.1701e-09, 5.9064e-04, 9.9348e-01, 5.0818e-07, 5.9273e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0019591269083321095 tensor([0.0020, 0.1037, 0.0441, 0.0734, 0.7769], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00025283588911406696 tensor([2.5284e-04, 9.9886e-03, 1.0164e-02, 4.1032e-02, 9.3856e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001300858217291534 tensor([0.0029, 0.8419, 0.0166, 0.0013, 0.1373], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.54974711930845e-06 tensor([6.1631e-01, 7.1500e-02, 9.5497e-06, 2.8166e-01, 3.0524e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.547746274534802e-08 tensor([3.5477e-08, 2.7307e-03, 9.6964e-01, 9.2301e-06, 2.7617e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.816775112383766e-06 tensor([6.0621e-02, 3.1983e-03, 6.8168e-06, 9.0085e-01, 3.5322e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.828918041103861e-09 tensor([3.4835e-01, 3.5070e-04, 8.8289e-09, 6.5011e-01, 1.1895e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00047065698890946805 tensor([1.8562e-01, 5.7902e-01, 4.7066e-04, 7.7503e-02, 1.5738e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017065831925719976 tensor([0.0017, 0.7977, 0.0368, 0.0017, 0.1620], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.047259096751077e-07 tensor([4.0473e-07, 1.1265e-02, 8.9321e-01, 5.1995e-05, 9.5473e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7559481168282218e-05 tensor([1.8522e-01, 2.7710e-02, 2.7559e-05, 7.4370e-01, 4.3340e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005168557167053223 tensor([0.0057, 0.0744, 0.0052, 0.1166, 0.7982], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0026919874362647533 tensor([0.0027, 0.4402, 0.0438, 0.0096, 0.5036], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000288716604700312 tensor([2.8872e-04, 7.2821e-01, 1.3755e-01, 4.7233e-04, 1.3348e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00011764978262363002 tensor([1.1765e-04, 8.1146e-02, 1.8057e-01, 3.6195e-03, 7.3454e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.624075700121466e-07 tensor([2.6635e-01, 2.1392e-03, 4.6241e-07, 7.2237e-01, 9.1410e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.979433398943911e-08 tensor([4.5909e-01, 1.5907e-03, 4.9794e-08, 5.3718e-01, 2.1398e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.435770385200158e-06 tensor([6.0724e-01, 3.7131e-01, 7.4358e-06, 1.1851e-02, 9.5896e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018049192149192095 tensor([0.0044, 0.8671, 0.0125, 0.0018, 0.1142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002759763738140464 tensor([0.0042, 0.7760, 0.0162, 0.0028, 0.2008], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0024127685464918613 tensor([0.0436, 0.8559, 0.0024, 0.0190, 0.0791], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5534210029422013e-10 tensor([9.4546e-01, 3.5737e-04, 1.5534e-10, 5.4141e-02, 4.1119e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.30893373706931e-07 tensor([8.3227e-01, 2.5026e-02, 5.3089e-07, 1.3685e-01, 5.8445e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011213832767680287 tensor([0.0011, 0.7021, 0.0717, 0.0020, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.661658699158579e-05 tensor([6.6617e-05, 5.9506e-02, 3.4084e-01, 4.8990e-03, 5.9469e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.958427441830281e-06 tensor([4.5652e-01, 1.7992e-02, 3.9584e-06, 4.9689e-01, 2.8598e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018710778094828129 tensor([0.0019, 0.1251, 0.0193, 0.0278, 0.8259], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.56251905739191e-06 tensor([4.2589e-01, 1.4325e-02, 2.5625e-06, 5.2292e-01, 3.6861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00046890127123333514 tensor([4.6890e-04, 5.3233e-01, 1.6134e-01, 1.4035e-03, 3.0446e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.924851403600769e-06 tensor([1.9249e-06, 4.8966e-02, 8.1776e-01, 5.7971e-05, 1.3321e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012492082314565778 tensor([0.1071, 0.2370, 0.0012, 0.3265, 0.3281], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015407610917463899 tensor([0.0383, 0.1063, 0.0015, 0.3326, 0.5212], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005043100100010633 tensor([1.3603e-01, 7.0804e-01, 5.0431e-04, 3.0420e-02, 1.2500e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010166021063923836 tensor([0.0135, 0.4534, 0.0102, 0.0320, 0.4910], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004944679094478488 tensor([4.9447e-04, 2.2079e-01, 1.2698e-01, 5.9939e-03, 6.4574e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008286685915663838 tensor([0.0191, 0.0238, 0.0008, 0.6291, 0.3271], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006519010639749467 tensor([0.1007, 0.2305, 0.0007, 0.2094, 0.4588], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0019899746403098106 tensor([0.0055, 0.8919, 0.0082, 0.0020, 0.0925], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006800613831728697 tensor([0.0068, 0.5475, 0.0345, 0.0231, 0.3880], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.151577736090985e-08 tensor([6.1516e-08, 4.2948e-03, 9.4539e-01, 1.2816e-05, 5.0301e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2742764055626594e-10 tensor([4.3427e-01, 6.5233e-05, 3.2743e-10, 5.6557e-01, 9.4973e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005412379978224635 tensor([1.3623e-02, 2.1911e-02, 5.4124e-04, 3.9106e-01, 5.7287e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00013119223876856267 tensor([4.0705e-01, 3.9331e-01, 1.3119e-04, 9.4266e-02, 1.0524e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.70124136377126e-05 tensor([6.7012e-05, 6.1111e-01, 3.2244e-01, 1.1465e-04, 6.6265e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4675530312379124e-06 tensor([2.4676e-06, 2.9356e-02, 7.8752e-01, 1.7629e-04, 1.8294e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.862213998218067e-05 tensor([3.0440e-01, 4.5403e-02, 3.8622e-05, 5.6154e-01, 8.8619e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.115831752140366e-07 tensor([6.1158e-07, 3.5843e-03, 6.0058e-01, 6.4574e-04, 3.9519e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.0217535140764085e-09 tensor([9.8570e-01, 9.3816e-03, 3.0218e-09, 4.7644e-03, 1.5378e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.173664587549865e-06 tensor([6.1737e-06, 2.4883e-01, 7.0016e-01, 2.3394e-05, 5.0987e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1230888219415647e-07 tensor([1.1231e-07, 9.4988e-03, 9.6367e-01, 9.1212e-06, 2.6819e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00028035903233103454 tensor([2.8036e-04, 2.5821e-02, 6.5511e-02, 6.1725e-02, 8.4666e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5658686436381686e-07 tensor([7.0748e-01, 5.9293e-03, 1.5659e-07, 2.8334e-01, 3.2449e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1174604424013523e-06 tensor([8.6726e-01, 5.8752e-02, 1.1175e-06, 6.5610e-02, 8.3738e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.313043741101865e-07 tensor([6.3130e-07, 3.0913e-02, 9.2430e-01, 2.4056e-05, 4.4765e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005977347027510405 tensor([5.9773e-04, 6.5019e-02, 4.2971e-02, 2.1759e-02, 8.6965e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00043843535240739584 tensor([4.1743e-02, 3.0928e-02, 4.3844e-04, 7.5065e-01, 1.7624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00048561987932771444 tensor([3.8536e-02, 5.8578e-02, 4.8562e-04, 3.4806e-01, 5.5434e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000999247538857162 tensor([0.0920, 0.2298, 0.0010, 0.1928, 0.4844], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.2944139860546784e-08 tensor([3.2944e-08, 2.8807e-03, 9.7040e-01, 9.7662e-06, 2.6707e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1958417189816828e-07 tensor([1.1958e-07, 6.0424e-03, 9.3272e-01, 2.1360e-05, 6.1218e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0023903120309114456 tensor([0.0221, 0.0974, 0.0024, 0.2034, 0.6746], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.055300107414951e-07 tensor([1.9070e-01, 1.9696e-03, 5.0553e-07, 7.9812e-01, 9.2133e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001453713048249483 tensor([3.5338e-01, 4.5149e-01, 1.4537e-04, 8.5203e-02, 1.0978e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.831703568517696e-05 tensor([2.8317e-05, 1.3927e-01, 6.3447e-01, 6.2140e-04, 2.2561e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003136651066597551 tensor([3.1367e-04, 4.4624e-01, 1.7785e-01, 1.0653e-03, 3.7454e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00046376249520108104 tensor([8.1694e-02, 5.2008e-02, 4.6376e-04, 5.9251e-01, 2.7332e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0035079801455140114 tensor([0.0096, 0.0922, 0.0035, 0.1112, 0.7835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00024813017807900906 tensor([3.3559e-01, 2.7361e-01, 2.4813e-04, 1.6936e-01, 2.2119e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00048579080612398684 tensor([1.5731e-03, 9.3363e-01, 2.3360e-02, 4.8579e-04, 4.0949e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4076862814581546e-07 tensor([2.4077e-07, 2.5035e-02, 9.4428e-01, 6.1443e-06, 3.0674e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011184844188392162 tensor([0.0011, 0.2128, 0.1069, 0.0271, 0.6522], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.084299118607305e-05 tensor([5.8216e-03, 1.2473e-03, 5.0843e-05, 9.2083e-01, 7.2055e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7102656784118153e-05 tensor([6.2774e-01, 3.0046e-01, 1.7103e-05, 5.0888e-02, 2.0897e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011226189089938998 tensor([0.0021, 0.8619, 0.0200, 0.0011, 0.1149], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.310454692633357e-05 tensor([2.3105e-05, 1.9474e-02, 2.7259e-01, 4.4657e-03, 7.0345e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017808080883696675 tensor([1.7808e-04, 2.5724e-02, 1.8162e-01, 6.4565e-02, 7.2791e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.182692231959663e-06 tensor([1.8358e-02, 8.1316e-04, 6.1827e-06, 9.3223e-01, 4.8588e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0029347692616283894 tensor([0.0162, 0.9118, 0.0029, 0.0041, 0.0649], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002132306108251214 tensor([3.3575e-01, 3.9054e-01, 2.1323e-04, 1.6270e-01, 1.1079e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1580468708416447e-05 tensor([1.1580e-05, 1.0495e-01, 7.3080e-01, 2.6095e-04, 1.6398e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7255729289900046e-08 tensor([3.1670e-01, 4.4224e-04, 1.7256e-08, 6.8208e-01, 7.7823e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005375468754209578 tensor([0.0267, 0.0373, 0.0005, 0.4199, 0.5156], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.364646659065329e-07 tensor([9.1972e-01, 2.6511e-02, 3.3646e-07, 4.9929e-02, 3.8383e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007660257164388895 tensor([0.0077, 0.2581, 0.0169, 0.0602, 0.6571], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.544671471900074e-06 tensor([3.5447e-06, 3.9624e-02, 7.4626e-01, 1.4204e-04, 2.1397e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005435320548713207 tensor([0.0054, 0.3277, 0.0502, 0.0626, 0.5542], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002764632226899266 tensor([2.7646e-04, 1.6690e-01, 2.4558e-01, 8.1994e-03, 5.7904e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006569963879883289 tensor([7.6586e-02, 7.6652e-01, 6.5700e-04, 1.4726e-02, 1.4151e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7170454157167114e-05 tensor([1.7170e-05, 3.0646e-01, 6.2399e-01, 6.4888e-05, 6.9466e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0025984535459429026 tensor([0.0026, 0.0262, 0.0096, 0.2654, 0.6962], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.855723124639553e-08 tensor([9.1300e-01, 4.3779e-03, 2.8557e-08, 8.1789e-02, 8.3078e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.764593541040085e-06 tensor([5.4126e-02, 2.9643e-03, 4.7646e-06, 8.9184e-01, 5.1065e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.743223037919961e-05 tensor([6.5308e-01, 1.3820e-01, 1.7432e-05, 1.5022e-01, 5.8485e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.076858893611643e-06 tensor([1.0769e-06, 6.7082e-02, 8.9495e-01, 1.4490e-05, 3.7953e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003741567488759756 tensor([0.0250, 0.7495, 0.0037, 0.0107, 0.2111], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.9133240526421105e-09 tensor([5.7582e-01, 4.4552e-04, 5.9133e-09, 4.2318e-01, 5.5504e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012733244511764497 tensor([8.6853e-03, 3.3762e-03, 1.2733e-04, 8.0255e-01, 1.8527e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.97939003657666e-06 tensor([7.0386e-01, 8.8242e-02, 6.9794e-06, 1.6423e-01, 4.3660e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2643867851002142e-05 tensor([2.2644e-05, 8.2982e-02, 5.2656e-01, 7.9319e-04, 3.8964e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8233899027109146e-05 tensor([1.1378e-01, 1.1568e-02, 1.8234e-05, 8.1966e-01, 5.4973e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003339223621878773 tensor([8.6098e-02, 6.1390e-02, 3.3392e-04, 6.2722e-01, 2.2496e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003921928582713008 tensor([3.9219e-04, 5.3915e-03, 4.7344e-03, 1.2474e-01, 8.6474e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2758122466038913e-05 tensor([6.9291e-01, 1.1291e-01, 1.2758e-05, 1.5410e-01, 4.0071e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003926292527467012 tensor([0.0039, 0.2047, 0.0244, 0.0346, 0.7323], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.331458669388667e-05 tensor([5.3315e-05, 6.8052e-02, 3.4788e-01, 3.3834e-03, 5.8063e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001310207531787455 tensor([0.1049, 0.2886, 0.0013, 0.2377, 0.3675], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.899542884435505e-05 tensor([2.6134e-02, 5.1504e-03, 5.8995e-05, 8.4969e-01, 1.1897e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.492253421834903e-06 tensor([7.5529e-01, 2.0869e-01, 5.4923e-06, 2.8314e-02, 7.6995e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002644075721036643 tensor([2.6441e-04, 2.8546e-01, 2.4536e-01, 3.2972e-03, 4.6562e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.740459295746405e-08 tensor([7.7405e-08, 5.5058e-03, 9.5062e-01, 1.2728e-05, 4.3861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.19339876553704e-09 tensor([4.7349e-01, 4.2926e-04, 7.1934e-09, 5.2538e-01, 7.0515e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00014157398254610598 tensor([1.4157e-04, 1.5788e-02, 3.4084e-02, 2.8011e-02, 9.2198e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005210467614233494 tensor([0.0168, 0.3651, 0.0052, 0.0342, 0.5787], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0157298675039783e-05 tensor([1.0157e-05, 2.1242e-01, 7.2654e-01, 6.4082e-05, 6.0968e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.10728854755871e-05 tensor([7.1073e-05, 2.0947e-02, 1.2897e-01, 1.8217e-02, 8.3179e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008299703476950526 tensor([0.0382, 0.0485, 0.0008, 0.5723, 0.3401], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.4064041351957712e-05 tensor([1.4064e-05, 2.6154e-02, 4.1982e-01, 2.9025e-03, 5.5111e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3006236915534828e-06 tensor([8.2729e-01, 6.0107e-02, 2.3006e-06, 9.4983e-02, 1.7615e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.538962482707575e-05 tensor([4.8071e-05, 7.2628e-01, 2.2826e-01, 3.5390e-05, 4.5375e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012296048225834966 tensor([1.2296e-04, 2.0254e-02, 7.6959e-02, 1.8718e-02, 8.8395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2115803350052374e-08 tensor([4.6973e-02, 9.9026e-05, 3.2116e-08, 9.5158e-01, 1.3528e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2780741087681236e-07 tensor([1.2781e-07, 1.2451e-02, 9.6435e-01, 7.2842e-06, 2.3187e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7117325796789373e-06 tensor([8.4463e-01, 6.5285e-02, 1.7117e-06, 7.7404e-02, 1.2678e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.8680806824231695e-07 tensor([3.8681e-07, 3.3875e-02, 9.4061e-01, 8.2308e-06, 2.5508e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.5312128349905834e-05 tensor([4.5312e-05, 4.6899e-02, 3.1019e-01, 3.0357e-03, 6.3983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015845712274312973 tensor([0.0205, 0.3958, 0.0158, 0.0996, 0.4683], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0046505918726325035 tensor([0.0047, 0.0865, 0.0104, 0.1164, 0.7820], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.403753285307175e-07 tensor([4.3959e-01, 3.5097e-03, 2.4038e-07, 5.5107e-01, 5.8311e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006755975191481411 tensor([0.0007, 0.6509, 0.1112, 0.0015, 0.2357], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002220997354015708 tensor([0.0403, 0.1549, 0.0022, 0.1993, 0.6033], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.329178414584021e-06 tensor([4.9543e-02, 1.4530e-03, 2.3292e-06, 9.3286e-01, 1.6139e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008529347251169384 tensor([0.0368, 0.0634, 0.0009, 0.4285, 0.4705], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.890391285996884e-05 tensor([1.8904e-05, 3.3044e-01, 5.9508e-01, 6.9854e-05, 7.4391e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.811165520481666e-11 tensor([6.8112e-11, 4.1332e-04, 9.9855e-01, 2.2933e-08, 1.0359e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.761413821252063e-05 tensor([9.7614e-05, 4.4132e-01, 3.5101e-01, 3.0425e-04, 2.0726e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.486485792933649e-10 tensor([8.1763e-01, 3.0216e-04, 6.4865e-10, 1.8190e-01, 1.7378e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1680448298866395e-06 tensor([1.3605e-01, 3.7432e-03, 2.1680e-06, 8.3442e-01, 2.5791e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.232652231119573e-05 tensor([2.1253e-01, 7.6714e-01, 6.2327e-05, 5.4117e-03, 1.4857e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006588688120245934 tensor([6.5887e-04, 8.0914e-01, 8.3123e-02, 8.0465e-04, 1.0627e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.0580172111504e-08 tensor([4.0580e-08, 4.2378e-03, 9.6380e-01, 5.4026e-06, 3.1956e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011239389423280954 tensor([0.0539, 0.0709, 0.0011, 0.5564, 0.3177], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004796431749127805 tensor([0.1631, 0.1975, 0.0005, 0.3240, 0.3149], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.391320805931855e-09 tensor([9.8192e-01, 8.7351e-03, 6.3913e-09, 9.0554e-03, 2.8469e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010978623759001493 tensor([0.0011, 0.8242, 0.0645, 0.0012, 0.1090], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010915489838225767 tensor([1.0915e-04, 1.1429e-01, 3.4283e-01, 4.2317e-03, 5.3853e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.96472114516655e-06 tensor([1.4584e-01, 4.9542e-03, 3.9647e-06, 8.2123e-01, 2.7973e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006301685702055693 tensor([0.0279, 0.7151, 0.0063, 0.0220, 0.2287], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.6119907488464378e-05 tensor([5.3479e-02, 6.5308e-03, 2.6120e-05, 8.3533e-01, 1.0464e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.3816751460544765e-05 tensor([1.7810e-04, 9.1417e-01, 5.3080e-02, 5.3817e-05, 3.2520e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00021285969705786556 tensor([2.1286e-04, 5.3052e-01, 2.1136e-01, 4.6846e-04, 2.5744e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4099350412143394e-05 tensor([4.8764e-01, 1.0185e-01, 2.4099e-05, 3.4988e-01, 6.0608e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.493649117241148e-05 tensor([2.4936e-05, 3.9612e-03, 5.3156e-02, 2.3642e-02, 9.1922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [0], [0], [2], [0], [0], [2], [2], [2], [0], [2], [3], [0], [2], [2], [2], [2], [0], [2], [0], [0], [3], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [2], [2], [3], [2], [0], [2], [2], [0], [0], [0], [2], [2], [2], [3], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [0], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [0], [0], [3], [2], [0], [2], [2], [2], [3], [0], [2], [2], [0], [0], [0], [2], [2], [2], [3], [3], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [2], [2], [2], [2], [0], [2], [2], [3], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [3], [0], [0], [2], [2], [3], [0], [0], [2], [2], [2], [0], [2], [2], [2], [0], [0], [0], [0], [2], [0], [0], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [3], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [2], [2], [2], [0], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [3], [0], [2], [0]]\n",
      "[[0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [0], [0], [2], [0], [0], [2], [2], [2], [0], [2], [3], [0], [2], [2], [2], [2], [0], [2], [0], [0], [3], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [2], [2], [3], [2], [0], [2], [2], [0], [0], [0], [2], [2], [2], [3], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [0], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [0], [2], [2], [0], [0], [0], [0], [3], [2], [0], [2], [2], [2], [3], [0], [2], [2], [0], [0], [0], [2], [2], [2], [3], [3], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [2], [2], [2], [2], [0], [2], [2], [3], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [3], [0], [0], [2], [2], [3], [0], [0], [2], [2], [2], [0], [2], [2], [2], [0], [0], [0], [0], [2], [0], [0], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [2], [0], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [3], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [2], [2], [2], [0], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [2], [2], [3], [0], [2], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0044202165603637695  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004420215606689453  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004420212745666504  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004420209407806397  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004420204639434815  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004420198440551758  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004420191287994385  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.00442018461227417  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00442017650604248  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004420168399810791  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004420158863067627  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004420148849487305  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004420138835906982  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.0044201288223266605  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00442011833190918  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004420105934143066  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0044200940132141115  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004420083999633789  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004420071125030517  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004420059680938721  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.013075292110443115 tensor([8.7722e-01, 5.9012e-02, 1.2052e-06, 5.0693e-02, 1.3075e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2807309985873871e-06 tensor([5.8817e-08, 1.9247e-02, 9.7185e-01, 1.2807e-06, 8.8964e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00406213104724884 tensor([1.8501e-04, 1.6353e-01, 2.3662e-01, 4.0621e-03, 5.9560e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008505976758897305 tensor([6.3339e-02, 8.5060e-03, 3.0371e-05, 8.6994e-01, 5.8188e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009937314316630363 tensor([1.5630e-04, 4.0261e-02, 7.8191e-02, 9.9373e-03, 8.7145e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.024691011756658554 tensor([0.0017, 0.1609, 0.0458, 0.0247, 0.7670], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002502512652426958 tensor([0.0149, 0.9233, 0.0022, 0.0025, 0.0571], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012889483012259007 tensor([0.0129, 0.2072, 0.0058, 0.0404, 0.7336], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00826284196227789 tensor([3.0873e-02, 8.2628e-03, 9.6317e-05, 8.4044e-01, 1.2033e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010302802547812462 tensor([7.9838e-05, 3.2003e-02, 1.3530e-01, 1.0303e-02, 8.2231e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008149469271302223 tensor([0.0348, 0.8791, 0.0020, 0.0081, 0.0759], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015390054322779179 tensor([1.5390e-03, 8.9887e-01, 2.2161e-02, 6.1811e-04, 7.6814e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.922024214058183e-05 tensor([3.4408e-08, 1.3243e-03, 9.6268e-01, 2.9220e-05, 3.5964e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007067349273711443 tensor([2.6346e-01, 7.0673e-03, 2.6179e-06, 7.0819e-01, 2.1280e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.542366751702502e-05 tensor([5.1688e-01, 9.5424e-05, 3.7298e-10, 4.8286e-01, 1.5473e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05981244519352913 tensor([0.0708, 0.5487, 0.0022, 0.0598, 0.3185], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13166242837905884 tensor([0.1317, 0.3018, 0.0010, 0.2175, 0.3481], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006941488245502114 tensor([5.2500e-06, 2.6104e-02, 6.7593e-01, 6.9415e-04, 2.9727e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0028441487811505795 tensor([1.5019e-01, 2.8441e-03, 1.0238e-06, 8.3755e-01, 9.4175e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02702413685619831 tensor([0.0021, 0.0966, 0.0270, 0.0643, 0.8100], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002590879565104842 tensor([9.9635e-05, 1.1445e-01, 2.7472e-01, 2.5909e-03, 6.0815e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001310123479925096 tensor([6.7590e-03, 9.7874e-01, 1.3101e-03, 4.3633e-04, 1.2756e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00602860189974308 tensor([0.0041, 0.6445, 0.0231, 0.0060, 0.3223], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07970133423805237 tensor([0.0797, 0.2363, 0.0019, 0.3282, 0.3539], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0030560267623513937 tensor([1.0564e-02, 3.0560e-03, 8.2123e-05, 7.7719e-01, 2.0911e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022522766143083572 tensor([7.5589e-01, 1.6456e-01, 7.3024e-06, 5.7024e-02, 2.2523e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009401903487741947 tensor([0.0029, 0.4452, 0.0329, 0.0094, 0.5097], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.156705396482721e-05 tensor([8.6594e-07, 1.8779e-02, 8.5474e-01, 7.1567e-05, 1.2640e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00015155352593865246 tensor([1.3049e-01, 1.5155e-04, 1.4506e-08, 8.6833e-01, 1.0257e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0019292637007310987 tensor([5.2854e-03, 1.9293e-03, 1.0740e-04, 6.8011e-01, 3.1257e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.036381207406520844 tensor([5.7678e-01, 3.4018e-01, 2.9745e-05, 4.6623e-02, 3.6381e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.452445035800338e-05 tensor([7.7891e-05, 7.0757e-01, 2.4006e-01, 8.4524e-05, 5.2209e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003135215083602816 tensor([1.0991e-04, 5.0264e-01, 3.1238e-01, 3.1352e-04, 1.8455e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1408899575471878 tensor([3.4496e-01, 2.2468e-01, 1.6132e-04, 2.8931e-01, 1.4089e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0032226971816271544 tensor([0.0010, 0.0089, 0.0032, 0.1932, 0.7936], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07018924504518509 tensor([2.0739e-01, 5.2993e-01, 3.7392e-04, 7.0189e-02, 1.9212e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010372948832809925 tensor([5.3249e-04, 1.5850e-01, 1.2627e-01, 1.0373e-02, 7.0433e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.823746035574004e-05 tensor([1.5391e-07, 5.7563e-03, 9.2999e-01, 2.8237e-05, 6.4224e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014465372078120708 tensor([4.1072e-01, 1.4465e-03, 7.0254e-08, 5.8603e-01, 1.8065e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015859396662563086 tensor([0.0016, 0.0047, 0.0015, 0.4853, 0.5069], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004605390131473541 tensor([0.0046, 0.8535, 0.0128, 0.0022, 0.1268], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03104349412024021 tensor([0.0627, 0.7744, 0.0016, 0.0310, 0.1302], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.772690646059345e-06 tensor([5.3650e-08, 4.1147e-03, 9.6312e-01, 9.7727e-06, 3.2756e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014207577332854271 tensor([4.1081e-01, 1.4208e-02, 3.6867e-06, 5.3852e-01, 3.6456e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030208129435777664 tensor([0.0302, 0.1403, 0.0020, 0.2265, 0.6010], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009214433841407299 tensor([0.0038, 0.4419, 0.0242, 0.0092, 0.5208], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015505729243159294 tensor([3.7270e-04, 4.5190e-01, 1.7227e-01, 1.5506e-03, 3.7391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3252297321741935e-06 tensor([1.2590e-08, 3.3297e-03, 9.8568e-01, 1.3252e-06, 1.0986e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08243200182914734 tensor([1.8729e-01, 8.2432e-02, 1.9475e-04, 5.9545e-01, 1.3463e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009073197841644287 tensor([6.0290e-02, 9.0732e-03, 3.2460e-05, 8.2635e-01, 1.0426e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0016767163760960102 tensor([8.9755e-01, 8.9755e-02, 3.5816e-07, 1.1020e-02, 1.6767e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006502600386738777 tensor([0.0065, 0.7998, 0.0143, 0.0045, 0.1750], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00018376161460764706 tensor([1.9462e-06, 2.4517e-02, 7.8133e-01, 1.8376e-04, 1.9396e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06492061913013458 tensor([0.0649, 0.4260, 0.0026, 0.1192, 0.3873], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.966524541785475e-05 tensor([2.7803e-01, 1.9665e-05, 8.3975e-11, 7.2189e-01, 5.6157e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00457696383818984 tensor([8.8144e-01, 8.9731e-02, 1.1411e-06, 2.4252e-02, 4.5770e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0023865702096372843 tensor([3.3323e-04, 3.9082e-01, 2.4641e-01, 2.3866e-03, 3.6004e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016690486518200487 tensor([1.6903e-06, 1.9770e-02, 7.5909e-01, 1.6690e-04, 2.2097e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0714821070432663 tensor([0.0715, 0.1865, 0.0016, 0.3816, 0.3588], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006346673239022493 tensor([0.0063, 0.0103, 0.0009, 0.4489, 0.5336], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02426336705684662 tensor([1.7404e-01, 7.3054e-01, 3.6455e-04, 2.4263e-02, 7.0789e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1762255098801688e-06 tensor([1.6435e-08, 5.8504e-03, 9.8687e-01, 1.1762e-06, 7.2754e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.852164122188697e-07 tensor([1.0910e-08, 5.3013e-03, 9.8859e-01, 5.8522e-07, 6.1111e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.033539265394210815 tensor([0.0017, 0.0517, 0.0335, 0.1503, 0.7627], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005803288077004254 tensor([9.6312e-02, 5.8033e-04, 2.1828e-07, 8.9854e-01, 4.5706e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.088862912496552e-05 tensor([6.9109e-06, 1.2907e-01, 7.9961e-01, 7.0889e-05, 7.1236e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11530394107103348 tensor([0.1987, 0.4756, 0.0006, 0.1153, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012387066381052136 tensor([4.8730e-05, 1.2676e-01, 4.8052e-01, 1.2387e-03, 3.9143e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021631161507684737 tensor([3.8946e-01, 2.1631e-04, 3.4499e-09, 6.0996e-01, 3.6487e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01774671860039234 tensor([0.0015, 0.0730, 0.0177, 0.0438, 0.8640], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005786242429167032 tensor([0.0330, 0.8282, 0.0014, 0.0058, 0.1317], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.180007297487464e-07 tensor([9.2077e-09, 7.3825e-03, 9.8742e-01, 3.1800e-07, 5.1947e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006615303573198617 tensor([4.8205e-05, 1.7867e-01, 4.3962e-01, 6.6153e-04, 3.8100e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017841358203440905 tensor([1.0588e-01, 1.7841e-03, 1.7897e-06, 8.7666e-01, 1.5672e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02135108783841133 tensor([0.0049, 0.1303, 0.0214, 0.1174, 0.7260], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07907048612833023 tensor([5.0715e-01, 2.0475e-01, 5.2849e-05, 2.0898e-01, 7.9070e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00043750699842348695 tensor([1.8743e-05, 1.0704e-01, 5.8785e-01, 4.3751e-04, 3.0466e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012692888267338276 tensor([0.0091, 0.4210, 0.0127, 0.0139, 0.5433], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002093403832986951 tensor([3.3096e-01, 2.0934e-03, 2.8874e-07, 6.6293e-01, 4.0215e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022903596982359886 tensor([0.0010, 0.1271, 0.0418, 0.0229, 0.8072], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01026272401213646 tensor([7.7914e-01, 1.7434e-01, 4.8438e-06, 3.6253e-02, 1.0263e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008248826488852501 tensor([3.6961e-05, 1.3444e-01, 5.4073e-01, 8.2488e-04, 3.2398e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010667307302355766 tensor([0.0009, 0.2740, 0.0976, 0.0107, 0.6168], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.025136206299066544 tensor([4.1307e-02, 2.5136e-02, 3.1365e-04, 7.4274e-01, 1.9051e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03740820661187172 tensor([0.0374, 0.0664, 0.0010, 0.4356, 0.4596], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05887952819466591 tensor([0.0958, 0.5946, 0.0016, 0.0589, 0.2491], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.863842837541597e-06 tensor([7.8882e-09, 1.4619e-03, 9.8591e-01, 2.8638e-06, 1.2628e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00043153419392183423 tensor([1.9019e-05, 1.2136e-01, 6.0120e-01, 4.3153e-04, 2.7699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0038075693883001804 tensor([2.3783e-01, 3.8076e-03, 9.3358e-07, 7.4624e-01, 1.2126e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010654509998857975 tensor([3.8539e-02, 1.0655e-02, 5.6410e-05, 7.2988e-01, 2.2087e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.028607064858078957 tensor([1.7988e-01, 6.8124e-01, 3.7666e-04, 2.8607e-02, 1.0990e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006050435360521078 tensor([2.1263e-05, 1.6573e-02, 3.2808e-01, 6.0504e-03, 6.4928e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.419617420760915e-05 tensor([1.4326e-07, 3.9608e-03, 9.3936e-01, 6.4196e-05, 5.6620e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0032026958651840687 tensor([7.9002e-05, 1.0127e-01, 3.5240e-01, 3.2027e-03, 5.4305e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007493375451304018 tensor([3.7819e-03, 7.4934e-04, 3.7829e-05, 8.5767e-01, 1.3777e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11100031435489655 tensor([0.2469, 0.3626, 0.0004, 0.1110, 0.2792], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018678357591852546 tensor([3.0439e-04, 3.9837e-01, 2.4542e-01, 1.8678e-03, 3.5404e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.150749302629265e-07 tensor([1.2093e-09, 6.0583e-04, 9.9339e-01, 5.1507e-07, 6.0028e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04370621219277382 tensor([0.0020, 0.1053, 0.0437, 0.0728, 0.7762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010073968209326267 tensor([2.5814e-04, 1.0160e-02, 1.0074e-02, 4.0884e-02, 9.3862e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0029168049804866314 tensor([0.0029, 0.8434, 0.0163, 0.0013, 0.1361], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03034016489982605 tensor([6.1997e-01, 7.1938e-02, 9.4387e-06, 2.7774e-01, 3.0340e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.330422471975908e-06 tensor([3.6599e-08, 2.8003e-03, 9.6926e-01, 9.3304e-06, 2.7928e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003274882910773158 tensor([6.2079e-02, 3.2749e-03, 6.8190e-06, 8.9909e-01, 3.5552e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00035639115958474576 tensor([3.5369e-01, 3.5639e-04, 8.7996e-09, 6.4476e-01, 1.1915e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07637576758861542 tensor([1.8653e-01, 5.8065e-01, 4.6283e-04, 7.6376e-02, 1.5598e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0017272081458941102 tensor([0.0017, 0.7998, 0.0362, 0.0017, 0.1606], grad_fn=<SoftmaxBackward0>)\n",
      "3 5.256074291537516e-05 tensor([4.1784e-07, 1.1552e-02, 8.9188e-01, 5.2561e-05, 9.6515e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028226396068930626 tensor([1.8878e-01, 2.8226e-02, 2.7443e-05, 7.3953e-01, 4.3444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0058039715513587 tensor([0.0058, 0.0755, 0.0051, 0.1160, 0.7976], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009553208947181702 tensor([0.0027, 0.4435, 0.0433, 0.0096, 0.5009], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004690811038017273 tensor([2.9149e-04, 7.3129e-01, 1.3533e-01, 4.6908e-04, 1.3262e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036174238193780184 tensor([1.2008e-04, 8.2368e-02, 1.7878e-01, 3.6174e-03, 7.3512e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002178503666073084 tensor([2.7112e-01, 2.1785e-03, 4.6097e-07, 7.1754e-01, 9.1617e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001611996442079544 tensor([4.6491e-01, 1.6120e-03, 4.9451e-08, 5.3135e-01, 2.1362e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00949771236628294 tensor([6.0757e-01, 3.7126e-01, 7.3209e-06, 1.1666e-02, 9.4977e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0044362773187458515 tensor([0.0044, 0.8683, 0.0123, 0.0018, 0.1132], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004244645591825247 tensor([0.0042, 0.7781, 0.0159, 0.0027, 0.1990], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01867152750492096 tensor([0.0438, 0.8569, 0.0024, 0.0187, 0.0782], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.091652590432204e-05 tensor([9.4632e-01, 3.5916e-04, 1.5418e-10, 5.3279e-02, 4.0917e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0057976809330284595 tensor([8.3454e-01, 2.5122e-02, 5.2406e-07, 1.3454e-01, 5.7977e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0019455536967143416 tensor([0.0011, 0.7050, 0.0705, 0.0019, 0.2215], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004899858962744474 tensor([6.8088e-05, 6.0551e-02, 3.3837e-01, 4.8999e-03, 5.9611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01820804923772812 tensor([4.6197e-01, 1.8208e-02, 3.9233e-06, 4.9131e-01, 2.8509e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.019083296880126 tensor([0.0019, 0.1269, 0.0191, 0.0277, 0.8244], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014507955871522427 tensor([4.3082e-01, 1.4508e-02, 2.5481e-06, 5.1784e-01, 3.6831e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013939072377979755 tensor([4.7383e-04, 5.3591e-01, 1.5916e-01, 1.3939e-03, 3.0306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.852276080986485e-05 tensor([1.9812e-06, 5.0059e-02, 8.1543e-01, 5.8523e-05, 1.3445e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10849545896053314 tensor([0.1085, 0.2399, 0.0012, 0.3233, 0.3271], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03899922966957092 tensor([0.0390, 0.1080, 0.0015, 0.3305, 0.5210], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.029982803389430046 tensor([1.3654e-01, 7.0919e-01, 4.9572e-04, 2.9983e-02, 1.2379e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013652308844029903 tensor([0.0137, 0.4568, 0.0100, 0.0317, 0.4879], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005974672734737396 tensor([5.0329e-04, 2.2352e-01, 1.2544e-01, 5.9747e-03, 6.4457e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01951942779123783 tensor([0.0195, 0.0243, 0.0008, 0.6270, 0.3284], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10206399857997894 tensor([0.1021, 0.2330, 0.0006, 0.2074, 0.4568], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005505444016307592 tensor([0.0055, 0.8928, 0.0081, 0.0020, 0.0916], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022884754464030266 tensor([0.0069, 0.5507, 0.0341, 0.0229, 0.3855], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2977971891814377e-05 tensor([6.3570e-08, 4.4061e-03, 9.4468e-01, 1.2978e-05, 5.0902e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.628252594964579e-05 tensor([4.4031e-01, 6.6283e-05, 3.2625e-10, 5.5953e-01, 9.5039e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013907122425734997 tensor([1.3907e-02, 2.2307e-02, 5.3759e-04, 3.8956e-01, 5.7369e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.092832550406456 tensor([4.0810e-01, 3.9454e-01, 1.2945e-04, 9.2833e-02, 1.0440e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011440543312346563 tensor([6.7852e-05, 6.1525e-01, 3.1842e-01, 1.1441e-04, 6.6148e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017799949273467064 tensor([2.5452e-06, 3.0063e-02, 7.8506e-01, 1.7800e-04, 1.8470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04605959728360176 tensor([3.0877e-01, 4.6060e-02, 3.8366e-05, 5.5659e-01, 8.8543e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006486516213044524 tensor([6.2780e-07, 3.6611e-03, 5.9819e-01, 6.4865e-04, 3.9750e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00015373485803138465 tensor([9.8570e-01, 9.4310e-03, 3.0173e-09, 4.7137e-03, 1.5373e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.358667552471161e-05 tensor([6.3205e-06, 2.5278e-01, 6.9586e-01, 2.3587e-05, 5.1334e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.226875590684358e-06 tensor([1.1567e-07, 9.7209e-03, 9.6314e-01, 9.2269e-06, 2.7125e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026273781433701515 tensor([2.8587e-04, 2.6274e-02, 6.5006e-02, 6.1409e-02, 8.4702e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032224904280155897 tensor([7.1183e-01, 5.9682e-03, 1.5467e-07, 2.7898e-01, 3.2225e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008303486742079258 tensor([8.6831e-01, 5.8915e-02, 1.1030e-06, 6.4469e-02, 8.3035e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.4284654500661418e-05 tensor([6.4968e-07, 3.1629e-02, 9.2313e-01, 2.4285e-05, 4.5212e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021699171513319016 tensor([6.0926e-04, 6.5986e-02, 4.2568e-02, 2.1699e-02, 8.6914e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.031579382717609406 tensor([4.2629e-02, 3.1579e-02, 4.3772e-04, 7.4828e-01, 1.7708e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.039245788007974625 tensor([3.9246e-02, 5.9534e-02, 4.8213e-04, 3.4617e-01, 5.5457e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09297029674053192 tensor([0.0930, 0.2321, 0.0010, 0.1911, 0.4829], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.879748176899739e-06 tensor([3.4044e-08, 2.9577e-03, 9.7001e-01, 9.8797e-06, 2.7023e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.1602472770609893e-05 tensor([1.2344e-07, 6.1963e-03, 9.3187e-01, 2.1602e-05, 6.1916e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.022465864196419716 tensor([0.0225, 0.0989, 0.0024, 0.2022, 0.6741], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0020097647793591022 tensor([1.9445e-01, 2.0098e-03, 5.0500e-07, 7.9428e-01, 9.2534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08390577137470245 tensor([3.5455e-01, 4.5260e-01, 1.4316e-04, 8.3906e-02, 1.0881e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006233823951333761 tensor([2.8976e-05, 1.4181e-01, 6.3086e-01, 6.2338e-04, 2.2668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010609938763082027 tensor([3.1790e-04, 4.4992e-01, 1.7549e-01, 1.0610e-03, 3.7321e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0529545359313488 tensor([8.3199e-02, 5.2955e-02, 4.6195e-04, 5.8944e-01, 2.7394e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009736920706927776 tensor([0.0097, 0.0935, 0.0035, 0.1106, 0.7826], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16718539595603943 tensor([3.3742e-01, 2.7526e-01, 2.4542e-04, 1.6719e-01, 2.1989e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001582132768817246 tensor([1.5821e-03, 9.3435e-01, 2.2962e-02, 4.8168e-04, 4.0629e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.220388968358748e-06 tensor([2.4787e-07, 2.5592e-02, 9.4338e-01, 6.2204e-06, 3.1023e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026874233037233353 tensor([0.0011, 0.2156, 0.1057, 0.0269, 0.6506], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001275635906495154 tensor([5.9571e-03, 1.2756e-03, 5.0884e-05, 9.2019e-01, 7.2531e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020660551264882088 tensor([6.2868e-01, 3.0070e-01, 1.6797e-05, 4.9942e-02, 2.0661e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0021169306710362434 tensor([0.0021, 0.8633, 0.0196, 0.0011, 0.1139], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004467031918466091 tensor([2.3623e-05, 1.9820e-02, 2.7055e-01, 4.4670e-03, 7.0514e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026138268411159515 tensor([1.8115e-04, 2.6138e-02, 1.8064e-01, 6.4279e-02, 7.2876e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008323690271936357 tensor([1.8796e-02, 8.3237e-04, 6.1897e-06, 9.3143e-01, 4.8937e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00406681140884757 tensor([0.0163, 0.9125, 0.0029, 0.0041, 0.0643], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10988525301218033 tensor([3.3752e-01, 3.9207e-01, 2.0998e-04, 1.6031e-01, 1.0989e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002625021443236619 tensor([1.1876e-05, 1.0701e-01, 7.2761e-01, 2.6250e-04, 1.6511e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004500830254983157 tensor([3.2184e-01, 4.5008e-04, 1.7226e-08, 6.7693e-01, 7.8049e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02726595848798752 tensor([0.0273, 0.0380, 0.0005, 0.4181, 0.5162], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003810151945799589 tensor([9.2049e-01, 2.6592e-02, 3.3276e-07, 4.9112e-02, 3.8102e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016746018081903458 tensor([0.0078, 0.2609, 0.0167, 0.0597, 0.6549], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00014342847862280905 tensor([3.6509e-06, 4.0507e-02, 7.4345e-01, 1.4343e-04, 2.1590e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04955136775970459 tensor([0.0055, 0.3313, 0.0496, 0.0620, 0.5517], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008171153254806995 tensor([2.8139e-04, 1.6929e-01, 2.4337e-01, 8.1712e-03, 5.7888e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014540055766701698 tensor([7.6911e-02, 7.6773e-01, 6.4625e-04, 1.4540e-02, 1.4018e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.516803114209324e-05 tensor([1.7523e-05, 3.1077e-01, 6.1939e-01, 6.5168e-05, 6.9757e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009508267045021057 tensor([0.0026, 0.0266, 0.0095, 0.2643, 0.6970], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008234356646426022 tensor([9.1453e-01, 4.3956e-03, 2.8204e-08, 8.0250e-02, 8.2344e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003031318075954914 tensor([5.5422e-02, 3.0313e-03, 4.7572e-06, 8.9020e-01, 5.1341e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05805947631597519 tensor([6.5521e-01, 1.3878e-01, 1.7222e-05, 1.4794e-01, 5.8059e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4655922313977499e-05 tensor([1.1082e-06, 6.8525e-02, 8.9311e-01, 1.4656e-05, 3.8353e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01057571079581976 tensor([0.0252, 0.7515, 0.0037, 0.0106, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00045047063031233847 tensor([5.8114e-01, 4.5047e-04, 5.8751e-09, 4.1786e-01, 5.5369e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0034518970642238855 tensor([8.8860e-03, 3.4519e-03, 1.2721e-04, 8.0121e-01, 1.8632e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.043357133865356445 tensor([7.0625e-01, 8.8669e-02, 6.9004e-06, 1.6171e-01, 4.3357e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007964239921420813 tensor([2.3202e-05, 8.4524e-02, 5.2315e-01, 7.9642e-04, 3.9150e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0117992227897048 tensor([1.1615e-01, 1.1799e-02, 1.8186e-05, 8.1683e-01, 5.5198e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06250495463609695 tensor([8.7744e-02, 6.2505e-02, 3.3230e-04, 6.2399e-01, 2.2543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004694275092333555 tensor([4.0047e-04, 5.4822e-03, 4.6943e-03, 1.2436e-01, 8.6506e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.039803363382816315 tensor([6.9502e-01, 1.1344e-01, 1.2617e-05, 1.5172e-01, 3.9803e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024116478860378265 tensor([0.0040, 0.2070, 0.0241, 0.0344, 0.7304], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0033837712835520506 tensor([5.4433e-05, 6.9175e-02, 3.4534e-01, 3.3838e-03, 5.8204e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10616924613714218 tensor([0.1062, 0.2914, 0.0013, 0.2353, 0.3659], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005268674343824387 tensor([2.6735e-02, 5.2687e-03, 5.9009e-05, 8.4823e-01, 1.1971e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007624280638992786 tensor([7.5568e-01, 2.0887e-01, 5.4083e-06, 2.7819e-02, 7.6243e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0032833360601216555 tensor([2.6839e-04, 2.8872e-01, 2.4282e-01, 3.2833e-03, 4.6491e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2877676454081666e-05 tensor([7.9929e-08, 5.6474e-03, 9.4997e-01, 1.2878e-05, 4.4367e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00043505587382242084 tensor([4.7932e-01, 4.3506e-04, 7.1524e-09, 5.1954e-01, 7.0451e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016052603721618652 tensor([1.4438e-04, 1.6053e-02, 3.3777e-02, 2.7884e-02, 9.2214e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01698799803853035 tensor([0.0170, 0.3680, 0.0051, 0.0339, 0.5759], grad_fn=<SoftmaxBackward0>)\n",
      "3 6.445912003982812e-05 tensor([1.0399e-05, 2.1610e-01, 7.2247e-01, 6.4459e-05, 6.1351e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01814086176455021 tensor([7.2399e-05, 2.1288e-02, 1.2798e-01, 1.8141e-02, 8.3251e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0389678031206131 tensor([0.0390, 0.0494, 0.0008, 0.5695, 0.3412], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002905283821746707 tensor([1.4398e-05, 2.6668e-02, 4.1736e-01, 2.9053e-03, 5.5306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01747831329703331 tensor([8.2875e-01, 6.0289e-02, 2.2729e-06, 9.3482e-02, 1.7478e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.862266359850764e-05 tensor([4.8623e-05, 7.2958e-01, 2.2507e-01, 3.5336e-05, 4.5271e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018670151010155678 tensor([1.2546e-04, 2.0580e-02, 7.6251e-02, 1.8670e-02, 8.8437e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001016079550026916 tensor([4.8182e-02, 1.0161e-04, 3.2202e-08, 9.5035e-01, 1.3642e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.365147212112788e-06 tensor([1.3165e-07, 1.2747e-02, 9.6380e-01, 7.3651e-06, 2.3450e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01255171000957489 tensor([8.4602e-01, 6.5389e-02, 1.6849e-06, 7.6042e-02, 1.2552e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.321046152559575e-06 tensor([3.9804e-07, 3.4637e-02, 9.3957e-01, 8.3210e-06, 2.5787e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003034015651792288 tensor([4.6225e-05, 4.7672e-02, 3.0810e-01, 3.0340e-03, 6.4114e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02072475664317608 tensor([0.0207, 0.3995, 0.0156, 0.0985, 0.4656], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010314332321286201 tensor([0.0047, 0.0878, 0.0103, 0.1158, 0.7814], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0035555041395127773 tensor([4.4489e-01, 3.5555e-03, 2.3905e-07, 5.4573e-01, 5.8274e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014579688431695104 tensor([0.0007, 0.6542, 0.1095, 0.0015, 0.2342], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04098566994071007 tensor([0.0410, 0.1569, 0.0022, 0.1980, 0.6019], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014870630111545324 tensor([5.0717e-02, 1.4871e-03, 2.3309e-06, 9.3154e-01, 1.6249e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03749556094408035 tensor([0.0375, 0.0644, 0.0008, 0.4264, 0.4708], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.017161988187581e-05 tensor([1.9293e-05, 3.3493e-01, 5.9031e-01, 7.0172e-05, 7.4679e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.322134839971568e-08 tensor([7.0035e-11, 4.2182e-04, 9.9853e-01, 2.3221e-08, 1.0477e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00030441483249887824 tensor([9.9407e-05, 4.4599e-01, 3.4647e-01, 3.0441e-04, 2.0714e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00017286016372963786 tensor([8.2071e-01, 3.0456e-04, 6.4356e-10, 1.7881e-01, 1.7286e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003817203687503934 tensor([1.3884e-01, 3.8172e-03, 2.1629e-06, 8.3144e-01, 2.5894e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0053440420888364315 tensor([2.1296e-01, 7.6691e-01, 6.1340e-05, 5.3440e-03, 1.4723e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007968996069394052 tensor([6.6404e-04, 8.1150e-01, 8.1645e-02, 7.9690e-04, 1.0539e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.473181772686075e-06 tensor([4.1931e-08, 4.3475e-03, 9.6330e-01, 5.4732e-06, 3.2346e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05490346625447273 tensor([0.0549, 0.0721, 0.0011, 0.5536, 0.3183], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16525022685527802 tensor([0.1653, 0.1997, 0.0005, 0.3209, 0.3137], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00028447640943340957 tensor([9.8198e-01, 8.7796e-03, 6.3775e-09, 8.9559e-03, 2.8448e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011895354837179184 tensor([0.0011, 0.8263, 0.0634, 0.0012, 0.1081], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004225907847285271 tensor([1.1129e-04, 1.1611e-01, 3.4027e-01, 4.2259e-03, 5.3929e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00505876773968339 tensor([1.4882e-01, 5.0588e-03, 3.9633e-06, 8.1801e-01, 2.8109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021771544590592384 tensor([0.0281, 0.7172, 0.0062, 0.0218, 0.2267], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006673146039247513 tensor([5.4636e-02, 6.6731e-03, 2.6121e-05, 8.3341e-01, 1.0526e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001795760908862576 tensor([1.7958e-04, 9.1527e-01, 5.2159e-02, 5.3556e-05, 3.2333e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00046624697279185057 tensor([2.1540e-04, 5.3433e-01, 2.0856e-01, 4.6625e-04, 2.5643e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.060305386781692505 tensor([4.9172e-01, 1.0276e-01, 2.3845e-05, 3.4519e-01, 6.0305e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004025486297905445 tensor([2.5444e-05, 4.0255e-03, 5.2677e-02, 2.3569e-02, 9.1970e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [0, 3], [2, 3], [2, 0], [2, 1], [0, 3], [2, 3], [3, 0], [0, 3], [2, 1], [2, 1], [2, 3], [2, 0], [0, 3], [2, 1], [0, 2], [0, 3], [3, 2], [0, 3], [2, 0], [2, 1], [2, 4], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [2, 3], [0, 3], [0, 3], [2, 1], [2, 0], [3, 0], [2, 3], [0, 3], [2, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [3, 0], [0, 3], [2, 0], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [2, 0], [2, 3], [0, 3], [0, 3], [0, 2], [2, 1], [0, 3], [2, 3], [0, 3], [2, 1], [0, 2], [2, 3], [0, 3], [0, 3], [2, 1], [0, 2], [2, 4], [0, 3], [0, 2], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 1], [2, 0], [2, 3], [0, 3], [0, 3], [2, 1], [2, 1], [2, 3], [0, 3], [0, 3], [0, 3], [2, 1], [2, 3], [0, 3], [0, 3], [0, 2], [0, 2], [3, 0], [2, 4], [0, 3], [2, 1], [2, 1], [2, 3], [3, 0], [0, 3], [2, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [3, 0], [3, 0], [2, 3], [2, 4], [2, 4], [0, 3], [0, 3], [2, 1], [0, 2], [2, 1], [0, 3], [0, 3], [2, 0], [2, 0], [2, 3], [2, 0], [0, 3], [2, 0], [2, 0], [3, 0], [0, 3], [0, 3], [2, 1], [2, 0], [2, 3], [0, 3], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [2, 4], [0, 3], [0, 3], [2, 1], [2, 0], [2, 0], [0, 3], [0, 3], [2, 0], [2, 1], [2, 3], [0, 3], [0, 3], [2, 1], [2, 0], [2, 3], [3, 0], [0, 3], [0, 3], [2, 1], [2, 4], [3, 0], [0, 3], [0, 1], [2, 1], [2, 3], [2, 4], [0, 3], [2, 1], [2, 0], [2, 4], [0, 2], [0, 3], [0, 2], [0, 3], [2, 3], [0, 3], [0, 2], [2, 4], [2, 1], [2, 4], [0, 3], [2, 3], [2, 1], [2, 1], [2, 4], [0, 3], [2, 1], [2, 1], [0, 2], [2, 4], [0, 2], [0, 3], [2, 0], [2, 1], [2, 4], [0, 3], [0, 3], [2, 1], [0, 1], [2, 0], [0, 3], [0, 3], [2, 0], [0, 3], [2, 4], [3, 0], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 0], [0, 2], [2, 1], [0, 3], [2, 0], [2, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 4], [2, 1], [2, 3], [0, 3], [0, 3], [2, 0], [2, 0], [2, 4], [0, 3], [0, 3], [2, 1], [2, 3], [2, 1], [3, 0], [0, 3], [2, 4], [0, 1]]\n",
      "[[0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 4], [1, 3, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 4], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [0, 3], [2, 3], [2, 0], [2, 1], [0, 3], [2, 3], [3, 0], [0, 3], [2, 1], [2, 1], [2, 3], [2, 0], [0, 3], [2, 1], [0, 2], [0, 3], [3, 2], [0, 3], [2, 0], [2, 1], [2, 4], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [2, 3], [0, 3], [0, 3], [2, 1], [2, 0], [3, 0], [2, 3], [0, 3], [2, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [3, 0], [0, 3], [2, 0], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [2, 0], [2, 3], [0, 3], [0, 3], [0, 2], [2, 1], [0, 3], [2, 3], [0, 3], [2, 1], [0, 2], [2, 3], [0, 3], [0, 3], [2, 1], [0, 2], [2, 4], [0, 3], [0, 2], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 1], [2, 0], [2, 3], [0, 3], [0, 3], [2, 1], [2, 1], [2, 3], [0, 3], [0, 3], [0, 3], [2, 1], [2, 3], [0, 3], [0, 3], [0, 2], [0, 2], [3, 0], [2, 4], [0, 3], [2, 1], [2, 1], [2, 3], [3, 0], [0, 3], [2, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [3, 0], [3, 0], [2, 3], [2, 4], [2, 4], [0, 3], [0, 3], [2, 1], [0, 2], [2, 1], [0, 3], [0, 3], [2, 0], [2, 0], [2, 3], [2, 0], [0, 3], [2, 0], [2, 0], [3, 0], [0, 3], [0, 3], [2, 1], [2, 0], [2, 3], [0, 3], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [2, 4], [0, 3], [0, 3], [2, 1], [2, 0], [2, 0], [0, 3], [0, 3], [2, 0], [2, 1], [2, 3], [0, 3], [0, 3], [2, 1], [2, 0], [2, 3], [3, 0], [0, 3], [0, 3], [2, 1], [2, 4], [3, 0], [0, 3], [0, 1], [2, 1], [2, 3], [2, 4], [0, 3], [2, 1], [2, 0], [2, 4], [0, 2], [0, 3], [0, 2], [0, 3], [2, 3], [0, 3], [0, 2], [2, 4], [2, 1], [2, 4], [0, 3], [2, 3], [2, 1], [2, 1], [2, 4], [0, 3], [2, 1], [2, 1], [0, 2], [2, 4], [0, 2], [0, 3], [2, 0], [2, 1], [2, 4], [0, 3], [0, 3], [2, 1], [0, 1], [2, 0], [0, 3], [0, 3], [2, 0], [0, 3], [2, 4], [3, 0], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 0], [0, 2], [2, 1], [0, 3], [2, 0], [2, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 4], [2, 1], [2, 3], [0, 3], [0, 3], [2, 0], [2, 0], [2, 4], [0, 3], [0, 3], [2, 1], [2, 3], [2, 1], [3, 0], [0, 3], [2, 4], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004494546413421631  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004494529247283935  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004494497776031494  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004494452953338623  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004494396209716797  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.0044943299293518065  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004494254589080811  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0044941725730895995  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004494083881378174  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004493989944458008  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004493892192840576  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004493791580200196  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004493687629699707  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004493582248687744  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004493475437164307  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004493368625640869  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004493261814117432  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004493155002593994  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004493048667907715  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004492943286895752  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.05056663602590561 tensor([8.6882e-01, 6.6050e-02, 1.4385e-06, 5.0567e-02, 1.4557e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00858447514474392 tensor([5.2044e-08, 1.8355e-02, 9.7306e-01, 1.1662e-06, 8.5845e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16260971128940582 tensor([1.6555e-04, 1.6261e-01, 2.4928e-01, 3.6684e-03, 5.8427e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06244128942489624 tensor([6.2441e-02, 9.4399e-03, 3.5849e-05, 8.6367e-01, 6.4412e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04053260758519173 tensor([1.4177e-04, 4.0533e-02, 8.3426e-02, 9.1103e-03, 8.6679e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04895734786987305 tensor([0.0015, 0.1618, 0.0490, 0.0224, 0.7653], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013410716317594051 tensor([0.0134, 0.9248, 0.0024, 0.0023, 0.0571], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03682384639978409 tensor([0.0116, 0.2090, 0.0062, 0.0368, 0.7364], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030296700075268745 tensor([3.0297e-02, 9.0959e-03, 1.1226e-04, 8.2857e-01, 1.3192e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03211783617734909 tensor([7.2011e-05, 3.2118e-02, 1.4399e-01, 9.3850e-03, 8.1443e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.031232444569468498 tensor([0.0312, 0.8833, 0.0021, 0.0074, 0.0760], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02371595799922943 tensor([1.3803e-03, 8.9748e-01, 2.3716e-02, 5.6699e-04, 7.6854e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001258073141798377 tensor([2.9594e-08, 1.2581e-03, 9.6494e-01, 2.5367e-05, 3.3774e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023703191429376602 tensor([2.6145e-01, 7.8969e-03, 3.1103e-06, 7.0695e-01, 2.3703e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00016997811326291412 tensor([5.1599e-01, 1.0394e-04, 4.2608e-10, 4.8374e-01, 1.6998e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06359725445508957 tensor([0.0636, 0.5580, 0.0024, 0.0541, 0.3219], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20179715752601624 tensor([0.1208, 0.3140, 0.0011, 0.2018, 0.3623], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.025208458304405212 tensor([4.5718e-06, 2.5208e-02, 6.9052e-01, 6.1042e-04, 2.8366e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010437257587909698 tensor([1.4960e-01, 3.1676e-03, 1.2033e-06, 8.3679e-01, 1.0437e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05782097950577736 tensor([0.0018, 0.0972, 0.0290, 0.0578, 0.8141], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11380254477262497 tensor([8.9271e-05, 1.1380e-01, 2.8840e-01, 2.3375e-03, 5.9537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006094904150813818 tensor([6.0949e-03, 9.7929e-01, 1.4032e-03, 4.0269e-04, 1.2809e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02468300424516201 tensor([0.0037, 0.6448, 0.0247, 0.0055, 0.3213], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24724620580673218 tensor([0.0738, 0.2472, 0.0021, 0.3067, 0.3701], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010207682847976685 tensor([1.0208e-02, 3.3429e-03, 9.5985e-05, 7.5767e-01, 2.2868e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05586974322795868 tensor([7.3727e-01, 1.8212e-01, 8.6175e-06, 5.5870e-02, 2.4737e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.035024918615818024 tensor([0.0026, 0.4458, 0.0350, 0.0086, 0.5081], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.017939219251275063 tensor([7.4672e-07, 1.7939e-02, 8.6271e-01, 6.2360e-05, 1.1929e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011256005382165313 tensor([1.2987e-01, 1.6556e-04, 1.6627e-08, 8.6884e-01, 1.1256e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005036870948970318 tensor([5.0369e-03, 2.0825e-03, 1.2412e-04, 6.5502e-01, 3.3774e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04467330500483513 tensor([5.4998e-01, 3.6638e-01, 3.4083e-05, 4.4673e-02, 3.8938e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05175964534282684 tensor([7.0305e-05, 6.9715e-01, 2.5094e-01, 7.8199e-05, 5.1760e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1798289716243744 tensor([9.8027e-05, 4.9446e-01, 3.2532e-01, 2.8360e-04, 1.7983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24098454415798187 tensor([3.3044e-01, 2.4098e-01, 1.8283e-04, 2.7805e-01, 1.5034e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009062837809324265 tensor([0.0009, 0.0091, 0.0035, 0.1762, 0.8103], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18963691592216492 tensor([1.8964e-01, 5.4820e-01, 4.1143e-04, 6.4425e-02, 1.9733e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.134041890501976 tensor([4.7516e-04, 1.5783e-01, 1.3404e-01, 9.3830e-03, 6.9827e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0054824380204081535 tensor([1.3249e-07, 5.4824e-03, 9.3409e-01, 2.4535e-05, 6.0398e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002003737259656191 tensor([4.0948e-01, 1.6107e-03, 8.2676e-08, 5.8691e-01, 2.0037e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004916027188301086 tensor([0.0015, 0.0049, 0.0017, 0.4566, 0.5353], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013703867793083191 tensor([0.0041, 0.8538, 0.0137, 0.0020, 0.1263], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05621390789747238 tensor([0.0562, 0.7826, 0.0017, 0.0280, 0.1314], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0039009673055261374 tensor([4.6084e-08, 3.9010e-03, 9.6533e-01, 8.5042e-06, 3.0759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.040536217391490936 tensor([4.0656e-01, 1.5823e-02, 4.3717e-06, 5.3707e-01, 4.0536e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14442752301692963 tensor([0.0275, 0.1444, 0.0022, 0.2078, 0.6181], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.025857683271169662 tensor([0.0034, 0.4434, 0.0259, 0.0084, 0.5190], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1816217005252838 tensor([3.3379e-04, 4.4922e-01, 1.8162e-01, 1.4055e-03, 3.6742e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0031852240208536386 tensor([1.1106e-08, 3.1852e-03, 9.8628e-01, 1.1941e-06, 1.0537e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14636442065238953 tensor([1.8164e-01, 8.9732e-02, 2.2542e-04, 5.8204e-01, 1.4636e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.059050943702459335 tensor([5.9051e-02, 1.0040e-02, 3.8288e-05, 8.1571e-01, 1.1516e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010905944742262363 tensor([8.8702e-01, 1.0022e-01, 4.2444e-07, 1.0906e-02, 1.8533e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01528004091233015 tensor([0.0058, 0.8002, 0.0153, 0.0041, 0.1746], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0235490370541811 tensor([1.6872e-06, 2.3549e-02, 7.9237e-01, 1.6084e-04, 1.8392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.108611099421978 tensor([0.0586, 0.4348, 0.0028, 0.1086, 0.3952], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.140070763649419e-05 tensor([2.7725e-01, 2.1325e-05, 9.5278e-11, 7.2266e-01, 6.1401e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02407042682170868 tensor([8.7088e-01, 9.9985e-02, 1.3517e-06, 2.4070e-02, 5.0650e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25836193561553955 tensor([2.9771e-04, 3.8679e-01, 2.5836e-01, 2.1576e-03, 3.5240e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01905095763504505 tensor([1.4730e-06, 1.9051e-02, 7.7063e-01, 1.4672e-04, 2.1017e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19633954763412476 tensor([0.0664, 0.1963, 0.0019, 0.3580, 0.3774], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010795810259878635 tensor([0.0059, 0.0108, 0.0010, 0.4215, 0.5608], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07206476479768753 tensor([1.5808e-01, 7.4724e-01, 3.9685e-04, 2.2224e-02, 7.2065e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005587544292211533 tensor([1.4465e-08, 5.5875e-03, 9.8743e-01, 1.0582e-06, 6.9794e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005122792907059193 tensor([9.8369e-09, 5.1228e-03, 9.8894e-01, 5.4003e-07, 5.9376e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05251330882310867 tensor([0.0015, 0.0525, 0.0360, 0.1372, 0.7728], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0050469255074858665 tensor([9.5490e-02, 6.3610e-04, 2.5299e-07, 8.9883e-01, 5.0469e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06770031899213791 tensor([5.9948e-06, 1.2326e-01, 8.0898e-01, 6.2377e-05, 6.7700e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18278168141841888 tensor([0.1828, 0.4932, 0.0006, 0.1067, 0.2167], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1239081397652626 tensor([4.2938e-05, 1.2391e-01, 4.9713e-01, 1.1031e-03, 3.7782e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004020150227006525 tensor([3.8773e-01, 2.3711e-04, 3.9794e-09, 6.1163e-01, 4.0202e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03926486521959305 tensor([0.0013, 0.0733, 0.0190, 0.0393, 0.8671], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.029631078243255615 tensor([0.0296, 0.8318, 0.0015, 0.0053, 0.1318], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0050878701731562614 tensor([8.4011e-09, 7.1630e-03, 9.8775e-01, 2.9720e-07, 5.0879e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17500346899032593 tensor([4.2708e-05, 1.7500e-01, 4.5533e-01, 5.9308e-04, 3.6903e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01744109019637108 tensor([1.0502e-01, 1.9898e-03, 2.1229e-06, 8.7554e-01, 1.7441e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10633895546197891 tensor([0.0044, 0.1320, 0.0230, 0.1063, 0.7342], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20206381380558014 tensor([4.8862e-01, 2.2352e-01, 6.1427e-05, 2.0206e-01, 8.5741e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1038316935300827 tensor([1.6366e-05, 1.0383e-01, 6.0403e-01, 3.8611e-04, 2.9174e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012778791598975658 tensor([0.0083, 0.4227, 0.0135, 0.0128, 0.5428], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004453449044376612 tensor([3.2951e-01, 2.3252e-03, 3.3928e-07, 6.6371e-01, 4.4534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04489646479487419 tensor([0.0009, 0.1274, 0.0449, 0.0207, 0.8061], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03555157035589218 tensor([7.6084e-01, 1.9235e-01, 5.6849e-06, 3.5552e-02, 1.1253e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1306094080209732 tensor([3.2351e-05, 1.3061e-01, 5.5727e-01, 7.3112e-04, 3.1135e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1037956029176712 tensor([0.0008, 0.2733, 0.1038, 0.0096, 0.6125], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.040006957948207855 tensor([4.0007e-02, 2.7509e-02, 3.6587e-04, 7.2426e-01, 2.0785e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06999997794628143 tensor([0.0348, 0.0700, 0.0011, 0.4096, 0.4845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08637294918298721 tensor([0.0864, 0.6062, 0.0017, 0.0534, 0.2523], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001385136740282178 tensor([6.7549e-09, 1.3851e-03, 9.8676e-01, 2.4804e-06, 1.1848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11765974014997482 tensor([1.6629e-05, 1.1766e-01, 6.1668e-01, 3.8138e-04, 2.6526e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013417878188192844 tensor([2.3676e-01, 4.2299e-03, 1.0943e-06, 7.4559e-01, 1.3418e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.037132006138563156 tensor([3.7132e-02, 1.1640e-02, 6.5941e-05, 7.0989e-01, 2.4127e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11183705180883408 tensor([1.6366e-01, 6.9785e-01, 4.1037e-04, 2.6239e-02, 1.1184e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016415255144238472 tensor([1.9011e-05, 1.6415e-02, 3.4331e-01, 5.4525e-03, 6.3480e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003769730217754841 tensor([1.2344e-07, 3.7697e-03, 9.4290e-01, 5.5814e-05, 5.3274e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0998542532324791 tensor([7.0246e-05, 9.9854e-02, 3.6805e-01, 2.8774e-03, 5.2915e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0036821470130234957 tensor([3.6821e-03, 8.2628e-04, 4.4631e-05, 8.4345e-01, 1.5199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22822146117687225 tensor([0.2282, 0.3791, 0.0004, 0.1030, 0.2892], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25762954354286194 tensor([2.7129e-04, 3.9403e-01, 2.5763e-01, 1.6859e-03, 3.4638e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005856183706782758 tensor([1.0853e-09, 5.8562e-04, 9.9360e-01, 4.7170e-07, 5.8156e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06598842144012451 tensor([0.0018, 0.1060, 0.0465, 0.0660, 0.7797], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010208215564489365 tensor([2.2987e-04, 1.0208e-02, 1.0779e-02, 3.6723e-02, 9.4206e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017459815368056297 tensor([0.0026, 0.8429, 0.0175, 0.0012, 0.1358], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07982107251882553 tensor([6.1103e-01, 7.9821e-02, 1.1141e-05, 2.7558e-01, 3.3557e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002656549448147416 tensor([3.1417e-08, 2.6565e-03, 9.7112e-01, 8.1013e-06, 2.6220e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03954390063881874 tensor([6.1403e-02, 3.6501e-03, 8.0862e-06, 8.9539e-01, 3.9544e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0013216654770076275 tensor([3.5162e-01, 3.9280e-04, 1.0259e-08, 6.4667e-01, 1.3217e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15999996662139893 tensor([1.7019e-01, 5.9942e-01, 5.0810e-04, 6.9881e-02, 1.6000e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03851469233632088 tensor([0.0015, 0.7984, 0.0385, 0.0015, 0.1600], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01100640557706356 tensor([3.5933e-07, 1.1006e-02, 8.9807e-01, 4.5687e-05, 9.0873e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.047919586300849915 tensor([1.8644e-01, 3.1231e-02, 3.2157e-05, 7.3438e-01, 4.7920e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.076471708714962 tensor([0.0052, 0.0765, 0.0055, 0.1047, 0.8081], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04624710604548454 tensor([0.0024, 0.4445, 0.0462, 0.0086, 0.4981], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13117243349552155 tensor([2.6037e-04, 7.2478e-01, 1.4336e-01, 4.2669e-04, 1.3117e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08243804425001144 tensor([1.0846e-04, 8.2438e-02, 1.8883e-01, 3.2932e-03, 7.2534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010135949589312077 tensor([2.6891e-01, 2.4006e-03, 5.3872e-07, 7.1855e-01, 1.0136e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0023901183158159256 tensor([4.6298e-01, 1.8085e-03, 5.9036e-08, 5.3282e-01, 2.3901e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011222395114600658 tensor([5.7881e-01, 3.9977e-01, 8.4233e-06, 1.1222e-02, 1.0183e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013101907446980476 tensor([0.0040, 0.8682, 0.0131, 0.0016, 0.1131], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016978377476334572 tensor([0.0038, 0.7779, 0.0170, 0.0025, 0.1988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03913406655192375 tensor([0.0391, 0.8629, 0.0025, 0.0168, 0.0786], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00038814794970676303 tensor([9.4683e-01, 3.8815e-04, 1.7206e-10, 5.2737e-02, 4.4216e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02824551984667778 tensor([8.3068e-01, 2.8246e-02, 6.2706e-07, 1.3459e-01, 6.4878e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07497333735227585 tensor([0.0010, 0.7020, 0.0750, 0.0018, 0.2203], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05990558862686157 tensor([6.1015e-05, 5.9906e-02, 3.5303e-01, 4.4288e-03, 5.8258e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.031547170132398605 tensor([4.5780e-01, 2.0178e-02, 4.6107e-06, 4.9047e-01, 3.1547e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025286760181188583 tensor([0.0017, 0.1274, 0.0204, 0.0253, 0.8251], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.041187264025211334 tensor([4.2699e-01, 1.6311e-02, 3.0542e-06, 5.1551e-01, 4.1187e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16797895729541779 tensor([4.2447e-04, 5.3232e-01, 1.6798e-01, 1.2664e-03, 2.9801e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04783610999584198 tensor([1.7097e-06, 4.7836e-02, 8.2501e-01, 5.1152e-05, 1.2710e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25171971321105957 tensor([0.1006, 0.2517, 0.0014, 0.3029, 0.3433], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11256229877471924 tensor([0.0359, 0.1126, 0.0017, 0.3069, 0.5430], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12320158630609512 tensor([1.2320e-01, 7.2346e-01, 5.3899e-04, 2.7253e-02, 1.2555e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.028734898194670677 tensor([0.0122, 0.4592, 0.0107, 0.0287, 0.4891], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13263283669948578 tensor([4.5479e-04, 2.2367e-01, 1.3263e-01, 5.4515e-03, 6.3779e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026128392666578293 tensor([0.0186, 0.0261, 0.0009, 0.6018, 0.3525], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19127926230430603 tensor([0.0930, 0.2414, 0.0007, 0.1913, 0.4735], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008644395507872105 tensor([0.0049, 0.8932, 0.0086, 0.0018, 0.0914], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03630370274186134 tensor([0.0062, 0.5515, 0.0363, 0.0208, 0.3853], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004180100746452808 tensor([5.4507e-08, 4.1801e-03, 9.4803e-01, 1.1268e-05, 4.7782e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00010298455890733749 tensor([4.4020e-01, 7.1473e-05, 3.6517e-10, 5.5963e-01, 1.0298e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.023315684869885445 tensor([0.0128, 0.0233, 0.0006, 0.3631, 0.6002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11002659052610397 tensor([3.8299e-01, 4.1955e-01, 1.4634e-04, 8.7289e-02, 1.1003e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06491032987833023 tensor([6.0583e-05, 6.0297e-01, 3.3195e-01, 1.0433e-04, 6.4910e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028822584077715874 tensor([2.2039e-06, 2.8823e-02, 7.9589e-01, 1.5591e-04, 1.7513e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09724854677915573 tensor([3.0223e-01, 5.0577e-02, 4.4845e-05, 5.4990e-01, 9.7249e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003545808605849743 tensor([5.4698e-07, 3.5458e-03, 6.1497e-01, 5.7216e-04, 3.8091e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004642124753445387 tensor([9.8480e-01, 1.0394e-02, 3.4381e-09, 4.6421e-03, 1.6641e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.049917127937078476 tensor([5.6278e-06, 2.4370e-01, 7.0636e-01, 2.1590e-05, 4.9917e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009242481552064419 tensor([1.0003e-07, 9.2425e-03, 9.6512e-01, 8.0941e-06, 2.5626e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0551103912293911 tensor([2.5453e-04, 2.6351e-02, 6.9387e-02, 5.5110e-02, 8.4890e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0066659096628427505 tensor([7.0993e-01, 6.6659e-03, 1.8304e-07, 2.7981e-01, 3.5882e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06398870795965195 tensor([8.6086e-01, 6.5929e-02, 1.3085e-06, 6.3989e-02, 9.2173e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03006087802350521 tensor([5.5992e-07, 3.0061e-02, 9.2733e-01, 2.1192e-05, 4.2589e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.045484963804483414 tensor([5.4742e-04, 6.6225e-02, 4.5485e-02, 1.9748e-02, 8.6800e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04138144105672836 tensor([4.1381e-02, 3.4417e-02, 5.0636e-04, 7.3116e-01, 1.9254e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06219273433089256 tensor([3.6108e-02, 6.2193e-02, 5.3957e-04, 3.2172e-01, 5.7944e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17549754679203033 tensor([0.0848, 0.2405, 0.0011, 0.1755, 0.4981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027991135139018297 tensor([2.9050e-08, 2.7991e-03, 9.7189e-01, 8.5313e-06, 2.5298e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005883962847292423 tensor([1.0568e-07, 5.8840e-03, 9.3599e-01, 1.8701e-05, 5.8111e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10142647475004196 tensor([0.0204, 0.1014, 0.0026, 0.1853, 0.6903], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010302422568202019 tensor([1.9324e-01, 2.2437e-03, 5.9867e-07, 7.9422e-01, 1.0302e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11397697031497955 tensor([3.2973e-01, 4.7788e-01, 1.6109e-04, 7.8258e-02, 1.1398e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13716810941696167 tensor([2.5290e-05, 1.3717e-01, 6.4562e-01, 5.4994e-04, 2.1664e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18484175205230713 tensor([2.8571e-04, 4.4684e-01, 1.8484e-01, 9.6782e-04, 3.6707e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07966774702072144 tensor([7.9668e-02, 5.6856e-02, 5.2745e-04, 5.6904e-01, 2.9391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09490535408258438 tensor([0.0087, 0.0949, 0.0038, 0.1001, 0.7925], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2320162057876587 tensor([3.1729e-01, 2.9275e-01, 2.7753e-04, 1.5766e-01, 2.3202e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024528803303837776 tensor([1.4252e-03, 9.3288e-01, 2.4529e-02, 4.4403e-04, 4.0725e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024502737447619438 tensor([2.1902e-07, 2.4503e-02, 9.4571e-01, 5.6163e-06, 2.9783e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11219970881938934 tensor([0.0010, 0.2149, 0.1122, 0.0241, 0.6478], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005852475296705961 tensor([5.8525e-03, 1.4146e-03, 6.0219e-05, 9.1228e-01, 8.0394e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.048306144773960114 tensor([6.0350e-01, 3.2584e-01, 1.9410e-05, 4.8306e-02, 2.2333e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021003741770982742 tensor([0.0019, 0.8623, 0.0210, 0.0010, 0.1138], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019707225263118744 tensor([2.1137e-05, 1.9707e-02, 2.8474e-01, 4.0322e-03, 6.9150e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05776046961545944 tensor([1.6151e-04, 2.6019e-02, 1.9059e-01, 5.7760e-02, 7.2546e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01851699687540531 tensor([1.8517e-02, 9.2043e-04, 7.2931e-06, 9.2628e-01, 5.4271e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014600477181375027 tensor([0.0146, 0.9146, 0.0031, 0.0037, 0.0640], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15113627910614014 tensor([3.1688e-01, 4.1587e-01, 2.3683e-04, 1.5114e-01, 1.1588e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10301658511161804 tensor([1.0336e-05, 1.0302e-01, 7.3968e-01, 2.3081e-04, 1.5706e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008604893810115755 tensor([3.1996e-01, 4.9325e-04, 1.9903e-08, 6.7868e-01, 8.6049e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03988073021173477 tensor([0.0253, 0.0399, 0.0006, 0.3916, 0.5427], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02986784465610981 tensor([9.1681e-01, 2.9868e-02, 3.9677e-07, 4.9071e-02, 4.2473e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.053832389414310455 tensor([0.0070, 0.2627, 0.0179, 0.0538, 0.6586], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03910059109330177 tensor([3.2030e-06, 3.9101e-02, 7.5481e-01, 1.2705e-04, 2.0596e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0561521016061306 tensor([0.0050, 0.3330, 0.0526, 0.0562, 0.5533], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16722455620765686 tensor([2.4989e-04, 1.6722e-01, 2.5598e-01, 7.3431e-03, 5.6920e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06903062015771866 tensor([6.9031e-02, 7.7588e-01, 6.9995e-04, 1.3253e-02, 1.4113e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06748255342245102 tensor([1.5448e-05, 2.9966e-01, 6.3279e-01, 5.8733e-05, 6.7483e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027484862133860588 tensor([0.0024, 0.0275, 0.0104, 0.2434, 0.7163], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004884252790361643 tensor([9.1352e-01, 4.8843e-03, 3.3097e-08, 8.0683e-02, 9.1245e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05456002429127693 tensor([5.4560e-02, 3.3837e-03, 5.6808e-06, 8.8477e-01, 5.7281e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14486174285411835 tensor([6.3891e-01, 1.5274e-01, 2.0145e-05, 1.4486e-01, 6.3468e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.036667123436927795 tensor([9.6873e-07, 6.5340e-02, 8.9798e-01, 1.3079e-05, 3.6667e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02274160273373127 tensor([0.0227, 0.7547, 0.0039, 0.0097, 0.2090], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006102534825913608 tensor([5.7929e-01, 4.9100e-04, 6.7507e-09, 4.1961e-01, 6.1025e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008608030155301094 tensor([8.6080e-03, 3.7955e-03, 1.4964e-04, 7.8275e-01, 2.0470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09867420792579651 tensor([6.9389e-01, 9.8674e-02, 8.1742e-06, 1.5951e-01, 4.7919e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0825270414352417 tensor([2.0481e-05, 8.2527e-02, 5.3921e-01, 7.1001e-04, 3.7753e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.061131495982408524 tensor([1.1456e-01, 1.3103e-02, 2.1476e-05, 8.1119e-01, 6.1131e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08439274877309799 tensor([8.4393e-02, 6.7580e-02, 3.8174e-04, 6.0441e-01, 2.4324e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005547132808715105 tensor([3.5802e-04, 5.5471e-03, 5.0829e-03, 1.1244e-01, 8.7657e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1255318522453308 tensor([6.8139e-01, 1.2553e-01, 1.4838e-05, 1.4930e-01, 4.3762e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03125697374343872 tensor([0.0036, 0.2082, 0.0257, 0.0313, 0.7312], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06827772408723831 tensor([4.8305e-05, 6.8278e-02, 3.6146e-01, 3.0332e-03, 5.6718e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21809597313404083 tensor([0.0974, 0.3028, 0.0014, 0.2181, 0.3802], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026113366708159447 tensor([2.6113e-02, 5.8042e-03, 6.9354e-05, 8.3616e-01, 1.3185e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.027164461091160774 tensor([7.3512e-01, 2.2939e-01, 6.3147e-06, 2.7164e-02, 8.3182e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25545617938041687 tensor([2.3904e-04, 2.8582e-01, 2.5546e-01, 2.9539e-03, 4.5554e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005355185829102993 tensor([6.8526e-08, 5.3552e-03, 9.5299e-01, 1.1182e-05, 4.1646e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000774247688241303 tensor([4.7813e-01, 4.7667e-04, 8.2147e-09, 5.2062e-01, 7.7425e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025110626593232155 tensor([1.2935e-04, 1.6145e-02, 3.6116e-02, 2.5111e-02, 9.2250e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03077731281518936 tensor([0.0152, 0.3713, 0.0055, 0.0308, 0.5771], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05906400829553604 tensor([9.1848e-06, 2.0833e-01, 7.3254e-01, 5.7808e-05, 5.9064e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02137654274702072 tensor([6.5381e-05, 2.1377e-02, 1.3581e-01, 1.6464e-02, 8.2628e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.052789174020290375 tensor([0.0370, 0.0528, 0.0009, 0.5451, 0.3642], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026196492835879326 tensor([1.2714e-05, 2.6196e-02, 4.3469e-01, 2.5918e-03, 5.3651e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06746973842382431 tensor([8.2031e-01, 6.7470e-02, 2.7039e-06, 9.2810e-02, 1.9410e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04509981721639633 tensor([4.4222e-05, 7.2000e-01, 2.3483e-01, 3.3015e-05, 4.5100e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02075064554810524 tensor([1.1379e-04, 2.0751e-02, 8.1378e-02, 1.7083e-02, 8.8067e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015003576409071684 tensor([4.7804e-02, 1.1101e-04, 3.7065e-08, 9.5058e-01, 1.5004e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012123245745897293 tensor([1.1474e-07, 1.2123e-02, 9.6551e-01, 6.5446e-06, 2.2355e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07334668189287186 tensor([8.3705e-01, 7.3347e-02, 2.0161e-06, 7.5602e-02, 1.3997e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024759434163570404 tensor([3.5114e-07, 3.3109e-02, 9.4212e-01, 7.5109e-06, 2.4759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04727243259549141 tensor([4.1269e-05, 4.7272e-02, 3.2328e-01, 2.7319e-03, 6.2668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08945882320404053 tensor([0.0187, 0.4041, 0.0168, 0.0895, 0.4709], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08895569294691086 tensor([0.0042, 0.0890, 0.0112, 0.1047, 0.7910], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006487782578915358 tensor([4.4383e-01, 3.9844e-03, 2.8346e-07, 5.4570e-01, 6.4878e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1162700206041336 tensor([6.0700e-04, 6.4939e-01, 1.1627e-01, 1.3232e-03, 2.3241e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1613093465566635 tensor([0.0373, 0.1613, 0.0024, 0.1815, 0.6175], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01806049421429634 tensor([5.0458e-02, 1.6613e-03, 2.7554e-06, 9.2982e-01, 1.8060e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06793997436761856 tensor([0.0350, 0.0679, 0.0010, 0.4006, 0.4956], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07234705984592438 tensor([1.7083e-05, 3.2405e-01, 6.0352e-01, 6.3389e-05, 7.2347e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00041385332588106394 tensor([6.4857e-11, 4.1385e-04, 9.9855e-01, 2.1947e-08, 1.0355e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20168916881084442 tensor([8.8731e-05, 4.3817e-01, 3.5978e-01, 2.7555e-04, 2.0169e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00033022198476828635 tensor([8.2141e-01, 3.3022e-04, 7.2445e-10, 1.7807e-01, 1.8815e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02871512621641159 tensor([1.3767e-01, 4.2436e-03, 2.5484e-06, 8.2936e-01, 2.8715e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0150400185957551 tensor([1.9420e-01, 7.8574e-01, 6.7093e-05, 4.9452e-03, 1.5040e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08680445700883865 tensor([5.9282e-04, 8.0707e-01, 8.6804e-02, 7.2504e-04, 1.0481e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00414548022672534 tensor([3.6469e-08, 4.1455e-03, 9.6522e-01, 4.8445e-06, 3.0629e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0767873078584671 tensor([0.0521, 0.0768, 0.0013, 0.5306, 0.3392], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21120142936706543 tensor([0.1539, 0.2112, 0.0005, 0.3023, 0.3321], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008835036307573318 tensor([9.8113e-01, 9.7218e-03, 7.3242e-09, 8.8350e-03, 3.0910e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06743296980857849 tensor([0.0010, 0.8230, 0.0674, 0.0011, 0.1075], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11478988826274872 tensor([9.9303e-05, 1.1479e-01, 3.5540e-01, 3.8012e-03, 5.2591e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.031089285388588905 tensor([1.4746e-01, 5.6049e-03, 4.6521e-06, 8.1584e-01, 3.1089e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02522921748459339 tensor([0.0252, 0.7213, 0.0066, 0.0198, 0.2270], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.053560514003038406 tensor([5.3561e-02, 7.4003e-03, 3.0883e-05, 8.2260e-01, 1.1641e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.032555051147937775 tensor([1.6396e-04, 9.1192e-01, 5.5312e-02, 5.0181e-05, 3.2555e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21929660439491272 tensor([1.9251e-04, 5.2878e-01, 2.1930e-01, 4.2312e-04, 2.5130e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11269309371709824 tensor([4.8199e-01, 1.1269e-01, 2.7613e-05, 3.3949e-01, 6.5798e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021362993866205215 tensor([2.2956e-05, 4.0618e-03, 5.6269e-02, 2.1363e-02, 9.1828e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3], [0, 3, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 2], [2, 3, 0], [2, 0, 3], [2, 1, 0], [0, 3, 1], [2, 3, 0], [3, 0, 2], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 3, 0], [2, 0], [0, 3, 1], [2, 1, 4], [0, 2, 3], [0, 3, 1], [3, 2, 0], [0, 3, 2], [2, 0], [2, 1, 0], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 4, 3], [0, 3, 4], [0, 3, 4], [2, 4], [0, 2, 1], [2, 3, 0], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 0, 1], [3, 0, 2], [2, 3, 0], [0, 3, 1], [2, 1, 4], [2, 0, 1], [0, 3, 2], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 4, 3], [3, 0, 2], [0, 3, 1], [2, 0, 3], [2, 1, 4], [2, 4, 3], [0, 3], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 3, 4], [0, 3, 1], [0, 3, 1], [0, 2, 1], [2, 1, 4], [0, 3, 4], [2, 3, 0], [0, 3, 1], [2, 1, 4], [0, 2, 3], [2, 3, 0], [0, 3, 4], [0, 3, 1], [2, 1, 4], [0, 2, 3], [2, 4], [0, 3, 1], [0, 2, 3], [2, 1, 4], [0, 3, 2], [2, 4, 3], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 3, 0], [0, 3, 1], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 3, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 1, 0], [2, 3], [0, 3], [0, 3, 1], [0, 2, 3], [0, 2, 1], [3, 0, 2], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 3, 4], [3, 0, 2], [0, 3, 1], [2, 1, 4], [2, 0, 1], [0, 3, 2], [0, 3, 4], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 4, 3], [3, 0, 2], [3, 0, 2], [2, 3, 0], [2, 4, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 1, 4], [0, 2, 3], [2, 1, 4], [0, 3, 2], [0, 3, 1], [2, 0], [2, 0, 1], [2, 3, 0], [2, 0, 3], [0, 3, 2], [2, 0, 1], [2, 0, 3], [3, 0, 2], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 3, 4], [0, 3, 4], [0, 3, 1], [2, 1, 4], [0, 3, 1], [2, 4, 3], [0, 3, 4], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 0, 3], [0, 3, 1], [0, 3, 1], [2, 0, 1], [2, 1, 4], [2, 3, 4], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 3], [3, 0, 2], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 4, 3], [3, 0, 2], [0, 3, 1], [0, 1, 3], [2, 1, 0], [2, 3, 0], [2, 4, 3], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 4, 1], [0, 2, 3], [0, 3, 1], [0, 2, 3], [0, 3, 1], [2, 3, 0], [0, 3, 4], [0, 2, 1], [2, 4, 1], [2, 1, 0], [2, 4, 3], [0, 3, 4], [2, 3, 0], [2, 1, 4], [2, 1, 0], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 1, 0], [0, 2, 1], [2, 4, 1], [0, 2, 3], [0, 3, 1], [2, 0], [2, 1, 0], [2, 4, 3], [0, 3], [0, 3, 1], [2, 1, 4], [0, 1, 3], [2, 0, 3], [0, 3, 4], [0, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [3, 0, 4], [0, 3, 1], [2, 1, 4], [0, 3, 1], [2, 4, 1], [0, 3, 4], [0, 3, 1], [2, 0, 3], [0, 2, 1], [2, 1, 4], [0, 3, 2], [2, 0, 1], [2, 1, 4], [2, 0, 1], [0, 3, 4], [0, 3, 1], [0, 3], [2, 4, 1], [2, 1, 4], [2, 3, 4], [0, 3, 2], [0, 3, 1], [2, 0, 1], [2, 0], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 3, 0], [2, 1, 0], [3, 0, 4], [0, 3], [2, 4, 1], [0, 1, 3]]\n",
      "[[0, 1], [1, 2], [2, 4], [3, 4], [2, 4], [1, 4], [1, 4], [1, 4], [3, 4], [2, 4], [1, 4], [1, 4], [2, 4], [0, 3], [0, 3], [1, 4], [1, 3, 4], [2, 4], [0, 3], [1, 4], [2, 4], [1, 4], [1, 4], [1, 3, 4], [3, 4], [0, 1], [1, 4], [2, 4], [0, 3], [3, 4], [0, 1], [1, 2], [1, 2], [0, 1, 3], [3, 4], [1, 4], [1, 4], [2, 4], [0, 3], [3, 4], [1, 4], [1, 4], [2, 4], [0, 3], [3, 4], [1, 4], [1, 4], [2, 4], [0, 3], [3, 4], [0, 1], [1, 4], [2, 4], [1, 4], [0, 3], [0, 1], [1, 2, 4], [2, 4], [3, 4], [3, 4], [0, 1], [2, 4], [2, 4], [3, 4], [0, 3], [1, 2], [1, 4], [2, 4], [0, 3], [1, 4], [1, 4], [1, 2], [2, 4], [0, 3], [1, 4], [0, 1, 3], [2, 4], [1, 4], [0, 3], [1, 4], [0, 1], [2, 4], [1, 4], [3, 4], [3, 4], [1, 4], [2, 4], [2, 4], [0, 3], [3, 4], [0, 1], [2, 4], [2, 4], [2, 4], [3, 4], [0, 1, 4], [1, 2, 4], [2, 4], [1, 4], [3, 4], [1, 4], [0, 3], [2, 4], [0, 3], [0, 3], [0, 1], [1, 4], [2, 4], [0, 3], [3, 4], [1, 4], [1, 2], [2, 4], [0, 3], [0, 3], [0, 1], [1, 4], [1, 4], [1, 4], [0, 3], [0, 3], [1, 4], [2, 4], [0, 3], [1, 4], [0, 3], [1, 4], [2, 4], [1, 3, 4], [3, 4], [1, 4], [1, 4], [1, 4], [3, 4], [1, 4], [1, 4], [1, 4], [2, 4], [0, 3], [3, 4], [0, 1], [1, 2], [2, 4], [0, 3], [2, 4], [0, 1], [1, 2], [2, 4], [2, 4], [0, 3], [0, 1], [2, 4], [1, 4], [3, 4], [3, 4], [1, 4], [2, 4], [2, 4], [3, 4], [0, 3], [0, 1], [2, 4], [1, 4], [3, 4], [3, 4], [0, 1, 4], [1, 4], [2, 4], [1, 4], [3, 4], [0, 1], [1, 4], [2, 4], [2, 4], [3, 4], [1, 4], [0, 1], [2, 4], [0, 3], [3, 4], [0, 3], [1, 4], [2, 4], [1, 4], [2, 4], [1, 4], [1, 2], [3, 4], [0, 3], [3, 4], [0, 1], [1, 2], [1, 4], [0, 3], [3, 4], [0, 3], [2, 4], [0, 3], [3, 4], [3, 4], [0, 3], [1, 4], [2, 4], [1, 3, 4], [3, 4], [0, 1], [1, 2, 4], [2, 4], [0, 3], [2, 4], [1, 4], [1, 2], [2, 4], [3, 4], [2, 4], [0, 3], [1, 2], [2, 4], [0, 3], [2, 4], [0, 3], [1, 2], [2, 4], [1, 4], [3, 4], [0, 3], [1, 4], [3, 4], [0, 3], [3, 4], [1, 2], [2, 4], [1, 2, 4], [0, 3], [0, 3], [0, 1], [1, 4], [2, 4], [3, 4], [1, 3, 4], [0, 1], [1, 4], [2, 4], [0, 3], [1, 4], [3, 4], [1, 2], [1, 2, 4], [0, 3], [2, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 3], [0, 3, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 2], [2, 3, 0], [2, 0, 3], [2, 1, 0], [0, 3, 1], [2, 3, 0], [3, 0, 2], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 3, 0], [0, 3, 1], [2, 1, 4], [0, 2, 3], [0, 3, 1], [3, 2, 0], [0, 3, 2], [2, 1, 0], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 4, 3], [0, 3, 4], [0, 3, 4], [0, 2, 1], [2, 3, 0], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 0, 1], [3, 0, 2], [2, 3, 0], [0, 3, 1], [2, 1, 4], [2, 0, 1], [0, 3, 2], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 4, 3], [3, 0, 2], [0, 3, 1], [2, 0, 3], [2, 1, 4], [2, 4, 3], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 3, 4], [0, 3, 1], [0, 3, 1], [0, 2, 1], [2, 1, 4], [0, 3, 4], [2, 3, 0], [0, 3, 1], [2, 1, 4], [0, 2, 3], [2, 3, 0], [0, 3, 4], [0, 3, 1], [2, 1, 4], [0, 2, 3], [0, 3, 1], [0, 2, 3], [2, 1, 4], [0, 3, 2], [2, 4, 3], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 3, 0], [0, 3, 1], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 3, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 2, 3], [0, 2, 1], [3, 0, 2], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 3, 4], [3, 0, 2], [0, 3, 1], [2, 1, 4], [2, 0, 1], [0, 3, 2], [0, 3, 4], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 4, 3], [3, 0, 2], [3, 0, 2], [2, 3, 0], [2, 4, 1], [2, 4, 1], [0, 3, 2], [0, 3, 1], [2, 1, 4], [0, 2, 3], [2, 1, 4], [0, 3, 2], [0, 3, 1], [2, 0, 1], [2, 3, 0], [2, 0, 3], [0, 3, 2], [2, 0, 1], [2, 0, 3], [3, 0, 2], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 3, 4], [0, 3, 4], [0, 3, 1], [2, 1, 4], [0, 3, 1], [2, 4, 3], [0, 3, 4], [0, 3, 1], [0, 1, 3], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 0, 3], [0, 3, 1], [0, 3, 1], [2, 0, 1], [2, 1, 4], [2, 3, 4], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [3, 0, 2], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 4, 3], [3, 0, 2], [0, 3, 1], [0, 1, 3], [2, 1, 0], [2, 3, 0], [2, 4, 3], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 4, 1], [0, 2, 3], [0, 3, 1], [0, 2, 3], [0, 3, 1], [2, 3, 0], [0, 3, 4], [0, 2, 1], [2, 4, 1], [2, 1, 0], [2, 4, 3], [0, 3, 4], [2, 3, 0], [2, 1, 4], [2, 1, 0], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 1, 0], [0, 2, 1], [2, 4, 1], [0, 2, 3], [0, 3, 1], [2, 1, 0], [2, 4, 3], [0, 3, 1], [2, 1, 4], [0, 1, 3], [2, 0, 3], [0, 3, 4], [0, 3, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1], [3, 0, 4], [0, 3, 1], [2, 1, 4], [0, 3, 1], [2, 4, 1], [0, 3, 4], [0, 3, 1], [2, 0, 3], [0, 2, 1], [2, 1, 4], [0, 3, 2], [2, 0, 1], [2, 1, 4], [2, 0, 1], [0, 3, 4], [0, 3, 1], [2, 4, 1], [2, 1, 4], [2, 3, 4], [0, 3, 2], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 4], [2, 3, 0], [2, 1, 0], [3, 0, 4], [2, 4, 1], [0, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004926464315188133  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004926413802777307  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004926315808700303  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004926177404694638  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004926004652249611  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004925800582109872  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.00492557075064061  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004925319198834694  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004925050472809097  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004924767098184359  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004924472105705132  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004924169031240173  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004923858885037697  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.00492354621321468  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004923230510647014  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004922916323451673  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004922600620884007  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004922288454185098  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004921979318230839  Accuracy on Support set:0.0\n",
      "torch.Size([236, 2048]) torch.Size([236])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004921672707897121  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.06585049629211426 tensor([8.6210e-01, 6.5850e-02, 1.6143e-06, 5.5378e-02, 1.6668e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016181660816073418 tensor([4.4926e-08, 1.6182e-02, 9.7527e-01, 1.0987e-06, 8.5446e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2510487735271454 tensor([1.4683e-04, 1.4327e-01, 2.5105e-01, 3.6540e-03, 6.0188e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06755034625530243 tensor([5.5961e-02, 8.4701e-03, 3.6795e-05, 8.6798e-01, 6.7550e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08226329833269119 tensor([1.2338e-04, 3.5058e-02, 8.2263e-02, 8.8689e-03, 8.7369e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14301003515720367 tensor([0.0013, 0.1430, 0.0491, 0.0220, 0.7846], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06571613252162933 tensor([0.0132, 0.9158, 0.0027, 0.0025, 0.0657], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18467290699481964 tensor([0.0103, 0.1847, 0.0063, 0.0366, 0.7622], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13760530948638916 tensor([2.7004e-02, 8.1258e-03, 1.1469e-04, 8.2715e-01, 1.3761e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1421910673379898 tensor([6.2428e-05, 2.7727e-02, 1.4219e-01, 9.1137e-03, 8.2091e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08657179027795792 tensor([0.0306, 0.8724, 0.0024, 0.0080, 0.0866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0873015746474266 tensor([1.3537e-03, 8.8439e-01, 2.6339e-02, 6.1266e-04, 8.7302e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.034586500376462936 tensor([2.6237e-08, 1.1093e-03, 9.6428e-01, 2.4993e-05, 3.4587e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24090346693992615 tensor([2.4090e-01, 7.2781e-03, 3.2545e-06, 7.2648e-01, 2.5331e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4903218746185303 tensor([4.9032e-01, 9.9195e-05, 4.5844e-10, 5.0939e-01, 1.8574e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3534884750843048 tensor([0.0596, 0.5278, 0.0025, 0.0566, 0.3535], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20787380635738373 tensor([0.1114, 0.2898, 0.0011, 0.2079, 0.3898], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2883385419845581 tensor([3.9752e-06, 2.1877e-02, 6.8918e-01, 5.9693e-04, 2.8834e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13591334223747253 tensor([1.3591e-01, 2.8753e-03, 1.2447e-06, 8.5017e-01, 1.1039e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08423829078674316 tensor([0.0016, 0.0842, 0.0290, 0.0560, 0.8292], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2893989384174347 tensor([7.8494e-05, 9.9892e-02, 2.8940e-01, 2.2999e-03, 6.0833e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01446501538157463 tensor([6.0175e-03, 9.7752e-01, 1.5674e-03, 4.3378e-04, 1.4465e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3558502793312073 tensor([0.0035, 0.6082, 0.0267, 0.0058, 0.3559], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2252207100391388 tensor([0.0673, 0.2252, 0.0022, 0.3124, 0.3929], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23678135871887207 tensor([8.9839e-03, 2.9394e-03, 9.7124e-05, 7.5120e-01, 2.3678e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18078705668449402 tensor([7.2959e-01, 1.8079e-01, 9.6570e-06, 6.1279e-02, 2.8330e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.408564031124115 tensor([0.0024, 0.4086, 0.0365, 0.0088, 0.5437], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1218688115477562 tensor([6.5530e-07, 1.5659e-02, 8.6241e-01, 6.1373e-05, 1.2187e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11869396269321442 tensor([1.1869e-01, 1.5188e-04, 1.7225e-08, 8.7997e-01, 1.1855e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34901803731918335 tensor([4.4282e-03, 1.8364e-03, 1.2537e-04, 6.4459e-01, 3.4902e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36366695165634155 tensor([5.4276e-01, 3.6367e-01, 3.8288e-05, 4.8938e-02, 4.4592e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.27241599559783936 tensor([6.6574e-05, 6.7133e-01, 2.7242e-01, 8.0721e-05, 5.6105e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.34387651085853577 tensor([9.1561e-05, 4.6201e-01, 3.4388e-01, 2.9321e-04, 1.9372e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2282247543334961 tensor([3.1209e-01, 2.2822e-01, 1.9740e-04, 2.9373e-01, 1.6576e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16959093511104584 tensor([7.8459e-04, 7.7389e-03, 3.4617e-03, 1.6959e-01, 8.1842e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2208106964826584 tensor([1.8096e-01, 5.2932e-01, 4.5352e-04, 6.8455e-02, 2.2081e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13958041369915009 tensor([4.2454e-04, 1.3958e-01, 1.3393e-01, 9.3124e-03, 7.1675e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06160435453057289 tensor([1.1573e-07, 4.7700e-03, 9.3360e-01, 2.4082e-05, 6.1604e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.38481825590133667 tensor([3.8482e-01, 1.5091e-03, 8.7257e-08, 6.1151e-01, 2.1650e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4446740448474884 tensor([0.0013, 0.0043, 0.0017, 0.4447, 0.5481], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14271844923496246 tensor([0.0040, 0.8359, 0.0151, 0.0022, 0.1427], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14906048774719238 tensor([0.0549, 0.7636, 0.0019, 0.0305, 0.1491], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03135380521416664 tensor([4.0404e-08, 3.4126e-03, 9.6523e-01, 8.3294e-06, 3.1354e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3807291090488434 tensor([3.8073e-01, 1.4765e-02, 4.6179e-06, 5.6065e-01, 4.3852e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2059265375137329 tensor([0.0241, 0.1269, 0.0022, 0.2059, 0.6409], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4072604179382324 tensor([0.0032, 0.4073, 0.0270, 0.0086, 0.5540], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3938927948474884 tensor([3.1200e-04, 4.1509e-01, 1.8924e-01, 1.4605e-03, 3.9389e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010571260936558247 tensor([9.5568e-09, 2.7739e-03, 9.8665e-01, 1.1427e-06, 1.0571e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16718365252017975 tensor([1.6718e-01, 8.2158e-02, 2.3267e-04, 5.9527e-01, 1.5515e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1205817386507988 tensor([5.2560e-02, 8.9502e-03, 3.9256e-05, 8.1787e-01, 1.2058e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0994376689195633 tensor([8.8677e-01, 9.9438e-02, 4.5930e-07, 1.1734e-02, 2.0629e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.197947695851326 tensor([0.0056, 0.7751, 0.0170, 0.0044, 0.1979], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18711328506469727 tensor([1.4689e-06, 2.0487e-02, 7.9224e-01, 1.5711e-04, 1.8711e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40346240997314453 tensor([0.0540, 0.4035, 0.0030, 0.1120, 0.4276], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25699400901794434 tensor([2.5699e-01, 1.9856e-05, 1.0029e-10, 7.4292e-01, 6.5615e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09971299767494202 tensor([8.6808e-01, 9.9713e-02, 1.5079e-06, 2.6421e-02, 5.7834e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26830345392227173 tensor([2.7480e-04, 3.5449e-01, 2.6830e-01, 2.2187e-03, 3.7472e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21430762112140656 tensor([1.2880e-06, 1.6567e-02, 7.6898e-01, 1.4414e-04, 2.1431e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.362762987613678 tensor([0.0600, 0.1770, 0.0019, 0.3628, 0.3983], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41083890199661255 tensor([0.0051, 0.0094, 0.0010, 0.4108, 0.5737], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15564103424549103 tensor([1.5564e-01, 7.3727e-01, 4.4337e-04, 2.4339e-02, 8.2306e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006998991593718529 tensor([1.2488e-08, 4.8824e-03, 9.8812e-01, 1.0119e-06, 6.9990e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0059585971757769585 tensor([8.5138e-09, 4.4997e-03, 9.8954e-01, 5.1544e-07, 5.9586e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13306576013565063 tensor([0.0013, 0.0455, 0.0358, 0.1331, 0.7843], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08645091205835342 tensor([8.6451e-02, 5.8189e-04, 2.6423e-07, 9.0763e-01, 5.3336e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10976378619670868 tensor([5.2811e-06, 1.0976e-01, 8.2105e-01, 6.0989e-05, 6.9116e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2424343228340149 tensor([0.1724, 0.4710, 0.0007, 0.1134, 0.2424], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38796737790107727 tensor([3.7781e-05, 1.0896e-01, 5.0194e-01, 1.0896e-03, 3.8797e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3640443682670593 tensor([3.6404e-01, 2.2425e-04, 4.2430e-09, 6.3530e-01, 4.3485e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06316887587308884 tensor([0.0011, 0.0632, 0.0188, 0.0380, 0.8789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14878687262535095 tensor([0.0288, 0.8151, 0.0017, 0.0057, 0.1488], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0063697779551148415 tensor([7.3678e-09, 6.3698e-03, 9.8852e-01, 2.8385e-07, 5.1094e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38214489817619324 tensor([3.7878e-05, 1.5490e-01, 4.6232e-01, 5.9054e-04, 3.8214e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09531430900096893 tensor([9.5314e-02, 1.8055e-03, 2.1832e-06, 8.8456e-01, 1.8315e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11552778631448746 tensor([0.0038, 0.1155, 0.0232, 0.1038, 0.7536], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.216678649187088 tensor([4.6980e-01, 2.1696e-01, 6.8026e-05, 2.1668e-01, 9.6498e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2997644245624542 tensor([1.4465e-05, 9.1453e-02, 6.0839e-01, 3.8220e-04, 2.9976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.38590821623802185 tensor([0.0075, 0.3859, 0.0141, 0.0131, 0.5794], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30822160840034485 tensor([3.0822e-01, 2.1619e-03, 3.5281e-07, 6.8487e-01, 4.7482e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11052915453910828 tensor([7.5331e-04, 1.1053e-01, 4.4822e-02, 2.0204e-02, 8.2369e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19184577465057373 tensor([7.5605e-01, 1.9185e-01, 6.4001e-06, 3.9165e-02, 1.2934e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32081338763237 tensor([2.8612e-05, 1.1519e-01, 5.6324e-01, 7.2574e-04, 3.2081e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24372364580631256 tensor([0.0007, 0.2437, 0.1056, 0.0097, 0.6403], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21656742691993713 tensor([3.5712e-02, 2.4575e-02, 3.7268e-04, 7.2277e-01, 2.1657e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.40507039427757263 tensor([0.0306, 0.0616, 0.0011, 0.4051, 0.5016], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27913928031921387 tensor([0.0822, 0.5804, 0.0019, 0.0564, 0.2791], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012058314867317677 tensor([5.8968e-09, 1.2072e-03, 9.8673e-01, 2.4248e-06, 1.2058e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2735181748867035 tensor([1.4779e-05, 1.0401e-01, 6.2208e-01, 3.7925e-04, 2.7352e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21804575622081757 tensor([2.1805e-01, 3.8687e-03, 1.1292e-06, 7.6387e-01, 1.4217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2510014772415161 tensor([3.2784e-02, 1.0288e-02, 6.7092e-05, 7.0586e-01, 2.5100e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1602811962366104 tensor([1.6028e-01, 6.8441e-01, 4.5342e-04, 2.8421e-02, 1.2644e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3397302031517029 tensor([1.6633e-05, 1.4304e-02, 3.3973e-01, 5.3073e-03, 6.4064e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.054461218416690826 tensor([1.0870e-07, 3.2967e-03, 9.4219e-01, 5.5068e-05, 5.4461e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3685949146747589 tensor([6.1886e-05, 8.7601e-02, 3.6859e-01, 2.8382e-03, 5.4090e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15854273736476898 tensor([3.2729e-03, 7.3658e-04, 4.5498e-05, 8.3740e-01, 1.5854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21610409021377563 tensor([0.2161, 0.3594, 0.0004, 0.1079, 0.3161], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26792672276496887 tensor([2.5054e-04, 3.6149e-01, 2.6793e-01, 1.7344e-03, 3.6860e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005864749196916819 tensor([9.4346e-10, 5.1328e-04, 9.9362e-01, 4.5614e-07, 5.8647e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09269437193870544 tensor([0.0016, 0.0927, 0.0464, 0.0642, 0.7951], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03518166020512581 tensor([1.9496e-04, 8.6484e-03, 1.0549e-02, 3.5182e-02, 9.4543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15362517535686493 tensor([0.0025, 0.8233, 0.0193, 0.0013, 0.1536], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29655954241752625 tensor([5.8937e-01, 7.6621e-02, 1.2102e-05, 2.9656e-01, 3.7435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02677226811647415 tensor([2.7616e-08, 2.3232e-03, 9.7090e-01, 7.9682e-06, 2.6772e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05525940656661987 tensor([5.5259e-02, 3.2792e-03, 8.2649e-06, 9.0009e-01, 4.1360e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32811540365219116 tensor([3.2812e-01, 3.6841e-04, 1.0878e-08, 6.7009e-01, 1.4253e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16471277177333832 tensor([1.6471e-01, 5.8065e-01, 5.5588e-04, 7.4983e-02, 1.7910e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18082746863365173 tensor([0.0015, 0.7737, 0.0423, 0.0017, 0.1808], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09261441975831985 tensor([3.1353e-07, 9.5752e-03, 8.9777e-01, 4.4774e-05, 9.2614e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17033329606056213 tensor([1.7033e-01, 2.8557e-02, 3.3500e-05, 7.5004e-01, 5.1035e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10162829607725143 tensor([0.0045, 0.0660, 0.0055, 0.1016, 0.8224], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.408643513917923 tensor([0.0022, 0.4086, 0.0483, 0.0088, 0.5320], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15671078860759735 tensor([2.4981e-04, 6.9648e-01, 1.5671e-01, 4.5436e-04, 1.4611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18813864886760712 tensor([9.4468e-05, 7.1555e-02, 1.8814e-01, 3.2251e-03, 7.3699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24903567135334015 tensor([2.4904e-01, 2.2209e-03, 5.6146e-07, 7.3796e-01, 1.0786e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4347330331802368 tensor([4.3473e-01, 1.7008e-03, 6.3459e-08, 5.6094e-01, 2.6229e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4010818600654602 tensor([5.7482e-01, 4.0108e-01, 9.5574e-06, 1.2331e-02, 1.1761e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12861311435699463 tensor([0.0039, 0.8512, 0.0145, 0.0018, 0.1286], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22369076311588287 tensor([0.0037, 0.7513, 0.0186, 0.0027, 0.2237], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08985670655965805 tensor([0.0383, 0.8507, 0.0029, 0.0183, 0.0899], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05675828829407692 tensor([9.4281e-01, 3.8087e-04, 1.8428e-10, 5.6758e-02, 4.8783e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1474139243364334 tensor([8.1741e-01, 2.7791e-02, 6.9833e-07, 1.4741e-01, 7.3871e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24548733234405518 tensor([0.0010, 0.6705, 0.0812, 0.0019, 0.2455], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3517531454563141 tensor([5.3293e-05, 5.2134e-02, 3.5175e-01, 4.3409e-03, 5.9172e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4328518211841583 tensor([4.3285e-01, 1.8964e-02, 4.8707e-06, 5.1399e-01, 3.4194e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1106661707162857 tensor([0.0015, 0.1107, 0.0205, 0.0246, 0.8428], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4004884958267212 tensor([4.0049e-01, 1.5228e-02, 3.2307e-06, 5.3957e-01, 4.4710e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32206565141677856 tensor([4.0160e-04, 4.9932e-01, 1.7688e-01, 1.3243e-03, 3.2207e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1302153319120407 tensor([1.5042e-06, 4.1966e-02, 8.2777e-01, 5.0350e-05, 1.3022e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22986210882663727 tensor([0.0916, 0.2299, 0.0015, 0.3099, 0.3672], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.304593026638031 tensor([0.0316, 0.0992, 0.0017, 0.3046, 0.5628], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.142741397023201 tensor([1.1909e-01, 7.0815e-01, 6.0406e-04, 2.9418e-02, 1.4274e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4234229326248169 tensor([0.0113, 0.4234, 0.0112, 0.0295, 0.5246], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1988411545753479 tensor([4.0622e-04, 1.9884e-01, 1.3455e-01, 5.4527e-03, 6.6075e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3641755282878876 tensor([0.0164, 0.0230, 0.0010, 0.5954, 0.3642], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2186480313539505 tensor([0.0835, 0.2186, 0.0007, 0.1932, 0.5039], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1043497622013092 tensor([0.0048, 0.8792, 0.0097, 0.0019, 0.1043], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4176483452320099 tensor([0.0058, 0.5166, 0.0384, 0.0216, 0.4176], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04881545901298523 tensor([4.7757e-08, 3.6464e-03, 9.4753e-01, 1.1063e-05, 4.8815e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4164507985115051 tensor([4.1645e-01, 6.7666e-05, 3.8689e-10, 5.8337e-01, 1.1094e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3545880615711212 tensor([1.1109e-02, 2.0208e-02, 6.0091e-04, 3.5459e-01, 6.1349e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.373080313205719 tensor([3.7308e-01, 4.0841e-01, 1.6090e-04, 9.4411e-02, 1.2394e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35631459951400757 tensor([5.6969e-05, 5.7353e-01, 3.5631e-01, 1.0754e-04, 6.9992e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17910170555114746 tensor([1.9322e-06, 2.5106e-02, 7.9564e-01, 1.5376e-04, 1.7910e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28095555305480957 tensor([2.8096e-01, 4.6870e-02, 4.6908e-05, 5.6798e-01, 1.0414e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38682183623313904 tensor([4.7739e-07, 3.0798e-03, 6.0954e-01, 5.5769e-04, 3.8682e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010088573209941387 tensor([9.8485e-01, 1.0089e-02, 3.5288e-09, 4.8822e-03, 1.7848e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2208789885044098 tensor([5.0024e-06, 2.2088e-01, 7.2784e-01, 2.1014e-05, 5.1250e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025950953364372253 tensor([8.7147e-08, 8.0910e-03, 9.6595e-01, 7.8410e-06, 2.5951e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06854506582021713 tensor([2.1735e-04, 2.2633e-02, 6.8545e-02, 5.2770e-02, 8.5584e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30246925354003906 tensor([6.8708e-01, 6.4308e-03, 2.0023e-07, 3.0247e-01, 4.0213e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06554113328456879 tensor([8.5348e-01, 6.5541e-02, 1.4697e-06, 7.0410e-02, 1.0569e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0435495525598526 tensor([4.9252e-07, 2.6411e-02, 9.3002e-01, 2.0820e-05, 4.3550e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05687957629561424 tensor([4.7116e-04, 5.6880e-02, 4.4885e-02, 1.9142e-02, 8.7862e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2010592520236969 tensor([3.6856e-02, 3.0725e-02, 5.1781e-04, 7.3084e-01, 2.0106e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31658467650413513 tensor([3.1607e-02, 5.4499e-02, 5.4332e-04, 3.1658e-01, 5.9677e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21843208372592926 tensor([0.0767, 0.2184, 0.0011, 0.1771, 0.5266], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02586198039352894 tensor([2.5491e-08, 2.4412e-03, 9.7169e-01, 8.4017e-06, 2.5862e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05940121412277222 tensor([9.2703e-08, 5.1309e-03, 9.3545e-01, 1.8404e-05, 5.9401e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18226009607315063 tensor([0.0179, 0.0887, 0.0026, 0.1823, 0.7086], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1769634485244751 tensor([1.7696e-01, 2.0497e-03, 6.1958e-07, 8.1007e-01, 1.0915e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32132643461227417 tensor([3.2133e-01, 4.6566e-01, 1.7700e-04, 8.4534e-02, 1.2830e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22439232468605042 tensor([2.2620e-05, 1.2184e-01, 6.5320e-01, 5.5020e-04, 2.2439e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39271584153175354 tensor([2.6656e-04, 4.1359e-01, 1.9243e-01, 9.9862e-04, 3.9272e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3069419860839844 tensor([7.1609e-02, 5.1222e-02, 5.3987e-04, 5.6969e-01, 3.0694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09747456759214401 tensor([0.0075, 0.0821, 0.0038, 0.0975, 0.8091], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25452837347984314 tensor([3.0109e-01, 2.7820e-01, 2.9782e-04, 1.6588e-01, 2.5453e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04592103883624077 tensor([1.4025e-03, 9.2497e-01, 2.7233e-02, 4.7719e-04, 4.5921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030021850019693375 tensor([1.9026e-07, 2.1528e-02, 9.4844e-01, 5.3934e-06, 3.0022e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1902095228433609 tensor([0.0009, 0.1902, 0.1131, 0.0240, 0.6718], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0839223712682724 tensor([5.2467e-03, 1.2702e-03, 6.1418e-05, 9.0950e-01, 8.3922e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3234465718269348 tensor([5.9756e-01, 3.2345e-01, 2.1845e-05, 5.3285e-02, 2.5690e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1296296864748001 tensor([0.0018, 0.8441, 0.0234, 0.0011, 0.1296], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.28148359060287476 tensor([1.8281e-05, 1.6969e-02, 2.8148e-01, 3.9210e-03, 6.9761e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18821218609809875 tensor([1.4137e-04, 2.2765e-02, 1.8821e-01, 5.5806e-02, 7.3308e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.056889504194259644 tensor([1.6525e-02, 8.2865e-04, 7.5377e-06, 9.2575e-01, 5.6890e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0731765404343605 tensor([0.0144, 0.9050, 0.0035, 0.0040, 0.0732], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30526599287986755 tensor([3.0527e-01, 4.0228e-01, 2.6029e-04, 1.6219e-01, 1.3000e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16178180277347565 tensor([9.1709e-06, 9.1074e-02, 7.4691e-01, 2.2915e-04, 1.6178e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2984313368797302 tensor([2.9843e-01, 4.6119e-04, 2.0918e-08, 7.0019e-01, 9.2114e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3846781849861145 tensor([0.0220, 0.0348, 0.0006, 0.3847, 0.5579], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05385546386241913 tensor([9.1154e-01, 2.9757e-02, 4.4382e-07, 5.3855e-02, 4.8482e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23408642411231995 tensor([0.0062, 0.2341, 0.0183, 0.0538, 0.6876], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2100931853055954 tensor([2.8074e-06, 3.4211e-02, 7.5557e-01, 1.2460e-04, 2.1009e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30120980739593506 tensor([0.0045, 0.3012, 0.0545, 0.0564, 0.5835], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25911945104599 tensor([2.1914e-04, 1.4683e-01, 2.5912e-01, 7.2427e-03, 5.8658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16008201241493225 tensor([0.0669, 0.7579, 0.0008, 0.0143, 0.1601], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27413037419319153 tensor([1.3974e-05, 2.7413e-01, 6.5557e-01, 5.8419e-05, 7.0231e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23656383156776428 tensor([0.0021, 0.0238, 0.0103, 0.2366, 0.7272], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08836644142866135 tensor([9.0579e-01, 4.8112e-03, 3.6369e-08, 8.8366e-02, 1.0291e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06036587432026863 tensor([4.8492e-02, 3.0282e-03, 5.8852e-06, 8.8811e-01, 6.0366e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1491871178150177 tensor([6.2273e-01, 1.4919e-01, 2.2212e-05, 1.5651e-01, 7.1550e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05772130936384201 tensor([8.4574e-07, 5.7721e-02, 9.0513e-01, 1.2633e-05, 3.7135e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23575802147388458 tensor([0.0218, 0.7277, 0.0043, 0.0104, 0.2358], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.44287195801734924 tensor([5.5599e-01, 4.7190e-04, 7.2290e-09, 4.4287e-01, 6.6565e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21260881423950195 tensor([7.5767e-03, 3.3483e-03, 1.5222e-04, 7.7631e-01, 2.1261e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17331986129283905 tensor([6.7604e-01, 9.6333e-02, 9.0661e-06, 1.7332e-01, 5.4300e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3863914906978607 tensor([1.8088e-05, 7.2523e-02, 5.4037e-01, 7.0074e-04, 3.8639e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10345904529094696 tensor([1.0346e-01, 1.1810e-02, 2.2070e-05, 8.2033e-01, 6.4381e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25575417280197144 tensor([7.5615e-02, 6.0774e-02, 3.9329e-04, 6.0746e-01, 2.5575e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1075429916381836 tensor([3.0482e-04, 4.7338e-03, 4.9891e-03, 1.0754e-01, 8.8243e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16199159622192383 tensor([6.6589e-01, 1.2271e-01, 1.6375e-05, 1.6199e-01, 4.9394e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18465577065944672 tensor([0.0032, 0.1847, 0.0259, 0.0309, 0.7553], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36056646704673767 tensor([4.2163e-05, 5.9390e-02, 3.6057e-01, 2.9740e-03, 5.7703e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22403858602046967 tensor([0.0897, 0.2782, 0.0015, 0.2240, 0.4066], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13733382523059845 tensor([2.3254e-02, 5.1775e-03, 7.0778e-05, 8.3416e-01, 1.3733e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22932834923267365 tensor([7.3124e-01, 2.2933e-01, 7.1076e-06, 2.9879e-02, 9.5480e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25755396485328674 tensor([2.1624e-04, 2.5755e-01, 2.6213e-01, 2.9855e-03, 4.7711e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04258409142494202 tensor([6.0114e-08, 4.6745e-03, 9.5273e-01, 1.0994e-05, 4.2584e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4535488784313202 tensor([4.5355e-01, 4.5298e-04, 8.7487e-09, 5.4516e-01, 8.3978e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.035581763833761215 tensor([1.1046e-04, 1.3769e-02, 3.5582e-02, 2.4126e-02, 9.2641e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3378340005874634 tensor([0.0138, 0.3378, 0.0058, 0.0310, 0.6116], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1878662258386612 tensor([8.1766e-06, 1.8787e-01, 7.5132e-01, 5.6845e-05, 6.0745e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13367699086666107 tensor([5.6691e-05, 1.8398e-02, 1.3368e-01, 1.6016e-02, 8.3185e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.377520352602005 tensor([0.0328, 0.0468, 0.0010, 0.5419, 0.3775], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.43264222145080566 tensor([1.0952e-05, 2.2596e-02, 4.3264e-01, 2.5125e-03, 5.4224e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10151414573192596 tensor([8.0961e-01, 6.6761e-02, 3.0184e-06, 1.0151e-01, 2.2111e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25541234016418457 tensor([4.1904e-05, 6.9563e-01, 2.5541e-01, 3.3972e-05, 4.8878e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08000019192695618 tensor([9.8708e-05, 1.7876e-02, 8.0000e-02, 1.6596e-02, 8.8543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.043117132037878036 tensor([4.3117e-02, 1.0063e-04, 3.8216e-08, 9.5521e-01, 1.5717e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022446244955062866 tensor([9.9078e-08, 1.0598e-02, 9.6695e-01, 6.2712e-06, 2.2446e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08321375399827957 tensor([8.2821e-01, 7.2550e-02, 2.2540e-06, 8.3214e-02, 1.6027e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02923974022269249 tensor([3.0747e-07, 2.9240e-02, 9.4574e-01, 7.2586e-06, 2.5015e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3207826614379883 tensor([3.6128e-05, 4.1139e-02, 3.2078e-01, 2.6754e-03, 6.3537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37159398198127747 tensor([0.0170, 0.3716, 0.0177, 0.0909, 0.5028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10181964933872223 tensor([0.0037, 0.0768, 0.0111, 0.1018, 0.8066], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4183468520641327 tensor([4.1835e-01, 3.7595e-03, 3.0242e-07, 5.7083e-01, 7.0660e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2561977207660675 tensor([5.7773e-04, 6.1703e-01, 1.2480e-01, 1.3962e-03, 2.5620e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1804376244544983 tensor([0.0329, 0.1425, 0.0024, 0.1804, 0.6417], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04530303180217743 tensor([4.5303e-02, 1.4916e-03, 2.8247e-06, 9.3428e-01, 1.8924e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.39557045698165894 tensor([0.0306, 0.0598, 0.0010, 0.3956, 0.5130], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2969778776168823 tensor([1.5429e-05, 2.9698e-01, 6.2753e-01, 6.2955e-05, 7.5415e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010459338082000613 tensor([5.7115e-11, 3.6919e-04, 9.9858e-01, 2.1095e-08, 1.0459e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21600861847400665 tensor([8.1806e-05, 4.0490e-01, 3.7872e-01, 2.8281e-04, 2.1601e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1918269395828247 tensor([8.0764e-01, 3.2317e-04, 7.8740e-10, 1.9183e-01, 2.0903e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1249205470085144 tensor([1.2492e-01, 3.8327e-03, 2.6148e-06, 8.4103e-01, 3.0217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19312456250190735 tensor([1.9312e-01, 7.8432e-01, 7.4656e-05, 5.3754e-03, 1.7104e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11837708950042725 tensor([5.7595e-04, 7.8462e-01, 9.5648e-02, 7.8314e-04, 1.1838e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.031143518164753914 tensor([3.1836e-08, 3.6180e-03, 9.6523e-01, 4.7197e-06, 3.1144e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35224059224128723 tensor([0.0469, 0.0689, 0.0013, 0.5306, 0.3522], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1934543251991272 tensor([0.1405, 0.1935, 0.0006, 0.3097, 0.3559], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009434107691049576 tensor([9.8093e-01, 9.4341e-03, 7.5266e-09, 9.3038e-03, 3.3181e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12177300453186035 tensor([0.0010, 0.8014, 0.0747, 0.0012, 0.1218], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35627326369285583 tensor([8.7870e-05, 1.0097e-01, 3.5627e-01, 3.7641e-03, 5.3890e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1347104012966156 tensor([1.3471e-01, 5.1080e-03, 4.7878e-06, 8.2744e-01, 3.2741e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2522696256637573 tensor([0.0243, 0.6952, 0.0072, 0.0211, 0.2523], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12223564088344574 tensor([4.7769e-02, 6.6475e-03, 3.1890e-05, 8.2332e-01, 1.2224e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06157402694225311 tensor([1.5946e-04, 9.0196e-01, 6.1574e-02, 5.3001e-05, 3.6253e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23164157569408417 tensor([1.8137e-04, 4.9559e-01, 2.3164e-01, 4.4127e-04, 2.7214e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3609439432621002 tensor([4.5914e-01, 1.0712e-01, 2.9781e-05, 3.6094e-01, 7.2763e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05510659143328667 tensor([1.9778e-05, 3.4827e-03, 5.5107e-02, 2.0575e-02, 9.2082e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 2, 1], [2, 3, 0, 4], [2, 0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 3, 0, 4], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4], [2, 1, 4], [2, 3, 0], [2, 0], [0, 3, 1], [2, 1, 4, 0], [0, 2, 3, 1], [0, 3, 1], [3, 2, 0, 4], [0, 3, 2], [2, 0], [2, 1, 0], [2, 4, 3, 1], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0], [2, 4, 3], [0, 3, 4], [0, 3, 4], [2, 4], [0, 2, 1, 3], [2, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [3, 0, 2, 4], [2, 3, 0, 4], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [0, 3, 2], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 0, 3], [2, 1, 4], [2, 4, 3, 1], [0, 3], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 2, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [2, 3, 0], [0, 3, 1], [2, 1, 4], [0, 2, 3, 1], [2, 3, 0, 4], [0, 3, 4, 1], [0, 3, 1], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4], [0, 3, 1], [0, 2, 3], [2, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 3, 0], [0, 3, 1, 4], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 3, 4, 0], [0, 3, 1], [0, 3, 1, 4], [0, 3, 1], [2, 1, 0, 4], [2, 3], [0, 3], [0, 3, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [3, 0, 2, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4], [2, 3, 4, 0], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 2], [0, 3, 4, 2], [0, 3, 1, 2], [2, 1, 4], [2, 1, 4], [2, 4, 3], [3, 0, 2, 4], [3, 0, 2], [2, 3, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 2], [0, 3, 1], [2, 1, 4], [0, 2, 3, 1], [2, 1, 4], [0, 3, 2], [0, 3, 1, 4], [2, 0], [2, 0, 1], [2, 3, 0, 4], [2, 0, 3], [0, 3, 2, 1], [2, 0, 1], [2, 0, 3], [3, 0, 2, 4], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [2, 3, 4], [0, 3, 4], [0, 3, 1, 4], [2, 1, 4], [0, 3, 1], [2, 4, 3, 1], [0, 3, 4], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0], [2, 0, 1], [2, 0, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 3, 4], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1, 3], [2, 3], [3, 0, 2, 4], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0, 4], [2, 4, 3], [3, 0, 2, 4], [0, 3, 1], [0, 1, 3, 2], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 3], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1], [0, 2, 3], [0, 3, 1], [2, 3, 0, 4], [0, 3, 4], [0, 2, 1], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 4, 1], [2, 3, 0], [2, 1, 4], [2, 1, 0], [2, 4, 1, 3], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0], [0, 2, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1], [2, 0], [2, 1, 0, 4], [2, 4, 3], [0, 3], [0, 3, 1, 4], [2, 1, 4], [0, 1, 3, 2], [2, 0, 3], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 1], [0, 3, 1], [2, 4, 1, 3], [3, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1], [2, 0, 3], [0, 2, 1, 3], [2, 1, 4], [0, 3, 2], [2, 0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [0, 3, 4], [0, 3, 1, 4], [0, 3], [2, 4, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 1], [2, 0, 1], [2, 4, 3, 1], [0, 3, 2, 4], [0, 3, 1], [2, 1, 4, 0], [2, 3, 0], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3], [2, 4, 1], [0, 1, 3, 2]]\n",
      "[[0], [2], [2, 4], [3], [4], [4], [1], [4], [3], [4], [1], [1], [2], [0, 3], [0, 3], [1, 4], [1, 3, 4], [2, 4], [3], [4], [2, 4], [1], [1, 4], [1, 3, 4], [3, 4], [0], [1, 4], [2], [3], [3, 4], [0, 1], [1, 2], [1, 2], [0, 1, 3], [4], [1, 4], [4], [2], [0, 3], [3, 4], [1], [1], [2], [0, 3], [3, 4], [1, 4], [1, 4], [2], [3], [3], [0], [1], [2], [1, 4], [0, 3], [0], [1, 2, 4], [2, 4], [3, 4], [3, 4], [1], [2], [2], [4], [3], [2], [1, 4], [2, 4], [0, 3], [4], [1], [2], [2, 4], [3], [4], [0, 1, 3], [2, 4], [1, 4], [0, 3], [4], [0], [2, 4], [1, 4], [3, 4], [3, 4], [1, 4], [2], [2, 4], [0, 3], [3, 4], [1], [2, 4], [2], [2, 4], [3], [0, 1, 4], [1, 2, 4], [2], [4], [4], [1], [0, 3], [2], [3], [0, 3], [1], [1], [2], [3], [4], [1, 4], [1], [4], [0, 3], [0, 3], [0, 1], [1], [1, 4], [1], [0], [0], [1, 4], [2, 4], [0, 3], [4], [0, 3], [1, 4], [2], [1, 3, 4], [3, 4], [1], [1, 4], [4], [3, 4], [1, 4], [1], [1, 4], [2], [0, 3], [3, 4], [0, 1], [1, 2], [2], [0, 3], [2, 4], [0], [1, 2], [2], [4], [0, 3], [0], [2], [4], [3, 4], [3, 4], [1, 4], [2], [2], [4], [3], [0, 1], [2, 4], [1, 4], [3, 4], [4], [0, 1, 4], [1], [2], [4], [3], [0, 1], [1], [2, 4], [4], [3], [1], [0, 1], [2], [0, 3], [3, 4], [0], [1, 4], [2, 4], [1, 4], [2, 4], [1], [1, 2], [3, 4], [0], [3], [0], [2], [1, 4], [0, 3], [3, 4], [0], [2, 4], [3], [3, 4], [4], [0], [4], [2, 4], [1, 3, 4], [3], [0, 1], [1, 2, 4], [2], [0, 3], [4], [1, 4], [2], [4], [3, 4], [2, 4], [0], [1, 2], [4], [3], [2], [0], [2], [2, 4], [1, 4], [4], [0, 3], [1, 4], [4], [3], [3, 4], [1, 2], [2], [1, 2, 4], [0], [3], [1], [1], [2], [3, 4], [3, 4], [0], [1], [2, 4], [3], [1, 4], [3], [1], [1, 2, 4], [0, 3], [4]]\n",
      "NL_pred of 3th iteration [[2, 4, 3, 1], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 2, 1], [2, 3, 0, 4], [2, 0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 3, 0, 4], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [3, 2, 0, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 1, 3], [0, 3, 2, 1], [0, 3, 1, 4], [3, 0, 2, 4], [2, 3, 0, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 2, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [0, 2, 3, 1], [2, 3, 0, 4], [0, 3, 4, 1], [2, 1, 4, 0], [0, 2, 3, 1], [0, 3, 2, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 4, 0], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 4, 2], [0, 3, 1, 2], [3, 0, 2, 4], [2, 3, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 3, 0, 4], [0, 3, 2, 1], [3, 0, 2, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [0, 3, 1, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 0, 1, 3], [3, 0, 2, 4], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0, 4], [3, 0, 2, 4], [0, 1, 3, 2], [2, 1, 0, 4], [2, 3, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 3, 0, 4], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 4, 0], [0, 2, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [0, 1, 3, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [0, 3, 1, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 2, 1, 3], [2, 0, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 1], [2, 4, 3, 1], [0, 3, 2, 4], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 2], [0, 1, 3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.00946119290131789  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.009461027842301588  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.009460717898148757  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.00946027865776649  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.009459730295034555  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.009459085647876446  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.009458363973177397  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.009457576274871825  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00945673447388869  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.009455853242140549  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.009454940832578218  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.009454005498152512  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.009453052740830642  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.009452091730557955  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00945112705230713  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.009450161457061767  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.009449198612800011  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.0094482421875  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.009447294015150804  Accuracy on Support set:0.0\n",
      "torch.Size([130, 2048]) torch.Size([130])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.009446351344768818  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.28553253412246704 tensor([1.2338e-04, 1.3719e-01, 2.8553e-01, 3.3484e-03, 5.7381e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22447296977043152 tensor([2.2447e-01, 7.6384e-03, 3.9942e-06, 7.4084e-01, 2.7045e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4687851071357727 tensor([4.6879e-01, 1.0145e-04, 5.3219e-10, 5.3092e-01, 1.9624e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3568722903728485 tensor([0.0533, 0.5322, 0.0030, 0.0546, 0.3569], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20294195413589478 tensor([0.1001, 0.2958, 0.0014, 0.2029, 0.3998], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2528538405895233 tensor([3.0500e-06, 1.9262e-02, 7.2738e-01, 5.0125e-04, 2.5285e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.325295627117157 tensor([6.6181e-05, 9.5228e-02, 3.2530e-01, 2.1075e-03, 5.7730e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35389456152915955 tensor([0.0030, 0.6059, 0.0317, 0.0055, 0.3539], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22889851033687592 tensor([0.0598, 0.2289, 0.0026, 0.3044, 0.4042], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24647735059261322 tensor([8.0417e-03, 2.9893e-03, 1.1738e-04, 7.4237e-01, 2.4648e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40573790669441223 tensor([0.0021, 0.4057, 0.0429, 0.0084, 0.5409], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3595069944858551 tensor([3.9824e-03, 1.8585e-03, 1.4858e-04, 6.3450e-01, 3.5951e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.39029353857040405 tensor([5.1180e-01, 3.9029e-01, 4.7979e-05, 4.9927e-02, 4.7930e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.30465248227119446 tensor([5.6680e-05, 6.4167e-01, 3.0465e-01, 7.4478e-05, 5.3546e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3849267065525055 tensor([7.4807e-05, 4.3515e-01, 3.8493e-01, 2.5908e-04, 1.7959e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23937541246414185 tensor([2.8623e-01, 2.3938e-01, 2.4689e-04, 2.9720e-01, 1.7695e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2253931611776352 tensor([1.6434e-01, 5.4305e-01, 5.3706e-04, 6.6680e-02, 2.2539e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3616495430469513 tensor([3.6165e-01, 1.5895e-03, 1.0780e-07, 6.3442e-01, 2.3396e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.43266111612319946 tensor([0.0011, 0.0043, 0.0020, 0.4327, 0.5599], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3583958148956299 tensor([3.5840e-01, 1.5725e-02, 5.7787e-06, 5.7831e-01, 4.7567e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19847561419010162 tensor([0.0214, 0.1282, 0.0026, 0.1985, 0.6493], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40760883688926697 tensor([0.0028, 0.4076, 0.0311, 0.0082, 0.5502], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3770206868648529 tensor([2.6464e-04, 4.0378e-01, 2.1759e-01, 1.3378e-03, 3.7702e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40601441264152527 tensor([0.0474, 0.4060, 0.0036, 0.1077, 0.4353], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24009545147418976 tensor([2.4010e-01, 1.9910e-05, 1.1469e-10, 7.5982e-01, 6.8155e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.30393195152282715 tensor([2.3007e-04, 3.3915e-01, 3.0393e-01, 2.0149e-03, 3.5467e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18300002813339233 tensor([9.4744e-07, 1.4180e-02, 8.0270e-01, 1.1686e-04, 1.8300e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3552861213684082 tensor([0.0540, 0.1803, 0.0023, 0.3553, 0.4082], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.40194517374038696 tensor([0.0046, 0.0094, 0.0011, 0.4019, 0.5830], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2512177526950836 tensor([0.1546, 0.4820, 0.0008, 0.1113, 0.2512], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35232383012771606 tensor([3.0130e-05, 9.9503e-02, 5.4719e-01, 9.4852e-04, 3.5232e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3414473533630371 tensor([3.4145e-01, 2.2958e-04, 5.0116e-09, 6.5786e-01, 4.6046e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3482336103916168 tensor([2.9993e-05, 1.4147e-01, 5.0975e-01, 5.1353e-04, 3.4823e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2208702713251114 tensor([4.4581e-01, 2.3086e-01, 8.3144e-05, 2.2087e-01, 1.0237e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26888251304626465 tensor([1.1609e-05, 8.2980e-02, 6.4779e-01, 3.3142e-04, 2.6888e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3841232657432556 tensor([0.0065, 0.3841, 0.0169, 0.0123, 0.5802], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2900826930999756 tensor([2.9008e-01, 2.2558e-03, 4.2403e-07, 7.0262e-01, 5.0412e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2888434827327728 tensor([2.2720e-05, 1.0422e-01, 6.0628e-01, 6.2857e-04, 2.8884e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23847636580467224 tensor([6.2314e-04, 2.3848e-01, 1.2358e-01, 9.0205e-03, 6.2830e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22481274604797363 tensor([3.2465e-02, 2.5190e-02, 4.4692e-04, 7.1709e-01, 2.2481e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3944133520126343 tensor([0.0272, 0.0624, 0.0013, 0.3944, 0.5146], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28048786520957947 tensor([0.0746, 0.5883, 0.0022, 0.0545, 0.2805], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2432122379541397 tensor([1.1581e-05, 9.3131e-02, 6.6332e-01, 3.2371e-04, 2.4321e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20112170279026031 tensor([2.0112e-01, 4.0148e-03, 1.3829e-06, 7.7971e-01, 1.5151e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25940701365470886 tensor([2.9777e-02, 1.0483e-02, 7.9773e-05, 7.0025e-01, 2.5941e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.37432998418807983 tensor([1.4131e-05, 1.3535e-02, 3.7433e-01, 4.8987e-03, 6.0722e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4102540612220764 tensor([5.0819e-05, 8.1852e-02, 4.1025e-01, 2.5511e-03, 5.0529e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19873327016830444 tensor([0.1987, 0.3695, 0.0005, 0.1066, 0.3247], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.30311962962150574 tensor([2.1030e-04, 3.4637e-01, 3.0312e-01, 1.5755e-03, 3.4872e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3084908723831177 tensor([5.6734e-01, 8.3346e-02, 1.5257e-05, 3.0849e-01, 4.0811e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30812013149261475 tensor([3.0812e-01, 3.7845e-04, 1.2843e-08, 6.9000e-01, 1.5046e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40580886602401733 tensor([0.0020, 0.4058, 0.0561, 0.0084, 0.5277], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23164002597332 tensor([2.3164e-01, 2.2914e-03, 6.7455e-07, 7.5463e-01, 1.1437e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4126845896244049 tensor([4.1268e-01, 1.8170e-03, 7.9077e-08, 5.8265e-01, 2.8459e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4277271032333374 tensor([5.4741e-01, 4.2773e-01, 1.1583e-05, 1.2473e-02, 1.2382e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22266952693462372 tensor([0.0032, 0.7494, 0.0222, 0.0025, 0.2227], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24036677181720734 tensor([0.0008, 0.6624, 0.0946, 0.0018, 0.2404], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.39534053206443787 tensor([4.3038e-05, 4.8481e-02, 3.9534e-01, 3.8653e-03, 5.5227e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.40956032276153564 tensor([4.0956e-01, 2.0066e-02, 6.0152e-06, 5.3347e-01, 3.6903e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3803856670856476 tensor([3.8039e-01, 1.6262e-02, 3.9962e-06, 5.5524e-01, 4.8111e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3096736967563629 tensor([3.4820e-04, 4.8803e-01, 2.0072e-01, 1.2313e-03, 3.0967e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23464679718017578 tensor([0.0817, 0.2346, 0.0018, 0.3028, 0.3791], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29601380228996277 tensor([0.0281, 0.1000, 0.0021, 0.2960, 0.5738], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.42216747999191284 tensor([0.0098, 0.4222, 0.0133, 0.0282, 0.5265], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3768017292022705 tensor([0.0145, 0.0233, 0.0012, 0.5842, 0.3768], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22141127288341522 tensor([0.0745, 0.2214, 0.0009, 0.1881, 0.5150], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41630104184150696 tensor([0.0051, 0.5132, 0.0447, 0.0207, 0.4163], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3922653794288635 tensor([3.9227e-01, 6.8592e-05, 4.5285e-10, 6.0755e-01, 1.1751e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3441455066204071 tensor([0.0098, 0.0203, 0.0007, 0.3441, 0.6250], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.34856194257736206 tensor([3.4856e-01, 4.2792e-01, 1.9316e-04, 9.4318e-02, 1.2901e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.39213261008262634 tensor([4.7899e-05, 5.4227e-01, 3.9213e-01, 9.7081e-05, 6.5455e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26187482476234436 tensor([2.6187e-01, 4.9179e-02, 5.7469e-05, 5.7793e-01, 1.1096e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34658345580101013 tensor([3.7597e-07, 2.7587e-03, 6.5018e-01, 4.8059e-04, 3.4658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1971995085477829 tensor([4.0095e-06, 1.9720e-01, 7.5698e-01, 1.8338e-05, 4.5798e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32141491770744324 tensor([6.6699e-01, 7.0924e-03, 2.5945e-07, 3.2141e-01, 4.4991e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21002376079559326 tensor([3.3143e-02, 3.1501e-02, 6.3127e-04, 7.2470e-01, 2.1002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3068862855434418 tensor([0.0279, 0.0548, 0.0006, 0.3069, 0.6098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22236204147338867 tensor([0.0691, 0.2224, 0.0013, 0.1718, 0.5354], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29664793610572815 tensor([2.9665e-01, 4.8571e-01, 2.1366e-04, 8.3956e-02, 1.3347e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19853906333446503 tensor([1.7706e-05, 1.0887e-01, 6.9210e-01, 4.6766e-04, 1.9854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3771756887435913 tensor([2.2639e-04, 4.0097e-01, 2.2071e-01, 9.1885e-04, 3.7718e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31810900568962097 tensor([0.0647, 0.0521, 0.0006, 0.5645, 0.3181], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26553621888160706 tensor([0.2789, 0.2898, 0.0004, 0.1654, 0.2655], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3508835732936859 tensor([5.6620e-01, 3.5088e-01, 2.7896e-05, 5.4915e-02, 2.7975e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31952428817749023 tensor([1.5141e-05, 1.6086e-02, 3.1952e-01, 3.5595e-03, 6.6081e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2819409966468811 tensor([2.8194e-01, 4.2013e-01, 3.1574e-04, 1.6185e-01, 1.3576e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27813756465911865 tensor([2.7814e-01, 4.7166e-04, 2.4782e-08, 7.2042e-01, 9.7409e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3736640512943268 tensor([0.0195, 0.0351, 0.0007, 0.3737, 0.5711], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23238134384155273 tensor([0.0053, 0.2324, 0.0218, 0.0507, 0.6898], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18090391159057617 tensor([2.1017e-06, 2.9607e-02, 7.8938e-01, 1.0215e-04, 1.8090e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2945857346057892 tensor([0.0038, 0.2946, 0.0643, 0.0533, 0.5840], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2927038073539734 tensor([1.8546e-04, 1.4052e-01, 2.9270e-01, 6.6674e-03, 5.5992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24668698012828827 tensor([1.1231e-05, 2.4669e-01, 6.9011e-01, 5.0886e-05, 6.3146e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22709034383296967 tensor([0.0018, 0.0237, 0.0122, 0.2271, 0.7352], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23691318929195404 tensor([0.0188, 0.7292, 0.0052, 0.0098, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4626092314720154 tensor([5.3620e-01, 4.8789e-04, 8.4411e-09, 4.6261e-01, 7.0779e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22143448889255524 tensor([6.8603e-03, 3.4399e-03, 1.8373e-04, 7.6808e-01, 2.2143e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34799081087112427 tensor([1.4245e-05, 6.5619e-02, 5.8577e-01, 6.0303e-04, 3.4799e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2673644423484802 tensor([6.7791e-02, 6.2248e-02, 4.8101e-04, 6.0212e-01, 2.6736e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40187785029411316 tensor([3.4975e-05, 5.6054e-02, 4.0188e-01, 2.6763e-03, 5.3936e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2184644490480423 tensor([0.0800, 0.2825, 0.0018, 0.2185, 0.4172], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25066861510276794 tensor([7.0769e-01, 2.5067e-01, 8.9977e-06, 3.1192e-02, 1.0444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24824602901935577 tensor([1.8546e-04, 2.4825e-01, 2.9408e-01, 2.7598e-03, 4.5473e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4290333688259125 tensor([4.2903e-01, 4.7152e-04, 1.0570e-08, 5.6959e-01, 9.0353e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3394068777561188 tensor([0.0124, 0.3394, 0.0067, 0.0300, 0.6115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38872310519218445 tensor([0.0293, 0.0472, 0.0011, 0.5336, 0.3887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.47425663471221924 tensor([8.9628e-06, 2.0863e-02, 4.7426e-01, 2.2556e-03, 5.0262e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2853640019893646 tensor([3.5917e-05, 6.6781e-01, 2.8536e-01, 3.1501e-05, 4.6754e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3599875867366791 tensor([3.0054e-05, 3.8944e-02, 3.5999e-01, 2.4227e-03, 5.9862e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3698330819606781 tensor([0.0150, 0.3698, 0.0207, 0.0876, 0.5069], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.39954856038093567 tensor([3.9955e-01, 3.9615e-03, 3.6295e-07, 5.8899e-01, 7.5034e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24985989928245544 tensor([4.9914e-04, 6.0473e-01, 1.4360e-01, 1.3060e-03, 2.4986e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38362348079681396 tensor([0.0271, 0.0607, 0.0012, 0.3836, 0.5275], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26874321699142456 tensor([1.2477e-05, 2.6874e-01, 6.6308e-01, 5.5090e-05, 6.8114e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19915351271629333 tensor([6.6021e-05, 3.7767e-01, 4.2286e-01, 2.4805e-04, 1.9915e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3621057868003845 tensor([0.0425, 0.0699, 0.0015, 0.5241, 0.3621], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3071516752243042 tensor([0.1272, 0.1972, 0.0007, 0.3072, 0.3679], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3978968560695648 tensor([7.1913e-05, 9.4162e-02, 3.9790e-01, 3.3879e-03, 5.0448e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2523362934589386 tensor([0.0216, 0.6975, 0.0084, 0.0202, 0.2523], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2583238482475281 tensor([1.5270e-04, 4.7763e-01, 2.6349e-01, 4.0062e-04, 2.5832e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.37363213300704956 tensor([4.3295e-01, 1.1446e-01, 3.7518e-05, 3.7363e-01, 7.8921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 2, 1], [2, 3, 0, 4], [2, 0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 3, 0, 4], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4], [2, 1, 4], [2, 3, 0], [2, 0], [0, 3, 1], [2, 1, 4, 0], [0, 2, 3, 1], [0, 3, 1], [3, 2, 0, 4], [0, 3, 2], [2, 0], [2, 1, 0], [2, 4, 3, 1], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0], [2, 4, 3], [0, 3, 4], [0, 3, 4], [2, 4], [0, 2, 1, 3], [2, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [3, 0, 2, 4], [2, 3, 0, 4], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1, 3], [0, 3, 2], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 0, 3], [2, 1, 4], [2, 4, 3, 1], [0, 3], [0, 3, 1, 4], [2, 0, 1], [2, 0, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 2, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [2, 3, 0], [0, 3, 1], [2, 1, 4], [0, 2, 3, 1], [2, 3, 0, 4], [0, 3, 4, 1], [0, 3, 1], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4], [0, 3, 1], [0, 2, 3], [2, 1, 4], [0, 3, 2, 1], [2, 4, 3, 1], [0, 3, 1], [0, 3, 2], [2, 1, 0], [2, 0, 1], [2, 3, 0], [0, 3, 1, 4], [0, 3, 1], [2, 1, 4], [2, 1, 0], [2, 3, 4, 0], [0, 3, 1], [0, 3, 1, 4], [0, 3, 1], [2, 1, 0, 4], [2, 3, 0], [0, 3], [0, 3, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [3, 0, 2, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4], [2, 3, 4, 0], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 2], [0, 3, 4, 2], [0, 3, 1, 2], [2, 1, 4], [2, 1, 4], [2, 4, 3], [3, 0, 2, 4], [3, 0, 2], [2, 3, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 2], [0, 3, 1], [2, 1, 4], [0, 2, 3, 1], [2, 1, 4], [0, 3, 2], [0, 3, 1, 4], [2, 0], [2, 0, 1], [2, 3, 0, 4], [2, 0, 3], [0, 3, 2, 1], [2, 0, 1], [2, 0, 3], [3, 0, 2, 4], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [2, 3, 4], [0, 3, 4], [0, 3, 1, 4], [2, 1, 4], [0, 3, 1], [2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0], [2, 0, 1], [2, 0, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 3, 4], [0, 3, 1, 4], [0, 3, 2], [2, 1, 0], [2, 0, 1, 3], [2, 3], [3, 0, 2, 4], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0, 4], [2, 4, 3], [3, 0, 2, 4], [0, 3, 1], [0, 1, 3, 2], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 3], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1], [2, 4, 1, 3], [0, 2, 3], [0, 3, 1, 4], [0, 2, 3], [0, 3, 1], [2, 3, 0, 4], [0, 3, 4], [0, 2, 1], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 4, 1], [2, 3, 0], [2, 1, 4], [2, 1, 0], [2, 4, 1, 3], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0], [0, 2, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1], [2, 0], [2, 1, 0, 4], [2, 4, 3], [0, 3], [0, 3, 1, 4], [2, 1, 4], [0, 1, 3, 2], [2, 0, 3], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 1], [0, 3, 1], [2, 4, 1, 3], [3, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1], [2, 0, 3], [0, 2, 1, 3], [2, 1, 4], [0, 3, 2], [2, 0, 1, 3], [2, 1, 4, 0], [2, 0, 1], [0, 3, 4], [0, 3, 1, 4], [0, 3, 4], [2, 4, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 1], [2, 0, 1], [2, 4, 3, 1], [0, 3, 2, 4], [0, 3, 1], [2, 1, 4, 0], [2, 3, 0], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3], [2, 4, 1], [0, 1, 3, 2]]\n",
      "[[0], [2], [2, 4], [3], [4], [4], [1], [4], [3], [4], [1], [1], [2], [0, 3], [0, 3], [1, 4], [1, 3, 4], [2, 4], [3], [4], [2, 4], [1], [1, 4], [1, 3, 4], [3, 4], [0], [1, 4], [2], [3], [3, 4], [0, 1], [1, 2], [1, 2], [0, 1, 3], [4], [1, 4], [4], [2], [0, 3], [3, 4], [1], [1], [2], [0, 3], [4], [1, 4], [1, 4], [2], [3], [3], [0], [1], [2], [1, 4], [0, 3], [0], [1, 2, 4], [2], [3, 4], [3, 4], [1], [2], [2], [4], [3], [2], [1, 4], [2, 4], [0, 3], [4], [1], [2], [2, 4], [3], [4], [0, 1, 3], [2, 4], [1, 4], [0, 3], [4], [0], [2, 4], [1, 4], [3, 4], [3, 4], [1, 4], [2], [2, 4], [0, 3], [3, 4], [1], [2, 4], [2], [2, 4], [3], [1, 4], [1, 2, 4], [2], [4], [4], [1], [0, 3], [2], [3], [0, 3], [1], [1], [2], [3], [4], [1, 4], [1], [4], [0, 3], [0, 3], [0, 1], [1], [1, 4], [1], [0], [0], [1, 4], [2, 4], [0, 3], [4], [0, 3], [1, 4], [2], [1, 3, 4], [3, 4], [1], [1, 4], [4], [3, 4], [1, 4], [1], [1, 4], [2], [0, 3], [3, 4], [0, 1], [1, 2], [2], [0, 3], [2, 4], [0], [2], [2], [4], [0, 3], [0], [2], [4], [3, 4], [3, 4], [1, 4], [2], [2], [4], [3], [0, 1], [2], [1, 4], [3, 4], [4], [0, 1, 4], [1], [2], [4], [3], [0, 1], [1], [2, 4], [4], [3], [1], [0, 1], [2], [0, 3], [3, 4], [0], [1, 4], [2], [1, 4], [2, 4], [1], [1, 2], [3, 4], [0], [3], [0], [2], [1, 4], [0, 3], [3, 4], [0], [2, 4], [3], [3, 4], [4], [0], [4], [2, 4], [1, 3, 4], [3], [0, 1], [1, 2, 4], [2], [0, 3], [4], [1, 4], [2], [4], [3, 4], [2, 4], [0], [1, 2], [4], [3], [2], [0], [2], [2, 4], [1, 4], [4], [0, 3], [1, 4], [4], [3], [3, 4], [1, 2], [2], [1, 2], [0], [3], [1], [1], [2], [3, 4], [3, 4], [0], [1], [2, 4], [3], [1, 4], [3], [1], [1, 2, 4], [0, 3], [4]]\n",
      "NL_pred of 4th iteration [[2, 0, 1, 3], [0, 3, 1, 4], [2, 3, 0], [0, 3, 4, 1], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.18745723792484828  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.1866635765348162  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.18521978173937118  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.18330238546643937  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.18109960215432302  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.17878316129956925  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.17649849823543004  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.17435198170798166  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.17241501808166504  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.17072299548557826  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.16928633621760777  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.16807266644069127  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.16706022194453649  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.16622294698442733  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.16552759919847762  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.16495633125305176  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.16448325770241873  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.16408870901380265  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.16375883987971715  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.1634793622153146  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.10945284366607666 tensor([2.3199e-06, 3.1698e-02, 8.5870e-01, 1.4661e-04, 1.0945e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09263872355222702 tensor([9.2639e-02, 4.9168e-02, 3.2228e-04, 6.6392e-01, 1.9395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.296236127614975 tensor([2.9624e-01, 5.0285e-04, 2.3127e-08, 7.0179e-01, 1.4733e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39401859045028687 tensor([0.0038, 0.5497, 0.0430, 0.0096, 0.3940], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03565744310617447 tensor([0.0079, 0.3756, 0.0234, 0.0357, 0.5575], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017895005643367767 tensor([1.8752e-08, 1.7078e-03, 9.8039e-01, 7.4503e-06, 1.7895e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09798486530780792 tensor([1.0840e-06, 1.9446e-02, 8.8249e-01, 8.1987e-05, 9.7985e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.233561173081398 tensor([1.3752e-04, 4.1733e-01, 3.4832e-01, 6.5512e-04, 2.3356e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.054366547614336014 tensor([0.0048, 0.3036, 0.0483, 0.0544, 0.5890], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2521447241306305 tensor([0.0012, 0.0080, 0.0045, 0.2521, 0.7342], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2729632556438446 tensor([1.0654e-04, 2.7296e-01, 3.9502e-01, 1.0302e-03, 3.3088e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16582615673542023 tensor([4.7090e-04, 3.8094e-03, 4.2974e-03, 1.6583e-01, 8.2560e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06399013847112656 tensor([0.0640, 0.8102, 0.0014, 0.0147, 0.1098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1547572761774063 tensor([1.8895e-06, 1.5476e-01, 8.2780e-01, 7.9680e-06, 1.7436e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07652026414871216 tensor([1.2785e-06, 7.6520e-02, 8.9179e-01, 1.2063e-05, 3.1673e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03222740441560745 tensor([0.0322, 0.4834, 0.0074, 0.0768, 0.4002], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30104008316993713 tensor([0.0128, 0.6637, 0.0094, 0.0130, 0.3010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20929133892059326 tensor([2.0929e-01, 1.2000e-02, 8.5894e-06, 7.5923e-01, 1.9470e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0821816623210907 tensor([1.0273e-04, 6.4347e-03, 3.9394e-02, 8.2182e-02, 8.7189e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13396035134792328 tensor([1.3396e-01, 9.0968e-02, 4.1646e-04, 4.6644e-01, 3.0822e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30040353536605835 tensor([1.5560e-04, 3.0040e-01, 3.2059e-01, 1.1183e-03, 3.7773e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08351130783557892 tensor([5.3710e-06, 1.0564e-01, 8.1078e-01, 6.7710e-05, 8.3511e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4317750930786133 tensor([0.0032, 0.4318, 0.0523, 0.0168, 0.4959], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13242337107658386 tensor([1.3242e-01, 7.4519e-05, 3.5036e-09, 8.6711e-01, 3.9484e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05974575877189636 tensor([3.5005e-06, 6.8172e-02, 8.7200e-01, 7.5539e-05, 5.9746e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06622768193483353 tensor([0.0046, 0.2560, 0.0443, 0.0662, 0.6289], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07533016055822372 tensor([3.9140e-04, 1.3072e-02, 2.1391e-02, 7.5330e-02, 8.8982e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3424350917339325 tensor([0.0118, 0.6113, 0.0150, 0.0195, 0.3424], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.035410817712545395 tensor([2.7922e-07, 1.2291e-02, 9.5228e-01, 2.1087e-05, 3.5411e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1925528645515442 tensor([1.9255e-01, 1.1883e-03, 2.3844e-07, 8.0301e-01, 3.2495e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03476383164525032 tensor([2.6242e-07, 1.7250e-02, 9.4797e-01, 1.1292e-05, 3.4764e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06511338800191879 tensor([0.0651, 0.5754, 0.0029, 0.0725, 0.2841], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024662157520651817 tensor([1.0374e-07, 9.1288e-03, 9.6620e-01, 7.1461e-06, 2.4662e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32750558853149414 tensor([3.5410e-04, 3.2751e-01, 2.1872e-01, 1.7280e-03, 4.5170e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15991437435150146 tensor([1.5991e-01, 1.5166e-02, 2.8364e-05, 7.8734e-01, 3.7554e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026923460885882378 tensor([1.9828e-07, 1.1836e-02, 9.6123e-01, 1.3212e-05, 2.6923e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0944424420595169 tensor([1.8397e-05, 9.4442e-02, 6.9067e-01, 6.3167e-04, 2.1424e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2611323893070221 tensor([0.0055, 0.0669, 0.0154, 0.2611, 0.6511], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07303784042596817 tensor([0.0023, 0.0898, 0.0270, 0.0730, 0.8079], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32219234108924866 tensor([0.0058, 0.6313, 0.0305, 0.0102, 0.3222], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0221576988697052 tensor([1.0457e-07, 1.0122e-02, 9.6771e-01, 7.0496e-06, 2.2158e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09021802991628647 tensor([9.0218e-02, 2.7620e-02, 1.1821e-04, 7.6451e-01, 1.1753e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22892239689826965 tensor([0.0044, 0.0268, 0.0029, 0.2289, 0.7370], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10677492618560791 tensor([2.8228e-07, 2.9052e-03, 8.9011e-01, 2.0939e-04, 1.0677e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07344373315572739 tensor([7.5545e-07, 1.4437e-02, 9.1203e-01, 8.6841e-05, 7.3444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.47664305567741394 tensor([0.0142, 0.4781, 0.0108, 0.0202, 0.4766], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06175907328724861 tensor([3.5580e-06, 7.3287e-02, 8.6489e-01, 6.4278e-05, 6.1759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1745830774307251 tensor([0.1746, 0.4049, 0.0009, 0.2042, 0.2154], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16355304419994354 tensor([1.6355e-01, 2.1140e-03, 7.6305e-07, 8.2258e-01, 1.1750e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24189570546150208 tensor([9.0893e-05, 2.4190e-01, 4.6144e-01, 9.6464e-04, 2.9561e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11510554701089859 tensor([1.1511e-01, 1.5360e-02, 4.9584e-05, 7.8345e-01, 8.6033e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23015013337135315 tensor([2.3015e-01, 1.7196e-02, 9.9756e-06, 7.2206e-01, 3.0587e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07345420122146606 tensor([7.3454e-02, 8.9369e-01, 3.2822e-04, 4.0768e-03, 2.8447e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16310718655586243 tensor([1.6248e-04, 5.6574e-01, 2.7064e-01, 3.5148e-04, 1.6311e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09706907719373703 tensor([2.8501e-05, 2.9370e-01, 6.0905e-01, 1.5275e-04, 9.7069e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08090616017580032 tensor([6.3586e-07, 8.7367e-03, 9.1023e-01, 1.2951e-04, 8.0906e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16641013324260712 tensor([1.6641e-01, 1.1924e-01, 4.2800e-04, 4.6849e-01, 2.4543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1284862458705902 tensor([1.2849e-01, 9.9525e-02, 3.5803e-04, 4.3092e-01, 3.4071e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0835476964712143 tensor([1.0058e-05, 1.5361e-01, 7.6275e-01, 8.4077e-05, 8.3548e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.053630467504262924 tensor([0.0064, 0.3261, 0.0355, 0.0536, 0.5783], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04579503834247589 tensor([0.0019, 0.1254, 0.0390, 0.0458, 0.7879], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3660873472690582 tensor([0.0006, 0.3661, 0.1602, 0.0045, 0.4687], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1456528753042221 tensor([0.0016, 0.0439, 0.0304, 0.1457, 0.7784], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2695532441139221 tensor([0.0051, 0.2696, 0.0163, 0.0305, 0.6785], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28553518652915955 tensor([2.7597e-04, 3.3884e-01, 3.7266e-01, 2.6846e-03, 2.8554e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2381376475095749 tensor([2.3814e-01, 3.0430e-04, 1.6284e-08, 7.6080e-01, 7.5458e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.057795148342847824 tensor([7.4427e-04, 2.6854e-02, 1.3554e-02, 5.7795e-02, 9.0105e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0332241915166378 tensor([0.0332, 0.7047, 0.0047, 0.0215, 0.2359], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1026189848780632 tensor([1.1668e-06, 1.0262e-01, 8.8108e-01, 7.4652e-06, 1.6292e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06782547384500504 tensor([0.0678, 0.1771, 0.0024, 0.3159, 0.4368], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03090844117105007 tensor([3.3581e-09, 3.1255e-04, 9.6877e-01, 9.6196e-06, 3.0908e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4331602156162262 tensor([4.3316e-01, 7.2109e-02, 3.2306e-05, 4.4467e-01, 5.0033e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2588563561439514 tensor([0.0053, 0.0856, 0.0239, 0.2589, 0.6264], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05130498483777046 tensor([0.0021, 0.0705, 0.0121, 0.0513, 0.8640], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2606186866760254 tensor([0.0049, 0.2606, 0.0225, 0.0285, 0.6835], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02734379656612873 tensor([0.0273, 0.7283, 0.0045, 0.0182, 0.2216], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08316182345151901 tensor([4.6631e-06, 1.0462e-01, 8.1217e-01, 4.7325e-05, 8.3162e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17103137075901031 tensor([0.0093, 0.1049, 0.0153, 0.1710, 0.6995], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02652350254356861 tensor([0.0265, 0.4587, 0.0080, 0.0375, 0.4692], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08474116027355194 tensor([0.0847, 0.8247, 0.0008, 0.0181, 0.0716], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10257214307785034 tensor([2.1166e-07, 3.1255e-03, 8.9419e-01, 1.1653e-04, 1.0257e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026764584705233574 tensor([0.0268, 0.6863, 0.0073, 0.0343, 0.2454], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15506374835968018 tensor([1.5506e-01, 2.8845e-03, 1.4672e-06, 8.3490e-01, 7.1534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06821893155574799 tensor([0.0016, 0.0493, 0.0141, 0.0682, 0.8668], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18260571360588074 tensor([2.7415e-04, 1.8261e-01, 2.5218e-01, 6.3344e-03, 5.5861e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16020047664642334 tensor([1.3917e-04, 1.6020e-01, 4.9985e-01, 4.5843e-03, 3.3523e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10229780524969101 tensor([3.1645e-06, 3.0682e-02, 8.6675e-01, 2.6596e-04, 1.0230e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030249744653701782 tensor([1.8445e-07, 3.0250e-02, 9.5961e-01, 2.7024e-06, 1.0137e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.028681952506303787 tensor([1.0444e-04, 2.3121e-02, 1.7278e-01, 2.8682e-02, 7.7531e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2282431274652481 tensor([0.0011, 0.6867, 0.0824, 0.0015, 0.2282], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.35323286056518555 tensor([3.5323e-01, 2.7788e-03, 4.1497e-07, 6.3854e-01, 5.4432e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.273171603679657 tensor([0.0011, 0.0101, 0.0078, 0.2732, 0.7077], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03456714004278183 tensor([1.3759e-07, 7.9624e-03, 9.5746e-01, 1.3837e-05, 3.4567e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18439583480358124 tensor([0.0096, 0.1396, 0.0141, 0.1844, 0.6524], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07141884416341782 tensor([4.2221e-07, 9.0274e-03, 9.1948e-01, 7.6898e-05, 7.1419e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03528757765889168 tensor([0.0057, 0.3534, 0.0324, 0.0353, 0.5731], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13762934505939484 tensor([1.3763e-01, 8.1003e-01, 3.9560e-04, 1.3843e-02, 3.8101e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05603646859526634 tensor([3.4038e-06, 5.6036e-02, 8.5934e-01, 1.1799e-04, 8.4504e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25588101148605347 tensor([2.5588e-01, 2.8154e-03, 6.0727e-07, 7.3417e-01, 7.1289e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3130752444267273 tensor([0.0008, 0.3131, 0.0908, 0.0050, 0.5903], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13489167392253876 tensor([0.0035, 0.0844, 0.0256, 0.1349, 0.7516], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05939289554953575 tensor([1.0147e-07, 3.0343e-03, 9.3751e-01, 5.9092e-05, 5.9393e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16804912686347961 tensor([1.3129e-06, 1.6805e-01, 8.1527e-01, 4.0540e-06, 1.6673e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09204306453466415 tensor([4.4528e-07, 7.2957e-03, 9.0058e-01, 8.4171e-05, 9.2043e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3127250373363495 tensor([0.0009, 0.3127, 0.2153, 0.0122, 0.4589], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2160440981388092 tensor([2.1604e-01, 3.1157e-02, 3.3591e-05, 6.8613e-01, 6.6639e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08111584931612015 tensor([1.5551e-05, 2.2105e-01, 6.9772e-01, 1.0041e-04, 8.1116e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06702335923910141 tensor([0.0021, 0.0862, 0.0249, 0.0670, 0.8198], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03585786744952202 tensor([2.3420e-07, 3.5858e-02, 9.5237e-01, 3.2332e-06, 1.1766e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.055562108755111694 tensor([8.3195e-07, 5.5562e-02, 9.1540e-01, 9.0742e-06, 2.9023e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14280293881893158 tensor([0.0055, 0.1243, 0.0310, 0.1428, 0.6964], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.061781808733940125 tensor([0.0114, 0.2985, 0.0143, 0.0618, 0.6140], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07290508598089218 tensor([1.0283e-06, 1.6569e-02, 9.1041e-01, 1.1162e-04, 7.2905e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24579061567783356 tensor([0.0017, 0.6515, 0.0973, 0.0037, 0.2458], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05421885848045349 tensor([2.9632e-06, 1.0894e-01, 8.3682e-01, 2.0787e-05, 5.4219e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09469788521528244 tensor([0.0947, 0.4113, 0.0018, 0.1807, 0.3115], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 2, 1], [2, 3, 0, 4], [2, 0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 3, 0, 4], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4], [2, 3, 0], [2, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [0, 3, 1, 4], [3, 2, 0, 4], [0, 3, 2], [2, 0, 3], [2, 1, 0], [2, 4, 3, 1], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 4, 0], [0, 2, 1, 3], [2, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1, 3], [3, 0, 2, 4], [2, 3, 0, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 2], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 0, 3], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 0, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 2, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [2, 3, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 3, 0, 4], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 0], [0, 3, 1, 4], [0, 2, 3], [2, 1, 4, 0], [0, 3, 2, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0], [2, 0, 1, 3], [2, 3, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 0], [0, 3, 4], [0, 3, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [3, 0, 2, 4], [2, 4, 1, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4, 0], [2, 3, 4, 0], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 2], [0, 3, 4, 2], [0, 3, 1, 2], [2, 1, 4, 0], [2, 1, 4], [2, 4, 3, 0], [3, 0, 2, 4], [3, 0, 2, 4], [2, 3, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 3], [2, 0, 1, 3], [2, 3, 0, 4], [2, 0, 3], [0, 3, 2, 1], [2, 0, 1, 3], [2, 0, 3], [3, 0, 2, 4], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1, 3], [2, 3, 4, 0], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0], [2, 0, 1, 3], [2, 0, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 2, 4], [2, 1, 0, 3], [2, 0, 1, 3], [2, 3, 0], [3, 0, 2, 4], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0, 4], [2, 4, 3, 0], [3, 0, 2, 4], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [0, 2, 3, 1], [0, 3, 1, 4], [2, 3, 0, 4], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 4, 1], [2, 3, 0], [2, 1, 4], [2, 1, 0], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 3], [0, 2, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 0, 3], [2, 1, 0, 4], [2, 4, 3, 0], [0, 3, 1], [0, 3, 1, 4], [2, 1, 4], [0, 1, 3, 2], [2, 0, 3], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 0, 4, 1], [0, 3, 1, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 0, 3], [0, 2, 1, 3], [2, 1, 4], [0, 3, 2, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 0, 1, 3], [2, 4, 3, 1], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3, 4], [2, 4, 1, 0], [0, 1, 3, 2]]\n",
      "[[0], [2], [2], [3], [4], [4], [1], [4], [3], [4], [1], [1], [2], [3], [0, 3], [1, 4], [1, 4], [2], [3], [4], [2], [1], [1, 4], [1, 4], [3, 4], [0], [1, 4], [2], [3], [4], [1], [2], [2], [1, 3], [4], [1, 4], [4], [2], [0, 3], [4], [1], [1], [2], [3], [4], [1, 4], [1], [2], [3], [3], [0], [1], [2], [1, 4], [3], [0], [1, 2], [2], [4], [4], [1], [2], [2], [4], [3], [2], [1, 4], [2], [3], [4], [1], [2], [2], [3], [4], [1, 3], [2], [1, 4], [3], [4], [0], [2], [4], [3, 4], [4], [1, 4], [2], [2], [3], [3, 4], [1], [2], [2], [2], [3], [1, 4], [1, 2], [2], [4], [4], [1], [3], [2], [3], [3], [1], [1], [2], [3], [4], [1, 4], [1], [4], [3], [0, 3], [1], [1], [1], [1], [0], [0], [1], [2], [3], [4], [3], [1], [2], [1, 4], [4], [1], [1, 4], [4], [4], [1, 4], [1], [1, 4], [2], [0, 3], [4], [1], [2], [2], [3], [2], [0], [2], [2], [4], [0, 3], [0], [2], [4], [3, 4], [4], [1, 4], [2], [2], [4], [3], [1], [2], [1], [4], [4], [1, 4], [1], [2], [4], [3], [1], [1], [2], [4], [3], [1], [1], [2], [3], [4], [0], [4], [2], [4], [2], [1], [2], [4], [0], [3], [0], [2], [1, 4], [0, 3], [3, 4], [0], [2], [3], [4], [4], [0], [4], [2], [1, 4], [3], [1], [2, 4], [2], [0, 3], [4], [1, 4], [2], [4], [4], [2], [0], [2], [4], [3], [2], [0], [2], [2], [1, 4], [4], [0, 3], [1], [4], [3], [4], [2], [2], [2], [0], [3], [1], [1], [2], [4], [4], [0], [1], [2], [3], [1, 4], [3], [1], [1, 2], [3], [4]]\n",
      "NL_pred of 5th iteration [[0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 0, 3], [2, 1, 0, 3], [2, 4, 3, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 4, 0], [2, 0, 1, 3], [2, 1, 4, 0], [0, 3, 2, 4], [2, 1, 4, 0], [0, 3, 4], [2, 0, 1, 3], [2, 0, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 3, 2, 1], [2, 0, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 4], [2, 4, 1, 0], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 0], [3, 0, 2, 4], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4, 0], [0, 3, 2, 4], [2, 0, 3], [2, 0, 1, 3], [2, 0, 1, 3], [2, 0, 1, 3], [2, 3, 4, 0], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 1, 3], [2, 3, 4, 0], [0, 3, 2, 4], [2, 1, 0, 3], [2, 3, 0], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 3, 0], [2, 1, 4, 0], [2, 0, 1, 3], [0, 2, 3, 1], [0, 2, 3, 1], [0, 3, 1, 4], [0, 3, 4, 1], [0, 2, 1, 3], [0, 3, 1, 4], [2, 1, 0, 3], [0, 3, 1, 4], [2, 0, 3], [2, 4, 3, 0], [0, 3, 1], [2, 0, 1, 3], [0, 3, 1, 4], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 2, 4], [2, 0, 1, 3], [0, 3, 4, 1], [0, 3, 4, 1], [2, 0, 1, 3], [2, 0, 1, 3], [0, 3, 1, 4], [0, 3, 4], [2, 4, 1, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01435004813330514  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.01433739633787246  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.014313859598977225  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014281417642320906  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.01424210128330049  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.014197972558793567  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.014150812512352354  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.014102108421779815  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.014053155978520712  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.014005018132073539  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.013958442778814407  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.013913964941388085  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01387196211587815  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.013832662786756243  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.013796127977825347  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.013762336401712327  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.013731195813133604  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.013702548685527983  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.013676290001188005  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.013652195533116659  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 2, 1], [2, 3, 0, 4], [2, 0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 3, 0, 4], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4], [2, 3, 0], [2, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [0, 3, 1, 4], [3, 2, 0, 4], [0, 3, 2], [2, 0, 3], [2, 1, 0], [2, 4, 3, 1], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 4, 0], [0, 2, 1, 3], [2, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1, 3], [3, 0, 2, 4], [2, 3, 0, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 2], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 0, 3], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 0, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 2, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [2, 3, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 3, 0, 4], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 0], [0, 3, 1, 4], [0, 2, 3], [2, 1, 4, 0], [0, 3, 2, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0], [2, 0, 1, 3], [2, 3, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 0], [0, 3, 4], [0, 3, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [3, 0, 2, 4], [2, 4, 1, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4, 0], [2, 3, 4, 0], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 2], [0, 3, 4, 2], [0, 3, 1, 2], [2, 1, 4, 0], [2, 1, 4], [2, 4, 3, 0], [3, 0, 2, 4], [3, 0, 2, 4], [2, 3, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 3], [2, 0, 1, 3], [2, 3, 0, 4], [2, 0, 3], [0, 3, 2, 1], [2, 0, 1, 3], [2, 0, 3], [3, 0, 2, 4], [0, 3, 2], [0, 3, 1, 4], [2, 1, 4], [2, 0, 1, 3], [2, 3, 4, 0], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0], [2, 0, 1, 3], [2, 0, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 2, 4], [2, 1, 0, 3], [2, 0, 1, 3], [2, 3, 0], [3, 0, 2, 4], [0, 3, 1, 4], [0, 3, 2, 1], [2, 1, 0, 4], [2, 4, 3, 0], [3, 0, 2, 4], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [0, 2, 3, 1], [0, 3, 1, 4], [2, 3, 0, 4], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 4, 1], [2, 3, 0], [2, 1, 4], [2, 1, 0], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 3], [0, 2, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [2, 0, 3], [2, 1, 0, 4], [2, 4, 3, 0], [0, 3, 1], [0, 3, 1, 4], [2, 1, 4], [0, 1, 3, 2], [2, 0, 3], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 0, 4, 1], [0, 3, 1, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 0, 3], [0, 2, 1, 3], [2, 1, 4], [0, 3, 2, 4], [2, 0, 1, 3], [2, 1, 4, 0], [2, 0, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 2, 4], [0, 3, 1, 4], [2, 0, 1, 3], [2, 0, 1, 3], [2, 4, 3, 1], [0, 3, 2, 4], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3, 4], [2, 4, 1, 0], [0, 1, 3, 2]]\n",
      "POSITION :  [[0], [2], [2], [3], [4], [4], [1], [4], [3], [4], [1], [1], [2], [3], [0, 3], [1, 4], [1, 4], [2], [3], [4], [2], [1], [1, 4], [1, 4], [3, 4], [0], [1, 4], [2], [3], [4], [1], [2], [2], [1, 3], [4], [1, 4], [4], [2], [0, 3], [4], [1], [1], [2], [3], [4], [1, 4], [1], [2], [3], [3], [0], [1], [2], [1, 4], [3], [0], [1, 2], [2], [4], [4], [1], [2], [2], [4], [3], [2], [1, 4], [2], [3], [4], [1], [2], [2], [3], [4], [1, 3], [2], [1, 4], [3], [4], [0], [2], [4], [3, 4], [4], [1, 4], [2], [2], [3], [3, 4], [1], [2], [2], [2], [3], [1, 4], [1, 2], [2], [4], [4], [1], [3], [2], [3], [3], [1], [1], [2], [3], [4], [1, 4], [1], [4], [3], [0, 3], [1], [1], [1], [1], [0], [0], [1], [2], [3], [4], [3], [1], [2], [1, 4], [4], [1], [1, 4], [4], [4], [1, 4], [1], [1, 4], [2], [0, 3], [4], [1], [2], [2], [3], [2], [0], [2], [2], [4], [0, 3], [0], [2], [4], [3, 4], [4], [1, 4], [2], [2], [4], [3], [1], [2], [1], [4], [4], [1, 4], [1], [2], [4], [3], [1], [1], [2], [4], [3], [1], [1], [2], [3], [4], [0], [4], [2], [4], [2], [1], [2], [4], [0], [3], [0], [2], [1, 4], [0, 3], [3, 4], [0], [2], [3], [4], [4], [0], [4], [2], [1, 4], [3], [1], [2, 4], [2], [0, 3], [4], [1, 4], [2], [4], [4], [2], [0], [2], [4], [3], [2], [0], [2], [2], [1, 4], [4], [0, 3], [1], [4], [3], [4], [2], [2], [2], [0], [3], [1], [1], [2], [4], [4], [0], [1], [2], [3], [1, 4], [3], [1], [1, 2], [3], [4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.54\n",
      "tensor([0, 2, 2, 3, 4, 4, 1, 4, 3, 4, 1, 1, 2, 3, 2, 3, 4, 2, 1, 0, 2, 3, 4, 1,\n",
      "        2, 2, 4, 4, 2, 4, 1, 1, 2, 3, 4, 1, 2, 3, 3, 0, 1, 2, 3, 0, 2, 4, 4, 1,\n",
      "        2, 2, 4, 3, 2, 2, 3, 4, 1, 2, 2, 3, 4, 2, 3, 4, 0, 2, 4, 4, 2, 2, 3, 1,\n",
      "        2, 2, 2, 3, 2, 4, 4, 1, 3, 2, 3, 3, 1, 1, 2, 3, 4, 1, 4, 3, 1, 1, 1, 1,\n",
      "        0, 0, 1, 2, 3, 4, 3, 1, 2, 4, 1, 4, 4, 1, 2, 4, 1, 2, 2, 3, 2, 0, 2, 2,\n",
      "        4, 0, 2, 4, 4, 2, 2, 4, 3, 1, 2, 1, 4, 4, 1, 2, 4, 3, 1, 1, 2, 4, 3, 1,\n",
      "        1, 2, 3, 4, 0, 4, 2, 4, 2, 1, 2, 4, 0, 3, 0, 2, 0, 2, 3, 4, 4, 0, 4, 2,\n",
      "        3, 1, 2, 4, 2, 4, 4, 2, 0, 2, 4, 3, 2, 0, 2, 2, 4, 1, 4, 3, 4, 2, 2, 2,\n",
      "        0, 3, 1, 1, 2, 4, 4, 0, 1, 2, 3, 3, 1, 3, 4])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 98.06763285024155\n",
      "Epoch: 1  Loss: 99.03381642512076\n",
      "Epoch: 2  Loss: 99.51690821256038\n",
      "Epoch: 3  Loss: 99.51690821256038\n",
      "Epoch: 4  Loss: 100.0\n",
      "Epoch: 5  Loss: 100.0\n",
      "Epoch: 6  Loss: 100.0\n",
      "Epoch: 7  Loss: 100.0\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 7/15 [05:21<06:21, 47.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  52.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 6.047709548100829  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 1  Train_Loss: 3.9696843177080154  Accuracy on Support set:32.0\n",
      "Train_Epoch: 2  Train_Loss: 2.519518627822399  Accuracy on Support set:44.0\n",
      "Train_Epoch: 3  Train_Loss: 1.7194629372656345  Accuracy on Support set:60.0\n",
      "Train_Epoch: 4  Train_Loss: 1.2275536930933595  Accuracy on Support set:68.0\n",
      "Train_Epoch: 5  Train_Loss: 0.8855012712627649  Accuracy on Support set:84.0\n",
      "Train_Epoch: 6  Train_Loss: 0.6742849391698837  Accuracy on Support set:84.0\n",
      "Train_Epoch: 7  Train_Loss: 0.5430589547008275  Accuracy on Support set:88.0\n",
      "Train_Epoch: 8  Train_Loss: 0.4547511560097337  Accuracy on Support set:92.0\n",
      "Train_Epoch: 9  Train_Loss: 0.3877532196417451  Accuracy on Support set:96.0\n",
      "Train_Epoch: 10  Train_Loss: 0.33288570400327444  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.2896040218509734  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.2548280359245837  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.22617603901773692  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.20297265460714697  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.1837524876743555  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.16782057255506516  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.15391758177429438  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.14171318190172313  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.13123344618827104  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.12198803192004562  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.11399234361946582  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.10689292188733816  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.10049236766993999  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.09472700012847782  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.08953019082546235  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.08489933663979173  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.08064330887049437  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.07680453404784203  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.07327522177249193  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.07008365469053388  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.0670960802398622  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.06434768611565232  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.061835636757314204  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.05946949513629079  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.05730950063094497  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.05526226622983813  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.05336481848731637  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.051571061965078116  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.04990847943350673  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.048328064922243355  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.04684864554554224  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.045442761331796644  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.04410641849040985  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.042836987692862746  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.04165028868243098  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.04051982840523124  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.03944097965955734  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.038421253059059385  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.03745168745517731  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  37.333333333333336\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 2.8035094146616757e-05 tensor([4.1572e-02, 2.3293e-03, 2.8035e-05, 8.8127e-01, 7.4804e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0021776852663606405 tensor([0.1448, 0.7891, 0.0022, 0.0252, 0.0386], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010018308239523321 tensor([1.0018e-04, 3.0722e-02, 8.2448e-01, 1.0299e-03, 1.4367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.7790720552877133e-10 tensor([3.3822e-01, 1.4031e-04, 3.7791e-10, 6.6153e-01, 1.1927e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0018234929302707314 tensor([0.1037, 0.0799, 0.0018, 0.2748, 0.5397], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009148697718046606 tensor([0.0009, 0.1198, 0.4434, 0.0035, 0.4323], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005099644884467125 tensor([0.0051, 0.0433, 0.0421, 0.0588, 0.8507], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010375651763752103 tensor([0.3134, 0.5694, 0.0010, 0.0878, 0.0283], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7561872784076513e-09 tensor([4.8003e-01, 5.6299e-04, 2.7562e-09, 5.1917e-01, 2.3888e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014850680017843843 tensor([0.0180, 0.0126, 0.0015, 0.2501, 0.7178], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0019265076844021678 tensor([0.0019, 0.0085, 0.0364, 0.1401, 0.8131], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014174471609294415 tensor([0.0142, 0.3416, 0.1065, 0.0313, 0.5064], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005548506509512663 tensor([0.0192, 0.8902, 0.0359, 0.0055, 0.0491], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4520205695589539e-06 tensor([3.7260e-01, 8.3649e-03, 1.4520e-06, 6.1005e-01, 8.9918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.05478612164734e-06 tensor([1.2244e-04, 8.0548e-06, 7.5064e-05, 6.2963e-01, 3.7017e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.521636093268171e-05 tensor([5.1690e-01, 1.2070e-01, 9.5216e-05, 3.3790e-01, 2.4392e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.531798211042769e-05 tensor([6.6025e-01, 2.2905e-01, 3.5318e-05, 8.6055e-02, 2.4607e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7986329769191798e-06 tensor([2.7986e-06, 2.1800e-03, 9.2821e-01, 1.0491e-04, 6.9506e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007774773053824902 tensor([0.1148, 0.5365, 0.0078, 0.0840, 0.2569], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016316435358021408 tensor([1.3502e-01, 2.4076e-02, 1.6316e-04, 7.0187e-01, 1.3887e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017377380281686783 tensor([0.0281, 0.7073, 0.0550, 0.0174, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.787847046827665e-06 tensor([4.7878e-06, 2.0967e-02, 9.6545e-01, 1.1276e-05, 1.3565e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0022690813057124615 tensor([0.0023, 0.1010, 0.2492, 0.0171, 0.6304], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024726366624236107 tensor([0.0609, 0.3661, 0.0247, 0.0954, 0.4528], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6678128304192796e-05 tensor([1.9909e-01, 1.0783e-02, 1.6678e-05, 7.5015e-01, 3.9962e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009765899740159512 tensor([0.0098, 0.3917, 0.1875, 0.0221, 0.3890], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018290710868313909 tensor([0.0018, 0.2815, 0.3520, 0.0036, 0.3610], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004009373020380735 tensor([0.1060, 0.8161, 0.0040, 0.0168, 0.0571], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.728777306198026e-06 tensor([3.3406e-01, 6.8957e-03, 1.7288e-06, 6.4934e-01, 9.7066e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015397832030430436 tensor([0.0956, 0.8808, 0.0015, 0.0063, 0.0158], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6847688755206036e-07 tensor([9.0276e-01, 2.9588e-02, 1.6848e-07, 6.7198e-02, 4.5048e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00233713211491704 tensor([0.1096, 0.8228, 0.0023, 0.0161, 0.0491], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.540752993724254e-09 tensor([8.5408e-09, 3.0596e-04, 9.9870e-01, 1.9481e-07, 9.9169e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.684769988079097e-08 tensor([3.2999e-01, 1.2637e-03, 5.6848e-08, 6.6738e-01, 1.3674e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007680146023631096 tensor([0.0458, 0.8493, 0.0077, 0.0095, 0.0877], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012384451110847294 tensor([4.1942e-01, 1.1778e-01, 1.2384e-04, 3.6974e-01, 9.2944e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00019674266513902694 tensor([3.9828e-02, 9.5869e-01, 1.9674e-04, 3.7127e-04, 9.1161e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017379810742568225 tensor([1.7380e-04, 1.9628e-02, 5.3258e-01, 3.5567e-03, 4.4406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.439301952238964e-10 tensor([8.4614e-02, 3.3767e-05, 7.4393e-10, 9.1515e-01, 2.0549e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.1044902900466695e-05 tensor([4.2592e-04, 4.1045e-05, 1.4641e-04, 5.8753e-01, 4.1186e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5547362270117446e-07 tensor([9.1219e-01, 5.1636e-02, 2.5547e-07, 3.5915e-02, 2.6026e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003408092597965151 tensor([1.1370e-01, 3.0229e-02, 3.4081e-04, 6.1591e-01, 2.3982e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00605199858546257 tensor([0.0061, 0.0854, 0.1099, 0.0614, 0.7373], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.337907869166884e-08 tensor([8.2867e-01, 1.1061e-02, 9.3379e-08, 1.5974e-01, 5.2821e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6036028682719916e-06 tensor([1.6036e-06, 2.6179e-03, 9.6726e-01, 4.0546e-05, 3.0081e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0025066144298762083 tensor([0.0025, 0.3084, 0.4262, 0.0060, 0.2569], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.54993790097069e-06 tensor([3.5499e-06, 3.4399e-04, 4.9126e-01, 1.8575e-03, 5.0654e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.604159458831418e-06 tensor([9.6042e-06, 3.1235e-03, 8.2702e-01, 6.2495e-04, 1.6922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00017077449592761695 tensor([9.9991e-02, 1.6312e-02, 1.7077e-04, 6.7770e-01, 2.0582e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006279379012994468 tensor([6.2794e-04, 1.3285e-03, 1.0773e-02, 1.3108e-01, 8.5619e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015275382611434907 tensor([1.5275e-04, 2.5641e-02, 7.0207e-01, 2.4162e-03, 2.6972e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00031217827927321196 tensor([3.4665e-01, 1.6141e-01, 3.1218e-04, 3.9405e-01, 9.7581e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014706887304782867 tensor([0.0147, 0.1415, 0.0823, 0.1092, 0.6523], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002734563546255231 tensor([0.0292, 0.0224, 0.0027, 0.4700, 0.4757], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002228710101917386 tensor([0.0022, 0.0078, 0.0121, 0.0824, 0.8955], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.052643610639393e-10 tensor([9.7575e-01, 2.8216e-03, 2.0526e-10, 2.1422e-02, 6.5986e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.304488811612828e-06 tensor([5.6637e-01, 4.2778e-01, 4.3045e-06, 4.8844e-03, 9.6478e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005592058878391981 tensor([0.0006, 0.0416, 0.4771, 0.0092, 0.4715], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00042377490899525583 tensor([1.1866e-01, 4.0436e-02, 4.2377e-04, 5.1322e-01, 3.2726e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.274214031989686e-05 tensor([3.2742e-05, 3.2721e-03, 3.6757e-01, 2.1463e-03, 6.2698e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014239199459552765 tensor([0.0225, 0.7514, 0.0630, 0.0142, 0.1490], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.36698984837858e-06 tensor([1.7329e-01, 8.2612e-01, 7.3670e-06, 4.1206e-04, 1.6763e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.478813320218933e-09 tensor([4.4788e-09, 1.0449e-04, 9.9868e-01, 3.0764e-07, 1.2106e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9838871373755183e-09 tensor([8.6112e-01, 2.4730e-03, 2.9839e-09, 1.3629e-01, 1.1934e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.814266377361491e-05 tensor([9.8143e-05, 1.4495e-02, 6.1893e-01, 2.1943e-03, 3.6428e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00037051361869089305 tensor([1.2611e-01, 3.5618e-02, 3.7051e-04, 6.9686e-01, 1.4104e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002491370076313615 tensor([0.0476, 0.9315, 0.0025, 0.0040, 0.0144], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.823916863941122e-05 tensor([1.8239e-05, 1.2199e-02, 9.1694e-01, 1.8468e-04, 7.0658e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.090979690678068e-07 tensor([7.8476e-01, 2.7330e-02, 8.0910e-07, 1.8630e-01, 1.6102e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002825109288096428 tensor([0.0028, 0.0748, 0.1950, 0.0358, 0.6916], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7880124048019752e-08 tensor([9.4702e-01, 1.2608e-02, 1.7880e-08, 4.0284e-02, 9.1116e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002615093835629523 tensor([1.7877e-01, 8.0886e-01, 2.6151e-04, 5.3706e-03, 6.7367e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004430447705090046 tensor([0.0044, 0.1929, 0.3245, 0.0198, 0.4584], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002237761509604752 tensor([4.3306e-01, 5.1700e-01, 2.2378e-04, 3.7428e-02, 1.2289e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007095625041984022 tensor([0.0007, 0.0492, 0.4209, 0.0106, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9280638500495115e-06 tensor([3.1161e-02, 5.8225e-04, 2.9281e-06, 9.4117e-01, 2.7089e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002454504370689392 tensor([0.0895, 0.0874, 0.0025, 0.3660, 0.4547], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.7803957790602e-06 tensor([6.7804e-06, 1.3712e-02, 9.6346e-01, 3.4920e-05, 2.2786e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.438645184265624e-07 tensor([3.1732e-01, 4.9389e-03, 7.4386e-07, 6.6979e-01, 7.9508e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00956222228705883 tensor([0.0637, 0.1918, 0.0096, 0.1486, 0.5863], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8419364639376e-07 tensor([2.8419e-07, 1.9187e-03, 9.9275e-01, 2.8411e-06, 5.3279e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007376788649708033 tensor([0.1406, 0.3872, 0.0074, 0.2025, 0.2623], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2666415588901145e-06 tensor([2.2666e-06, 1.2800e-03, 8.8779e-01, 2.0634e-04, 1.1072e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013700379058718681 tensor([0.0014, 0.0024, 0.0053, 0.1346, 0.8562], grad_fn=<SoftmaxBackward0>)\n",
      "1 5.235114076640457e-05 tensor([1.0152e-04, 5.2351e-05, 1.4385e-03, 1.8512e-01, 8.1329e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00022764934692531824 tensor([2.1828e-01, 4.7076e-02, 2.2765e-04, 6.7701e-01, 5.7402e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5368615297338692e-06 tensor([4.6126e-01, 5.3632e-01, 2.5369e-06, 2.0607e-03, 3.5703e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9044058824645447e-10 tensor([2.9044e-10, 1.6883e-05, 9.9925e-01, 6.2732e-08, 7.3298e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011291209608316422 tensor([0.0580, 0.8701, 0.0113, 0.0118, 0.0489], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.818196081643691e-07 tensor([1.1354e-03, 7.7420e-06, 7.8182e-07, 9.6582e-01, 3.3035e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004612431861460209 tensor([0.0046, 0.4900, 0.3178, 0.0047, 0.1829], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011784857138991356 tensor([0.0597, 0.8022, 0.0118, 0.0200, 0.1064], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002255630912259221 tensor([0.0023, 0.0985, 0.2888, 0.0187, 0.5918], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.029419828206300735 tensor([0.0310, 0.6016, 0.0553, 0.0294, 0.2826], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009444578317925334 tensor([0.0009, 0.0164, 0.0803, 0.0202, 0.8822], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003421240486204624 tensor([0.0831, 0.8854, 0.0034, 0.0103, 0.0178], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000759877439122647 tensor([1.3229e-02, 9.6509e-01, 7.2562e-03, 7.5988e-04, 1.3667e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001876379013992846 tensor([0.0019, 0.1132, 0.3342, 0.0122, 0.5384], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5065997338581383e-09 tensor([1.5760e-01, 1.0045e-04, 1.5066e-09, 8.4210e-01, 2.0127e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.445194695610553e-05 tensor([4.7193e-04, 2.4452e-05, 5.0963e-05, 7.2866e-01, 2.7079e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.927599827122322e-08 tensor([9.4337e-01, 3.7598e-02, 6.9276e-08, 1.8851e-02, 1.8228e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.397149546595756e-05 tensor([1.2325e-01, 8.7601e-01, 2.3971e-05, 4.4845e-04, 2.6402e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005009512882679701 tensor([0.0050, 0.1761, 0.2652, 0.0240, 0.5297], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.433457157418161e-07 tensor([8.1485e-01, 1.1395e-02, 1.4335e-07, 1.7304e-01, 7.1166e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008237734436988831 tensor([0.0008, 0.0296, 0.2332, 0.0186, 0.7178], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000779458845499903 tensor([0.3505, 0.2779, 0.0008, 0.2634, 0.1074], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008925859583541751 tensor([1.1997e-02, 9.6935e-01, 7.3134e-03, 8.9259e-04, 1.0443e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.505200947984122e-05 tensor([3.5052e-05, 3.5209e-04, 3.0347e-02, 1.1879e-02, 9.5739e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6354148257846646e-08 tensor([8.5992e-01, 5.6172e-03, 1.6354e-08, 1.3402e-01, 4.4424e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004163149744272232 tensor([0.0042, 0.0064, 0.0052, 0.1199, 0.8643], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001967446645721793 tensor([0.0025, 0.5041, 0.3772, 0.0020, 0.1142], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.311032845791487e-07 tensor([5.6685e-01, 9.9157e-03, 7.3110e-07, 4.1573e-01, 7.5002e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.5727700253265198e-09 tensor([1.5728e-09, 1.3257e-05, 9.9555e-01, 1.0879e-06, 4.4386e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007034436566755176 tensor([0.3561, 0.3298, 0.0007, 0.1908, 0.1226], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017391625442542136 tensor([1.7392e-04, 1.4345e-03, 3.3204e-02, 3.0665e-02, 9.3452e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.665903907152824e-05 tensor([5.2463e-01, 9.0280e-02, 4.6659e-05, 3.6578e-01, 1.9262e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5588587959646247e-07 tensor([1.0087e-01, 6.1685e-04, 2.5589e-07, 8.9345e-01, 5.0555e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.808442887835554e-07 tensor([7.8952e-01, 2.5961e-02, 7.8084e-07, 1.8231e-01, 2.2127e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015022692270576954 tensor([0.0905, 0.2267, 0.0150, 0.2699, 0.3978], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000777592184022069 tensor([1.0631e-03, 7.7759e-04, 2.0393e-03, 2.0091e-01, 7.9521e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0037172706797719 tensor([0.0238, 0.0204, 0.0037, 0.7046, 0.2475], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001765994355082512 tensor([0.0018, 0.0236, 0.0412, 0.0245, 0.9090], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008414890617132187 tensor([0.0008, 0.2622, 0.6166, 0.0009, 0.1195], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.837983394485491e-07 tensor([4.6509e-01, 6.6927e-03, 5.8380e-07, 5.2392e-01, 4.2889e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.986178727354854e-05 tensor([2.7316e-01, 4.4378e-02, 8.9862e-05, 5.8596e-01, 9.6407e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001098854627343826 tensor([5.5075e-01, 2.0245e-01, 1.0989e-04, 2.0883e-01, 3.7865e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004230928316246718 tensor([4.4851e-02, 9.5161e-01, 4.2309e-04, 7.8685e-04, 2.3337e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012227615807205439 tensor([0.1915, 0.1201, 0.0012, 0.3856, 0.3016], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9890698865765444e-08 tensor([5.1273e-01, 1.6579e-03, 1.9891e-08, 4.8479e-01, 8.1844e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.2704633011016995e-05 tensor([7.9304e-02, 4.5332e-03, 2.2705e-05, 8.5057e-01, 6.5566e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007816582801751792 tensor([0.0425, 0.0149, 0.0008, 0.6483, 0.2934], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1940339383897935e-08 tensor([9.4077e-01, 9.3771e-03, 1.1940e-08, 4.9757e-02, 9.7028e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.2502591668380774e-08 tensor([4.2503e-08, 3.6475e-04, 9.9576e-01, 1.6178e-06, 3.8772e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.364210664993152e-06 tensor([6.7354e-01, 4.6653e-02, 5.3642e-06, 2.7264e-01, 7.1615e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004058642080053687 tensor([4.0586e-04, 4.7281e-02, 4.1132e-01, 2.8323e-03, 5.3816e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.764246330779542e-09 tensor([9.7143e-01, 1.6031e-02, 4.7642e-09, 1.2513e-02, 2.3733e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009560376638546586 tensor([0.0090, 0.9531, 0.0222, 0.0010, 0.0148], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.344643381657079e-05 tensor([7.3446e-05, 4.6209e-02, 8.9689e-01, 2.7277e-04, 5.6555e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.8429334381362423e-07 tensor([1.7339e-01, 1.3907e-03, 3.8429e-07, 8.1890e-01, 6.3235e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0073013645596802235 tensor([0.1585, 0.4011, 0.0073, 0.2096, 0.2235], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008081529289484024 tensor([0.0081, 0.3582, 0.2171, 0.0180, 0.3987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011038719676434994 tensor([0.0024, 0.6559, 0.2823, 0.0011, 0.0583], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.411036883946508e-05 tensor([6.4110e-05, 5.7652e-02, 8.7493e-01, 1.9082e-04, 6.7163e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.301171057681131e-09 tensor([1.1591e-01, 1.1975e-04, 5.3012e-09, 8.8327e-01, 6.9770e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2480378902400844e-05 tensor([2.2480e-05, 4.3726e-04, 7.6532e-02, 7.8000e-03, 9.1521e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8699209931583027e-06 tensor([7.2253e-01, 2.6957e-01, 1.8699e-06, 7.6691e-03, 2.2424e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8584092686069198e-05 tensor([1.3516e-01, 7.7133e-03, 1.8584e-05, 7.6384e-01, 9.3263e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1572338962650974e-06 tensor([1.1572e-06, 1.0984e-04, 5.6065e-01, 2.0755e-03, 4.3716e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.69072318385588e-05 tensor([1.1824e-01, 5.9740e-03, 1.6907e-05, 8.0329e-01, 7.2477e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016067804244812578 tensor([7.5260e-03, 9.8415e-01, 6.0966e-03, 1.6068e-04, 2.0714e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00015850306954234838 tensor([3.1551e-01, 6.5834e-02, 1.5850e-04, 5.2893e-01, 8.9570e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.601678659175377e-07 tensor([8.6453e-01, 1.2715e-01, 2.6017e-07, 8.0585e-03, 2.6784e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.009117175549306e-10 tensor([7.0091e-10, 5.8040e-05, 9.9946e-01, 4.5670e-08, 4.8650e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4025434419705505e-10 tensor([1.7880e-01, 4.8848e-05, 2.4025e-10, 8.2103e-01, 1.2058e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00015052773233037442 tensor([7.3081e-04, 1.5053e-04, 4.2668e-04, 3.7373e-01, 6.2496e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00262777809984982 tensor([0.0026, 0.2447, 0.3381, 0.0066, 0.4080], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005385133903473616 tensor([0.0058, 0.5784, 0.1561, 0.0054, 0.2543], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.932006175157767e-08 tensor([5.9320e-08, 1.4306e-04, 9.8477e-01, 1.1060e-05, 1.5077e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.812360548887227e-07 tensor([2.4117e-01, 1.7335e-03, 2.8124e-07, 7.5236e-01, 4.7351e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001645330572500825 tensor([0.1851, 0.2405, 0.0016, 0.1512, 0.4215], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016112728917505592 tensor([3.8580e-01, 5.6325e-01, 1.6113e-04, 3.2044e-02, 1.8744e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006491014501079917 tensor([7.6720e-02, 9.1112e-01, 6.4910e-04, 2.6358e-03, 8.8742e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001973184524103999 tensor([0.0020, 0.2756, 0.3744, 0.0038, 0.3442], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4622102500870824e-05 tensor([3.1963e-01, 2.7488e-02, 2.4622e-05, 6.1276e-01, 4.0096e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010048800613731146 tensor([0.0540, 0.0322, 0.0010, 0.4128, 0.5000], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005503643187694252 tensor([5.5036e-04, 1.0939e-01, 6.0548e-01, 2.7713e-03, 2.8180e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.892357537755743e-05 tensor([3.5932e-01, 4.4684e-02, 4.8924e-05, 5.3606e-01, 5.9881e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012399458326399326 tensor([0.0124, 0.1467, 0.0704, 0.0580, 0.7125], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.270956924301572e-05 tensor([5.3426e-01, 9.7994e-02, 4.2710e-05, 3.5036e-01, 1.7336e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1133945321262217e-07 tensor([1.1134e-07, 1.1703e-03, 9.9701e-01, 1.4581e-06, 1.8173e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004951247829012573 tensor([4.9512e-04, 6.2171e-02, 5.8209e-01, 5.6963e-03, 3.4954e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.293828599813196e-09 tensor([9.7984e-01, 1.2004e-02, 1.2938e-09, 8.1320e-03, 2.0957e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0342722564237192e-06 tensor([2.0343e-06, 2.5882e-03, 9.7637e-01, 5.7051e-05, 2.0981e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.627477375087949e-09 tensor([9.6378e-01, 5.5232e-03, 1.6275e-09, 3.0659e-02, 3.5905e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004007346578873694 tensor([5.2959e-04, 4.0073e-04, 2.3957e-03, 1.7803e-01, 8.1865e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.505740112241256e-08 tensor([9.2907e-01, 5.7506e-02, 8.5057e-08, 1.3304e-02, 1.2106e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017948145978152752 tensor([0.0296, 0.9583, 0.0033, 0.0018, 0.0071], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.613764708163217e-05 tensor([6.6138e-05, 2.2655e-02, 7.3792e-01, 8.1518e-04, 2.3855e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.467999138389132e-07 tensor([6.4097e-02, 3.2723e-04, 2.4680e-07, 9.2982e-01, 5.7567e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0479897355253343e-05 tensor([1.0480e-05, 4.8341e-05, 1.3561e-02, 2.3184e-02, 9.6320e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.5640263124369085e-06 tensor([7.6547e-01, 2.0861e-01, 4.5640e-06, 2.4831e-02, 1.0899e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.815633918857202e-05 tensor([1.9700e-03, 1.4338e-04, 8.8156e-05, 7.6101e-01, 2.3679e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.374879123529411e-10 tensor([8.3749e-10, 2.4801e-05, 9.9867e-01, 1.7136e-07, 1.3004e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005948802572675049 tensor([5.9488e-04, 1.2361e-02, 1.3698e-01, 2.2213e-02, 8.2785e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.1211689424235374e-05 tensor([5.1212e-05, 3.5520e-04, 1.6678e-02, 1.2223e-02, 9.7069e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9094068193226121e-07 tensor([1.7179e-01, 1.0936e-03, 1.9094e-07, 8.2399e-01, 3.1261e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.371110572445104e-08 tensor([6.8348e-01, 4.5764e-03, 6.3711e-08, 3.1010e-01, 1.8403e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0031579758506268263 tensor([0.0032, 0.0352, 0.0750, 0.0665, 0.8202], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0064460476860404015 tensor([0.1443, 0.2662, 0.0064, 0.3026, 0.2805], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.690817043709103e-06 tensor([8.5257e-01, 1.1713e-01, 1.6908e-06, 2.9239e-02, 1.0582e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018915403634309769 tensor([0.0019, 0.0031, 0.0072, 0.1970, 0.7908], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.797079837881029e-05 tensor([7.2730e-04, 7.8438e-01, 2.0187e-01, 9.7971e-05, 1.2924e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.9111514859559975e-07 tensor([1.9112e-07, 7.6002e-04, 9.9203e-01, 5.4014e-06, 7.2013e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008978525293059647 tensor([0.0019, 0.0009, 0.0015, 0.3597, 0.6360], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005597235867753625 tensor([0.3573, 0.2567, 0.0006, 0.1922, 0.1933], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.662309009504042e-09 tensor([7.5075e-01, 2.4103e-03, 7.6623e-09, 2.4663e-01, 2.1367e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010173268674407154 tensor([9.4682e-03, 9.8829e-01, 1.1372e-03, 1.0173e-04, 1.0061e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.553642262791982e-07 tensor([7.5536e-07, 7.8608e-04, 9.3774e-01, 5.7012e-05, 6.1416e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0034821631852537394 tensor([0.0800, 0.8844, 0.0035, 0.0080, 0.0242], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5491294107050635e-05 tensor([2.1966e-05, 1.5491e-05, 1.1657e-03, 6.3800e-02, 9.3500e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.126493694111559e-08 tensor([9.3848e-01, 3.5238e-02, 9.1265e-08, 2.6131e-02, 1.5224e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.001403226538969e-07 tensor([4.0618e-02, 2.4949e-04, 4.0014e-07, 9.4841e-01, 1.0722e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.3782450827857247e-06 tensor([3.3782e-06, 3.6295e-03, 9.6825e-01, 8.7552e-05, 2.8031e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012801858596503735 tensor([0.0258, 0.0647, 0.0128, 0.2560, 0.6407], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007985749281942844 tensor([0.0194, 0.0335, 0.0080, 0.2334, 0.7057], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009609941043891013 tensor([0.1974, 0.1355, 0.0010, 0.3267, 0.3395], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003379289119038731 tensor([6.6288e-02, 9.2913e-01, 3.3793e-04, 1.3407e-03, 2.9008e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020265890285372734 tensor([0.0529, 0.8224, 0.0206, 0.0203, 0.0838], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.110656729812035e-06 tensor([7.6249e-01, 8.4000e-02, 5.1107e-06, 1.4870e-01, 4.7982e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.816584570718987e-07 tensor([7.8166e-07, 2.2197e-03, 9.8198e-01, 1.2676e-05, 1.5784e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.4460445072472794e-06 tensor([1.4460e-06, 4.3123e-03, 9.8270e-01, 1.4708e-05, 1.2971e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.826099317753688e-05 tensor([7.8261e-05, 2.4363e-04, 1.4764e-02, 5.0100e-02, 9.3481e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00021073145035188645 tensor([4.5156e-01, 3.9931e-01, 2.1073e-04, 1.0418e-01, 4.4745e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4496336575575697e-07 tensor([2.4095e-01, 1.3669e-03, 1.4496e-07, 7.5227e-01, 5.4150e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00039733422454446554 tensor([9.3777e-03, 2.3187e-03, 3.9733e-04, 4.1984e-01, 5.6807e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.439274147851393e-05 tensor([2.0109e-01, 2.2562e-02, 6.4393e-05, 7.3378e-01, 4.2503e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.08618431922514e-05 tensor([4.5138e-02, 9.5434e-01, 5.0862e-05, 1.6499e-04, 3.0546e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2291710760337082e-08 tensor([1.2292e-08, 4.1720e-05, 9.9102e-01, 5.1690e-06, 8.9290e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02060633897781372 tensor([0.0502, 0.6754, 0.0206, 0.0301, 0.2237], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005332363303750753 tensor([0.0053, 0.0378, 0.0271, 0.0571, 0.8726], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.664529680871055e-07 tensor([3.0497e-01, 3.8920e-03, 5.6645e-07, 6.8487e-01, 6.2771e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.4373260859865695e-07 tensor([8.1694e-01, 1.7853e-01, 3.4373e-07, 4.4043e-03, 1.2907e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.11014623043593e-05 tensor([5.4669e-01, 4.1829e-01, 5.1101e-05, 3.2368e-02, 2.5937e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4532853128912393e-05 tensor([7.3118e-01, 1.9027e-01, 1.4533e-05, 7.1289e-02, 7.2554e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001451146963518113 tensor([1.4511e-04, 5.6097e-03, 2.0602e-01, 1.0924e-02, 7.7730e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8983097334057675e-06 tensor([8.3780e-01, 1.0376e-01, 1.8983e-06, 5.7160e-02, 1.2814e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005950641818344593 tensor([0.0060, 0.4232, 0.2047, 0.0091, 0.3571], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7309246419804367e-08 tensor([2.7309e-08, 3.2532e-04, 9.9722e-01, 9.3178e-07, 2.4565e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5799174434505403e-05 tensor([2.5001e-02, 1.0543e-03, 1.5799e-05, 9.1814e-01, 5.5790e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6383852496915097e-08 tensor([1.6384e-08, 2.2290e-05, 9.4438e-01, 1.9784e-05, 5.5578e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0724241292336956e-05 tensor([3.9942e-01, 5.9672e-01, 1.0724e-05, 3.0743e-03, 7.7376e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.373146111902315e-06 tensor([7.0251e-01, 5.0183e-02, 4.3731e-06, 2.3727e-01, 1.0036e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1283306776022073e-05 tensor([1.1283e-05, 1.9475e-02, 9.6619e-01, 4.9478e-05, 1.4272e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0968641753493777e-11 tensor([5.6937e-01, 8.7314e-05, 2.0969e-11, 4.3053e-01, 1.3809e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9991008230135776e-05 tensor([1.7625e-04, 1.9991e-05, 1.5683e-04, 5.4850e-01, 4.5115e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007063182536512613 tensor([0.0071, 0.4181, 0.2174, 0.0131, 0.3443], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012759589590132236 tensor([0.0279, 0.0782, 0.0128, 0.2138, 0.6673], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.0927245120437874e-07 tensor([3.0927e-07, 1.2011e-03, 9.9081e-01, 6.0176e-06, 7.9818e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.767004499472982e-11 tensor([5.5511e-01, 1.2226e-04, 4.7670e-11, 4.4474e-01, 2.3216e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.826214353670366e-05 tensor([8.8618e-04, 4.8262e-05, 5.2113e-05, 6.8385e-01, 3.1516e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005230876267887652 tensor([2.3618e-01, 9.1005e-02, 5.2309e-04, 5.2711e-01, 1.4518e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.532727755053202e-06 tensor([4.9797e-01, 4.9701e-01, 5.5327e-06, 4.2462e-03, 7.6636e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.918483812043632e-08 tensor([9.9185e-08, 2.6305e-03, 9.9657e-01, 4.3283e-07, 7.9530e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4987921278807335e-05 tensor([6.7369e-01, 2.3140e-01, 2.4988e-05, 8.6830e-02, 8.0492e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00019794654508586973 tensor([1.9795e-04, 2.3152e-03, 3.2643e-02, 1.6870e-02, 9.4797e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3444505057857992e-11 tensor([9.2721e-01, 4.2036e-04, 1.3445e-11, 7.2371e-02, 3.0250e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004259324341546744 tensor([4.2593e-04, 1.6452e-02, 2.0170e-01, 1.5282e-02, 7.6614e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4492672423548356e-07 tensor([2.4493e-07, 1.1558e-03, 9.9145e-01, 4.8246e-06, 7.3907e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5729979168099817e-06 tensor([6.9144e-01, 2.4984e-02, 1.5730e-06, 2.8028e-01, 3.2932e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1315872683326234e-07 tensor([1.1316e-07, 7.3411e-05, 8.3196e-01, 7.5594e-05, 1.6789e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [2], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [3], [2], [1], [2], [2], [0], [2], [2], [3], [0], [0], [2], [2], [0], [0], [2], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [1], [2], [2], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [2], [2], [0], [2], [0], [3], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [0], [2], [0], [0], [1], [2], [2], [0], [2], [2], [0], [2], [0], [3], [0], [2], [3], [0], [2], [1], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [3], [2], [0], [2], [0], [2], [2], [2], [2], [1], [2], [0], [0], [2], [2], [2], [2], [2], [2], [2], [2], [2], [0], [2], [0], [2], [3], [0], [2], [2], [0], [3], [0], [2], [0], [2], [2], [0], [2], [3], [2], [2], [0], [2], [1], [0], [3], [0], [2], [2], [2], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [1], [2], [3], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [2], [0], [3], [0], [1], [2], [2], [3], [0], [2], [1], [2], [2], [0], [2], [2], [2], [2], [3], [2], [0], [0], [0], [2], [2], [2], [2], [2], [0], [2], [0], [2], [2], [2], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [1], [0], [2], [0], [2], [1], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0]]\n",
      "[[0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [2], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [3], [2], [1], [2], [2], [0], [2], [2], [3], [0], [0], [2], [2], [0], [0], [2], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [1], [2], [2], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [2], [2], [0], [2], [0], [3], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [0], [2], [0], [0], [1], [2], [2], [0], [2], [2], [0], [2], [0], [3], [0], [2], [3], [0], [2], [1], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [3], [2], [0], [2], [0], [2], [2], [2], [2], [1], [2], [0], [0], [2], [2], [2], [2], [2], [2], [2], [2], [2], [0], [2], [0], [2], [3], [0], [2], [2], [0], [3], [0], [2], [0], [2], [2], [0], [2], [3], [2], [2], [0], [2], [1], [0], [3], [0], [2], [2], [2], [2], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [2], [1], [2], [3], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [2], [0], [3], [0], [1], [2], [2], [3], [0], [2], [1], [2], [2], [0], [2], [2], [2], [2], [3], [2], [0], [0], [0], [2], [2], [2], [2], [2], [0], [2], [0], [2], [2], [2], [2], [0], [2], [0], [0], [2], [0], [2], [2], [0], [2], [1], [0], [2], [0], [2], [1], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004438243865966797  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004438241481781006  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004438239574432373  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004438235759735108  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004438230991363526  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004438225269317627  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004438218593597412  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004438211441040039  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004438203334808349  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004438194274902343  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004438186168670654  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004438176155090332  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00443816614151001  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004438156127929687  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0044381461143493655  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004438135147094726  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004438124179840088  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004438112735748291  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0044381022453308105  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004438090801239014  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.0023749058600515127 tensor([4.2638e-02, 2.3749e-03, 2.7472e-05, 8.8096e-01, 7.3999e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.024970440194010735 tensor([0.1463, 0.7889, 0.0021, 0.0250, 0.0377], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010615369537845254 tensor([1.0568e-04, 3.1989e-02, 8.2113e-01, 1.0615e-03, 1.4572e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00011780647764680907 tensor([3.4332e-01, 1.4261e-04, 3.7221e-10, 6.5642e-01, 1.1781e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08142571151256561 tensor([0.1067, 0.0814, 0.0018, 0.2760, 0.5342], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0035529339220374823 tensor([0.0010, 0.1235, 0.4383, 0.0036, 0.4336], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.041472166776657104 tensor([0.0053, 0.0446, 0.0415, 0.0595, 0.8492], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027613207697868347 tensor([0.3161, 0.5686, 0.0010, 0.0867, 0.0276], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002346205001231283 tensor([4.8600e-01, 5.6911e-04, 2.6917e-09, 5.1319e-01, 2.3462e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012919439934194088 tensor([0.0186, 0.0129, 0.0015, 0.2527, 0.7143], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0087434696033597 tensor([0.0020, 0.0087, 0.0358, 0.1419, 0.8115], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0314522422850132 tensor([0.0145, 0.3477, 0.1044, 0.0315, 0.5020], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01949469745159149 tensor([0.0195, 0.8924, 0.0346, 0.0055, 0.0480], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008437643758952618 tensor([3.7831e-01, 8.4376e-03, 1.4097e-06, 6.0444e-01, 8.8177e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.372912659775466e-05 tensor([1.2620e-04, 8.2401e-06, 7.3729e-05, 6.3259e-01, 3.6720e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02380494773387909 tensor([5.2184e-01, 1.2105e-01, 9.1951e-05, 3.3321e-01, 2.3805e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023905782029032707 tensor([6.6308e-01, 2.2842e-01, 3.3954e-05, 8.4562e-02, 2.3906e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001080987713066861 tensor([2.9522e-06, 2.2729e-03, 9.2696e-01, 1.0810e-04, 7.0659e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08356717228889465 tensor([0.1169, 0.5403, 0.0075, 0.0836, 0.2517], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024443311616778374 tensor([1.3812e-01, 2.4443e-02, 1.5913e-04, 7.0035e-01, 1.3692e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.028499485924839973 tensor([0.0285, 0.7121, 0.0533, 0.0173, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1585820175241679e-05 tensor([5.0253e-06, 2.1771e-02, 9.6444e-01, 1.1586e-05, 1.3770e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017336957156658173 tensor([0.0024, 0.1039, 0.2459, 0.0173, 0.6305], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06258054822683334 tensor([0.0626, 0.3714, 0.0240, 0.0957, 0.4464], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010940490290522575 tensor([2.0306e-01, 1.0940e-02, 1.6301e-05, 7.4660e-01, 3.9390e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022189298644661903 tensor([0.0100, 0.3986, 0.1834, 0.0222, 0.3857], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003670123405754566 tensor([0.0019, 0.2884, 0.3459, 0.0037, 0.3601], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01663713902235031 tensor([0.1072, 0.8166, 0.0039, 0.0166, 0.0557], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0069686551578342915 tensor([3.3964e-01, 6.9687e-03, 1.6803e-06, 6.4387e-01, 9.5255e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006191898137331009 tensor([0.0967, 0.8803, 0.0015, 0.0062, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00043859443394467235 tensor([9.0417e-01, 2.9516e-02, 1.6275e-07, 6.5880e-02, 4.3859e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01588570326566696 tensor([0.1109, 0.8231, 0.0022, 0.0159, 0.0479], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9835478326513112e-07 tensor([8.8792e-09, 3.1560e-04, 9.9868e-01, 1.9835e-07, 1.0013e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012808829778805375 tensor([3.3530e-01, 1.2809e-03, 5.5626e-08, 6.6207e-01, 1.3462e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00943585205823183 tensor([0.0465, 0.8510, 0.0074, 0.0094, 0.0857], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09089671075344086 tensor([4.2479e-01, 1.1841e-01, 1.1981e-04, 3.6578e-01, 9.0897e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003698906803037971 tensor([4.0288e-02, 9.5825e-01, 1.9031e-04, 3.6989e-04, 8.9665e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036389492452144623 tensor([1.8189e-04, 2.0308e-02, 5.2800e-01, 3.6389e-03, 4.4787e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.4518354368628934e-05 tensor([8.6626e-02, 3.4518e-05, 7.3321e-10, 9.1314e-01, 2.0364e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014384815585799515 tensor([4.3889e-04, 4.1966e-05, 1.4385e-04, 5.9068e-01, 4.0870e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002543306036386639 tensor([9.1303e-01, 5.1440e-02, 2.4716e-07, 3.5276e-02, 2.5433e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030772116035223007 tensor([1.1653e-01, 3.0772e-02, 3.3327e-04, 6.1547e-01, 2.3690e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06203540042042732 tensor([0.0063, 0.0878, 0.1084, 0.0620, 0.7355], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005142402951605618 tensor([8.3150e-01, 1.1028e-02, 8.9970e-08, 1.5696e-01, 5.1424e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.1811756091192365e-05 tensor([1.6916e-06, 2.7291e-03, 9.6662e-01, 4.1812e-05, 3.0606e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006074605975300074 tensor([0.0026, 0.3157, 0.4193, 0.0061, 0.2563], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00035528530133888125 tensor([3.7026e-06, 3.5529e-04, 4.8773e-01, 1.8922e-03, 5.1002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006412787479348481 tensor([1.0088e-05, 3.2469e-03, 8.2460e-01, 6.4128e-04, 1.7150e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016611406579613686 tensor([1.0259e-01, 1.6611e-02, 1.6681e-04, 6.7739e-01, 2.0324e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0013655575457960367 tensor([6.5183e-04, 1.3656e-03, 1.0624e-02, 1.3276e-01, 8.5460e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0024745843838900328 tensor([1.6014e-04, 2.6601e-02, 6.9837e-01, 2.4746e-03, 2.7239e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09554494917392731 tensor([3.5152e-01, 1.6254e-01, 3.0245e-04, 3.9009e-01, 9.5545e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0807955414056778 tensor([0.0152, 0.1449, 0.0808, 0.1102, 0.6489], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022913239896297455 tensor([0.0301, 0.0229, 0.0027, 0.4729, 0.4714], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007977488450706005 tensor([0.0023, 0.0080, 0.0120, 0.0834, 0.8944], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.541310085594887e-06 tensor([9.7596e-01, 2.8305e-03, 2.0259e-10, 2.1207e-02, 6.5413e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009450915385968983 tensor([5.6818e-01, 4.2604e-01, 4.1676e-06, 4.8288e-03, 9.4509e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009418987669050694 tensor([0.0006, 0.0430, 0.4727, 0.0094, 0.4744], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04110707342624664 tensor([1.2183e-01, 4.1107e-02, 4.1263e-04, 5.1377e-01, 3.2288e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002183389849960804 tensor([3.4115e-05, 3.3781e-03, 3.6416e-01, 2.1834e-03, 6.3025e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.022816922515630722 tensor([0.0228, 0.7558, 0.0610, 0.0141, 0.1462], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00016593454347457737 tensor([1.7468e-01, 8.2473e-01, 7.1815e-06, 4.1161e-04, 1.6593e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.1319189019995974e-07 tensor([4.6594e-09, 1.0785e-04, 9.9867e-01, 3.1319e-07, 1.2220e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00011676459689624608 tensor([8.6328e-01, 2.4785e-03, 2.9068e-09, 1.3412e-01, 1.1676e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0022447663359344006 tensor([1.0273e-04, 1.5017e-02, 6.1497e-01, 2.2448e-03, 3.6767e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03620516136288643 tensor([1.2896e-01, 3.6205e-02, 3.6239e-04, 6.9519e-01, 1.3928e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003914323169738054 tensor([0.0481, 0.9315, 0.0024, 0.0039, 0.0141], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019008165691047907 tensor([1.9185e-05, 1.2684e-02, 9.1531e-01, 1.9008e-04, 7.1795e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015665132086724043 tensor([7.8809e-01, 2.7256e-02, 7.7883e-07, 1.8309e-01, 1.5665e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03622465953230858 tensor([0.0029, 0.0770, 0.1926, 0.0362, 0.6912], grad_fn=<SoftmaxBackward0>)\n",
      "4 8.976975368568674e-05 tensor([9.4752e-01, 1.2606e-02, 1.7508e-08, 3.9785e-02, 8.9770e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005307130049914122 tensor([1.8039e-01, 8.0749e-01, 2.5104e-04, 5.3071e-03, 6.5621e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02011779136955738 tensor([0.0046, 0.1980, 0.3194, 0.0201, 0.4579], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011931772343814373 tensor([4.3607e-01, 5.1491e-01, 2.1409e-04, 3.6876e-02, 1.1932e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010861053131520748 tensor([0.0007, 0.0509, 0.4164, 0.0109, 0.5211], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000593838922213763 tensor([3.1967e-02, 5.9384e-04, 2.8691e-06, 9.4065e-01, 2.6788e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08900852501392365 tensor([0.0919, 0.0890, 0.0024, 0.3668, 0.4498], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.600699346861802e-05 tensor([7.1491e-06, 1.4286e-02, 9.6249e-01, 3.6007e-05, 2.3182e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004996099043637514 tensor([3.2274e-01, 4.9961e-03, 7.2444e-07, 6.6445e-01, 7.8114e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06552714109420776 tensor([0.0655, 0.1955, 0.0093, 0.1492, 0.5804], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.8917611416545697e-06 tensor([2.9529e-07, 1.9781e-03, 9.9264e-01, 2.8918e-06, 5.3786e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14327025413513184 tensor([0.1433, 0.3905, 0.0071, 0.2017, 0.2574], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00021181536430958658 tensor([2.3824e-06, 1.3318e-03, 8.8617e-01, 2.1182e-04, 1.1229e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002511930651962757 tensor([0.0014, 0.0025, 0.0052, 0.1366, 0.8543], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010547452257014811 tensor([1.0547e-04, 5.3827e-05, 1.4165e-03, 1.8747e-01, 8.1095e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04762108623981476 tensor([2.2247e-01, 4.7621e-02, 2.2128e-04, 6.7326e-01, 5.6427e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00035136466613039374 tensor([4.6319e-01, 5.3441e-01, 2.4656e-06, 2.0455e-03, 3.5136e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.372064120796495e-08 tensor([3.0065e-10, 1.7358e-05, 9.9924e-01, 6.3721e-08, 7.3879e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011696125380694866 tensor([0.0587, 0.8709, 0.0109, 0.0117, 0.0478], grad_fn=<SoftmaxBackward0>)\n",
      "1 7.88967008702457e-06 tensor([1.1633e-03, 7.8897e-06, 7.6757e-07, 9.6615e-01, 3.2679e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0047234282828867435 tensor([0.0047, 0.4981, 0.3108, 0.0047, 0.1816], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01979788951575756 tensor([0.0605, 0.8044, 0.0114, 0.0198, 0.1040], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018956251442432404 tensor([0.0023, 0.1013, 0.2852, 0.0190, 0.5922], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03161350265145302 tensor([0.0316, 0.6074, 0.0537, 0.0293, 0.2780], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01688435487449169 tensor([0.0010, 0.0169, 0.0792, 0.0205, 0.8824], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010151890106499195 tensor([0.0839, 0.8853, 0.0033, 0.0102, 0.0174], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006989832501858473 tensor([1.3386e-02, 9.6550e-01, 6.9898e-03, 7.5420e-04, 1.3366e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012435120530426502 tensor([0.0020, 0.1166, 0.3300, 0.0124, 0.5390], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001025218516588211 tensor([1.6108e-01, 1.0252e-04, 1.4821e-09, 8.3862e-01, 1.9909e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.997460564482026e-05 tensor([4.8585e-04, 2.4980e-05, 4.9975e-05, 7.3123e-01, 2.6821e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00017932728223968297 tensor([9.4369e-01, 3.7517e-02, 6.7654e-08, 1.8616e-02, 1.7933e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000260515371337533 tensor([1.2440e-01, 8.7487e-01, 2.3263e-05, 4.4717e-04, 2.6052e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02433912083506584 tensor([0.0052, 0.1807, 0.2610, 0.0243, 0.5288], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006924801855348051 tensor([8.1808e-01, 1.1379e-02, 1.3820e-07, 1.6985e-01, 6.9248e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018860042095184326 tensor([0.0009, 0.0306, 0.2306, 0.0189, 0.7191], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10492640733718872 tensor([0.3548, 0.2790, 0.0008, 0.2605, 0.1049], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007047514896839857 tensor([1.2119e-02, 9.6975e-01, 7.0475e-03, 8.8330e-04, 1.0201e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003614781890064478 tensor([3.6318e-05, 3.6148e-04, 2.9936e-02, 1.2024e-02, 9.5764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00043279252713546157 tensor([8.6234e-01, 5.6200e-03, 1.5845e-08, 1.3161e-01, 4.3279e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005134062375873327 tensor([0.0043, 0.0066, 0.0051, 0.1216, 0.8624], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002620469080284238 tensor([0.0026, 0.5129, 0.3690, 0.0020, 0.1134], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007330157794058323 tensor([5.7252e-01, 9.9715e-03, 7.0941e-07, 4.1018e-01, 7.3302e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.106890067603672e-06 tensor([1.6327e-09, 1.3654e-05, 9.9551e-01, 1.1069e-06, 4.4769e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1195301041007042 tensor([0.3604, 0.3308, 0.0007, 0.1886, 0.1195], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014771604910492897 tensor([1.8106e-04, 1.4772e-03, 3.2766e-02, 3.1146e-02, 9.3443e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.018799131736159325 tensor([5.2988e-01, 9.0600e-02, 4.5076e-05, 3.6067e-01, 1.8799e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006285725394263864 tensor([1.0318e-01, 6.2857e-04, 2.5126e-07, 8.9119e-01, 4.9984e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0021508869249373674 tensor([7.9292e-01, 2.5903e-02, 7.5151e-07, 1.7902e-01, 2.1509e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0927882269024849 tensor([0.0928, 0.2300, 0.0146, 0.2702, 0.3924], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011025090934708714 tensor([0.0011, 0.0008, 0.0020, 0.2033, 0.7927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.020776722580194473 tensor([0.0245, 0.0208, 0.0036, 0.7059, 0.2452], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024257903918623924 tensor([0.0018, 0.0243, 0.0405, 0.0249, 0.9085], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009648156701587141 tensor([0.0009, 0.2701, 0.6081, 0.0010, 0.1200], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004201695788651705 tensor([4.7090e-01, 6.7383e-03, 5.6679e-07, 5.1816e-01, 4.2017e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.044891294091939926 tensor([2.7830e-01, 4.4891e-02, 8.7290e-05, 5.8208e-01, 9.4640e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0368565171957016 tensor([5.5497e-01, 2.0250e-01, 1.0579e-04, 2.0557e-01, 3.6857e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007806092617101967 tensor([4.5333e-02, 9.5120e-01, 4.0755e-04, 7.8061e-04, 2.2827e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12167565524578094 tensor([0.1958, 0.1217, 0.0012, 0.3846, 0.2967], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008017047657631338 tensor([5.1873e-01, 1.6725e-03, 1.9355e-08, 4.7880e-01, 8.0170e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004611488897353411 tensor([8.1209e-02, 4.6115e-03, 2.2200e-05, 8.4941e-01, 6.4746e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015191899612545967 tensor([0.0437, 0.0152, 0.0008, 0.6494, 0.2909], grad_fn=<SoftmaxBackward0>)\n",
      "4 9.514780686004087e-05 tensor([9.4154e-01, 9.3770e-03, 1.1646e-08, 4.8991e-02, 9.5148e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.6464917962366599e-06 tensor([4.4204e-08, 3.7643e-04, 9.9571e-01, 1.6465e-06, 3.9133e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006961317267268896 tensor([6.7817e-01, 4.6584e-02, 5.1509e-06, 2.6828e-01, 6.9613e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002887171693146229 tensor([4.2298e-04, 4.8754e-02, 4.0696e-01, 2.8872e-03, 5.4097e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.3520657123299316e-05 tensor([9.7152e-01, 1.6050e-02, 4.6935e-09, 1.2403e-02, 2.3521e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009114343672990799 tensor([9.1143e-03, 9.5411e-01, 2.1345e-02, 9.4930e-04, 1.4483e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00028058598400093615 tensor([7.7195e-05, 4.8008e-02, 8.9430e-01, 2.8059e-04, 5.7333e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014147836482152343 tensor([1.7717e-01, 1.4148e-03, 3.7585e-07, 8.1519e-01, 6.2326e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1613590568304062 tensor([0.1614, 0.4043, 0.0071, 0.2082, 0.2190], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018101414665579796 tensor([0.0083, 0.3650, 0.2125, 0.0181, 0.3960], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0024631256237626076 tensor([0.0025, 0.6640, 0.2748, 0.0011, 0.0576], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019601370149757713 tensor([6.7313e-05, 5.9875e-02, 8.7181e-01, 1.9601e-04, 6.8049e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00012233882443979383 tensor([1.1860e-01, 1.2234e-04, 5.2196e-09, 8.8058e-01, 6.9070e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00044898537453264 tensor([2.3291e-05, 4.4899e-04, 7.5587e-02, 7.8992e-03, 9.1604e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00022090060519985855 tensor([7.2388e-01, 2.6830e-01, 1.8162e-06, 7.5951e-03, 2.2090e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007842001505196095 tensor([1.3817e-01, 7.8420e-03, 1.8189e-05, 7.6188e-01, 9.2091e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00011371501022949815 tensor([1.2101e-06, 1.1372e-04, 5.5693e-01, 2.1186e-03, 4.4083e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006068154703825712 tensor([1.2100e-01, 6.0682e-03, 1.6485e-05, 8.0149e-01, 7.1427e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0020325782243162394 tensor([7.6242e-03, 9.8430e-01, 5.8794e-03, 1.5999e-04, 2.0326e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0664328932762146 tensor([3.2078e-01, 6.6433e-02, 1.5370e-04, 5.2480e-01, 8.7831e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00026301233447156847 tensor([8.6518e-01, 1.2659e-01, 2.5297e-07, 7.9630e-03, 2.6301e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.640875062023042e-08 tensor([7.2531e-10, 5.9650e-05, 9.9945e-01, 4.6409e-08, 4.9070e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.984418774256483e-05 tensor([1.8223e-01, 4.9844e-05, 2.3736e-10, 8.1760e-01, 1.1951e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00042011437471956015 tensor([7.5550e-04, 1.5431e-04, 4.2011e-04, 3.7698e-01, 6.2169e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00668703205883503 tensor([0.0027, 0.2509, 0.3326, 0.0067, 0.4070], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005893373861908913 tensor([0.0059, 0.5857, 0.1519, 0.0054, 0.2511], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1309921319480054e-05 tensor([6.1997e-08, 1.4803e-04, 9.8458e-01, 1.1310e-05, 1.5257e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017572446959093213 tensor([2.4585e-01, 1.7572e-03, 2.7412e-07, 7.4774e-01, 4.6550e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1510198563337326 tensor([0.1891, 0.2434, 0.0016, 0.1510, 0.4148], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01823209784924984 tensor([3.8825e-01, 5.6179e-01, 1.5480e-04, 3.1572e-02, 1.8232e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002610307652503252 tensor([7.7506e-02, 9.1060e-01, 6.2451e-04, 2.6103e-03, 8.6626e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003825761843472719 tensor([0.0020, 0.2824, 0.3681, 0.0038, 0.3437], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02774299681186676 tensor([3.2506e-01, 2.7743e-02, 2.3868e-05, 6.0787e-01, 3.9303e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03287490829825401 tensor([0.0556, 0.0329, 0.0010, 0.4150, 0.4955], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002832908881828189 tensor([5.7614e-04, 1.1315e-01, 5.9982e-01, 2.8329e-03, 2.8362e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.045065708458423615 tensor([3.6476e-01, 4.5066e-02, 4.7494e-05, 5.3141e-01, 5.8715e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0585162453353405 tensor([0.0128, 0.1502, 0.0692, 0.0585, 0.7092], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016899798065423965 tensor([5.3927e-01, 9.8044e-02, 4.1104e-05, 3.4575e-01, 1.6900e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4842552218397032e-06 tensor([1.1553e-07, 1.2056e-03, 9.9696e-01, 1.4843e-06, 1.8347e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005813091527670622 tensor([5.1767e-04, 6.4340e-02, 5.7763e-01, 5.8131e-03, 3.5170e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.073396899504587e-05 tensor([9.7991e-01, 1.2017e-02, 1.2727e-09, 8.0558e-03, 2.0734e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.858030999661423e-05 tensor([2.1368e-06, 2.6924e-03, 9.7595e-01, 5.8580e-05, 2.1293e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.546503285178915e-05 tensor([9.6409e-01, 5.5317e-03, 1.6003e-09, 3.0339e-02, 3.5465e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00054911058396101 tensor([5.4911e-04, 4.1152e-04, 2.3617e-03, 1.8017e-01, 8.1651e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00011913825437659398 tensor([9.2941e-01, 5.7333e-02, 8.2929e-08, 1.3137e-02, 1.1914e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003173895413056016 tensor([0.0299, 0.9582, 0.0032, 0.0018, 0.0069], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008385139517486095 tensor([6.9662e-05, 2.3569e-02, 7.3388e-01, 8.3851e-04, 2.4165e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003337632806506008 tensor([6.5669e-02, 3.3376e-04, 2.4208e-07, 9.2831e-01, 5.6901e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.9731534090824425e-05 tensor([1.0892e-05, 4.9732e-05, 1.3361e-02, 2.3515e-02, 9.6306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010583217954263091 tensor([7.6694e-01, 2.0764e-01, 4.3859e-06, 2.4353e-02, 1.0583e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00014628043572884053 tensor([2.0228e-03, 1.4628e-04, 8.6642e-05, 7.6297e-01, 2.3477e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.7402831531398988e-07 tensor([8.6730e-10, 2.5512e-05, 9.9866e-01, 1.7403e-07, 1.3106e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012731005437672138 tensor([6.1911e-04, 1.2731e-02, 1.3526e-01, 2.2566e-02, 8.2882e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00036454354994930327 tensor([5.3040e-05, 3.6454e-04, 1.6454e-02, 1.2370e-02, 9.7076e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011124901939183474 tensor([1.7533e-01, 1.1125e-03, 1.8737e-07, 8.2046e-01, 3.0885e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001798239885829389 tensor([6.8823e-01, 4.5957e-03, 6.1889e-08, 3.0538e-01, 1.7982e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.036155909299850464 tensor([0.0033, 0.0362, 0.0739, 0.0674, 0.8193], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14736808836460114 tensor([0.1474, 0.2692, 0.0062, 0.3016, 0.2756], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001029552542604506 tensor([8.5365e-01, 1.1662e-01, 1.6300e-06, 2.8701e-02, 1.0296e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003212078707292676 tensor([0.0020, 0.0032, 0.0071, 0.1992, 0.7885], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007421480258926749 tensor([7.4215e-04, 7.9039e-01, 1.9601e-01, 9.8166e-05, 1.2756e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.524942935153376e-06 tensor([1.9987e-07, 7.8701e-04, 9.9192e-01, 5.5249e-06, 7.2914e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014975593658164144 tensor([0.0020, 0.0009, 0.0015, 0.3631, 0.6324], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18917283415794373 tensor([0.3618, 0.2580, 0.0005, 0.1904, 0.1892], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00020951156329829246 tensor([7.5480e-01, 2.4249e-03, 7.4847e-09, 2.4257e-01, 2.0951e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000989166204817593 tensor([9.5885e-03, 9.8822e-01, 1.0989e-03, 1.0141e-04, 9.8917e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.859478915226646e-05 tensor([7.9470e-07, 8.1794e-04, 9.3677e-01, 5.8595e-05, 6.2355e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007920447736978531 tensor([0.0808, 0.8843, 0.0034, 0.0079, 0.0236], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2840657038614154e-05 tensor([2.2841e-05, 1.5955e-05, 1.1514e-03, 6.4686e-02, 9.3412e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00014961963461246341 tensor([9.3895e-01, 3.5126e-02, 8.8827e-08, 2.5776e-02, 1.4962e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002547766489442438 tensor([4.1620e-02, 2.5478e-04, 3.9344e-07, 9.4751e-01, 1.0612e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.023142047226429e-05 tensor([3.5634e-06, 3.7864e-03, 9.6763e-01, 9.0231e-05, 2.8491e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026688871905207634 tensor([0.0267, 0.0661, 0.0125, 0.2584, 0.6363], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020034320652484894 tensor([0.0200, 0.0343, 0.0078, 0.2357, 0.7021], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1373276561498642 tensor([0.2018, 0.1373, 0.0009, 0.3258, 0.3341], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00132856541313231 tensor([6.6937e-02, 9.2857e-01, 3.2565e-04, 1.3286e-03, 2.8367e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.019853489473462105 tensor([0.0536, 0.8246, 0.0199, 0.0201, 0.0819], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004658686928451061 tensor([7.6561e-01, 8.3734e-02, 4.9075e-06, 1.4599e-01, 4.6587e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3085603313811589e-05 tensor([8.2495e-07, 2.3140e-03, 9.8160e-01, 1.3086e-05, 1.6071e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5189742043730803e-05 tensor([1.5270e-06, 4.4974e-03, 9.8227e-01, 1.5190e-05, 1.3212e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000250697776209563 tensor([8.1301e-05, 2.5070e-04, 1.4587e-02, 5.0796e-02, 9.3429e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.043510157614946365 tensor([4.5494e-01, 3.9870e-01, 2.0236e-04, 1.0264e-01, 4.3510e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0013872876297682524 tensor([2.4561e-01, 1.3873e-03, 1.4159e-07, 7.4768e-01, 5.3277e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023766146041452885 tensor([9.6967e-03, 2.3766e-03, 3.9030e-04, 4.2306e-01, 5.6448e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022890638560056686 tensor([2.0529e-01, 2.2891e-02, 6.2756e-05, 7.2992e-01, 4.1839e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016470340779051185 tensor([4.5627e-02, 9.5386e-01, 4.9354e-05, 1.6470e-04, 3.0136e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.264600076770876e-06 tensor([1.2787e-08, 4.3052e-05, 9.9094e-01, 5.2646e-06, 9.0109e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02997392974793911 tensor([0.0512, 0.6798, 0.0199, 0.0300, 0.2192], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026769326999783516 tensor([0.0055, 0.0388, 0.0268, 0.0578, 0.8710], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003944238647818565 tensor([3.1042e-01, 3.9442e-03, 5.5285e-07, 6.7946e-01, 6.1743e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00012761475227307528 tensor([8.1735e-01, 1.7815e-01, 3.3671e-07, 4.3680e-03, 1.2761e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0025223756674677134 tensor([5.4924e-01, 4.1634e-01, 4.8991e-05, 3.1842e-02, 2.5224e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007036188151687384 tensor([7.3374e-01, 1.8917e-01, 1.3911e-05, 7.0038e-02, 7.0362e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005795458797365427 tensor([1.5129e-04, 5.7955e-03, 2.0379e-01, 1.1098e-02, 7.7917e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0012439345009624958 tensor([8.3938e-01, 1.0341e-01, 1.8260e-06, 5.5961e-02, 1.2439e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00916839949786663 tensor([0.0061, 0.4307, 0.2000, 0.0092, 0.3540], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.47992077726667e-07 tensor([2.8375e-08, 3.3548e-04, 9.9718e-01, 9.4799e-07, 2.4790e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010750091169029474 tensor([2.5660e-02, 1.0750e-03, 1.5466e-05, 9.1808e-01, 5.5169e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.0097468222957104e-05 tensor([1.6932e-08, 2.2869e-05, 9.4394e-01, 2.0097e-05, 5.6013e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007584672421216965 tensor([4.0167e-01, 5.9452e-01, 1.0362e-05, 3.0459e-03, 7.5847e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00976000726222992 tensor([7.0678e-01, 5.0127e-02, 4.2068e-06, 2.3333e-01, 9.7600e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.106278695166111e-05 tensor([1.1908e-05, 2.0309e-02, 9.6511e-01, 5.1063e-05, 1.4517e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3659154319611844e-05 tensor([5.7373e-01, 8.8334e-05, 2.0701e-11, 4.2617e-01, 1.3659e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001542469544801861 tensor([1.8174e-04, 2.0460e-05, 1.5425e-04, 5.5164e-01, 4.4800e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013194598257541656 tensor([0.0073, 0.4254, 0.2126, 0.0132, 0.3415], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.028843451291322708 tensor([0.0288, 0.0800, 0.0125, 0.2155, 0.6631], grad_fn=<SoftmaxBackward0>)\n",
      "3 6.151353318273323e-06 tensor([3.2310e-07, 1.2429e-03, 9.9067e-01, 6.1514e-06, 8.0834e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.294093428645283e-05 tensor([5.5958e-01, 1.2362e-04, 4.6999e-11, 4.4027e-01, 2.2941e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.1163464377168566e-05 tensor([9.1241e-04, 4.9324e-05, 5.1163e-05, 6.8660e-01, 3.1239e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0920603945851326 tensor([2.4069e-01, 9.2060e-02, 5.0840e-04, 5.2408e-01, 1.4266e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007508631679229438 tensor([5.0013e-01, 4.9491e-01, 5.3475e-06, 4.2019e-03, 7.5086e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.4064935877941025e-07 tensor([1.0278e-07, 2.7061e-03, 9.9649e-01, 4.4065e-07, 8.0321e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007815091870725155 tensor([6.7631e-01, 2.3061e-01, 2.3996e-05, 8.5243e-02, 7.8151e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023861469235271215 tensor([2.0598e-04, 2.3861e-03, 3.2240e-02, 1.7115e-02, 9.4805e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.001473032782087e-06 tensor([9.2801e-01, 4.2299e-04, 1.3315e-11, 7.1563e-02, 3.0015e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015515215694904327 tensor([4.4331e-04, 1.6966e-02, 1.9957e-01, 1.5515e-02, 7.6750e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.9445993681729306e-06 tensor([2.5684e-07, 1.2001e-03, 9.9130e-01, 4.9446e-06, 7.4943e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003204318229109049 tensor([6.9618e-01, 2.4973e-02, 1.5139e-06, 2.7565e-01, 3.2043e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.591978646814823e-05 tensor([1.1835e-07, 7.5920e-05, 8.2995e-01, 7.7368e-05, 1.6990e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1], [2, 3], [0, 3], [2, 4], [2, 1], [0, 3], [0, 2], [2, 4], [2, 4], [2, 1], [0, 1], [0, 3], [3, 0], [2, 1], [1, 2], [2, 4], [2, 4], [0, 3], [2, 3], [2, 1], [3, 0], [0, 3], [0, 3], [2, 0], [2, 1], [0, 3], [0, 3], [2, 3], [2, 1], [2, 3], [2, 4], [2, 3], [0, 3], [2, 1], [2, 3], [2, 4], [2, 3], [0, 3], [2, 1], [1, 2], [2, 4], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [0, 1], [0, 3], [2, 1], [0, 1], [0, 3], [2, 4], [0, 2], [2, 1], [0, 1], [2, 4], [2, 4], [0, 3], [2, 1], [0, 3], [3, 0], [2, 4], [0, 3], [2, 4], [0, 3], [2, 1], [2, 3], [0, 3], [2, 4], [0, 3], [2, 4], [2, 3], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [0, 3], [2, 1], [2, 0], [0, 3], [2, 0], [0, 3], [0, 1], [1, 0], [2, 1], [2, 4], [0, 3], [2, 3], [2, 1], [0, 3], [2, 3], [0, 3], [3, 0], [0, 1], [2, 3], [3, 2], [0, 3], [2, 1], [1, 2], [2, 4], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 2], [0, 1], [2, 4], [0, 2], [3, 0], [2, 4], [0, 3], [2, 4], [0, 1], [2, 4], [2, 1], [2, 4], [2, 0], [1, 0], [2, 1], [0, 1], [0, 3], [2, 4], [2, 1], [2, 4], [2, 3], [2, 1], [2, 4], [2, 1], [2, 1], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 0], [0, 3], [2, 1], [2, 0], [0, 3], [3, 0], [0, 3], [2, 1], [0, 1], [2, 4], [2, 1], [0, 1], [2, 1], [3, 4], [2, 1], [2, 4], [0, 3], [2, 1], [1, 2], [0, 3], [3, 0], [0, 3], [2, 1], [2, 3], [2, 4], [2, 3], [0, 3], [2, 1], [2, 1], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 4], [1, 0], [2, 4], [3, 2], [0, 3], [2, 1], [0, 1], [2, 4], [2, 1], [0, 3], [0, 1], [0, 1], [2, 1], [2, 4], [0, 1], [2, 0], [2, 4], [0, 1], [3, 0], [0, 3], [1, 2], [2, 4], [2, 4], [3, 4], [0, 3], [2, 3], [1, 0], [2, 4], [2, 1], [0, 3], [2, 0], [2, 0], [2, 1], [2, 3], [3, 2], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [2, 1], [2, 1], [2, 1], [2, 3], [0, 3], [2, 3], [0, 2], [2, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [2, 4], [2, 4], [0, 3], [2, 4], [1, 2], [0, 3], [2, 0], [0, 3], [2, 4], [1, 2], [2, 1], [2, 4], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0, 1]]\n",
      "[[0, 3, 4], [0, 1, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 4], [0, 1, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 4], [2, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 3, 4], [2, 3, 4], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 4], [2, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 3, 4], [0, 1, 4], [0, 1, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [0, 1, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 1], [2, 3], [0, 3], [2, 4], [2, 1], [0, 3], [0, 2], [2, 4], [2, 4], [2, 1], [0, 1], [0, 3], [3, 0], [2, 1], [1, 2], [2, 4], [2, 4], [0, 3], [2, 3], [2, 1], [3, 0], [0, 3], [0, 3], [2, 0], [2, 1], [0, 3], [0, 3], [2, 3], [2, 1], [2, 3], [2, 4], [2, 3], [0, 3], [2, 1], [2, 3], [2, 4], [2, 3], [0, 3], [2, 1], [1, 2], [2, 4], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [0, 1], [0, 3], [2, 1], [0, 1], [0, 3], [2, 4], [0, 2], [2, 1], [0, 1], [2, 4], [2, 4], [0, 3], [2, 1], [0, 3], [3, 0], [2, 4], [0, 3], [2, 4], [0, 3], [2, 1], [2, 3], [0, 3], [2, 4], [0, 3], [2, 4], [2, 3], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [0, 3], [2, 1], [2, 0], [0, 3], [2, 0], [0, 3], [0, 1], [1, 0], [2, 1], [2, 4], [0, 3], [2, 3], [2, 1], [0, 3], [2, 3], [0, 3], [3, 0], [0, 1], [2, 3], [3, 2], [0, 3], [2, 1], [1, 2], [2, 4], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 2], [0, 1], [2, 4], [0, 2], [3, 0], [2, 4], [0, 3], [2, 4], [0, 1], [2, 4], [2, 1], [2, 4], [2, 0], [1, 0], [2, 1], [0, 1], [0, 3], [2, 4], [2, 1], [2, 4], [2, 3], [2, 1], [2, 4], [2, 1], [2, 1], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 0], [0, 3], [2, 1], [2, 0], [0, 3], [3, 0], [0, 3], [2, 1], [0, 1], [2, 4], [2, 1], [0, 1], [2, 1], [3, 4], [2, 1], [2, 4], [0, 3], [2, 1], [1, 2], [0, 3], [3, 0], [0, 3], [2, 1], [2, 3], [2, 4], [2, 3], [0, 3], [2, 1], [2, 1], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 4], [1, 0], [2, 4], [3, 2], [0, 3], [2, 1], [0, 1], [2, 4], [2, 1], [0, 3], [0, 1], [0, 1], [2, 1], [2, 4], [0, 1], [2, 0], [2, 4], [0, 1], [3, 0], [0, 3], [1, 2], [2, 4], [2, 4], [3, 4], [0, 3], [2, 3], [1, 0], [2, 4], [2, 1], [0, 3], [2, 0], [2, 0], [2, 1], [2, 3], [3, 2], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [2, 1], [2, 1], [2, 1], [2, 3], [0, 3], [2, 3], [0, 2], [2, 1], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [2, 4], [2, 4], [0, 3], [2, 4], [1, 2], [0, 3], [2, 0], [0, 3], [2, 4], [1, 2], [2, 1], [2, 4], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.00450163459777832  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004501628875732422  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004501615047454834  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004501596450805664  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004501574516296387  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.0045015478134155275  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004501518249511719  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004501483917236328  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004501448154449463  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0045014100074768065  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.00450136947631836  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004501327991485595  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004501284122467041  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004501240253448486  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004501194477081299  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004501149177551269  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0045011029243469235  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004501057147979736  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004501010417938232  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004500964164733887  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.040602393448352814 tensor([4.0602e-02, 2.1882e-03, 2.6262e-05, 8.8410e-01, 7.3084e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.040033191442489624 tensor([0.1496, 0.7814, 0.0021, 0.0268, 0.0400], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030914943665266037 tensor([1.0535e-04, 3.0915e-02, 8.1714e-01, 1.1054e-03, 1.5073e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000134569316287525 tensor([3.3248e-01, 1.3457e-04, 3.6411e-10, 6.6727e-01, 1.1768e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10351302474737167 tensor([0.1035, 0.0763, 0.0017, 0.2819, 0.5366], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11791279911994934 tensor([0.0009, 0.1179, 0.4327, 0.0037, 0.4447], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0417913980782032 tensor([0.0051, 0.0418, 0.0401, 0.0605, 0.8525], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0920962244272232 tensor([0.3203, 0.5578, 0.0010, 0.0921, 0.0288], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005401605740189552 tensor([4.7313e-01, 5.4016e-04, 2.6568e-09, 5.2609e-01, 2.3651e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018023107200860977 tensor([0.0180, 0.0121, 0.0014, 0.2568, 0.7118], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03451156243681908 tensor([0.0019, 0.0081, 0.0345, 0.1441, 0.8113], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1035706177353859 tensor([0.0144, 0.3343, 0.1036, 0.0325, 0.5153], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03564813360571861 tensor([0.0199, 0.8872, 0.0356, 0.0059, 0.0513], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008832208812236786 tensor([3.6621e-01, 7.8813e-03, 1.3645e-06, 6.1707e-01, 8.8322e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012160259939264506 tensor([1.2160e-04, 7.7505e-06, 7.1515e-05, 6.3544e-01, 3.6436e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11521804332733154 tensor([5.1402e-01, 1.1522e-01, 9.0761e-05, 3.4636e-01, 2.4319e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08936378359794617 tensor([6.6522e-01, 2.2060e-01, 3.3866e-05, 8.9364e-02, 2.4780e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022006642539054155 tensor([2.9515e-06, 2.2007e-03, 9.2436e-01, 1.1304e-04, 7.3324e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1170516386628151 tensor([0.1171, 0.5254, 0.0076, 0.0879, 0.2621], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1323976367712021 tensor([1.3240e-01, 2.2611e-02, 1.5264e-04, 7.0873e-01, 1.3611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05417076498270035 tensor([0.0288, 0.7002, 0.0542, 0.0183, 0.1985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014069989323616028 tensor([4.9581e-06, 2.1102e-02, 9.6481e-01, 1.1836e-05, 1.4070e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09816966205835342 tensor([0.0023, 0.0982, 0.2411, 0.0177, 0.6407], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09934816509485245 tensor([0.0617, 0.3554, 0.0239, 0.0993, 0.4597], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03907925635576248 tensor([1.9559e-01, 1.0181e-02, 1.5648e-05, 7.5514e-01, 3.9079e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1835041344165802 tensor([0.0099, 0.3841, 0.1835, 0.0231, 0.3994], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2780954837799072 tensor([0.0019, 0.2781, 0.3443, 0.0038, 0.3719], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.059144843369722366 tensor([0.1094, 0.8097, 0.0039, 0.0178, 0.0591], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009522844105958939 tensor([3.2804e-01, 6.4993e-03, 1.6243e-06, 6.5594e-01, 9.5228e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016371844336390495 tensor([0.0990, 0.8765, 0.0015, 0.0066, 0.0164], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0284496508538723 tensor([9.0183e-01, 2.8450e-02, 1.6256e-07, 6.9267e-02, 4.5266e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.051024600863456726 tensor([0.1134, 0.8162, 0.0023, 0.0170, 0.0510], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003040499868802726 tensor([8.6634e-09, 3.0405e-04, 9.9868e-01, 2.0061e-07, 1.0160e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0013491505524143577 tensor([3.2418e-01, 1.2097e-03, 5.4646e-08, 6.7326e-01, 1.3492e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.047568630427122116 tensor([0.0476, 0.8439, 0.0075, 0.0101, 0.0909], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11213895678520203 tensor([4.1768e-01, 1.1214e-01, 1.1679e-04, 3.7812e-01, 9.1944e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009397559915669262 tensor([4.1256e-02, 9.5722e-01, 1.9380e-04, 3.9074e-04, 9.3976e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019335586577653885 tensor([1.7810e-04, 1.9336e-02, 5.2018e-01, 3.7279e-03, 4.5658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00020167705952189863 tensor([8.2824e-02, 3.2250e-05, 7.1241e-10, 9.1694e-01, 2.0168e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004228092439007014 tensor([4.2281e-04, 3.9460e-05, 1.3956e-04, 5.9375e-01, 4.0565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.037025220692157745 tensor([9.1293e-01, 4.9780e-02, 2.4649e-07, 3.7025e-02, 2.6095e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11203968524932861 tensor([1.1204e-01, 2.8512e-02, 3.1914e-04, 6.2375e-01, 2.3538e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0824589803814888 tensor([0.0061, 0.0825, 0.1058, 0.0632, 0.7425], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01058885082602501 tensor([8.2512e-01, 1.0589e-02, 8.9388e-08, 1.6376e-01, 5.2710e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0026405309326946735 tensor([1.6852e-06, 2.6405e-03, 9.6561e-01, 4.3556e-05, 3.1706e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26610034704208374 tensor([0.0026, 0.3060, 0.4189, 0.0064, 0.2661], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0019411288667470217 tensor([3.6385e-06, 3.3906e-04, 4.7848e-01, 1.9411e-03, 5.1923e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0031210577581077814 tensor([9.9913e-06, 3.1211e-03, 8.1940e-01, 6.6527e-04, 1.7681e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09797389060258865 tensor([9.7974e-02, 1.5331e-02, 1.6027e-04, 6.8443e-01, 2.0210e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010247346945106983 tensor([6.2975e-04, 1.2771e-03, 1.0247e-02, 1.3459e-01, 8.5326e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.025508616119623184 tensor([1.5826e-04, 2.5509e-02, 6.9168e-01, 2.5601e-03, 2.8010e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15395215153694153 tensor([3.4580e-01, 1.5395e-01, 2.9467e-04, 4.0330e-01, 9.6647e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11339744180440903 tensor([0.0149, 0.1367, 0.0785, 0.1134, 0.6565], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.028832776471972466 tensor([0.0288, 0.0212, 0.0026, 0.4782, 0.4692], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011554860509932041 tensor([0.0022, 0.0075, 0.0116, 0.0846, 0.8941], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027476598042994738 tensor([9.7528e-01, 2.7477e-03, 2.0020e-10, 2.1968e-02, 6.5943e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005082112271338701 tensor([5.7625e-01, 4.1769e-01, 4.1663e-06, 5.0821e-03, 9.7673e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04096893221139908 tensor([0.0006, 0.0410, 0.4649, 0.0097, 0.4839], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11721296608448029 tensor([1.1721e-01, 3.8223e-02, 3.9823e-04, 5.2170e-01, 3.2246e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0031944606453180313 tensor([3.3305e-05, 3.1945e-03, 3.5534e-01, 2.2325e-03, 6.3920e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06246278062462807 tensor([0.0231, 0.7448, 0.0625, 0.0150, 0.1547], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004310004005674273 tensor([1.7882e-01, 8.2057e-01, 7.1974e-06, 4.3100e-04, 1.7115e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010377309081377462 tensor([4.5522e-09, 1.0377e-04, 9.9865e-01, 3.1810e-07, 1.2446e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023991833440959454 tensor([8.5750e-01, 2.3992e-03, 2.9198e-09, 1.3998e-01, 1.1985e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014393530786037445 tensor([1.0129e-04, 1.4394e-02, 6.0731e-01, 2.3088e-03, 3.7588e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12351629883050919 tensor([1.2352e-01, 3.3537e-02, 3.4926e-04, 7.0373e-01, 1.3887e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015013459138572216 tensor([0.0494, 0.9289, 0.0025, 0.0042, 0.0150], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012250702828168869 tensor([1.9077e-05, 1.2251e-02, 9.1326e-01, 1.9775e-04, 7.4271e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02614709548652172 tensor([7.8112e-01, 2.6147e-02, 7.7475e-07, 1.9112e-01, 1.6082e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07259786874055862 tensor([0.0029, 0.0726, 0.1879, 0.0369, 0.6997], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012156111188232899 tensor([9.4632e-01, 1.2156e-02, 1.7232e-08, 4.1430e-02, 9.1114e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0069792792201042175 tensor([1.8488e-01, 8.0218e-01, 2.5752e-04, 5.6977e-03, 6.9793e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18937735259532928 tensor([0.0045, 0.1894, 0.3163, 0.0207, 0.4691], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03939574584364891 tensor([4.4221e-01, 5.0561e-01, 2.1809e-04, 3.9396e-02, 1.2561e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.048246778547763824 tensor([0.0007, 0.0482, 0.4093, 0.0111, 0.5306], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026393843814730644 tensor([3.0527e-02, 5.5145e-04, 2.7589e-06, 9.4252e-01, 2.6394e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08894884586334229 tensor([0.0889, 0.0829, 0.0023, 0.3751, 0.4507], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01383800245821476 tensor([7.1018e-06, 1.3838e-02, 9.6221e-01, 3.7340e-05, 2.3906e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007811079733073711 tensor([3.1192e-01, 4.6952e-03, 7.0687e-07, 6.7557e-01, 7.8111e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1530982106924057 tensor([0.0640, 0.1852, 0.0092, 0.1531, 0.5885], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0019116589101031423 tensor([2.9017e-07, 1.9117e-03, 9.9260e-01, 2.9469e-06, 5.4878e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21094363927841187 tensor([0.1418, 0.3744, 0.0071, 0.2109, 0.2658], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012884868774563074 tensor([2.3777e-06, 1.2885e-03, 8.8231e-01, 2.2088e-04, 1.1617e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005059255287051201 tensor([0.0014, 0.0024, 0.0051, 0.1385, 0.8527], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013683140277862549 tensor([1.0204e-04, 5.0466e-05, 1.3683e-03, 1.8993e-01, 8.0855e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05646058917045593 tensor([2.1409e-01, 4.4251e-02, 2.1365e-04, 6.8498e-01, 5.6461e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0021512594539672136 tensor([4.7167e-01, 5.2581e-01, 2.4581e-06, 2.1513e-03, 3.6212e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.6793546819826588e-05 tensor([2.9495e-10, 1.6794e-05, 9.9923e-01, 6.4659e-08, 7.5182e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05066573619842529 tensor([0.0601, 0.8657, 0.0111, 0.0125, 0.0507], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011145705357193947 tensor([1.1146e-03, 7.3919e-06, 7.4377e-07, 9.6660e-01, 3.2282e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1895168423652649 tensor([0.0048, 0.4874, 0.3133, 0.0050, 0.1895], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06155961751937866 tensor([0.0616, 0.7956, 0.0116, 0.0211, 0.1102], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09642605483531952 tensor([0.0023, 0.0964, 0.2799, 0.0194, 0.6020], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05437471717596054 tensor([0.0317, 0.5921, 0.0544, 0.0308, 0.2910], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02073930762708187 tensor([0.0009, 0.0158, 0.0766, 0.0207, 0.8858], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.018578045070171356 tensor([0.0860, 0.8811, 0.0034, 0.0109, 0.0186], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013748670928180218 tensor([1.3749e-02, 9.6407e-01, 7.1705e-03, 8.0540e-04, 1.4207e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11111468821763992 tensor([0.0019, 0.1111, 0.3241, 0.0128, 0.5500], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00019810916273854673 tensor([1.5412e-01, 9.5823e-05, 1.4459e-09, 8.4559e-01, 1.9811e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00046636429033242166 tensor([4.6636e-04, 2.3365e-05, 4.8289e-05, 7.3401e-01, 2.6545e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.019359618425369263 tensor([9.4424e-01, 3.6216e-02, 6.6528e-08, 1.9360e-02, 1.8214e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000469771446660161 tensor([1.2740e-01, 8.7184e-01, 2.3454e-05, 4.6977e-04, 2.6994e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17251941561698914 tensor([0.0051, 0.1725, 0.2573, 0.0250, 0.5401], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01089577004313469 tensor([8.1042e-01, 1.0896e-02, 1.3776e-07, 1.7797e-01, 7.1311e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028761690482497215 tensor([0.0008, 0.0288, 0.2250, 0.0192, 0.7262], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26755788922309875 tensor([0.3524, 0.2676, 0.0007, 0.2719, 0.1075], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01088574156165123 tensor([1.2486e-02, 9.6844e-01, 7.2413e-03, 9.5046e-04, 1.0886e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012135634198784828 tensor([3.5111e-05, 3.3884e-04, 2.8843e-02, 1.2136e-02, 9.5865e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005447516683489084 tensor([8.5679e-01, 5.4475e-03, 1.5941e-08, 1.3731e-01, 4.4453e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006107604131102562 tensor([0.0042, 0.0061, 0.0049, 0.1235, 0.8612], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11892910301685333 tensor([0.0026, 0.5025, 0.3739, 0.0021, 0.1189], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009510665200650692 tensor([5.6078e-01, 9.5107e-03, 7.0156e-07, 4.2229e-01, 7.4137e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.3243821740616113e-05 tensor([1.6227e-09, 1.3244e-05, 9.9536e-01, 1.1439e-06, 4.6240e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19803392887115479 tensor([0.3597, 0.3185, 0.0007, 0.1980, 0.1231], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.031604230403900146 tensor([1.7521e-04, 1.3839e-03, 3.1611e-02, 3.1604e-02, 9.3523e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08596985042095184 tensor([5.2064e-01, 8.5970e-02, 4.4365e-05, 3.7418e-01, 1.9161e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004934630822390318 tensor([9.8775e-02, 5.8567e-04, 2.4244e-07, 8.9570e-01, 4.9346e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024838510900735855 tensor([7.8575e-01, 2.4839e-02, 7.4891e-07, 1.8720e-01, 2.2123e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21749447286128998 tensor([0.0905, 0.2175, 0.0144, 0.2783, 0.3993], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019299566047266126 tensor([1.0682e-03, 7.4693e-04, 1.9300e-03, 2.0636e-01, 7.8990e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.023313166573643684 tensor([0.0233, 0.0191, 0.0035, 0.7110, 0.2431], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02512458711862564 tensor([0.0018, 0.0227, 0.0392, 0.0251, 0.9112], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.125131294131279 tensor([0.0009, 0.2627, 0.6103, 0.0010, 0.1251], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006347724236547947 tensor([4.5842e-01, 6.3477e-03, 5.5334e-07, 5.3100e-01, 4.2310e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09478051960468292 tensor([2.6902e-01, 4.1818e-02, 8.4253e-05, 5.9430e-01, 9.4781e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19464565813541412 tensor([5.5178e-01, 1.9465e-01, 1.0538e-04, 2.1550e-01, 3.7969e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0024225946981459856 tensor([4.6568e-02, 9.4976e-01, 4.1829e-04, 8.3369e-04, 2.4226e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.189925417304039 tensor([0.1899, 0.1138, 0.0012, 0.3953, 0.2998], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015914581017568707 tensor([5.0573e-01, 1.5915e-03, 1.9187e-08, 4.9187e-01, 8.1110e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06388159841299057 tensor([7.7606e-02, 4.2542e-03, 2.1168e-05, 8.5424e-01, 6.3882e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.041825711727142334 tensor([0.0418, 0.0140, 0.0007, 0.6550, 0.2884], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00901822466403246 tensor([9.3964e-01, 9.0182e-03, 1.1480e-08, 5.1244e-02, 9.7055e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00036265933886170387 tensor([4.3339e-08, 3.6266e-04, 9.9564e-01, 1.6787e-06, 3.9925e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04444291070103645 tensor([6.6934e-01, 4.4443e-02, 5.0934e-06, 2.7909e-01, 7.1196e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04657868668437004 tensor([4.1622e-04, 4.6579e-02, 3.9958e-01, 2.9592e-03, 5.5046e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012859788723289967 tensor([9.7159e-01, 1.5522e-02, 4.6113e-09, 1.2860e-02, 2.3729e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015404676087200642 tensor([0.0093, 0.9522, 0.0220, 0.0010, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04664382338523865 tensor([7.7273e-05, 4.6644e-02, 8.9352e-01, 2.9293e-04, 5.9464e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006195938680320978 tensor([1.6966e-01, 1.3209e-03, 3.6567e-07, 8.2282e-01, 6.1959e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21807457506656647 tensor([0.1595, 0.3883, 0.0071, 0.2181, 0.2271], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21211081743240356 tensor([0.0083, 0.3521, 0.2121, 0.0188, 0.4087], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06092243641614914 tensor([0.0025, 0.6552, 0.2802, 0.0012, 0.0609], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05802280083298683 tensor([6.7161e-05, 5.8023e-02, 8.7120e-01, 2.0424e-04, 7.0509e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006835500826127827 tensor([1.1339e-01, 1.1404e-04, 5.0575e-09, 8.8581e-01, 6.8355e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007976168766617775 tensor([2.2547e-05, 4.2225e-04, 7.3113e-02, 7.9762e-03, 9.1847e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007919572293758392 tensor([7.2986e-01, 2.6199e-01, 1.8034e-06, 7.9196e-03, 2.2448e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09119750559329987 tensor([1.3247e-01, 7.2552e-03, 1.7402e-05, 7.6906e-01, 9.1198e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00217971159145236 tensor([1.1886e-06, 1.0844e-04, 5.4792e-01, 2.1797e-03, 4.4979e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07078168541193008 tensor([1.1592e-01, 5.6373e-03, 1.5868e-05, 8.0765e-01, 7.0782e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006013638339936733 tensor([7.7647e-03, 9.8393e-01, 6.0136e-03, 1.6793e-04, 2.1239e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08833535015583038 tensor([3.1095e-01, 6.1943e-02, 1.4863e-04, 5.3862e-01, 8.8335e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008315971121191978 tensor([8.6900e-01, 1.2241e-01, 2.4853e-07, 8.3160e-03, 2.6750e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.775175304734148e-05 tensor([7.1041e-10, 5.7752e-05, 9.9944e-01, 4.6897e-08, 4.9742e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00011895297939190641 tensor([1.7515e-01, 4.6817e-05, 2.3198e-10, 8.2468e-01, 1.1895e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000731710169930011 tensor([7.3171e-04, 1.4567e-04, 4.0779e-04, 3.8049e-01, 6.1822e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24095900356769562 tensor([0.0027, 0.2410, 0.3299, 0.0069, 0.4195], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1530919224023819 tensor([0.0059, 0.5723, 0.1531, 0.0057, 0.2630], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001435356098227203 tensor([6.1805e-08, 1.4354e-04, 9.8402e-01, 1.1774e-05, 1.5821e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004627427086234093 tensor([2.3675e-01, 1.6467e-03, 2.6623e-07, 7.5697e-01, 4.6274e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18655534088611603 tensor([0.1866, 0.2331, 0.0016, 0.1556, 0.4232], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.033701784908771515 tensor([3.9524e-01, 5.5180e-01, 1.5638e-04, 3.3702e-02, 1.9103e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009224734269082546 tensor([7.9595e-02, 9.0774e-01, 6.4129e-04, 2.7967e-03, 9.2247e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27203571796417236 tensor([0.0020, 0.2720, 0.3670, 0.0040, 0.3550], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03935159370303154 tensor([3.1466e-01, 2.5922e-02, 2.3075e-05, 6.2004e-01, 3.9352e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.053508929908275604 tensor([0.0535, 0.0305, 0.0009, 0.4211, 0.4939], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10878844559192657 tensor([5.7206e-04, 1.0879e-01, 5.9488e-01, 2.9452e-03, 2.9281e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05882694944739342 tensor([3.5454e-01, 4.2092e-02, 4.5728e-05, 5.4449e-01, 5.8827e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06774596124887466 tensor([0.0124, 0.1419, 0.0677, 0.0596, 0.7183], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09343121945858002 tensor([5.3110e-01, 9.3431e-02, 4.0528e-05, 3.5821e-01, 1.7217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011656471760943532 tensor([1.1310e-07, 1.1656e-03, 9.9697e-01, 1.5006e-06, 1.8583e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06150757893919945 tensor([5.1064e-04, 6.1508e-02, 5.7066e-01, 6.0128e-03, 3.6131e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008366509340703487 tensor([9.8002e-01, 1.1594e-02, 1.2486e-09, 8.3665e-03, 2.0929e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0026082319673150778 tensor([2.1337e-06, 2.6082e-03, 9.7523e-01, 6.1204e-05, 2.2102e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005360990297049284 tensor([9.6312e-01, 5.3610e-03, 1.5832e-09, 3.1483e-02, 3.5951e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002276943065226078 tensor([5.3159e-04, 3.8550e-04, 2.2769e-03, 1.8267e-01, 8.1413e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013686250895261765 tensor([9.3096e-01, 5.5234e-02, 8.1189e-08, 1.3686e-02, 1.2056e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0073403362184762955 tensor([0.0307, 0.9567, 0.0033, 0.0019, 0.0073], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022644026204943657 tensor([6.9134e-05, 2.2644e-02, 7.2720e-01, 8.7117e-04, 2.4922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005634576082229614 tensor([6.2571e-02, 3.1062e-04, 2.3505e-07, 9.3148e-01, 5.6346e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012913159094750881 tensor([1.0504e-05, 4.6494e-05, 1.2913e-02, 2.3810e-02, 9.6322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025827696546912193 tensor([7.7141e-01, 2.0165e-01, 4.4124e-06, 2.5828e-02, 1.1028e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001947534503415227 tensor([1.9475e-03, 1.3741e-04, 8.3784e-05, 7.6551e-01, 2.3232e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.4640048650326207e-05 tensor([8.5233e-10, 2.4640e-05, 9.9864e-01, 1.7749e-07, 1.3381e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022797616198658943 tensor([5.9506e-04, 1.1890e-02, 1.3131e-01, 2.2798e-02, 8.3341e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012484180741012096 tensor([5.1383e-05, 3.4275e-04, 1.5867e-02, 1.2484e-02, 9.7125e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0030653721187263727 tensor([1.6829e-01, 1.0411e-03, 1.8207e-07, 8.2760e-01, 3.0654e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004419922363013029 tensor([6.7814e-01, 4.4199e-03, 6.1688e-08, 3.1561e-01, 1.8280e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06866665184497833 tensor([0.0032, 0.0338, 0.0715, 0.0687, 0.8228], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25535908341407776 tensor([0.1441, 0.2554, 0.0062, 0.3123, 0.2820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030070805922150612 tensor([8.5582e-01, 1.1305e-01, 1.6214e-06, 3.0071e-02, 1.0578e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006791801191866398 tensor([0.0019, 0.0030, 0.0068, 0.2020, 0.7863], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013330100104212761 tensor([7.5037e-04, 7.8534e-01, 2.0048e-01, 1.0256e-04, 1.3330e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000759078306145966 tensor([1.9654e-07, 7.5908e-04, 9.9177e-01, 5.6548e-06, 7.4606e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0019310354255139828 tensor([0.0019, 0.0009, 0.0014, 0.3669, 0.6289], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1979476660490036 tensor([0.3612, 0.2481, 0.0005, 0.1979, 0.1922], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002336443169042468 tensor([7.4518e-01, 2.3364e-03, 7.5103e-09, 2.5227e-01, 2.1449e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011240646708756685 tensor([9.7762e-03, 9.8796e-01, 1.1241e-03, 1.0672e-04, 1.0351e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007937475456856191 tensor([7.9657e-07, 7.9375e-04, 9.3436e-01, 6.1378e-05, 6.4781e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02500052936375141 tensor([0.0828, 0.8803, 0.0034, 0.0085, 0.0250], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001117223291657865 tensor([2.2205e-05, 1.5059e-05, 1.1172e-03, 6.5680e-02, 9.3317e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026802217587828636 tensor([9.3917e-01, 3.3875e-02, 8.7124e-08, 2.6802e-02, 1.5137e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010449160821735859 tensor([3.9728e-02, 2.3680e-04, 3.7879e-07, 9.4959e-01, 1.0449e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0036556783597916365 tensor([3.5419e-06, 3.6557e-03, 9.6673e-01, 9.3911e-05, 2.9521e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06170652061700821 tensor([0.0257, 0.0617, 0.0121, 0.2628, 0.6377], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.032063934952020645 tensor([0.0194, 0.0321, 0.0076, 0.2398, 0.7012], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1965491622686386 tensor([0.1965, 0.1286, 0.0009, 0.3362, 0.3378], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0030129177030175924 tensor([6.8838e-02, 9.2639e-01, 3.3400e-04, 1.4233e-03, 3.0129e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.054572075605392456 tensor([0.0546, 0.8166, 0.0204, 0.0214, 0.0870], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08056151866912842 tensor([7.6150e-01, 8.0562e-02, 4.8988e-06, 1.5312e-01, 4.8065e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002236742991954088 tensor([8.2127e-07, 2.2367e-03, 9.8108e-01, 1.3644e-05, 1.6667e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004357139579951763 tensor([1.5175e-06, 4.3571e-03, 9.8199e-01, 1.5759e-05, 1.3631e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01409764401614666 tensor([7.8687e-05, 2.3509e-04, 1.4098e-02, 5.1524e-02, 9.3406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10875824838876724 tensor([4.5854e-01, 3.8729e-01, 2.0259e-04, 1.0876e-01, 4.5207e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005300180055201054 tensor([2.3689e-01, 1.3046e-03, 1.3795e-07, 7.5651e-01, 5.3002e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009323984384536743 tensor([9.3240e-03, 2.2054e-03, 3.7408e-04, 4.2794e-01, 5.6015e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.041577331721782684 tensor([1.9748e-01, 2.1235e-02, 6.0154e-05, 7.3965e-01, 4.1577e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00031318870605900884 tensor([4.6602e-02, 9.5286e-01, 5.0106e-05, 1.7265e-04, 3.1319e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.177347000222653e-05 tensor([1.2758e-08, 4.1773e-05, 9.9060e-01, 5.4839e-06, 9.3525e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.051692280918359756 tensor([0.0517, 0.6663, 0.0201, 0.0317, 0.2302], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.036292437463998795 tensor([0.0053, 0.0363, 0.0258, 0.0586, 0.8739], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006168576423078775 tensor([2.9962e-01, 3.6996e-03, 5.3853e-07, 6.9051e-01, 6.1686e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004536096006631851 tensor([8.2252e-01, 1.7282e-01, 3.3008e-07, 4.5361e-03, 1.2877e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.033727165311574936 tensor([5.5555e-01, 4.0805e-01, 4.9440e-05, 3.3727e-02, 2.6205e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07382848858833313 tensor([7.3578e-01, 1.8309e-01, 1.3894e-05, 7.3828e-02, 7.2807e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01126027014106512 tensor([1.4601e-04, 5.4259e-03, 1.9795e-01, 1.1260e-02, 7.8522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05908529832959175 tensor([8.3965e-01, 9.9974e-02, 1.8310e-06, 5.9085e-02, 1.2900e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2003818303346634 tensor([0.0061, 0.4168, 0.2004, 0.0096, 0.3672], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00032286145142279565 tensor([2.7724e-08, 3.2286e-04, 9.9715e-01, 9.6273e-07, 2.5237e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.024469686672091484 tensor([2.4470e-02, 9.9551e-04, 1.4839e-05, 9.2014e-01, 5.4379e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.235143256257288e-05 tensor([1.7051e-08, 2.2351e-05, 9.4185e-01, 2.0994e-05, 5.8104e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0032222748268395662 tensor([4.0918e-01, 5.8680e-01, 1.0456e-05, 3.2223e-03, 7.8894e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04782096669077873 tensor([6.9868e-01, 4.7821e-02, 4.1611e-06, 2.4350e-01, 9.9987e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014931758865714073 tensor([1.1782e-05, 1.9635e-02, 9.6537e-01, 5.2736e-05, 1.4932e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.440778765361756e-05 tensor([5.6244e-01, 8.4408e-05, 2.0510e-11, 4.3747e-01, 1.3803e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017498202214483172 tensor([1.7498e-04, 1.9222e-05, 1.4970e-04, 5.5477e-01, 4.4488e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.212857186794281 tensor([0.0072, 0.4118, 0.2129, 0.0138, 0.3544], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07512977719306946 tensor([0.0280, 0.0751, 0.0122, 0.2195, 0.6652], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012052974198013544 tensor([3.1948e-07, 1.2053e-03, 9.9052e-01, 6.3059e-06, 8.2717e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001179960963781923 tensor([5.4826e-01, 1.1800e-04, 4.6422e-11, 4.5160e-01, 2.3137e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008759651682339609 tensor([8.7597e-04, 4.6222e-05, 4.9605e-05, 6.8931e-01, 3.0972e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14333504438400269 tensor([2.3311e-01, 8.5754e-02, 4.9096e-04, 5.3731e-01, 1.4334e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00443162489682436 tensor([5.0820e-01, 4.8658e-01, 5.3623e-06, 4.4316e-03, 7.7693e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008110617636702955 tensor([1.0047e-07, 2.6203e-03, 9.9657e-01, 4.4345e-07, 8.1106e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08999744802713394 tensor([6.7844e-01, 2.2344e-01, 2.4019e-05, 8.9997e-02, 8.0979e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017253529280424118 tensor([1.9792e-04, 2.2240e-03, 3.1102e-02, 1.7254e-02, 9.4922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00040950265247374773 tensor([9.2549e-01, 4.0950e-04, 1.3163e-11, 7.4098e-02, 3.0285e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015971776098012924 tensor([4.2956e-04, 1.5972e-02, 1.9423e-01, 1.5750e-02, 7.7362e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011546625755727291 tensor([2.5192e-07, 1.1547e-03, 9.9117e-01, 5.0564e-06, 7.6659e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.023776929825544357 tensor([6.8578e-01, 2.3777e-02, 1.5000e-06, 2.8716e-01, 3.2815e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.010075544007123e-05 tensor([1.1799e-07, 7.3655e-05, 8.2498e-01, 8.0101e-05, 1.7487e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 0], [2, 3, 4], [0, 3, 1], [2, 4, 1], [2, 1, 0], [0, 3, 1], [0, 2, 1], [2, 4, 3], [2, 4, 1], [2, 1, 0], [0, 1, 2], [0, 3, 2], [3, 0, 2], [2, 1, 4], [1, 2, 0], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 3, 0], [2, 1, 0], [3, 0, 2], [0, 3, 4], [0, 3, 1], [2, 0, 3], [2, 1, 4], [0, 3, 2], [0, 3], [2, 3, 4], [2, 1, 4], [2, 3, 4], [2, 4, 1], [2, 3, 4], [0, 3, 1], [2, 1, 4], [2, 3, 0], [2, 4, 1], [2, 3, 4], [0, 3, 1], [2, 1, 4], [1, 2, 0], [2, 4, 3], [2, 1, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3], [0, 1, 3], [0, 3, 1], [2, 1, 0], [0, 1, 2], [0, 3, 1], [2, 4, 1], [0, 2, 3], [2, 1, 0], [0, 1, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [3, 0, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 1, 0], [2, 3, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 3, 4], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [2, 1, 4], [2, 0, 3], [0, 3, 1], [2, 0], [0, 3, 1], [0, 1, 2], [1, 0, 2], [2, 1, 4], [2, 4, 3], [0, 3, 1], [2, 3, 4], [2, 1, 0], [0, 3, 4], [2, 3, 0], [0, 3, 1], [3, 0, 2], [0, 1, 3], [2, 3, 4], [3, 2, 0], [0, 3, 1], [2, 1, 4], [1, 2, 0], [2, 4, 3], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4], [3, 2, 4], [0, 1, 3], [2, 4, 1], [0, 2, 1], [3, 0, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 0], [1, 0, 2], [2, 1, 0], [0, 1, 3], [0, 3, 4], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 3, 4], [2, 1, 0], [2, 4, 1], [2, 1, 4], [2, 1, 0], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 0], [0, 3], [3, 0, 4], [0, 3, 1], [2, 1, 4], [0, 1, 3], [2, 4, 3], [2, 1, 4], [0, 1, 3], [2, 1, 4], [3, 4, 2], [2, 1, 4], [2, 4, 3], [0, 3, 1], [2, 1, 4], [1, 2, 0], [0, 3], [3, 0, 2], [0, 3, 1], [2, 1, 4], [2, 3, 0], [2, 4, 3], [2, 3, 4], [0, 3], [2, 1, 4], [2, 1, 0], [0, 3, 1], [2, 1, 4], [0, 3, 2], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [1, 0, 2], [2, 4, 3], [3, 2, 4], [0, 3, 1], [2, 1, 4], [0, 1, 2], [2, 4, 3], [2, 1, 0], [0, 3, 1], [0, 1, 3], [0, 1, 3], [2, 1, 4], [2, 4, 1], [0, 1, 3], [2, 0], [2, 4, 3], [0, 1, 2], [3, 0, 4], [0, 3, 1], [1, 2, 0], [2, 4, 3], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 3, 4], [1, 0, 2], [2, 4, 3], [2, 1, 4], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 1, 0], [2, 3, 4], [3, 2, 0], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 1, 4], [2, 1, 0], [2, 1, 4], [2, 3, 4], [0, 3, 1], [2, 3, 0], [0, 2, 1], [2, 1, 4], [2, 4, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [2, 4, 3], [2, 4, 1], [0, 3, 4], [2, 4, 1], [1, 2, 0], [0, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1], [1, 2, 0], [2, 1, 4], [2, 4, 3], [0, 3, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 1, 3]]\n",
      "[[3, 4], [0, 1], [2, 4], [0, 3], [3, 4], [2, 4], [3, 4], [0, 1], [0, 3], [3, 4], [3, 4], [1, 4], [1, 4], [0, 3], [3, 4], [0, 3], [0, 1], [2, 4], [1, 4], [3, 4], [1, 4], [1, 2], [2, 4], [1, 4], [0, 3], [1, 4], [1, 2, 4], [0, 1], [0, 3], [0, 1], [0, 3], [0, 1], [2, 4], [0, 3], [1, 4], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [0, 1], [3, 4], [2, 4], [0, 3], [2, 4], [1, 2, 4], [2, 4], [2, 4], [3, 4], [3, 4], [2, 4], [0, 3], [1, 4], [3, 4], [3, 4], [0, 3], [0, 1], [2, 4], [3, 4], [2, 4], [1, 4], [0, 1], [2, 4], [0, 3], [2, 4], [3, 4], [0, 1], [2, 4], [0, 3], [2, 4], [0, 3], [0, 1], [2, 4], [0, 1], [2, 4], [0, 3], [3, 4], [2, 4], [0, 3], [1, 4], [2, 4], [1, 3, 4], [2, 4], [3, 4], [3, 4], [0, 3], [0, 1], [2, 4], [0, 1], [3, 4], [1, 2], [1, 4], [2, 4], [1, 4], [2, 4], [0, 1], [1, 4], [2, 4], [0, 3], [3, 4], [0, 1], [0, 1], [2, 4], [0, 3], [2, 4], [0, 1, 3], [0, 1], [2, 4], [0, 3], [3, 4], [1, 2], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [0, 3], [0, 3], [1, 3, 4], [3, 4], [3, 4], [2, 4], [1, 2], [0, 3], [0, 3], [0, 3], [0, 1], [3, 4], [0, 3], [0, 3], [3, 4], [0, 3], [2, 4], [0, 3], [2, 4], [0, 1], [1, 2], [2, 4], [0, 3], [1, 3, 4], [1, 2, 4], [1, 2], [2, 4], [0, 3], [2, 4], [0, 1], [0, 3], [2, 4], [0, 3], [0, 1], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [1, 2, 4], [1, 4], [2, 4], [0, 3], [1, 4], [0, 1], [0, 1], [1, 2, 4], [0, 3], [3, 4], [2, 4], [0, 3], [1, 4], [0, 3], [2, 4], [2, 4], [0, 1], [2, 4], [0, 3], [3, 4], [0, 1], [0, 1], [2, 4], [0, 3], [3, 4], [0, 1], [3, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [1, 3, 4], [0, 1], [3, 4], [1, 2], [2, 4], [3, 4], [0, 1], [0, 3], [0, 1], [2, 4], [0, 1], [3, 4], [0, 1], [0, 3], [2, 4], [3, 4], [3, 4], [3, 4], [0, 1], [1, 4], [0, 3], [2, 4], [2, 4], [3, 4], [0, 1], [0, 3], [3, 4], [0, 3], [0, 1], [2, 4], [1, 4], [3, 4], [0, 3], [0, 1], [0, 1], [0, 1], [2, 4], [0, 1], [1, 2, 4], [2, 4], [3, 4], [2, 4], [0, 1], [0, 3], [1, 2], [0, 3], [3, 4], [1, 2, 4], [3, 4], [2, 4], [0, 3], [3, 4], [0, 3], [0, 1], [1, 2], [0, 1], [2, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 4]]\n",
      "NL_pred of 2th iteration [[2, 1, 0], [2, 3, 4], [0, 3, 1], [2, 4, 1], [2, 1, 0], [0, 3, 1], [0, 2, 1], [2, 4, 3], [2, 4, 1], [2, 1, 0], [0, 1, 2], [0, 3, 2], [3, 0, 2], [2, 1, 4], [1, 2, 0], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 3, 0], [2, 1, 0], [3, 0, 2], [0, 3, 4], [0, 3, 1], [2, 0, 3], [2, 1, 4], [0, 3, 2], [2, 3, 4], [2, 1, 4], [2, 3, 4], [2, 4, 1], [2, 3, 4], [0, 3, 1], [2, 1, 4], [2, 3, 0], [2, 4, 1], [2, 3, 4], [0, 3, 1], [2, 1, 4], [1, 2, 0], [2, 4, 3], [2, 1, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 1, 0], [0, 1, 2], [0, 3, 1], [2, 4, 1], [0, 2, 3], [2, 1, 0], [0, 1, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [3, 0, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 1, 0], [2, 3, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [2, 3, 4], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [2, 1, 4], [2, 0, 3], [0, 3, 1], [0, 3, 1], [0, 1, 2], [1, 0, 2], [2, 1, 4], [2, 4, 3], [0, 3, 1], [2, 3, 4], [2, 1, 0], [0, 3, 4], [2, 3, 0], [0, 3, 1], [3, 0, 2], [0, 1, 3], [2, 3, 4], [3, 2, 0], [0, 3, 1], [2, 1, 4], [1, 2, 0], [2, 4, 3], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [3, 2, 4], [0, 1, 3], [2, 4, 1], [0, 2, 1], [3, 0, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 1, 4], [2, 4, 1], [1, 0, 2], [2, 1, 0], [0, 1, 3], [0, 3, 4], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 3, 4], [2, 1, 0], [2, 4, 1], [2, 1, 4], [2, 1, 0], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 3, 1], [2, 1, 4], [3, 0, 4], [0, 3, 1], [2, 1, 4], [0, 1, 3], [2, 4, 3], [2, 1, 4], [0, 1, 3], [2, 1, 4], [3, 4, 2], [2, 1, 4], [2, 4, 3], [0, 3, 1], [2, 1, 4], [1, 2, 0], [3, 0, 2], [0, 3, 1], [2, 1, 4], [2, 3, 0], [2, 4, 3], [2, 3, 4], [2, 1, 4], [2, 1, 0], [0, 3, 1], [2, 1, 4], [0, 3, 2], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [1, 0, 2], [2, 4, 3], [3, 2, 4], [0, 3, 1], [2, 1, 4], [0, 1, 2], [2, 4, 3], [2, 1, 0], [0, 3, 1], [0, 1, 3], [0, 1, 3], [2, 1, 4], [2, 4, 1], [0, 1, 3], [2, 4, 3], [0, 1, 2], [3, 0, 4], [0, 3, 1], [1, 2, 0], [2, 4, 3], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 3, 4], [1, 0, 2], [2, 4, 3], [2, 1, 4], [0, 3, 1], [2, 0, 1], [2, 0, 1], [2, 1, 0], [2, 3, 4], [3, 2, 0], [2, 4, 1], [0, 3, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 1, 4], [2, 1, 0], [2, 1, 4], [2, 3, 4], [0, 3, 1], [2, 3, 0], [0, 2, 1], [2, 1, 4], [2, 4, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [2, 4, 3], [2, 4, 1], [0, 3, 4], [2, 4, 1], [1, 2, 0], [2, 0, 1], [0, 3, 1], [2, 4, 1], [1, 2, 0], [2, 1, 4], [2, 4, 3], [0, 3, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004841757421733953  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004841707834676534  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004841614671114112  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004841482438960997  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004841318150528339  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004841126313730448  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004840911937361004  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004840679028454949  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004840433597564697  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00484017764820772  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004839915187418962  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004839649220474628  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004839381750892191  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004839115783947857  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00483885282227973  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004838593867646546  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0048383404226864084  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004838092988278686  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0048378535679408485  Accuracy on Support set:0.0\n",
      "torch.Size([238, 2048]) torch.Size([238])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004837620158155425  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.0757639929652214 tensor([3.4925e-02, 1.8357e-03, 2.6385e-05, 8.8745e-01, 7.5764e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.150882288813591 tensor([0.1509, 0.7674, 0.0025, 0.0311, 0.0481], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15429037809371948 tensor([8.8960e-05, 2.5710e-02, 8.1883e-01, 1.0790e-03, 1.5429e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30348485708236694 tensor([3.0348e-01, 1.1937e-04, 3.7675e-10, 6.9627e-01, 1.2524e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2814931273460388 tensor([0.0890, 0.0649, 0.0018, 0.2815, 0.5628], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4373979866504669 tensor([0.0008, 0.0977, 0.4374, 0.0036, 0.4605], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05852382630109787 tensor([0.0042, 0.0341, 0.0396, 0.0585, 0.8635], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3178318440914154 tensor([0.3178, 0.5426, 0.0012, 0.1046, 0.0339], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.43798235058784485 tensor([4.3798e-01, 4.8993e-04, 2.8472e-09, 5.6127e-01, 2.5881e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2506563663482666 tensor([0.0151, 0.0100, 0.0014, 0.2507, 0.7229], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13989685475826263 tensor([0.0016, 0.0067, 0.0337, 0.1399, 0.8181], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29305896162986755 tensor([0.0129, 0.2931, 0.1075, 0.0335, 0.5531], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06144952028989792 tensor([0.0198, 0.8702, 0.0419, 0.0067, 0.0614], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33260947465896606 tensor([3.3261e-01, 6.9542e-03, 1.4275e-06, 6.5087e-01, 9.5646e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3753572404384613 tensor([1.0429e-04, 6.6417e-06, 7.3095e-05, 6.2446e-01, 3.7536e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3820541799068451 tensor([4.8460e-01, 1.0561e-01, 9.9701e-05, 3.8205e-01, 2.7639e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21262937784194946 tensor([6.5701e-01, 2.1263e-01, 3.8488e-05, 1.0129e-01, 2.9033e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0748467817902565 tensor([2.5114e-06, 1.8360e-03, 9.2320e-01, 1.1128e-04, 7.4847e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3004593849182129 tensor([0.1094, 0.4855, 0.0085, 0.0960, 0.3005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14392144978046417 tensor([1.1487e-01, 1.9193e-02, 1.5685e-04, 7.2186e-01, 1.4392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22998324036598206 tensor([0.0278, 0.6611, 0.0608, 0.0203, 0.2300], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01781896874308586 tensor([4.2002e-06, 1.7819e-02, 9.6805e-01, 1.1380e-05, 1.4117e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24086327850818634 tensor([0.0019, 0.0806, 0.2409, 0.0173, 0.6593], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3159327208995819 tensor([0.0557, 0.3159, 0.0253, 0.1032, 0.4998], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17361564934253693 tensor([1.7362e-01, 8.7697e-03, 1.5920e-05, 7.7641e-01, 4.1184e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3390600085258484 tensor([0.0090, 0.3391, 0.1927, 0.0241, 0.4352], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24021756649017334 tensor([0.0016, 0.2402, 0.3580, 0.0039, 0.3962], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10860460251569748 tensor([0.1086, 0.7955, 0.0047, 0.0203, 0.0710], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2960539758205414 tensor([2.9605e-01, 5.7009e-03, 1.6911e-06, 6.8800e-01, 1.0248e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09986595809459686 tensor([0.0999, 0.8708, 0.0018, 0.0077, 0.0199], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07914163172245026 tensor([8.9306e-01, 2.7273e-02, 1.8276e-07, 7.9142e-02, 5.2816e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11367358267307281 tensor([0.1137, 0.8030, 0.0027, 0.0196, 0.0611], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010208775056526065 tensor([7.3974e-09, 2.5938e-04, 9.9872e-01, 1.9429e-07, 1.0209e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2938949763774872 tensor([2.9389e-01, 1.0771e-03, 5.7597e-08, 7.0358e-01, 1.4496e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10900792479515076 tensor([0.0471, 0.8235, 0.0088, 0.0115, 0.1090], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.38685595989227295 tensor([3.8686e-01, 1.0086e-01, 1.2574e-04, 4.0953e-01, 1.0263e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.041788943111896515 tensor([4.1789e-02, 9.5645e-01, 2.2257e-04, 4.4004e-04, 1.0995e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4644308090209961 tensor([1.5177e-04, 1.6117e-02, 5.1565e-01, 3.6542e-03, 4.6443e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0726880356669426 tensor([7.2688e-02, 2.7853e-05, 7.3069e-10, 9.2707e-01, 2.1020e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41610512137413025 tensor([3.6308e-04, 3.3768e-05, 1.4174e-04, 5.8336e-01, 4.1611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.048119544982910156 tensor([9.0944e-01, 4.8120e-02, 2.7593e-07, 4.2143e-02, 2.9953e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24823640286922455 tensor([9.6784e-02, 2.4266e-02, 3.2953e-04, 6.3038e-01, 2.4824e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10533878207206726 tensor([0.0051, 0.0677, 0.1053, 0.0619, 0.7600], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18417109549045563 tensor([8.0520e-01, 1.0024e-02, 9.9405e-08, 1.8417e-01, 6.0657e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03228060156106949 tensor([1.4245e-06, 2.1951e-03, 9.6548e-01, 4.2636e-05, 3.2281e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2680399417877197 tensor([0.0023, 0.2680, 0.4376, 0.0065, 0.2855], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.47444555163383484 tensor([3.0690e-06, 2.8327e-04, 4.7445e-01, 1.8854e-03, 5.2338e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1806771457195282 tensor([8.5108e-06, 2.5984e-03, 8.1606e-01, 6.5705e-04, 1.8068e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2126757949590683 tensor([8.4100e-02, 1.2950e-02, 1.6480e-04, 6.9011e-01, 2.1268e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13033290207386017 tensor([5.2648e-04, 1.0475e-03, 9.9961e-03, 1.3033e-01, 8.5810e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2866936922073364 tensor([1.3301e-04, 2.1036e-02, 6.8963e-01, 2.5047e-03, 2.8669e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31984466314315796 tensor([3.1984e-01, 1.3787e-01, 3.1399e-04, 4.3493e-01, 1.0704e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11493266373872757 tensor([0.0127, 0.1149, 0.0797, 0.1122, 0.6805], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4723249673843384 tensor([0.0243, 0.0176, 0.0026, 0.4723, 0.4832], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08205774426460266 tensor([0.0019, 0.0061, 0.0113, 0.0821, 0.8987], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02366947941482067 tensor([9.7369e-01, 2.6340e-03, 2.0806e-10, 2.3669e-02, 6.9993e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4109855890274048 tensor([5.8220e-01, 4.1099e-01, 4.6405e-06, 5.6948e-03, 1.1186e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.463818222284317 tensor([4.7709e-04, 3.3665e-02, 4.6382e-01, 9.3589e-03, 4.9268e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.33931514620780945 tensor([1.0094e-01, 3.2318e-02, 4.0863e-04, 5.2702e-01, 3.3932e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.351212739944458 tensor([2.7956e-05, 2.6346e-03, 3.5121e-01, 2.1692e-03, 6.4396e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18094857037067413 tensor([0.0225, 0.7090, 0.0707, 0.0168, 0.1809], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18116192519664764 tensor([1.8116e-01, 8.1818e-01, 7.8394e-06, 4.6545e-04, 1.8812e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0012632326688617468 tensor([3.9015e-09, 8.8205e-05, 9.9865e-01, 3.1194e-07, 1.2632e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15582123398780823 tensor([8.4176e-01, 2.2811e-03, 3.1984e-09, 1.5582e-01, 1.3543e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.381779283285141 tensor([8.5596e-05, 1.1944e-02, 6.0394e-01, 2.2520e-03, 3.8178e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14685165882110596 tensor([1.0706e-01, 2.8395e-02, 3.5826e-04, 7.1733e-01, 1.4685e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.050435278564691544 tensor([0.0504, 0.9235, 0.0029, 0.0049, 0.0183], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07539493590593338 tensor([1.6247e-05, 1.0291e-02, 9.1410e-01, 1.9319e-04, 7.5395e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21455015242099762 tensor([7.5898e-01, 2.4621e-02, 8.5793e-07, 2.1455e-01, 1.8473e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18686331808567047 tensor([0.0024, 0.0595, 0.1869, 0.0360, 0.7152], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.045538753271102905 tensor([9.4274e-01, 1.1622e-02, 1.8192e-08, 4.5539e-02, 9.9844e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18775413930416107 tensor([1.8775e-01, 7.9685e-01, 3.0239e-04, 6.6324e-03, 8.4571e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.323882132768631 tensor([0.0038, 0.1592, 0.3239, 0.0206, 0.4925], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4416288733482361 tensor([4.4163e-01, 4.9744e-01, 2.5762e-04, 4.5524e-02, 1.5151e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4080153703689575 tensor([0.0006, 0.0394, 0.4080, 0.0108, 0.5412], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02635873109102249 tensor([2.6359e-02, 4.7021e-04, 2.8298e-06, 9.4569e-01, 2.7480e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3782162666320801 tensor([0.0770, 0.0704, 0.0024, 0.3782, 0.4720], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024208800867199898 tensor([6.0166e-06, 1.1590e-02, 9.6416e-01, 3.6411e-05, 2.4209e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2816029489040375 tensor([2.8160e-01, 4.1576e-03, 7.4400e-07, 7.0584e-01, 8.3977e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16012917459011078 tensor([0.0557, 0.1601, 0.0095, 0.1536, 0.6211], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0055768475867807865 tensor([2.5064e-07, 1.6383e-03, 9.9278e-01, 2.8950e-06, 5.5768e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22598259150981903 tensor([0.1312, 0.3394, 0.0077, 0.2260, 0.2957], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11868206411600113 tensor([2.0346e-06, 1.0793e-03, 8.8002e-01, 2.1824e-04, 1.1868e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13362707197666168 tensor([0.0011, 0.0019, 0.0050, 0.1336, 0.8583], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18365149199962616 tensor([8.5434e-05, 4.1785e-05, 1.3523e-03, 1.8365e-01, 8.1487e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1909179389476776 tensor([1.9092e-01, 3.8434e-02, 2.2105e-04, 7.1007e-01, 6.0356e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4789930582046509 tensor([4.7899e-01, 5.1822e-01, 2.6708e-06, 2.3766e-03, 4.0496e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000764962809626013 tensor([2.5822e-10, 1.4581e-05, 9.9922e-01, 6.3672e-08, 7.6496e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.060062963515520096 tensor([0.0601, 0.8530, 0.0129, 0.0142, 0.0600], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.033601704984903336 tensor([9.7351e-04, 6.4224e-06, 7.6581e-07, 9.6542e-01, 3.3602e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33958399295806885 tensor([0.0044, 0.4435, 0.3396, 0.0051, 0.2074], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13063132762908936 tensor([0.0607, 0.7713, 0.0134, 0.0239, 0.1306], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2800278663635254 tensor([0.0019, 0.0802, 0.2800, 0.0191, 0.6188], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3309708535671234 tensor([0.0297, 0.5458, 0.0602, 0.0334, 0.3310], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07540294528007507 tensor([7.7903e-04, 1.2844e-02, 7.5403e-02, 1.9806e-02, 8.9117e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08675567060709 tensor([0.0868, 0.8739, 0.0040, 0.0127, 0.0226], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01713906228542328 tensor([1.3913e-02, 9.5963e-01, 8.3961e-03, 9.2562e-04, 1.7139e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3259452283382416 tensor([0.0016, 0.0921, 0.3259, 0.0125, 0.5678], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13581784069538116 tensor([1.3582e-01, 8.2691e-05, 1.4846e-09, 8.6389e-01, 2.0798e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27407827973365784 tensor([4.0015e-04, 1.9960e-05, 4.9330e-05, 7.2545e-01, 2.7408e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03473428636789322 tensor([9.4377e-01, 3.4734e-02, 7.0463e-08, 2.1294e-02, 2.0063e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12923726439476013 tensor([1.2924e-01, 8.6992e-01, 2.5871e-05, 5.1291e-04, 3.0050e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2622220814228058 tensor([0.0043, 0.1446, 0.2622, 0.0248, 0.5641], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2007109820842743 tensor([7.8821e-01, 1.0253e-02, 1.5288e-07, 2.0071e-01, 8.2170e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22131496667861938 tensor([6.9253e-04, 2.3538e-02, 2.2131e-01, 1.8581e-02, 7.3587e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24653275310993195 tensor([0.3323, 0.2465, 0.0008, 0.2983, 0.1221], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012732523493468761 tensor([0.0127, 0.9644, 0.0085, 0.0011, 0.0132], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.028139885514974594 tensor([2.9564e-05, 2.7968e-04, 2.8140e-02, 1.1755e-02, 9.5980e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15460458397865295 tensor([8.3965e-01, 5.2292e-03, 1.8061e-08, 1.5460e-01, 5.1542e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11941605806350708 tensor([0.0035, 0.0050, 0.0049, 0.1194, 0.8672], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4075682461261749 tensor([0.0024, 0.4562, 0.4076, 0.0022, 0.1316], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.45698902010917664 tensor([5.2593e-01, 8.7970e-03, 7.7111e-07, 4.5699e-01, 8.2798e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004826391115784645 tensor([1.4473e-09, 1.1540e-05, 9.9516e-01, 1.1592e-06, 4.8264e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2984878718852997 tensor([0.3419, 0.2985, 0.0008, 0.2175, 0.1413], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030773647129535675 tensor([1.4486e-04, 1.1258e-03, 3.0774e-02, 3.0346e-02, 9.3761e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4102150499820709 tensor([4.8988e-01, 7.8319e-02, 4.7995e-05, 4.1022e-01, 2.1538e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08680430799722672 tensor([8.6804e-02, 5.0623e-04, 2.4813e-07, 9.0754e-01, 5.1459e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21146461367607117 tensor([7.6247e-01, 2.3487e-02, 8.4510e-07, 2.1146e-01, 2.5781e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18967702984809875 tensor([0.0803, 0.1897, 0.0151, 0.2869, 0.4280], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1995982825756073 tensor([8.9337e-04, 6.1606e-04, 1.8994e-03, 1.9960e-01, 7.9699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25087419152259827 tensor([0.0199, 0.0159, 0.0035, 0.7098, 0.2509], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03865043818950653 tensor([0.0015, 0.0185, 0.0387, 0.0240, 0.9174], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2275743931531906 tensor([0.0008, 0.2276, 0.6384, 0.0010, 0.1322], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4227267801761627 tensor([4.2273e-01, 5.7526e-03, 5.9751e-07, 5.6685e-01, 4.6669e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24079565703868866 tensor([2.4080e-01, 3.6707e-02, 8.8966e-05, 6.1975e-01, 1.0266e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24290816485881805 tensor([5.3115e-01, 1.8177e-01, 1.1783e-04, 2.4291e-01, 4.4052e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.047443095594644547 tensor([4.7443e-02, 9.4821e-01, 4.8670e-04, 9.5738e-04, 2.9010e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32420939207077026 tensor([0.1673, 0.0988, 0.0012, 0.4085, 0.3242], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.46984153985977173 tensor([4.6984e-01, 1.4500e-03, 2.0725e-08, 5.2781e-01, 8.9422e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06725798547267914 tensor([6.7258e-02, 3.5845e-03, 2.1265e-05, 8.6279e-01, 6.6343e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29878824949264526 tensor([0.0356, 0.0117, 0.0007, 0.6532, 0.2988], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.057496801018714905 tensor([9.3381e-01, 8.5807e-03, 1.2381e-08, 5.7497e-02, 1.0927e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004056453239172697 tensor([3.7301e-08, 3.0920e-04, 9.9563e-01, 1.6515e-06, 4.0565e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31034472584724426 tensor([6.4000e-01, 4.1462e-02, 5.6841e-06, 3.1034e-01, 8.1918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40134069323539734 tensor([3.4897e-04, 3.8787e-02, 4.0134e-01, 2.8495e-03, 5.5667e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014739913865923882 tensor([9.7132e-01, 1.4740e-02, 4.7203e-09, 1.3914e-02, 2.5161e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.025718607008457184 tensor([0.0094, 0.9453, 0.0257, 0.0012, 0.0184], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06113487109541893 tensor([6.6210e-05, 3.9261e-02, 8.9925e-01, 2.8865e-04, 6.1135e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14937293529510498 tensor([1.4937e-01, 1.1450e-03, 3.8051e-07, 8.4292e-01, 6.5626e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23663917183876038 tensor([0.1477, 0.3520, 0.0078, 0.2366, 0.2559], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22340089082717896 tensor([0.0073, 0.3087, 0.2234, 0.0193, 0.4412], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31381139159202576 tensor([0.0024, 0.6132, 0.3138, 0.0013, 0.0694], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07245295494794846 tensor([5.6916e-05, 4.8379e-02, 8.7891e-01, 2.0047e-04, 7.2453e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0996096208691597 tensor([9.9610e-02, 9.8398e-05, 5.1886e-09, 8.9958e-01, 7.1449e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07172033190727234 tensor([1.8992e-05, 3.4995e-04, 7.1720e-02, 7.7222e-03, 9.2019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2579471170902252 tensor([7.3317e-01, 2.5795e-01, 1.9780e-06, 8.6371e-03, 2.4596e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11641395837068558 tensor([1.1641e-01, 6.1832e-03, 1.7504e-05, 7.8234e-01, 9.5050e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.45547083020210266 tensor([1.0064e-06, 9.0228e-05, 5.4231e-01, 2.1320e-03, 4.5547e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10127614438533783 tensor([1.0128e-01, 4.8213e-03, 1.6206e-05, 8.1973e-01, 7.4154e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007680858019739389 tensor([7.6809e-03, 9.8269e-01, 6.9980e-03, 1.8393e-04, 2.4453e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28155717253685 tensor([2.8156e-01, 5.4744e-02, 1.5704e-04, 5.6734e-01, 9.6200e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11860886961221695 tensor([8.7195e-01, 1.1861e-01, 2.6608e-07, 9.1483e-03, 2.9635e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005016488721594214 tensor([6.1875e-10, 5.0083e-05, 9.9945e-01, 4.5757e-08, 5.0165e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1562141627073288 tensor([1.5621e-01, 4.0799e-05, 2.3815e-10, 8.4362e-01, 1.2486e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.37029650807380676 tensor([6.2294e-04, 1.2373e-04, 4.1113e-04, 3.7030e-01, 6.2855e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2055545598268509 tensor([0.0023, 0.2056, 0.3410, 0.0070, 0.4441], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2957523465156555 tensor([0.0056, 0.5267, 0.1658, 0.0062, 0.2958], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01642046496272087 tensor([5.4363e-08, 1.2362e-04, 9.8344e-01, 1.1863e-05, 1.6420e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21136103570461273 tensor([2.1136e-01, 1.4508e-03, 2.8008e-07, 7.8224e-01, 4.9468e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20604771375656128 tensor([0.1666, 0.2060, 0.0017, 0.1618, 0.4639], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.39576300978660583 tensor([3.9576e-01, 5.4231e-01, 1.8282e-04, 3.8831e-02, 2.2913e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08073076605796814 tensor([8.0731e-02, 9.0408e-01, 7.5626e-04, 3.2404e-03, 1.1194e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23551011085510254 tensor([0.0018, 0.2355, 0.3807, 0.0040, 0.3780], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2832781672477722 tensor([2.8328e-01, 2.2728e-02, 2.4205e-05, 6.5131e-01, 4.2655e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41784629225730896 tensor([0.0453, 0.0253, 0.0010, 0.4178, 0.5106], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30307596921920776 tensor([4.8369e-04, 9.0743e-02, 6.0280e-01, 2.8964e-03, 3.0308e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32410651445388794 tensor([3.2411e-01, 3.7434e-02, 4.8031e-05, 5.7453e-01, 6.3878e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1184762567281723 tensor([0.0106, 0.1185, 0.0679, 0.0590, 0.7440], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.39229273796081543 tensor([5.0212e-01, 8.6105e-02, 4.4356e-05, 3.9229e-01, 1.9437e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001864396734163165 tensor([9.8118e-08, 1.0071e-03, 9.9713e-01, 1.4568e-06, 1.8644e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37284645438194275 tensor([4.3073e-04, 5.0821e-02, 5.6997e-01, 5.9307e-03, 3.7285e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011024672538042068 tensor([9.7989e-01, 1.1025e-02, 1.2874e-09, 9.0605e-03, 2.2385e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022713659331202507 tensor([1.8342e-06, 2.1889e-03, 9.7503e-01, 6.0847e-05, 2.2714e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.034192368388175964 tensor([9.6064e-01, 5.1303e-03, 1.6601e-09, 3.4192e-02, 3.8944e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17705181241035461 tensor([4.4584e-04, 3.1809e-04, 2.2343e-03, 1.7705e-01, 8.1995e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.053014084696769714 tensor([9.3180e-01, 5.3014e-02, 8.5805e-08, 1.5054e-02, 1.3151e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03114127926528454 tensor([0.0311, 0.9540, 0.0039, 0.0022, 0.0088], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2536710798740387 tensor([5.7955e-05, 1.8681e-02, 7.2674e-01, 8.4711e-04, 2.5367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05435066670179367 tensor([5.4351e-02, 2.6590e-04, 2.4109e-07, 9.3951e-01, 5.8739e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023175526410341263 tensor([8.8401e-06, 3.8240e-05, 1.2595e-02, 2.3176e-02, 9.6418e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19621780514717102 tensor([7.7268e-01, 1.9622e-01, 5.0521e-06, 2.9786e-02, 1.3078e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24035833775997162 tensor([1.6905e-03, 1.1910e-04, 8.6041e-05, 7.5775e-01, 2.4036e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0013679666444659233 tensor([7.4717e-10, 2.1318e-05, 9.9861e-01, 1.7612e-07, 1.3680e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12890522181987762 tensor([4.9156e-04, 9.6796e-03, 1.2891e-01, 2.1880e-02, 8.3904e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015646006911993027 tensor([4.3130e-05, 2.8471e-04, 1.5646e-02, 1.2027e-02, 9.7200e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14850762486457825 tensor([1.4851e-01, 9.0319e-04, 1.8834e-07, 8.4736e-01, 3.2322e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3469037413597107 tensor([6.4688e-01, 4.1462e-03, 6.8523e-08, 3.4690e-01, 2.0683e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07045295834541321 tensor([0.0026, 0.0275, 0.0705, 0.0666, 0.8327], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22513532638549805 tensor([0.1296, 0.2251, 0.0066, 0.3293, 0.3093], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10998095571994781 tensor([8.5472e-01, 1.0998e-01, 1.8262e-06, 3.4073e-02, 1.2277e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19627991318702698 tensor([0.0016, 0.0025, 0.0066, 0.1963, 0.7930], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2269115000963211 tensor([7.2249e-04, 7.5712e-01, 2.2691e-01, 1.1045e-04, 1.5132e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007577936165034771 tensor([1.6846e-07, 6.4524e-04, 9.9177e-01, 5.5567e-06, 7.5779e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35768401622772217 tensor([0.0016, 0.0007, 0.0014, 0.3577, 0.6386], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2312200665473938 tensor([0.3399, 0.2312, 0.0006, 0.2125, 0.2157], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.280412882566452 tensor([7.1716e-01, 2.1878e-03, 8.2696e-09, 2.8041e-01, 2.4232e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009796066209673882 tensor([9.7961e-03, 9.8758e-01, 1.3025e-03, 1.1899e-04, 1.2044e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06624892354011536 tensor([6.7581e-07, 6.6108e-04, 9.3303e-01, 6.0597e-05, 6.6249e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08290055394172668 tensor([0.0829, 0.8736, 0.0040, 0.0096, 0.0298], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06299867480993271 tensor([1.8495e-05, 1.2494e-05, 1.1071e-03, 6.2999e-02, 9.3586e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.032602615654468536 tensor([9.3755e-01, 3.2603e-02, 9.3631e-08, 2.9683e-02, 1.6703e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03464044630527496 tensor([3.4640e-02, 2.0359e-04, 3.8756e-07, 9.5430e-01, 1.0859e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030162230134010315 tensor([2.9850e-06, 3.0235e-03, 9.6672e-01, 9.1884e-05, 3.0162e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2601601183414459 tensor([0.0218, 0.0512, 0.0121, 0.2602, 0.6547], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23554760217666626 tensor([0.0164, 0.0265, 0.0075, 0.2355, 0.7141], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34688618779182434 tensor([0.1732, 0.1124, 0.0010, 0.3469, 0.3665], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07030434906482697 tensor([7.0304e-02, 9.2404e-01, 3.8862e-04, 1.6476e-03, 3.6213e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10457255691289902 tensor([0.0537, 0.7933, 0.0239, 0.0245, 0.1046], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1740911602973938 tensor([7.4376e-01, 7.6511e-02, 5.5475e-06, 1.7409e-01, 5.6315e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016913771629333496 tensor([6.9489e-07, 1.8667e-03, 9.8121e-01, 1.3306e-05, 1.6914e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013657648116350174 tensor([1.2675e-06, 3.6253e-03, 9.8270e-01, 1.5128e-05, 1.3658e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04966183006763458 tensor([6.5699e-05, 1.9325e-04, 1.3756e-02, 4.9662e-02, 9.3632e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3731003701686859 tensor([4.4958e-01, 3.7310e-01, 2.3435e-04, 1.2351e-01, 5.3573e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2117265909910202 tensor([2.1173e-01, 1.1472e-03, 1.4420e-07, 7.8148e-01, 5.6433e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42005473375320435 tensor([7.8488e-03, 1.8191e-03, 3.7071e-04, 4.2005e-01, 5.6991e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17450770735740662 tensor([1.7451e-01, 1.8361e-02, 6.2496e-05, 7.6266e-01, 4.4407e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04707217216491699 tensor([4.7072e-02, 9.5233e-01, 5.5921e-05, 1.8867e-04, 3.5212e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009792761877179146 tensor([1.1316e-08, 3.6112e-05, 9.9017e-01, 5.5850e-06, 9.7928e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26783859729766846 tensor([0.0492, 0.6251, 0.0228, 0.0350, 0.2678], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05648335814476013 tensor([0.0044, 0.0295, 0.0254, 0.0565, 0.8841], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2685212194919586 tensor([2.6852e-01, 3.2680e-03, 5.7219e-07, 7.2154e-01, 6.6734e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16711288690567017 tensor([8.2787e-01, 1.6711e-01, 3.4389e-07, 4.8837e-03, 1.3727e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40198764204978943 tensor([5.5617e-01, 4.0199e-01, 5.7652e-05, 3.8690e-02, 3.0945e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17793500423431396 tensor([7.2969e-01, 1.7794e-01, 1.5925e-05, 8.3797e-02, 8.5610e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19451075792312622 tensor([1.2008e-04, 4.3895e-03, 1.9451e-01, 1.0822e-02, 7.9016e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09658997505903244 tensor([8.3355e-01, 9.6590e-02, 2.1081e-06, 6.8323e-02, 1.5359e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21238940954208374 tensor([0.0055, 0.3701, 0.2124, 0.0100, 0.4020], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0025580788496881723 tensor([2.3824e-08, 2.7554e-04, 9.9717e-01, 9.4283e-07, 2.5581e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05651293694972992 tensor([2.1054e-02, 8.4494e-04, 1.5160e-05, 9.2157e-01, 5.6513e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06072240322828293 tensor([1.5643e-08, 1.9935e-05, 9.3924e-01, 2.1545e-05, 6.0722e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4134824872016907 tensor([4.1348e-01, 5.8198e-01, 1.1850e-05, 3.6144e-03, 9.0876e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27160346508026123 tensor([6.7232e-01, 4.4620e-02, 4.5932e-06, 2.7160e-01, 1.1447e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01628211699426174 tensor([9.8302e-06, 1.6282e-02, 9.6862e-01, 5.0704e-05, 1.5039e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4651046693325043 tensor([5.3480e-01, 7.7076e-05, 2.1205e-11, 4.6510e-01, 1.4744e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4555322229862213 tensor([1.4957e-04, 1.6352e-05, 1.5153e-04, 5.4415e-01, 4.5553e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22660915553569794 tensor([0.0065, 0.3646, 0.2266, 0.0144, 0.3879], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2177620530128479 tensor([0.0238, 0.0626, 0.0121, 0.2178, 0.6837], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008347603492438793 tensor([2.7298e-07, 1.0244e-03, 9.9062e-01, 6.1521e-06, 8.3476e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.48068636655807495 tensor([5.1918e-01, 1.0734e-04, 4.7999e-11, 4.8069e-01, 2.4781e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31977465748786926 tensor([7.5053e-04, 3.9551e-05, 5.0816e-05, 6.7938e-01, 3.1977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20874084532260895 tensor([2.0874e-01, 7.4888e-02, 5.1364e-04, 5.6112e-01, 1.5474e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.479818195104599 tensor([5.1433e-01, 4.7982e-01, 5.9725e-06, 4.9634e-03, 8.8554e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022632204927504063 tensor([8.6707e-08, 2.2632e-03, 9.9693e-01, 4.2645e-07, 8.0919e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21592794358730316 tensor([6.7171e-01, 2.1593e-01, 2.7481e-05, 1.0279e-01, 9.5499e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03037121891975403 tensor([1.6214e-04, 1.7941e-03, 3.0371e-02, 1.6497e-02, 9.5118e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07996334880590439 tensor([9.1965e-01, 3.8785e-04, 1.3544e-11, 7.9963e-02, 3.2104e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19052039086818695 tensor([3.5687e-04, 1.3034e-02, 1.9052e-01, 1.5231e-02, 7.8086e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007716171443462372 tensor([2.1196e-07, 9.6587e-04, 9.9131e-01, 4.8929e-06, 7.7162e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3195703625679016 tensor([6.5462e-01, 2.2058e-02, 1.6578e-06, 3.1957e-01, 3.7540e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1785413920879364 tensor([1.0233e-07, 6.3003e-05, 8.2132e-01, 7.9343e-05, 1.7854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1], [2, 1, 0], [0, 3, 1], [0, 2, 1, 3], [2, 4, 3], [2, 4, 1], [2, 1, 0], [0, 1, 2, 3], [0, 3, 2], [3, 0, 2, 4], [2, 1, 4], [1, 2, 0], [2, 4, 1], [2, 4, 3], [0, 3, 1, 4], [2, 3, 0], [2, 1, 0, 4], [3, 0, 2], [0, 3, 4, 1], [0, 3, 1], [2, 0, 3], [2, 1, 4, 0], [0, 3, 2], [0, 3], [2, 3, 4, 0], [2, 1, 4], [2, 3, 4, 0], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4], [2, 3, 0, 4], [2, 4, 1], [2, 3, 4, 0], [0, 3, 1], [2, 1, 4, 0], [1, 2, 0], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 2], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3], [0, 1, 3], [0, 3, 1, 4], [2, 1, 0], [0, 1, 2, 3], [0, 3, 1], [2, 4, 1], [0, 2, 3, 1], [2, 1, 0], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [3, 0, 2, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1], [2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0], [0, 3, 1, 4], [2, 1, 4], [2, 0, 3, 1], [0, 3, 1, 4], [2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [1, 0, 2, 3], [2, 1, 4, 0], [2, 4, 3], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 0, 4], [0, 3, 4], [2, 3, 0, 4], [0, 3, 1], [3, 0, 2], [0, 1, 3, 2], [2, 3, 4, 0], [3, 2, 0, 4], [0, 3, 1], [2, 1, 4, 0], [1, 2, 0], [2, 4, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4], [3, 2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 1, 3], [3, 0, 4], [2, 4, 1], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1], [2, 1, 4, 0], [2, 4, 1], [2, 0, 1], [1, 0, 2, 3], [2, 1, 0], [0, 1, 3, 2], [0, 3, 4], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 3, 4, 0], [2, 1, 0], [2, 4, 1], [2, 1, 4, 0], [2, 1, 0], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0], [0, 3], [3, 0, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [0, 1, 3], [2, 1, 4, 0], [3, 4, 2, 0], [2, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 4, 0], [1, 2, 0], [0, 3], [3, 0, 2], [0, 3, 1, 4], [2, 1, 4], [2, 3, 0], [2, 4, 3], [2, 3, 4, 0], [0, 3], [2, 1, 4], [2, 1, 0], [0, 3, 1], [2, 1, 4], [0, 3, 2, 1], [2, 4, 1], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2, 3], [2, 4, 3, 1], [3, 2, 4, 0], [0, 3, 1], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1], [0, 1, 3, 2], [2, 0], [2, 4, 3, 1], [0, 1, 2, 3], [3, 0, 4], [0, 3, 1, 4], [1, 2, 0], [2, 4, 3], [2, 4, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 3, 4, 0], [1, 0, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 1], [2, 0, 1], [2, 1, 0], [2, 3, 4, 0], [3, 2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 3], [2, 1, 4], [2, 1, 0], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [2, 3, 0], [0, 2, 1, 3], [2, 1, 4], [2, 4, 3, 1], [2, 4, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1], [0, 3, 4, 1], [2, 4, 1], [1, 2, 0], [0, 3], [2, 0, 1], [0, 3, 1, 4], [2, 4, 1], [1, 2, 0], [2, 1, 4], [2, 4, 3], [0, 3, 4, 1], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1, 3], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1], [0, 1, 3, 4]]\n",
      "[[3], [1], [2], [0, 3], [3, 4], [2, 4], [4], [0, 1], [0, 3], [3, 4], [4], [1, 4], [1], [0, 3], [3, 4], [0, 3], [0, 1], [2], [1, 4], [3], [1, 4], [2], [2, 4], [1, 4], [3], [1, 4], [1, 2, 4], [1], [0, 3], [1], [0], [1], [2], [0, 3], [1], [0, 3], [1], [2, 4], [3], [3, 4], [0], [3, 4], [4], [0], [2], [1, 2, 4], [2, 4], [2], [3, 4], [4], [2, 4], [0, 3], [4], [3, 4], [4], [0], [0, 1], [2, 4], [3, 4], [2, 4], [1], [1], [2], [0], [2, 4], [3], [1], [2], [0, 3], [4], [0], [1], [2, 4], [0, 1], [2, 4], [3], [3, 4], [2], [0, 3], [4], [2], [1, 3, 4], [2], [4], [4], [3], [0, 1], [2], [1], [3], [1, 2], [1], [2, 4], [1, 4], [4], [1], [1], [2, 4], [3], [3, 4], [0], [1], [2, 4], [0, 3], [2, 4], [0, 1, 3], [1], [4], [0], [4], [1, 2], [0, 3], [2], [0, 1], [4], [0, 3], [3], [0, 3], [3, 4], [4], [3, 4], [4], [1, 2], [0, 3], [0, 3], [0, 3], [1], [3, 4], [0, 3], [3], [3, 4], [0], [2], [0, 3], [2, 4], [0], [1], [2], [3], [1, 3, 4], [1, 2, 4], [1, 2], [2], [3], [4], [0, 1], [3], [2, 4], [3], [1], [0, 3], [0], [2], [3], [3, 4], [1, 2, 4], [1, 4], [2], [0, 3], [1, 4], [0, 1], [1], [1, 2, 4], [0, 3], [3, 4], [2, 4], [0, 3], [4], [0, 3], [2], [2, 4], [0], [2], [0], [4], [0], [1], [2, 4], [3], [4], [0], [3, 4], [2], [4], [4], [3], [0, 3], [4], [1, 3, 4], [0], [4], [1, 2], [2], [3, 4], [0, 1], [0, 3], [1], [2], [1], [4], [0], [3], [2], [3, 4], [3, 4], [3, 4], [1], [1], [0], [2], [2], [4], [0, 1], [0, 3], [3, 4], [3], [1], [2], [1, 4], [4], [0, 3], [0], [0, 1], [0], [4], [0], [1, 2, 4], [2], [3], [2], [0, 1], [0, 3], [2], [0, 3], [3, 4], [1, 2, 4], [3, 4], [2], [0, 3], [3, 4], [0, 3], [0, 1], [2], [0, 1], [4], [0], [4], [2], [0, 3], [2]]\n",
      "NL_pred of 3th iteration [[2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [0, 2, 1, 3], [0, 1, 2, 3], [3, 0, 2, 4], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 4, 1], [2, 1, 4, 0], [2, 3, 4, 0], [2, 3, 4, 0], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [2, 3, 0, 4], [2, 3, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 1, 2], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [0, 2, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 2, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 2], [2, 4, 1, 3], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 3, 1], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 0, 2, 3], [2, 1, 4, 0], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 0, 4], [2, 3, 0, 4], [0, 1, 3, 2], [2, 3, 4, 0], [3, 2, 0, 4], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 0], [3, 2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 4, 0], [2, 0, 1], [1, 0, 2, 3], [0, 1, 3, 2], [2, 3, 4, 0], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 3, 2], [2, 1, 4, 0], [2, 1, 4, 0], [3, 4, 2, 0], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 3, 4, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2, 3], [2, 4, 3, 1], [3, 2, 4, 0], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 3, 1], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [0, 1, 2, 3], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4], [2, 3, 4, 0], [1, 0, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 3, 4, 0], [3, 2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 3, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 1, 3], [0, 3, 1, 2], [0, 3, 1, 4], [0, 1, 3, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.008984352041173864  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.008983939665335196  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.008983164363437228  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.008982082649513527  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.008980746622438784  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.00897921014715124  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.008977526205557364  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.008975736300150554  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.008973881050392433  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.008971990479363336  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.008970103440461336  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.008968244658576117  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.008966429145247848  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.008964670145953143  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.00896298090616862  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.008961367607116699  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00895983378092448  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.008958379427591959  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.008957013377436885  Accuracy on Support set:0.0\n",
      "torch.Size([135, 2048]) torch.Size([135])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00895573651349103  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.26692819595336914 tensor([2.6693e-01, 1.1992e-04, 5.3645e-10, 7.3279e-01, 1.6523e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23852522671222687 tensor([0.0618, 0.0577, 0.0025, 0.2385, 0.6395], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.416085809469223 tensor([4.5187e-04, 7.1583e-02, 5.0936e-01, 2.5175e-03, 4.1609e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2639710009098053 tensor([0.2640, 0.5821, 0.0020, 0.1058, 0.0462], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.38883158564567566 tensor([3.8883e-01, 5.1367e-04, 4.4383e-09, 6.1029e-01, 3.6155e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19613675773143768 tensor([0.0096, 0.0083, 0.0019, 0.1961, 0.7841], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24380594491958618 tensor([0.0084, 0.2438, 0.1407, 0.0269, 0.5801], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2884184718132019 tensor([2.8842e-01, 7.6915e-03, 2.4456e-06, 6.9033e-01, 1.3554e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.45212820172309875 tensor([7.4023e-05, 6.0063e-06, 1.0507e-04, 5.4769e-01, 4.5213e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41248324513435364 tensor([4.2795e-01, 1.1946e-01, 1.7700e-04, 4.1248e-01, 3.9922e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24700020253658295 tensor([5.9787e-01, 2.4700e-01, 6.9605e-05, 1.1212e-01, 4.2945e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36108681559562683 tensor([0.0803, 0.4597, 0.0128, 0.0861, 0.3611], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2661745548248291 tensor([0.0198, 0.6069, 0.0893, 0.0178, 0.2662], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2934820055961609 tensor([0.0011, 0.0606, 0.2935, 0.0125, 0.6323], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2775612473487854 tensor([0.0382, 0.2776, 0.0354, 0.0873, 0.5616], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27604949474334717 tensor([0.0056, 0.2760, 0.2527, 0.0187, 0.4470], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18207576870918274 tensor([0.0010, 0.1821, 0.4354, 0.0028, 0.3788], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25305473804473877 tensor([2.5305e-01, 6.2132e-03, 2.9082e-06, 7.2616e-01, 1.4567e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25310051441192627 tensor([2.5310e-01, 1.1023e-03, 8.8652e-08, 7.4381e-01, 1.9845e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32593074440956116 tensor([3.2593e-01, 1.0924e-01, 2.1472e-04, 4.2192e-01, 1.4270e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4057764410972595 tensor([8.5520e-05, 1.1509e-02, 5.8013e-01, 2.4988e-03, 4.0578e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4948514997959137 tensor([2.5616e-04, 3.0373e-05, 2.0133e-04, 5.0466e-01, 4.9485e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3127799928188324 tensor([7.3550e-02, 2.3563e-02, 5.0591e-04, 5.8960e-01, 3.1278e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20279555022716522 tensor([0.0014, 0.2028, 0.5201, 0.0047, 0.2709], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4650445580482483 tensor([1.7977e-06, 2.0962e-04, 5.3341e-01, 1.3382e-03, 4.6504e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2724372148513794 tensor([6.4171e-02, 1.2735e-02, 2.5935e-04, 6.5040e-01, 2.7244e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24088796973228455 tensor([7.2125e-05, 1.4401e-02, 7.4299e-01, 1.6536e-03, 2.4089e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26578065752983093 tensor([0.2658, 0.1463, 0.0005, 0.4417, 0.1457], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4006577134132385 tensor([0.0168, 0.0158, 0.0037, 0.4007, 0.5630], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4723411798477173 tensor([5.2008e-01, 4.7234e-01, 8.0019e-06, 6.0143e-03, 1.5554e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44107675552368164 tensor([2.6058e-04, 2.3844e-02, 5.2849e-01, 6.3235e-03, 4.4108e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42155811190605164 tensor([0.0739, 0.0312, 0.0006, 0.4727, 0.4216], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4089501202106476 tensor([1.6638e-05, 1.9723e-03, 4.0895e-01, 1.5789e-03, 5.8748e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3289157450199127 tensor([4.8653e-05, 8.4887e-03, 6.6099e-01, 1.5541e-03, 3.2892e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24576282501220703 tensor([7.2173e-01, 2.9713e-02, 1.5664e-06, 2.4576e-01, 2.7934e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3944588303565979 tensor([0.0022, 0.1200, 0.3945, 0.0148, 0.4686], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3778950273990631 tensor([3.7790e-01, 5.5299e-01, 4.5343e-04, 4.7300e-02, 2.1359e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.46994322538375854 tensor([3.3544e-04, 2.8410e-02, 4.6994e-01, 7.4574e-03, 4.9385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32678428292274475 tensor([0.0543, 0.0640, 0.0034, 0.3268, 0.5515], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24055765569210052 tensor([2.4056e-01, 4.4595e-03, 1.2431e-06, 7.4313e-01, 1.1856e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20218046009540558 tensor([0.0963, 0.3262, 0.0119, 0.2022, 0.3634], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4169531464576721 tensor([4.1695e-01, 5.8016e-01, 4.2951e-06, 2.3638e-03, 5.1991e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.356974333524704 tensor([0.0027, 0.3570, 0.4285, 0.0040, 0.2078], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33638638257980347 tensor([0.0011, 0.0601, 0.3364, 0.0137, 0.5886], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37407752871513367 tensor([0.0207, 0.4905, 0.0860, 0.0287, 0.3741], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3887764513492584 tensor([0.0009, 0.0679, 0.3888, 0.0089, 0.5335], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34471142292022705 tensor([2.9193e-04, 1.8748e-05, 7.4699e-05, 6.5490e-01, 3.4471e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32506322860717773 tensor([0.0025, 0.1101, 0.3251, 0.0180, 0.5443], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23475998640060425 tensor([7.5163e-01, 1.2346e-02, 2.8337e-07, 2.3476e-01, 1.2686e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2655543088912964 tensor([4.0191e-04, 1.7556e-02, 2.6555e-01, 1.3424e-02, 7.0306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26245081424713135 tensor([0.2724, 0.2625, 0.0014, 0.2979, 0.1658], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.35841912031173706 tensor([0.0015, 0.3584, 0.5100, 0.0016, 0.1285], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.471491277217865 tensor([4.7149e-01, 9.6239e-03, 1.3013e-06, 5.0670e-01, 1.2184e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2777910828590393 tensor([0.2778, 0.3129, 0.0013, 0.2170, 0.1911], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4330541789531708 tensor([4.3305e-01, 8.8599e-02, 8.5905e-05, 4.4677e-01, 3.1486e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24483899772167206 tensor([7.2264e-01, 2.8515e-02, 1.5930e-06, 2.4484e-01, 3.9998e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24903373420238495 tensor([0.0563, 0.1708, 0.0217, 0.2490, 0.5021], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31137579679489136 tensor([0.0149, 0.0153, 0.0053, 0.6531, 0.3114], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16109782457351685 tensor([4.1831e-04, 1.6110e-01, 7.2087e-01, 6.8063e-04, 1.1694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3749820590019226 tensor([3.7498e-01, 6.4373e-03, 1.0213e-06, 6.1189e-01, 6.6880e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19780950248241425 tensor([1.9781e-01, 3.8307e-02, 1.4556e-04, 6.2507e-01, 1.3867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2620590031147003 tensor([4.6671e-01, 2.0658e-01, 2.1331e-04, 2.6206e-01, 6.4434e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.374049574136734 tensor([0.1246, 0.0952, 0.0019, 0.3740, 0.4042], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4209074079990387 tensor([4.2091e-01, 1.5342e-03, 3.2432e-08, 5.7630e-01, 1.2536e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3659689724445343 tensor([0.0264, 0.0111, 0.0011, 0.5955, 0.3660], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3493252396583557 tensor([5.8820e-01, 4.9830e-02, 1.0972e-05, 3.4933e-01, 1.2637e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.46411776542663574 tensor([2.0602e-04, 2.8772e-02, 4.6412e-01, 2.0450e-03, 5.0486e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21771953999996185 tensor([0.1108, 0.3409, 0.0120, 0.2177, 0.3186], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24734577536582947 tensor([0.0045, 0.2473, 0.2890, 0.0146, 0.4445], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.41558200120925903 tensor([0.0015, 0.5102, 0.4156, 0.0010, 0.0717], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30642253160476685 tensor([6.8417e-01, 3.0642e-01, 3.3376e-06, 9.0810e-03, 3.2723e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39558476209640503 tensor([5.6638e-07, 6.4563e-05, 6.0290e-01, 1.4513e-03, 3.9558e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23242813348770142 tensor([2.3243e-01, 5.8561e-02, 2.6898e-04, 5.7579e-01, 1.3295e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3023529350757599 tensor([4.1632e-04, 1.0470e-04, 5.4278e-04, 3.0235e-01, 6.9658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15621048212051392 tensor([0.0014, 0.1562, 0.4115, 0.0051, 0.4258], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31715869903564453 tensor([0.0037, 0.4479, 0.2263, 0.0050, 0.3172], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17804355919361115 tensor([1.7804e-01, 1.5014e-03, 4.4811e-07, 8.1359e-01, 6.8664e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19081398844718933 tensor([0.1202, 0.1908, 0.0025, 0.1423, 0.5443], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33864420652389526 tensor([3.3864e-01, 5.8923e-01, 3.0707e-04, 4.0308e-02, 3.1515e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17804676294326782 tensor([0.0010, 0.1780, 0.4604, 0.0029, 0.3576], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23947079479694366 tensor([2.3947e-01, 2.4805e-02, 4.1971e-05, 6.7560e-01, 6.0080e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3528469502925873 tensor([0.0313, 0.0228, 0.0014, 0.3528, 0.5917], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2627652585506439 tensor([2.6419e-04, 6.3540e-02, 6.7150e-01, 1.9323e-03, 2.6277e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2760889232158661 tensor([2.7609e-01, 4.0704e-02, 8.1310e-05, 5.9444e-01, 8.8689e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42498263716697693 tensor([4.4765e-01, 9.8955e-02, 7.9943e-05, 4.2498e-01, 2.8336e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32724854350090027 tensor([2.3105e-04, 3.5188e-02, 6.3339e-01, 3.9452e-03, 3.2725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20859940350055695 tensor([3.0312e-05, 1.2502e-02, 7.7833e-01, 5.4272e-04, 2.0860e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29751506447792053 tensor([1.2836e-03, 1.1290e-04, 1.2510e-04, 7.0096e-01, 2.9752e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3925522267818451 tensor([5.9983e-01, 4.5909e-03, 1.1255e-07, 3.9255e-01, 3.0300e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21631507575511932 tensor([0.0947, 0.2163, 0.0103, 0.2956, 0.3830], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3153723478317261 tensor([4.9601e-04, 6.6734e-01, 3.1537e-01, 9.3138e-05, 1.6695e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2853643298149109 tensor([1.0583e-03, 6.0936e-04, 1.9823e-03, 2.8536e-01, 7.1099e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23619231581687927 tensor([0.2733, 0.2362, 0.0009, 0.2082, 0.2814], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3232412040233612 tensor([6.7400e-01, 2.4092e-03, 1.3444e-08, 3.2324e-01, 3.5442e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20740874111652374 tensor([0.0143, 0.0439, 0.0167, 0.2074, 0.7178], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18546223640441895 tensor([0.0105, 0.0222, 0.0101, 0.1855, 0.7718], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31295114755630493 tensor([0.1278, 0.1074, 0.0015, 0.3130, 0.4504], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.38202929496765137 tensor([0.3820, 0.4134, 0.0004, 0.1281, 0.0760], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17934082448482513 tensor([1.7934e-01, 1.1777e-03, 2.2418e-07, 8.1174e-01, 7.7386e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34590667486190796 tensor([5.2554e-03, 1.5834e-03, 5.1849e-04, 3.4591e-01, 6.4674e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31361672282218933 tensor([0.0355, 0.5857, 0.0340, 0.0311, 0.3136], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22790023684501648 tensor([2.2790e-01, 3.4460e-03, 9.3661e-07, 7.5931e-01, 9.3382e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.46003881096839905 tensor([4.9385e-01, 4.6004e-01, 1.0301e-04, 4.1599e-02, 4.4115e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2774324119091034 tensor([0.0034, 0.2997, 0.2774, 0.0077, 0.4117], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.35575711727142334 tensor([3.5576e-01, 6.3948e-01, 1.8981e-05, 3.5846e-03, 1.1631e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30742254853248596 tensor([6.2215e-01, 5.2965e-02, 8.5815e-06, 3.0742e-01, 1.7455e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.49920037388801575 tensor([5.0070e-01, 7.9090e-05, 2.8467e-11, 4.9920e-01, 1.8736e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4654758870601654 tensor([1.0439e-04, 1.4502e-05, 2.1121e-04, 4.6548e-01, 5.3419e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29551398754119873 tensor([0.0040, 0.2955, 0.2956, 0.0110, 0.3938], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17440694570541382 tensor([0.0155, 0.0526, 0.0162, 0.1744, 0.7413], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4838029146194458 tensor([4.8380e-01, 1.1095e-04, 6.5956e-11, 5.1605e-01, 3.1955e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.392093688249588 tensor([5.4467e-04, 3.6498e-05, 7.4173e-05, 6.0725e-01, 3.9209e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16670644283294678 tensor([0.1667, 0.0776, 0.0008, 0.5488, 0.2061], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.44854143261909485 tensor([4.4854e-01, 5.4521e-01, 1.0141e-05, 5.0487e-03, 1.1873e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25324955582618713 tensor([6.1740e-01, 2.5325e-01, 5.0199e-05, 1.1501e-01, 1.4288e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3645375967025757 tensor([6.0317e-01, 2.6456e-02, 3.1810e-06, 3.6454e-01, 5.8279e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1], [2, 1, 0], [0, 3, 1], [0, 2, 1, 3], [2, 4, 3], [2, 4, 1], [2, 1, 0, 3], [0, 1, 2, 3], [0, 3, 2], [3, 0, 2, 4], [2, 1, 4], [1, 2, 0], [2, 4, 1], [2, 4, 3], [0, 3, 1, 4], [2, 3, 0], [2, 1, 0, 4], [3, 0, 2], [0, 3, 4, 1], [0, 3, 1], [2, 0, 3], [2, 1, 4, 0], [0, 3, 2], [0, 3, 1], [2, 3, 4, 0], [2, 1, 4], [2, 3, 4, 0], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4], [2, 3, 0, 4], [2, 4, 1], [2, 3, 4, 0], [0, 3, 1], [2, 1, 4, 0], [1, 2, 0], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 2], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3], [0, 1, 3], [0, 3, 1, 4], [2, 1, 0], [0, 1, 2, 3], [0, 3, 1], [2, 4, 1], [0, 2, 3, 1], [2, 1, 0], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [3, 0, 2, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1], [2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0], [0, 3, 1, 4], [2, 1, 4], [2, 0, 3, 1], [0, 3, 1, 4], [2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [1, 0, 2, 3], [2, 1, 4, 0], [2, 4, 3], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 0, 4], [0, 3, 4], [2, 3, 0, 4], [0, 3, 1], [3, 0, 2], [0, 1, 3, 2], [2, 3, 4, 0], [3, 2, 0, 4], [0, 3, 1], [2, 1, 4, 0], [1, 2, 0], [2, 4, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4], [3, 2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 1, 3], [3, 0, 4], [2, 4, 1], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1], [2, 1, 4, 0], [2, 4, 1], [2, 0, 1], [1, 0, 2, 3], [2, 1, 0], [0, 1, 3, 2], [0, 3, 4, 1], [2, 4, 1], [2, 1, 4, 0], [2, 4, 1], [2, 3, 4, 0], [2, 1, 0], [2, 4, 1], [2, 1, 4, 0], [2, 1, 0], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0], [0, 3], [3, 0, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [0, 1, 3], [2, 1, 4, 0], [3, 4, 2, 0], [2, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 4, 0], [1, 2, 0], [0, 3, 1], [3, 0, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0, 1], [2, 4, 3], [2, 3, 4, 0], [0, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [2, 1, 4], [0, 3, 2, 1], [2, 4, 1], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2, 3], [2, 4, 3, 1], [3, 2, 4, 0], [0, 3, 1], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1], [0, 1, 3, 2], [2, 0], [2, 4, 3, 1], [0, 1, 2, 3], [3, 0, 4], [0, 3, 1, 4], [1, 2, 0], [2, 4, 3], [2, 4, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 3, 4, 0], [1, 0, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 1], [2, 0, 1, 3], [2, 1, 0], [2, 3, 4, 0], [3, 2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 3], [2, 1, 4, 0], [2, 1, 0], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [2, 3, 0], [0, 2, 1, 3], [2, 1, 4], [2, 4, 3, 1], [2, 4, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1], [0, 3, 4, 1], [2, 4, 1], [1, 2, 0], [0, 3], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 1], [1, 2, 0], [2, 1, 4, 0], [2, 4, 3], [0, 3, 4, 1], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1, 3], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1], [0, 1, 3, 4]]\n",
      "[[3], [1], [2], [0, 3], [3, 4], [2, 4], [4], [0, 1], [0, 3], [4], [4], [1, 4], [1], [0, 3], [3, 4], [0, 3], [0, 1], [2], [1, 4], [3], [1, 4], [2], [2, 4], [1, 4], [3], [1, 4], [2, 4], [1], [0, 3], [1], [0], [1], [2], [0, 3], [1], [0, 3], [1], [2, 4], [3], [3, 4], [0], [3, 4], [4], [0], [2], [1, 2, 4], [2, 4], [2], [3, 4], [4], [2, 4], [0, 3], [4], [3, 4], [4], [0], [0, 1], [2, 4], [3, 4], [2, 4], [1], [1], [2], [0], [2, 4], [3], [1], [2], [0, 3], [4], [0], [1], [2, 4], [0, 1], [2, 4], [3], [3, 4], [2], [0, 3], [4], [2], [1, 3, 4], [2], [4], [4], [3], [0, 1], [2], [1], [3], [1, 2], [1], [2, 4], [1, 4], [4], [1], [1], [2, 4], [3], [3, 4], [0], [1], [2, 4], [0, 3], [2, 4], [0, 1, 3], [1], [4], [0], [4], [1, 2], [0, 3], [2], [0, 1], [4], [0, 3], [3], [0, 3], [3, 4], [4], [3, 4], [4], [2], [0, 3], [3], [0, 3], [1], [3, 4], [0, 3], [3], [3, 4], [0], [2], [0, 3], [2, 4], [0], [1], [2], [3], [1, 3, 4], [1, 2, 4], [1, 2], [2], [3], [4], [0, 1], [3], [2, 4], [3], [1], [0, 3], [0], [2], [3], [3, 4], [2, 4], [1, 4], [2], [3], [4], [0, 1], [1], [2, 4], [0, 3], [3, 4], [2, 4], [0, 3], [4], [0, 3], [2], [2, 4], [0], [2], [0], [4], [0], [1], [2, 4], [3], [4], [0], [3, 4], [2], [4], [4], [3], [0, 3], [4], [1, 3, 4], [0], [4], [1, 2], [2], [3, 4], [0, 1], [0, 3], [1], [2], [1], [4], [0], [3], [2], [3, 4], [4], [3, 4], [1], [1], [0], [2], [2], [4], [0, 1], [3], [3, 4], [3], [1], [2], [1, 4], [4], [0, 3], [0], [0, 1], [0], [4], [0], [1, 2, 4], [2], [3], [2], [0, 1], [0, 3], [2], [0, 3], [3, 4], [1, 2, 4], [4], [2], [0, 3], [3, 4], [3], [0, 1], [2], [0, 1], [4], [0], [4], [2], [0, 3], [2]]\n",
      "NL_pred of 4th iteration [[2, 1, 0, 3], [0, 3, 1], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1], [2, 1, 4, 0], [2, 3, 0, 1], [0, 3, 1], [2, 0, 1, 3], [2, 1, 4, 0], [2, 0, 1, 3], [2, 1, 4, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.1079168717066447  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.10745691259702046  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.10662586490313213  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.10553536812464397  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.1043005883693695  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.10302433371543884  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.10178448756535848  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.10063364108403523  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.09960106015205383  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0986994206905365  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.097928653160731  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0972788433233897  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.09673568606376648  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.09628529349962871  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.09591389695803325  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.09560697277386983  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.09535372257232666  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.09514464934666951  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.09497172633806865  Accuracy on Support set:0.0\n",
      "torch.Size([12, 2048]) torch.Size([12])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.09482810894648235  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.04686843231320381 tensor([4.6868e-02, 4.8048e-05, 6.0314e-09, 9.5199e-01, 1.0946e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05659133940935135 tensor([0.0019, 0.0054, 0.0089, 0.0566, 0.9272], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2156505584716797 tensor([7.3649e-06, 3.0033e-03, 7.8103e-01, 3.1147e-04, 2.1565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.050565775483846664 tensor([0.0506, 0.3531, 0.0441, 0.1482, 0.4041], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07698739320039749 tensor([7.6987e-02, 2.3294e-04, 5.7610e-08, 9.2005e-01, 2.7247e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.017713133245706558 tensor([2.1364e-04, 1.7713e-02, 3.7886e-01, 5.2478e-03, 5.9797e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04337279126048088 tensor([4.3373e-02, 3.5881e-03, 4.5205e-05, 8.4827e-01, 1.0472e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1474435031414032 tensor([3.2648e-06, 8.8371e-07, 4.7320e-04, 1.4744e-01, 8.5208e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07110133022069931 tensor([0.0711, 0.0627, 0.0036, 0.5364, 0.3261], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14288797974586487 tensor([0.1429, 0.1797, 0.0019, 0.2009, 0.4747], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.062389470636844635 tensor([0.0036, 0.0624, 0.0767, 0.0337, 0.8236], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07503946125507355 tensor([0.0008, 0.0750, 0.4363, 0.0058, 0.4820], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42523932456970215 tensor([1.8766e-05, 2.8582e-03, 5.7020e-01, 1.6851e-03, 4.2524e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0256455447524786 tensor([0.0012, 0.0256, 0.1356, 0.0224, 0.8152], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014654946513473988 tensor([9.9013e-05, 1.4655e-02, 6.0157e-01, 2.7740e-03, 3.8090e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22283518314361572 tensor([1.4220e-05, 7.1905e-03, 7.6961e-01, 3.4572e-04, 2.2284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03717606142163277 tensor([3.7176e-02, 2.8718e-03, 5.1927e-05, 8.5189e-01, 1.0801e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0425044409930706 tensor([4.2504e-02, 4.8243e-04, 1.1947e-06, 9.4303e-01, 1.3986e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.028898634016513824 tensor([0.0289, 0.0308, 0.0024, 0.2938, 0.6442], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1927441507577896 tensor([1.4626e-06, 5.0644e-04, 8.0645e-01, 2.9524e-04, 1.9274e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13173115253448486 tensor([1.0700e-05, 4.1126e-06, 8.4623e-04, 1.3173e-01, 8.6741e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21902750432491302 tensor([0.0035, 0.0036, 0.0032, 0.2190, 0.7707], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00868696253746748 tensor([2.1553e-05, 8.6870e-03, 8.3521e-01, 5.5576e-04, 1.5553e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2520771026611328 tensor([4.7823e-08, 1.4790e-05, 7.4771e-01, 1.9483e-04, 2.5208e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2606833577156067 tensor([0.0031, 0.0020, 0.0018, 0.2607, 0.7325], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10039699077606201 tensor([9.9780e-07, 5.0854e-04, 8.9893e-01, 1.6735e-04, 1.0040e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.022393349558115005 tensor([0.0224, 0.0390, 0.0057, 0.2966, 0.6363], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08966559171676636 tensor([4.7451e-04, 1.5134e-03, 1.5490e-02, 8.9666e-02, 8.9286e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2552563548088074 tensor([2.5526e-01, 6.8699e-01, 4.3444e-04, 2.2881e-02, 3.4435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2200869768857956 tensor([3.9960e-06, 9.8492e-04, 7.7823e-01, 6.9501e-04, 2.2009e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13904902338981628 tensor([0.0026, 0.0036, 0.0033, 0.1390, 0.8514], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3248056173324585 tensor([3.2345e-07, 1.0703e-04, 6.7487e-01, 2.1680e-04, 3.2481e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14971111714839935 tensor([8.6378e-07, 3.8096e-04, 8.4972e-01, 1.8532e-04, 1.4971e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25947290658950806 tensor([2.5947e-01, 3.1673e-02, 5.5576e-05, 6.6579e-01, 4.3005e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2939331531524658 tensor([3.2869e-05, 5.1268e-03, 6.9917e-01, 1.7401e-03, 2.9393e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10444504022598267 tensor([0.1044, 0.4864, 0.0160, 0.1009, 0.2922], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26906660199165344 tensor([5.6712e-06, 1.2520e-03, 7.2876e-01, 9.1191e-04, 2.6907e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08006072044372559 tensor([0.0017, 0.0063, 0.0144, 0.0801, 0.8976], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03403560072183609 tensor([3.4036e-02, 2.0210e-03, 2.3248e-05, 8.7291e-01, 9.1006e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04336782544851303 tensor([0.0041, 0.0434, 0.0691, 0.0698, 0.8137], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20258642733097076 tensor([2.0259e-01, 7.8335e-01, 1.4366e-04, 6.7737e-03, 7.1473e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019055888056755066 tensor([5.2482e-05, 1.9056e-02, 8.3496e-01, 5.5935e-04, 1.4537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38036397099494934 tensor([2.0124e-05, 2.9469e-03, 6.1486e-01, 1.8111e-03, 3.8036e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05042770504951477 tensor([0.0007, 0.0504, 0.3558, 0.0081, 0.5849], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32157787680625916 tensor([1.4545e-05, 2.9263e-03, 6.7437e-01, 1.1152e-03, 3.2158e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20907257497310638 tensor([1.4621e-05, 3.2170e-06, 4.2774e-04, 2.0907e-01, 7.9048e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3717930018901825 tensor([4.2796e-05, 5.3486e-03, 6.2046e-01, 2.3574e-03, 3.7179e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27997007966041565 tensor([2.7997e-01, 1.4583e-02, 1.2086e-05, 6.8277e-01, 2.2663e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48074114322662354 tensor([8.2303e-06, 9.7467e-04, 5.1632e-01, 1.9579e-03, 4.8074e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02071564644575119 tensor([0.0207, 0.0659, 0.0156, 0.1868, 0.7110], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015727171674370766 tensor([2.2692e-05, 1.5727e-02, 9.0493e-01, 1.9681e-04, 7.9125e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09441553056240082 tensor([9.4416e-02, 5.7764e-03, 2.8511e-05, 7.8116e-01, 1.1862e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020381644368171692 tensor([0.0204, 0.0731, 0.0128, 0.1309, 0.7628], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0712805986404419 tensor([0.0713, 0.0463, 0.0019, 0.6050, 0.2756], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24586282670497894 tensor([2.4586e-01, 3.0937e-02, 6.6459e-05, 6.5542e-01, 6.7712e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06535384058952332 tensor([0.0019, 0.0178, 0.0904, 0.0654, 0.8245], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22557534277439117 tensor([7.2807e-04, 2.5212e-03, 3.2384e-02, 2.2558e-01, 7.3879e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06724277883768082 tensor([6.7243e-02, 3.6093e-03, 2.1528e-05, 8.6994e-01, 5.9187e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06925590336322784 tensor([0.0693, 0.1008, 0.0044, 0.3111, 0.5144], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11768711358308792 tensor([0.0047, 0.0116, 0.0102, 0.1177, 0.8559], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08255799114704132 tensor([8.2558e-02, 7.5063e-04, 5.0268e-07, 9.0596e-01, 1.0728e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19067981839179993 tensor([0.0011, 0.0016, 0.0063, 0.1907, 0.8004], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13899557292461395 tensor([1.3900e-01, 3.8789e-02, 3.5633e-04, 6.6300e-01, 1.5885e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2740035057067871 tensor([4.3490e-06, 1.5365e-03, 7.2416e-01, 2.9637e-04, 2.7400e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05107789859175682 tensor([0.0052, 0.0511, 0.0765, 0.0827, 0.7844], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013547414913773537 tensor([8.4715e-05, 1.3547e-02, 6.3177e-01, 2.1689e-03, 3.5243e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02785819210112095 tensor([2.9147e-05, 2.7858e-02, 9.1715e-01, 1.4894e-04, 5.4819e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3869164288043976 tensor([3.8692e-01, 5.7982e-01, 1.6922e-04, 2.8048e-02, 5.0473e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22123213112354279 tensor([1.9688e-08, 5.9718e-06, 7.7855e-01, 2.1441e-04, 2.2123e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018178202211856842 tensor([0.0182, 0.0152, 0.0031, 0.3732, 0.5903], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06475752592086792 tensor([1.4626e-05, 1.1745e-05, 1.7759e-03, 6.4758e-02, 9.3344e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25642451643943787 tensor([1.9551e-05, 6.1511e-03, 7.3678e-01, 6.2106e-04, 2.5642e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0326714850962162 tensor([9.9645e-05, 3.2671e-02, 6.3576e-01, 1.0463e-03, 3.3043e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08689535409212112 tensor([0.0869, 0.4474, 0.0090, 0.0815, 0.3752], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20099897682666779 tensor([1.5522e-05, 7.1850e-03, 7.9145e-01, 3.4886e-04, 2.0100e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027130424976348877 tensor([2.7130e-02, 8.8217e-03, 5.9912e-04, 6.1397e-01, 3.4948e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08180874586105347 tensor([0.0009, 0.0021, 0.0053, 0.0818, 0.9098], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11435627937316895 tensor([3.2556e-06, 2.0713e-03, 8.8338e-01, 1.8772e-04, 1.1436e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03008822351694107 tensor([0.0301, 0.0139, 0.0010, 0.4900, 0.4649], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08105719834566116 tensor([0.0811, 0.0564, 0.0018, 0.6069, 0.2539], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1475822478532791 tensor([2.8436e-06, 1.1756e-03, 8.5086e-01, 3.7453e-04, 1.4758e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08028248697519302 tensor([3.5209e-07, 3.8121e-04, 9.1929e-01, 5.0211e-05, 8.0282e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27186480164527893 tensor([7.8630e-05, 2.1490e-05, 7.1093e-04, 2.7186e-01, 7.2732e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1672777384519577 tensor([1.6728e-01, 3.2520e-03, 2.3432e-06, 7.9535e-01, 3.4122e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027463102713227272 tensor([0.0037, 0.0275, 0.0571, 0.0948, 0.8170], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.059707120060920715 tensor([1.6527e-05, 5.9707e-02, 9.2365e-01, 1.9886e-05, 1.6602e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.052884191274642944 tensor([2.7348e-05, 5.4639e-05, 6.8682e-03, 5.2884e-02, 9.4017e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01873989775776863 tensor([0.0187, 0.0475, 0.0065, 0.1044, 0.8229], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2151840776205063 tensor([2.1518e-01, 1.8977e-03, 3.0825e-07, 7.7847e-01, 4.4445e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.040461551398038864 tensor([3.5661e-04, 3.4655e-03, 5.4084e-02, 4.0462e-02, 9.0163e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09310825914144516 tensor([0.0046, 0.0122, 0.0073, 0.0931, 0.8828], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0571955181658268 tensor([0.0572, 0.1988, 0.0082, 0.1496, 0.5861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07099010050296783 tensor([1.5777e-04, 1.6261e-04, 1.9102e-03, 7.0990e-02, 9.2678e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07705092430114746 tensor([0.0015, 0.0771, 0.2052, 0.0119, 0.7043], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03172364458441734 tensor([3.1724e-02, 1.5228e-03, 1.7248e-05, 8.9410e-01, 7.2639e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19791994988918304 tensor([0.1979, 0.5929, 0.0050, 0.1237, 0.0805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016485808417201042 tensor([6.6802e-05, 1.6486e-02, 6.4252e-01, 1.2146e-03, 3.3972e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14640571177005768 tensor([0.1464, 0.8190, 0.0010, 0.0115, 0.0221], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14894600212574005 tensor([1.4895e-01, 4.0400e-02, 2.6705e-04, 5.9223e-01, 2.1815e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1562040001153946 tensor([1.5620e-01, 3.9260e-05, 2.1067e-10, 8.4365e-01, 1.0795e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1124994307756424 tensor([4.1636e-06, 1.8937e-06, 8.4866e-04, 1.1250e-01, 8.8665e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015484640374779701 tensor([7.2581e-05, 1.5485e-02, 6.6646e-01, 1.6258e-03, 3.1636e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13316872715950012 tensor([1.3317e-01, 5.4943e-05, 6.0927e-10, 8.6656e-01, 2.1150e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.186010479927063 tensor([2.5072e-05, 5.5424e-06, 3.7664e-04, 1.8601e-01, 8.1358e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20958617329597473 tensor([2.0959e-01, 7.5123e-01, 4.6400e-04, 1.6762e-02, 2.1962e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2056950032711029 tensor([0.2057, 0.2577, 0.0020, 0.3014, 0.2332], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15156742930412292 tensor([1.5157e-01, 2.1957e-02, 1.1026e-04, 7.4768e-01, 7.8688e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [0, 3, 1], [0, 2, 1, 3], [2, 4, 3, 0], [2, 4, 1, 0], [2, 1, 0, 3], [0, 1, 2, 3], [0, 3, 2, 1], [3, 0, 2, 4], [2, 1, 4, 0], [1, 2, 0, 3], [2, 4, 1, 0], [2, 4, 3, 0], [0, 3, 1, 4], [2, 3, 0, 1], [2, 1, 0, 4], [3, 0, 2, 1], [0, 3, 4, 1], [0, 3, 1], [2, 0, 3, 1], [2, 1, 4, 0], [0, 3, 2, 1], [0, 3, 1], [2, 3, 4, 0], [2, 1, 4, 0], [2, 3, 4, 0], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0, 4], [2, 4, 1, 0], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [1, 2, 0, 3], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 2], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1], [0, 1, 3], [0, 3, 1, 4], [2, 1, 0], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 0], [0, 2, 3, 1], [2, 1, 0, 3], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0, 3], [0, 3, 1], [3, 0, 2, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3, 1], [0, 3, 1, 4], [2, 0, 1], [0, 3, 1, 4], [0, 1, 2, 3], [1, 0, 2, 3], [2, 1, 4, 0], [2, 4, 3], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 0, 4], [0, 3, 4, 1], [2, 3, 0, 4], [0, 3, 1], [3, 0, 2, 1], [0, 1, 3, 2], [2, 3, 4, 0], [3, 2, 0, 4], [0, 3, 1], [2, 1, 4, 0], [1, 2, 0], [2, 4, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 0], [3, 2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 1, 3], [3, 0, 4, 1], [2, 4, 1, 0], [0, 3, 1, 4], [2, 4, 3, 0], [0, 1, 3, 2], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1], [2, 0, 1, 3], [1, 0, 2, 3], [2, 1, 0], [0, 1, 3, 2], [0, 3, 4, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 0], [2, 3, 4, 0], [2, 1, 0, 3], [2, 4, 1, 0], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [0, 3, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1], [0, 3, 1], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [0, 1, 3], [2, 1, 4, 0], [3, 4, 2, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 4, 0], [1, 2, 0, 3], [0, 3, 1], [3, 0, 2, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0, 1], [2, 4, 3, 0], [2, 3, 4, 0], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2, 1], [2, 4, 1, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2, 3], [2, 4, 3, 1], [3, 2, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1, 0], [0, 1, 3, 2], [2, 0, 1], [2, 4, 3, 1], [0, 1, 2, 3], [3, 0, 4, 1], [0, 3, 1, 4], [1, 2, 0, 3], [2, 4, 3, 0], [2, 4, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 3, 4, 0], [1, 0, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 1, 3], [2, 0, 1, 3], [2, 1, 0, 3], [2, 3, 4, 0], [3, 2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 3, 0], [2, 1, 4, 0], [2, 1, 0, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [2, 3, 0, 1], [0, 2, 1, 3], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 0], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 0], [2, 4, 1, 0], [0, 3, 4, 1], [2, 4, 1, 0], [1, 2, 0, 3], [0, 3, 1], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [1, 2, 0, 3], [2, 1, 4, 0], [2, 4, 3], [0, 3, 4, 1], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1, 3], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1, 0], [0, 1, 3, 4]]\n",
      "[[3], [1], [2], [3], [4], [2, 4], [4], [1], [3], [4], [4], [4], [1], [3], [4], [3], [1], [2], [4], [3], [4], [2], [2, 4], [4], [3], [4], [2, 4], [1], [3], [1], [0], [1], [2], [3], [1], [3], [1], [2], [3], [4], [0], [3, 4], [4], [0], [2], [2, 4], [2, 4], [2], [3, 4], [4], [2], [3], [4], [4], [4], [0], [0, 1], [2, 4], [4], [2, 4], [1], [1], [2], [0], [2], [3], [1], [2], [0, 3], [4], [0], [1], [2, 4], [1], [2, 4], [3], [4], [2], [3], [4], [2], [3, 4], [2], [4], [4], [3], [0, 1], [2], [1], [3], [2], [1], [2, 4], [4], [4], [1], [1], [2, 4], [3], [3, 4], [0], [1], [2, 4], [0, 3], [2, 4], [1, 3], [1], [4], [0], [4], [2], [3], [2], [1], [4], [3], [3], [0, 3], [4], [4], [3, 4], [4], [2], [3], [3], [3], [1], [4], [3], [3], [4], [0], [2], [3], [2, 4], [0], [1], [2], [3], [3, 4], [2, 4], [2], [2], [3], [4], [0, 1], [3], [2, 4], [3], [1], [3], [0], [2], [3], [4], [2, 4], [4], [2], [3], [4], [1], [1], [2, 4], [3], [4], [2], [3], [4], [3], [2], [2], [0], [2], [0], [4], [0], [1], [2], [3], [4], [0], [3, 4], [2], [4], [4], [3], [3], [4], [3, 4], [0], [4], [2], [2], [4], [1], [0, 3], [1], [2], [1], [4], [0], [3], [2], [4], [4], [4], [1], [1], [0], [2], [2], [4], [1], [3], [4], [3], [1], [2], [4], [4], [3], [0], [1], [0], [4], [0], [2, 4], [2], [3], [2], [1], [3], [2], [3], [4], [2, 4], [4], [2], [3], [4], [3], [0, 1], [2], [0, 1], [4], [0], [4], [2], [3], [2]]\n",
      "NL_pred of 5th iteration [[2, 4, 1, 0], [2, 1, 0, 3], [2, 4, 3, 0], [2, 4, 1, 0], [0, 3, 2, 1], [2, 1, 4, 0], [1, 2, 0, 3], [2, 4, 1, 0], [2, 4, 3, 0], [2, 3, 0, 1], [3, 0, 2, 1], [2, 0, 3, 1], [0, 3, 2, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 1, 0], [0, 3, 1, 4], [1, 2, 0, 3], [0, 3, 1], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [2, 1, 0, 3], [0, 3, 1, 4], [2, 4, 3, 0], [2, 1, 0, 3], [2, 1, 4, 0], [2, 0, 1], [0, 3, 4, 1], [3, 0, 2, 1], [2, 4, 0], [3, 0, 4, 1], [2, 4, 1, 0], [2, 4, 3, 0], [2, 4, 1, 0], [2, 0, 1, 3], [2, 4, 1, 0], [2, 4, 1, 0], [2, 1, 0, 3], [2, 4, 1, 0], [2, 1, 0, 3], [2, 4, 1, 0], [2, 0, 1], [0, 3, 1], [3, 0, 4, 1], [2, 1, 4, 0], [1, 2, 0, 3], [3, 0, 2, 1], [2, 4, 3, 0], [2, 1, 4, 0], [2, 1, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 1, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 0], [2, 0, 1], [3, 0, 4, 1], [1, 2, 0, 3], [2, 4, 3, 0], [2, 0, 1, 3], [2, 1, 0, 3], [2, 4, 3, 0], [2, 1, 0, 3], [2, 3, 0, 1], [2, 1, 4, 0], [2, 4, 3, 0], [0, 3, 1], [2, 4, 3, 0], [2, 4, 1, 0], [2, 4, 1, 0], [1, 2, 0, 3], [0, 3, 1], [2, 4, 1, 0], [1, 2, 0, 3], [2, 4, 1, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.015494691861140263  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.015482690427210424  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.015460396741891836  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.015429747569096553  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.015392639420249245  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.015350954873221261  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.015306417044107016  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.01526054623839143  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.015214594927701082  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.015169484274727958  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.015125983721250064  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.01508459642335966  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.015045557703290666  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.015009123009520692  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.014975315564638608  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.014944180265649573  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.014915744979660233  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.014889904430934362  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.014866381496578068  Accuracy on Support set:0.0\n",
      "torch.Size([77, 2048]) torch.Size([77])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.014844991944052956  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [0, 3, 1], [0, 2, 1, 3], [2, 4, 3, 0], [2, 4, 1, 0], [2, 1, 0, 3], [0, 1, 2, 3], [0, 3, 2, 1], [3, 0, 2, 4], [2, 1, 4, 0], [1, 2, 0, 3], [2, 4, 1, 0], [2, 4, 3, 0], [0, 3, 1, 4], [2, 3, 0, 1], [2, 1, 0, 4], [3, 0, 2, 1], [0, 3, 4, 1], [0, 3, 1], [2, 0, 3, 1], [2, 1, 4, 0], [0, 3, 2, 1], [0, 3, 1], [2, 3, 4, 0], [2, 1, 4, 0], [2, 3, 4, 0], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0, 4], [2, 4, 1, 0], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [1, 2, 0, 3], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 2], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1], [0, 1, 3], [0, 3, 1, 4], [2, 1, 0], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 0], [0, 2, 3, 1], [2, 1, 0, 3], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0, 3], [0, 3, 1], [3, 0, 2, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 2], [2, 4, 1, 3], [2, 3, 4, 0], [0, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3, 1], [0, 3, 1, 4], [2, 0, 1], [0, 3, 1, 4], [0, 1, 2, 3], [1, 0, 2, 3], [2, 1, 4, 0], [2, 4, 3], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 0, 4], [0, 3, 4, 1], [2, 3, 0, 4], [0, 3, 1], [3, 0, 2, 1], [0, 1, 3, 2], [2, 3, 4, 0], [3, 2, 0, 4], [0, 3, 1], [2, 1, 4, 0], [1, 2, 0], [2, 4, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 0], [3, 2, 4, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 2, 1, 3], [3, 0, 4, 1], [2, 4, 1, 0], [0, 3, 1, 4], [2, 4, 3, 0], [0, 1, 3, 2], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1], [2, 0, 1, 3], [1, 0, 2, 3], [2, 1, 0], [0, 1, 3, 2], [0, 3, 4, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 0], [2, 3, 4, 0], [2, 1, 0, 3], [2, 4, 1, 0], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [0, 3, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1], [0, 3, 1], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [0, 1, 3], [2, 1, 4, 0], [3, 4, 2, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 4, 0], [1, 2, 0, 3], [0, 3, 1], [3, 0, 2, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 0, 1], [2, 4, 3, 0], [2, 3, 4, 0], [0, 3, 1], [2, 1, 4, 0], [2, 1, 0, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2, 1], [2, 4, 1, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2, 3], [2, 4, 3, 1], [3, 2, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 0], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1, 0], [0, 1, 3, 2], [2, 0, 1], [2, 4, 3, 1], [0, 1, 2, 3], [3, 0, 4, 1], [0, 3, 1, 4], [1, 2, 0, 3], [2, 4, 3, 0], [2, 4, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 3, 4, 0], [1, 0, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 1, 3], [2, 0, 1, 3], [2, 1, 0, 3], [2, 3, 4, 0], [3, 2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 3, 0], [2, 1, 4, 0], [2, 1, 0, 3], [2, 1, 4, 0], [2, 3, 4, 0], [0, 3, 1, 4], [2, 3, 0, 1], [0, 2, 1, 3], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 0], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 0], [2, 4, 1, 0], [0, 3, 4, 1], [2, 4, 1, 0], [1, 2, 0, 3], [0, 3, 1], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [1, 2, 0, 3], [2, 1, 4, 0], [2, 4, 3], [0, 3, 4, 1], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1, 3], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1, 0], [0, 1, 3, 4]]\n",
      "POSITION :  [[3], [1], [2], [3], [4], [2, 4], [4], [1], [3], [4], [4], [4], [1], [3], [4], [3], [1], [2], [4], [3], [4], [2], [2, 4], [4], [3], [4], [2, 4], [1], [3], [1], [0], [1], [2], [3], [1], [3], [1], [2], [3], [4], [0], [3, 4], [4], [0], [2], [2, 4], [2, 4], [2], [3, 4], [4], [2], [3], [4], [4], [4], [0], [0, 1], [2, 4], [4], [2, 4], [1], [1], [2], [0], [2], [3], [1], [2], [0, 3], [4], [0], [1], [2, 4], [1], [2, 4], [3], [4], [2], [3], [4], [2], [3, 4], [2], [4], [4], [3], [0, 1], [2], [1], [3], [2], [1], [2, 4], [4], [4], [1], [1], [2, 4], [3], [3, 4], [0], [1], [2, 4], [0, 3], [2, 4], [1, 3], [1], [4], [0], [4], [2], [3], [2], [1], [4], [3], [3], [0, 3], [4], [4], [3, 4], [4], [2], [3], [3], [3], [1], [4], [3], [3], [4], [0], [2], [3], [2, 4], [0], [1], [2], [3], [3, 4], [2, 4], [2], [2], [3], [4], [0, 1], [3], [2, 4], [3], [1], [3], [0], [2], [3], [4], [2, 4], [4], [2], [3], [4], [1], [1], [2, 4], [3], [4], [2], [3], [4], [3], [2], [2], [0], [2], [0], [4], [0], [1], [2], [3], [4], [0], [3, 4], [2], [4], [4], [3], [3], [4], [3, 4], [0], [4], [2], [2], [4], [1], [0, 3], [1], [2], [1], [4], [0], [3], [2], [4], [4], [4], [1], [1], [0], [2], [2], [4], [1], [3], [4], [3], [1], [2], [4], [4], [3], [0], [1], [0], [4], [0], [2, 4], [2], [3], [2], [1], [3], [2], [3], [4], [2, 4], [4], [2], [3], [4], [3], [0, 1], [2], [0, 1], [4], [0], [4], [2], [3], [2]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.532\n",
      "tensor([3, 1, 2, 3, 4, 4, 1, 3, 4, 4, 4, 1, 3, 4, 3, 1, 2, 4, 3, 4, 2, 4, 3, 4,\n",
      "        1, 3, 1, 0, 1, 2, 3, 1, 3, 1, 2, 3, 4, 0, 4, 0, 2, 2, 4, 2, 3, 4, 4, 4,\n",
      "        0, 4, 1, 1, 2, 0, 2, 3, 1, 2, 4, 0, 1, 1, 3, 4, 2, 3, 4, 2, 2, 4, 4, 3,\n",
      "        2, 1, 3, 2, 1, 4, 4, 1, 1, 3, 0, 1, 1, 4, 0, 4, 2, 3, 2, 1, 4, 3, 3, 4,\n",
      "        4, 4, 2, 3, 3, 3, 1, 4, 3, 3, 4, 0, 2, 3, 0, 1, 2, 3, 2, 2, 3, 4, 3, 3,\n",
      "        1, 3, 0, 2, 3, 4, 4, 2, 3, 4, 1, 1, 3, 4, 2, 3, 4, 3, 2, 2, 0, 2, 0, 4,\n",
      "        0, 1, 2, 3, 4, 0, 2, 4, 4, 3, 3, 4, 0, 4, 2, 2, 4, 1, 1, 2, 1, 4, 0, 3,\n",
      "        2, 4, 4, 4, 1, 1, 0, 2, 2, 4, 1, 3, 4, 3, 1, 2, 4, 4, 3, 0, 1, 0, 4, 0,\n",
      "        2, 3, 2, 1, 3, 2, 3, 4, 4, 2, 3, 4, 3, 2, 4, 0, 4, 2, 3, 2])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 88.67924528301887\n",
      "Epoch: 1  Loss: 92.45283018867924\n",
      "Epoch: 2  Loss: 94.81132075471697\n",
      "Epoch: 3  Loss: 97.64150943396226\n",
      "Epoch: 4  Loss: 98.11320754716981\n",
      "Epoch: 5  Loss: 99.52830188679245\n",
      "Epoch: 6  Loss: 100.0\n",
      "Epoch: 7  Loss: 100.0\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 8/15 [06:17<05:51, 50.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  48.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 5.0579196079541  Accuracy on Support set:36.0\n",
      "Train_Epoch: 1  Train_Loss: 3.1527445031120442  Accuracy on Support set:52.0\n",
      "Train_Epoch: 2  Train_Loss: 1.6545069549418985  Accuracy on Support set:64.0\n",
      "Train_Epoch: 3  Train_Loss: 1.0153223549714312  Accuracy on Support set:64.0\n",
      "Train_Epoch: 4  Train_Loss: 0.7341793250106275  Accuracy on Support set:80.0\n",
      "Train_Epoch: 5  Train_Loss: 0.6115109387598932  Accuracy on Support set:92.0\n",
      "Train_Epoch: 6  Train_Loss: 0.394065782725811  Accuracy on Support set:100.0\n",
      "Train_Epoch: 7  Train_Loss: 0.2515586636029184  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.19256705461069942  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.16076631973963232  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.13932872612960637  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.1227440054854378  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.11006294978316873  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.10003557139076293  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.09167559921741486  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.08476225451100618  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.07882703634910286  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.07370875594671816  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.06923069488257169  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.06531224243342876  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.06178298295475543  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.058655801373533906  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.055821073870174584  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.053226419258862734  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.05090161395259202  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.048753066705539826  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.0467953775357455  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.044980418970808386  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.04330852088052779  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.041760384761728345  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.040324946627952156  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.03896962986793369  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.03771439578384161  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.03652939049527049  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.03541525881737471  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.034379357784055174  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.033386415252462026  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.032460428532212975  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.031585475909523666  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.03075413895305246  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.029970647492446004  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.02921363680623472  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.02850779130589217  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.027829024344682693  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.027182550630532206  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.026569258170202375  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.025978342443704606  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.025417653894983232  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.024876348301768304  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.02436379389371723  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  47.333333333333336\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.015381285920739174 tensor([0.0192, 0.4797, 0.0154, 0.0349, 0.4509], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.5598985861689507e-12 tensor([1.5599e-12, 1.9827e-06, 9.9978e-01, 4.0942e-09, 2.2195e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006878217100165784 tensor([0.0007, 0.4024, 0.4424, 0.0027, 0.1518], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.745424969116897e-12 tensor([2.7346e-02, 1.9078e-06, 5.7454e-12, 9.7263e-01, 1.7530e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.025499991821562e-08 tensor([1.3844e-01, 1.1920e-03, 9.0255e-08, 8.5588e-01, 4.4826e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.922488893223999e-08 tensor([9.3072e-01, 4.0684e-02, 6.9225e-08, 2.7960e-02, 6.3745e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.905940485608554e-11 tensor([9.7176e-01, 1.2556e-03, 7.9059e-11, 2.6985e-02, 3.8063e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006316887680441141 tensor([0.0063, 0.4907, 0.1108, 0.0297, 0.3624], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.223503184330184e-06 tensor([4.6568e-01, 6.5768e-02, 9.2235e-06, 4.3511e-01, 3.3432e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4045443776922184e-06 tensor([7.7237e-02, 2.3070e-03, 1.4045e-06, 9.0251e-01, 1.7948e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.309851071113371e-07 tensor([7.3099e-07, 1.4179e-03, 7.3010e-01, 1.8433e-04, 2.6829e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.499988376162946e-05 tensor([2.4607e-04, 7.4500e-01, 2.4369e-01, 6.5000e-05, 1.0994e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.4520463997858002e-10 tensor([1.4520e-10, 5.9139e-05, 9.9969e-01, 2.3362e-08, 2.5110e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4416852156529103e-09 tensor([6.4503e-01, 9.9538e-04, 1.4417e-09, 3.5379e-01, 1.8498e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0021590152755379677 tensor([0.0531, 0.1987, 0.0022, 0.2227, 0.5233], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003349705075379461 tensor([3.3497e-04, 6.1137e-02, 1.6023e-01, 7.8860e-03, 7.7041e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.497654248960316e-05 tensor([6.4977e-05, 2.1127e-01, 7.7029e-01, 1.7680e-04, 1.8195e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.945739682530984e-05 tensor([4.4636e-01, 4.0142e-01, 8.9457e-05, 1.4149e-01, 1.0641e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014131374191492796 tensor([0.0014, 0.0090, 0.0030, 0.0888, 0.8978], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00037327874451875687 tensor([6.9025e-03, 6.0681e-03, 3.7328e-04, 4.9610e-01, 4.9056e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.500318257465551e-08 tensor([2.9989e-01, 1.6725e-03, 3.5003e-08, 6.9318e-01, 5.2605e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.812963811355985e-08 tensor([8.5650e-01, 1.4130e-02, 3.8130e-08, 1.2895e-01, 4.2102e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6458803656860255e-05 tensor([1.6459e-05, 5.5033e-02, 9.2324e-01, 1.6347e-04, 2.1547e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1588578274768224e-07 tensor([7.2068e-01, 2.7828e-01, 1.1589e-07, 1.0244e-03, 1.8348e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009491482749581337 tensor([0.0095, 0.5626, 0.0528, 0.0308, 0.3443], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016839313320815563 tensor([3.1168e-01, 2.2042e-01, 1.6839e-04, 3.7082e-01, 9.6914e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003793518408201635 tensor([5.4623e-04, 6.3265e-01, 3.2990e-01, 3.7935e-04, 3.6520e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.229863978049252e-06 tensor([3.2299e-06, 2.5204e-02, 9.5758e-01, 2.8659e-05, 1.7183e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.2542610167874955e-05 tensor([4.6598e-01, 1.1919e-01, 2.2543e-05, 3.5891e-01, 5.5902e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00016005303768906742 tensor([1.6005e-04, 1.4238e-02, 7.3250e-02, 1.3176e-02, 8.9918e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0047295670956373215 tensor([0.0132, 0.0969, 0.0047, 0.1288, 0.7563], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004628267779480666 tensor([2.1360e-01, 2.0019e-01, 4.6283e-04, 5.2306e-01, 6.2691e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006612835451960564 tensor([0.0462, 0.9136, 0.0066, 0.0181, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007816413417458534 tensor([0.0271, 0.1320, 0.0078, 0.3772, 0.4559], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00026147186872549355 tensor([7.7548e-02, 6.2070e-02, 2.6147e-04, 5.9728e-01, 2.6284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0025195740163326263 tensor([0.0752, 0.7718, 0.0025, 0.0341, 0.1164], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1042234904223847e-09 tensor([9.6464e-01, 3.4372e-02, 1.1042e-09, 9.8005e-04, 3.7822e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.614137201817357e-06 tensor([2.6141e-06, 2.8935e-03, 7.5263e-01, 7.4103e-04, 2.4373e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00037333168438635767 tensor([2.3620e-03, 1.8521e-03, 3.7333e-04, 6.4522e-01, 3.5019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.937250650982605e-06 tensor([2.0327e-01, 1.6951e-02, 4.9373e-06, 7.5267e-01, 2.7112e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.768790237605572e-05 tensor([7.7688e-05, 1.5019e-02, 1.5332e-01, 4.3712e-03, 8.2721e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5625879541403265e-06 tensor([2.2716e-06, 1.0381e-01, 8.9470e-01, 1.5626e-06, 1.4940e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.17086061468941e-11 tensor([5.1709e-11, 2.2099e-05, 9.9976e-01, 2.0807e-08, 2.2079e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.966071072781482e-14 tensor([1.5697e-01, 1.7531e-06, 9.9661e-14, 8.4302e-01, 1.8399e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008939188905060291 tensor([0.0373, 0.4596, 0.0089, 0.0787, 0.4155], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.671245612073108e-07 tensor([4.2708e-01, 1.4696e-02, 7.6712e-07, 5.4861e-01, 9.6101e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.444646831165301e-07 tensor([5.4446e-07, 5.4795e-03, 9.7787e-01, 1.7959e-05, 1.6633e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013457974418997765 tensor([0.0464, 0.5321, 0.0135, 0.1249, 0.2832], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.720543955727408e-10 tensor([9.6616e-01, 3.1753e-03, 4.7205e-10, 3.0652e-02, 1.7048e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0750500223366544e-06 tensor([1.0751e-06, 7.3627e-04, 4.0660e-01, 1.2420e-03, 5.9142e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004924453678540885 tensor([4.9245e-04, 2.0583e-02, 1.4230e-02, 1.2393e-02, 9.5230e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.66255392995663e-05 tensor([3.4512e-01, 1.7727e-01, 9.6626e-05, 4.4323e-01, 3.4290e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0487816243530688e-07 tensor([2.5464e-01, 3.2918e-03, 2.0488e-07, 7.3541e-01, 6.6592e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003604905214160681 tensor([3.6049e-04, 3.8229e-03, 6.1002e-03, 5.3353e-02, 9.3636e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.9915368183283135e-05 tensor([5.9915e-05, 8.6093e-04, 4.7769e-03, 3.1080e-02, 9.6322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0555246010189876e-05 tensor([3.5694e-01, 6.3077e-01, 2.0555e-05, 9.5418e-03, 2.7316e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0033891163766384125 tensor([0.0487, 0.8912, 0.0034, 0.0193, 0.0374], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00024956371635198593 tensor([2.4956e-04, 2.3553e-01, 6.3669e-01, 1.1338e-03, 1.2639e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3434085782980674e-09 tensor([4.8999e-01, 7.5405e-04, 2.3434e-09, 5.0904e-01, 2.2115e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.589375410228968e-05 tensor([1.5139e-02, 3.4714e-03, 3.5894e-05, 8.0398e-01, 1.7737e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005160329747013748 tensor([2.1315e-03, 2.5193e-03, 5.1603e-04, 3.8759e-01, 6.0724e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.22911254507585e-08 tensor([1.2291e-08, 4.6456e-04, 9.9472e-01, 1.0202e-06, 4.8098e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.344091722596204e-08 tensor([9.3441e-08, 9.7801e-03, 9.8953e-01, 3.2601e-07, 6.8963e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1046422514482401e-06 tensor([7.8177e-01, 7.2054e-02, 1.1046e-06, 1.4241e-01, 3.7734e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00010011265840148553 tensor([2.6319e-02, 1.0033e-02, 1.0011e-04, 6.0933e-01, 3.5422e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.445738710501246e-08 tensor([2.7266e-01, 1.7405e-03, 4.4457e-08, 7.2140e-01, 4.2004e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.66879009763943e-06 tensor([8.5239e-06, 1.9569e-01, 8.0039e-01, 4.6688e-06, 3.9146e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012595716398209333 tensor([0.0013, 0.1570, 0.3007, 0.0250, 0.5160], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.1349295568361413e-06 tensor([6.7007e-02, 3.5272e-03, 3.1349e-06, 8.7263e-01, 5.6834e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.335519745131023e-05 tensor([3.6071e-01, 1.2888e-01, 5.3355e-05, 4.7949e-01, 3.0864e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.3423717954738095e-08 tensor([2.0634e-01, 1.2413e-03, 4.3424e-08, 7.8815e-01, 4.2721e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0026288034860044718 tensor([0.0031, 0.7888, 0.1055, 0.0026, 0.1000], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018260173499584198 tensor([0.0249, 0.4302, 0.0183, 0.1180, 0.4086], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012070576194673777 tensor([0.0409, 0.9505, 0.0012, 0.0028, 0.0046], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004672542680054903 tensor([0.0405, 0.8767, 0.0047, 0.0150, 0.0631], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002748660044744611 tensor([0.0189, 0.0547, 0.0027, 0.3055, 0.6181], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00028899070457555354 tensor([1.6995e-02, 9.8137e-01, 3.1827e-04, 2.8899e-04, 1.0295e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00020213362586218864 tensor([2.0213e-04, 4.6857e-02, 2.7940e-01, 8.1960e-03, 6.6534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.044856116713902e-10 tensor([6.1205e-01, 5.2751e-04, 4.0449e-10, 3.8733e-01, 8.6795e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006351533229462802 tensor([0.2042, 0.5339, 0.0006, 0.1512, 0.1101], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5697331036790274e-05 tensor([4.5462e-01, 8.5061e-02, 1.5697e-05, 4.3516e-01, 2.5141e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8624058739646898e-08 tensor([1.8624e-08, 2.1718e-03, 9.9688e-01, 1.9492e-07, 9.4602e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.264271219802442e-12 tensor([6.2643e-12, 3.5784e-06, 9.9964e-01, 1.3342e-08, 3.5210e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.3715663448674604e-05 tensor([5.1624e-01, 1.8243e-01, 3.3716e-05, 2.7174e-01, 2.9557e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0020342711359262466 tensor([0.0020, 0.0137, 0.0052, 0.1533, 0.8258], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00042886126902885735 tensor([4.2886e-04, 7.1615e-02, 1.5843e-01, 9.6674e-03, 7.5986e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00013909806148149073 tensor([1.3910e-04, 3.6844e-01, 6.0275e-01, 2.1389e-04, 2.8455e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.68711095386243e-08 tensor([6.6871e-08, 6.3353e-04, 9.6949e-01, 1.4145e-05, 2.9863e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005872101755812764 tensor([0.0150, 0.0165, 0.0006, 0.5540, 0.4139], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01312468945980072 tensor([0.0131, 0.2934, 0.0209, 0.0711, 0.6015], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3443074431407354e-10 tensor([9.7846e-01, 2.0655e-03, 1.3443e-10, 1.9465e-02, 1.1671e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.89768156323123e-10 tensor([9.8977e-10, 4.1848e-05, 9.9136e-01, 7.9973e-07, 8.5976e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011283186031505466 tensor([0.0011, 0.5307, 0.3958, 0.0030, 0.0694], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1197124649697798e-06 tensor([3.5759e-01, 1.3939e-02, 1.1197e-06, 6.2418e-01, 4.2892e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2966569304117e-06 tensor([2.2967e-06, 4.8575e-04, 1.9004e-01, 4.7442e-03, 8.0472e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.881737928983057e-08 tensor([8.5621e-01, 1.8431e-02, 7.8817e-08, 1.2458e-01, 7.7622e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7892750154023815e-07 tensor([8.8308e-01, 8.3896e-02, 2.7893e-07, 3.2290e-02, 7.3111e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016123599198181182 tensor([1.5486e-01, 6.3629e-02, 1.6124e-04, 7.2866e-01, 5.2694e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.993635064285627e-07 tensor([1.1604e-01, 1.6536e-03, 2.9936e-07, 8.7639e-01, 5.9103e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.3529868294695e-09 tensor([9.0546e-01, 7.7934e-03, 9.3530e-09, 8.6642e-02, 1.0232e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.495252829248784e-06 tensor([2.6533e-01, 1.3764e-02, 2.4953e-06, 6.8842e-01, 3.2485e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.697590665931784e-08 tensor([5.6976e-08, 3.8738e-03, 9.9508e-01, 5.2691e-07, 1.0421e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.2355660550820176e-06 tensor([3.2356e-06, 2.4780e-02, 9.6080e-01, 3.1543e-05, 1.4383e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3407321603153832e-05 tensor([3.5511e-02, 4.3083e-03, 1.3407e-05, 8.7261e-01, 8.7552e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.038677979181557e-07 tensor([6.5081e-03, 5.8158e-05, 1.0387e-07, 9.7941e-01, 1.4024e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.795360884577107e-11 tensor([9.8923e-01, 2.4876e-03, 7.7954e-11, 8.2774e-03, 3.9494e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015311517054215074 tensor([0.0015, 0.6173, 0.3011, 0.0032, 0.0768], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004090939648449421 tensor([0.0072, 0.9115, 0.0452, 0.0041, 0.0319], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1058714335376862e-06 tensor([5.6702e-03, 1.7757e-04, 1.1059e-06, 9.6218e-01, 3.1967e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010213250294327736 tensor([0.0188, 0.1217, 0.0102, 0.2327, 0.6165], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.482947926793713e-06 tensor([1.3370e-01, 1.1185e-02, 5.4829e-06, 8.1575e-01, 3.9361e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.812628038867842e-06 tensor([5.9193e-04, 9.9272e-01, 6.2737e-03, 8.8126e-06, 4.0103e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.016538544097784e-10 tensor([8.0165e-10, 1.5347e-04, 9.9936e-01, 7.9235e-08, 4.8207e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.971071833395399e-05 tensor([6.5388e-02, 9.9011e-03, 1.9711e-05, 8.1726e-01, 1.0743e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1049688034291414e-11 tensor([9.6884e-01, 6.6940e-04, 2.1050e-11, 3.0484e-02, 3.4678e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007419122848659754 tensor([0.0074, 0.2213, 0.0302, 0.0584, 0.6827], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2482346392062027e-05 tensor([2.6316e-01, 7.3311e-01, 1.2482e-05, 2.7020e-03, 1.0172e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02136799320578575 tensor([0.0214, 0.4997, 0.0286, 0.0605, 0.3899], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1219794032513164e-05 tensor([1.4984e-02, 1.7325e-03, 1.1220e-05, 8.7490e-01, 1.0837e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.600841200883224e-08 tensor([7.6008e-08, 1.9135e-05, 7.8022e-02, 1.4494e-03, 9.2051e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.242037438321859e-06 tensor([8.2420e-06, 4.2216e-03, 2.5825e-01, 1.0330e-03, 7.3649e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007242138381116092 tensor([0.0009, 0.7236, 0.2090, 0.0007, 0.0658], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.1712983652832918e-05 tensor([2.1713e-05, 1.0750e-01, 8.8428e-01, 6.0256e-05, 8.1387e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013810915406793356 tensor([0.0014, 0.1419, 0.2121, 0.0343, 0.6103], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011769260745495558 tensor([0.1219, 0.2303, 0.0012, 0.4640, 0.1827], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002056367084151134 tensor([2.0564e-04, 8.3668e-03, 2.0050e-02, 1.8365e-02, 9.5301e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015629757195711136 tensor([0.0261, 0.8156, 0.0156, 0.0267, 0.1161], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.505247564954516e-08 tensor([2.5052e-08, 1.0015e-03, 9.9758e-01, 9.1460e-07, 1.4155e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001259055861737579 tensor([1.2591e-04, 1.5465e-02, 1.4438e-01, 1.3478e-02, 8.2656e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.946068550751079e-05 tensor([1.9461e-05, 3.6762e-02, 8.6030e-01, 3.8382e-04, 1.0254e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.014290849203462e-08 tensor([2.4926e-01, 1.8629e-03, 5.0143e-08, 7.4502e-01, 3.8596e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001273173838853836 tensor([0.0872, 0.8792, 0.0013, 0.0161, 0.0163], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.9010574255371466e-05 tensor([2.8396e-04, 8.6189e-01, 1.3558e-01, 3.9011e-05, 2.2121e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.69800657912856e-06 tensor([3.4930e-01, 4.3226e-02, 9.6980e-06, 5.9758e-01, 9.8931e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00017340622434858233 tensor([3.1196e-02, 1.5846e-02, 1.7341e-04, 5.9033e-01, 3.6245e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.9180162586527345e-11 tensor([9.9026e-01, 1.7678e-03, 3.9180e-11, 7.9646e-03, 2.7081e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.043165517941816e-06 tensor([4.0432e-06, 4.5497e-02, 9.4068e-01, 1.3854e-05, 1.3801e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017821351066231728 tensor([0.1615, 0.4959, 0.0018, 0.2215, 0.1194], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010172154288738966 tensor([0.0010, 0.1273, 0.2474, 0.0303, 0.5940], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011777463369071484 tensor([0.0898, 0.1347, 0.0012, 0.4494, 0.3249], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00031590627622790635 tensor([3.2062e-01, 5.4790e-01, 3.1591e-04, 8.8599e-02, 4.2561e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.470934532539104e-07 tensor([2.4709e-07, 7.1305e-04, 7.7402e-01, 1.2677e-04, 2.2514e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.102118171713353e-13 tensor([3.1021e-13, 9.7698e-07, 9.9995e-01, 1.0442e-09, 4.8396e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000537645595613867 tensor([5.6485e-02, 4.6945e-02, 5.3765e-04, 7.7233e-01, 1.2370e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010506625287234783 tensor([0.0308, 0.7523, 0.0105, 0.0231, 0.1832], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.415929106416471e-13 tensor([9.9068e-01, 2.4983e-04, 3.4159e-13, 9.0736e-03, 2.7484e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.6895747851085616e-06 tensor([2.6896e-06, 4.0975e-02, 9.4406e-01, 1.0843e-05, 1.4953e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.6383145925356075e-05 tensor([5.1335e-04, 9.5190e-01, 4.4836e-02, 3.6383e-05, 2.7108e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.860690048895776e-05 tensor([4.7430e-01, 1.4954e-01, 3.8607e-05, 3.2505e-01, 5.1068e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.517228884040378e-05 tensor([6.0870e-01, 2.2850e-01, 1.5172e-05, 1.5261e-01, 1.0165e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002984963357448578 tensor([0.0030, 0.2365, 0.0647, 0.0138, 0.6820], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002740079944487661 tensor([2.7401e-04, 5.2547e-01, 4.4834e-01, 3.2248e-04, 2.5597e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019545206800103188 tensor([0.0195, 0.7969, 0.0280, 0.0360, 0.1195], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00044574911589734256 tensor([0.0004, 0.1860, 0.3941, 0.0063, 0.4132], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.046412868774496e-06 tensor([4.0464e-06, 3.5701e-03, 4.1326e-01, 8.0371e-04, 5.8236e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018713448662310839 tensor([0.0019, 0.0469, 0.0203, 0.0440, 0.8869], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009731759200803936 tensor([0.0010, 0.0266, 0.0792, 0.1085, 0.7847], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008005730924196541 tensor([0.0008, 0.7013, 0.2406, 0.0008, 0.0565], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3405119620178994e-08 tensor([3.0739e-01, 1.4480e-03, 2.3405e-08, 6.9026e-01, 8.9975e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007802632637321949 tensor([0.0125, 0.1244, 0.0078, 0.1212, 0.7341], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.048086263239384e-05 tensor([1.0999e-01, 3.0708e-02, 7.0481e-05, 7.0665e-01, 1.5258e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.856312701420393e-05 tensor([9.1708e-04, 9.9248e-01, 6.1938e-03, 1.8563e-05, 3.8715e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.882297773496248e-05 tensor([5.8823e-05, 2.9631e-02, 6.5378e-01, 3.9350e-03, 3.1259e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00010422951891086996 tensor([5.9294e-04, 2.6732e-04, 1.0423e-04, 4.6356e-01, 5.3547e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.509720722329803e-05 tensor([3.5818e-01, 1.2658e-01, 5.5097e-05, 4.4362e-01, 7.1564e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2652571967919357e-05 tensor([5.8744e-01, 1.0778e-01, 1.2653e-05, 2.7562e-01, 2.9152e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.795483180141673e-08 tensor([9.7955e-08, 4.1827e-03, 9.8971e-01, 1.1555e-06, 6.1079e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1304474583084811e-06 tensor([1.1304e-06, 2.7777e-02, 9.6784e-01, 2.8886e-06, 4.3814e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.331999778311001e-06 tensor([2.7197e-02, 1.4861e-03, 3.3320e-06, 8.9997e-01, 7.1348e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.2118765122722834e-05 tensor([4.2119e-05, 6.1366e-02, 7.3172e-01, 6.0201e-04, 2.0627e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.202273003375609e-11 tensor([9.6595e-01, 5.0172e-04, 1.2023e-11, 3.3542e-02, 2.4334e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00017422452219761908 tensor([2.3856e-01, 7.2355e-01, 1.7422e-04, 2.3496e-02, 1.4217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.379018643405288e-05 tensor([7.3790e-05, 2.6597e-02, 4.2038e-01, 5.6670e-03, 5.4728e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1362148161507832e-11 tensor([9.5085e-02, 1.3586e-05, 2.1362e-11, 9.0485e-01, 5.2056e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0792743365684032e-09 tensor([1.3688e-02, 1.0981e-05, 1.0793e-09, 9.8536e-01, 9.4231e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0344125459814677e-06 tensor([2.6629e-02, 1.0028e-03, 2.0344e-06, 9.3843e-01, 3.3935e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009755204431712627 tensor([0.0098, 0.5083, 0.0270, 0.0241, 0.4308], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8284305244596908e-06 tensor([2.8284e-06, 3.7506e-02, 9.5367e-01, 1.4499e-05, 8.8096e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.932724498805328e-07 tensor([2.9327e-07, 1.8177e-04, 3.5007e-01, 1.2055e-03, 6.4854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4953408511431121e-09 tensor([9.3419e-01, 3.4733e-03, 1.4953e-09, 6.2282e-02, 5.4705e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015763893316034228 tensor([1.5764e-04, 3.5145e-03, 1.2115e-02, 3.4094e-02, 9.5012e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006657117628492415 tensor([0.1725, 0.3845, 0.0007, 0.2048, 0.2376], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.67472113743861e-09 tensor([5.6747e-09, 9.7360e-04, 9.9843e-01, 1.2352e-07, 5.9162e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.388593127264585e-09 tensor([2.5723e-02, 4.8753e-05, 5.3886e-09, 9.7333e-01, 8.9799e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.540810550679453e-05 tensor([6.6024e-04, 1.5747e-04, 4.5408e-05, 5.7455e-01, 4.2459e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.075950657688736e-09 tensor([2.9867e-01, 8.0688e-04, 9.0760e-09, 6.9941e-01, 1.1139e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001202545827254653 tensor([0.0012, 0.1611, 0.2602, 0.0192, 0.5583], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001685165916569531 tensor([1.6852e-04, 2.4197e-02, 3.1454e-01, 2.3650e-02, 6.3744e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.289622156898986e-07 tensor([3.2896e-07, 4.8221e-03, 9.7981e-01, 8.4375e-06, 1.5362e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.066309485153738e-09 tensor([9.8276e-01, 9.4761e-03, 1.0663e-09, 7.7514e-03, 1.2221e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003856040770187974 tensor([0.0487, 0.7568, 0.0039, 0.0244, 0.1663], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3969430256111082e-05 tensor([5.9026e-05, 5.8165e-01, 4.1471e-01, 1.3969e-05, 3.5677e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00038621347630396485 tensor([1.7509e-03, 9.3343e-01, 5.9971e-02, 3.8621e-04, 4.4655e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.472594930670653e-11 tensor([6.1797e-01, 2.5307e-04, 9.4726e-11, 3.8174e-01, 3.2788e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9656194727435405e-10 tensor([9.2390e-01, 1.2760e-03, 1.9656e-10, 7.4790e-02, 3.2581e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.4461626885473606e-09 tensor([9.3143e-01, 5.5561e-03, 3.4462e-09, 6.2927e-02, 8.2706e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5032327610242646e-05 tensor([1.0325e-04, 7.2756e-01, 2.6703e-01, 1.5032e-05, 5.2935e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00028983617085032165 tensor([2.8984e-04, 2.5884e-01, 6.1950e-01, 1.1849e-03, 1.2019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.011479177279398e-05 tensor([4.0115e-05, 3.6860e-02, 5.8153e-01, 1.0450e-03, 3.8052e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000943066319450736 tensor([0.0009, 0.0324, 0.0410, 0.0423, 0.8834], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003123923670500517 tensor([2.8765e-01, 5.4664e-01, 3.1239e-04, 8.6525e-02, 7.8876e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001514358795247972 tensor([0.0472, 0.9201, 0.0015, 0.0082, 0.0229], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016852542757987976 tensor([0.0169, 0.5094, 0.0256, 0.0511, 0.3971], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.878741854952295e-09 tensor([6.0333e-01, 1.7675e-03, 4.8787e-09, 3.9456e-01, 3.4571e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.225801373853756e-07 tensor([5.0104e-01, 1.3322e-02, 5.2258e-07, 4.7656e-01, 9.0725e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00020456232596188784 tensor([8.5390e-03, 5.9809e-03, 2.0456e-04, 2.9452e-01, 6.9075e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.952151661490433e-10 tensor([3.9522e-10, 1.3471e-04, 9.9938e-01, 3.1966e-08, 4.8153e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.051130643110355e-08 tensor([2.0511e-08, 1.0651e-03, 9.9768e-01, 6.7693e-07, 1.2546e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.1990806999383494e-05 tensor([2.1991e-05, 1.1107e-02, 4.7775e-01, 2.4293e-03, 5.0870e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2761442508567598e-08 tensor([2.2761e-08, 1.4485e-03, 9.9605e-01, 5.7115e-07, 2.5046e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013168806210160255 tensor([0.0367, 0.4348, 0.0132, 0.1184, 0.3969], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.3666959754773416e-06 tensor([4.3667e-06, 5.1582e-02, 9.4015e-01, 1.1241e-05, 8.2515e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.290501914918423e-06 tensor([5.6175e-01, 7.0654e-02, 6.2905e-06, 3.5627e-01, 1.1327e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.106625056010671e-05 tensor([2.5059e-03, 5.3661e-04, 4.1066e-05, 7.1534e-01, 2.8158e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.160097866659271e-07 tensor([1.2043e-02, 1.0434e-04, 1.1601e-07, 9.6995e-01, 1.7907e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007690507918596268 tensor([0.0305, 0.7676, 0.0077, 0.0309, 0.1633], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.7816418120637536e-05 tensor([1.2381e-01, 3.8625e-02, 5.7816e-05, 5.8565e-01, 2.5185e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014250158332288265 tensor([0.0442, 0.4664, 0.0143, 0.1546, 0.3206], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.213397131191954e-10 tensor([2.2916e-01, 1.3357e-04, 4.2134e-10, 7.7056e-01, 1.4939e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012706328416243196 tensor([0.0016, 0.0025, 0.0013, 0.4835, 0.5111], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005366749246604741 tensor([0.1848, 0.2829, 0.0005, 0.2417, 0.2901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016413338482379913 tensor([0.0310, 0.8128, 0.0164, 0.0306, 0.1092], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.148196588474093e-06 tensor([1.1482e-06, 8.3713e-03, 9.7421e-01, 3.3644e-05, 1.7381e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.991539105323682e-08 tensor([9.2893e-01, 2.3019e-02, 2.9915e-08, 4.7908e-02, 1.3919e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4974661227972206e-10 tensor([9.3755e-01, 1.5057e-03, 2.4975e-10, 6.0905e-02, 4.2747e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5494799887605382e-11 tensor([8.2871e-01, 2.0833e-04, 1.5495e-11, 1.7107e-01, 8.7696e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013720252551138401 tensor([0.0137, 0.2405, 0.0170, 0.0754, 0.6534], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2951895200785657e-07 tensor([1.2952e-07, 4.5208e-03, 9.9325e-01, 1.6857e-06, 2.2262e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013402075273916125 tensor([0.0162, 0.9761, 0.0021, 0.0013, 0.0044], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00025006086798384786 tensor([2.7264e-03, 2.1623e-03, 2.5006e-04, 3.6429e-01, 6.3057e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0207013900753736e-08 tensor([4.5373e-01, 1.4285e-03, 1.0207e-08, 5.4319e-01, 1.6555e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006165939848870039 tensor([5.0380e-03, 9.7634e-01, 9.4758e-03, 6.1659e-04, 8.5287e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0049609500129009e-06 tensor([1.0050e-06, 1.4169e-04, 8.0823e-02, 4.0913e-03, 9.1494e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.306383054128673e-08 tensor([1.9832e-02, 1.4877e-04, 7.3064e-08, 9.7753e-01, 2.4889e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00035931228194385767 tensor([3.5931e-04, 1.9702e-02, 8.1018e-02, 4.5045e-02, 8.5388e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1945920050493442e-05 tensor([3.7372e-02, 4.3921e-03, 1.1946e-05, 8.6783e-01, 9.0392e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004141705576330423 tensor([0.0082, 0.0287, 0.0041, 0.3613, 0.5977], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.229086053049059e-10 tensor([5.2291e-10, 5.9963e-05, 9.9790e-01, 1.4869e-07, 2.0434e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017984876176342368 tensor([0.0119, 0.0315, 0.0018, 0.3848, 0.5701], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012839454575441778 tensor([1.2839e-04, 3.3013e-03, 1.5241e-02, 2.2418e-02, 9.5891e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006174183217808604 tensor([1.2969e-01, 8.2863e-01, 6.1742e-04, 2.1214e-02, 1.9853e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.8030367502651643e-06 tensor([1.1091e-01, 8.8891e-01, 2.8030e-06, 1.2381e-04, 4.9051e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8416510033935083e-08 tensor([2.8417e-08, 1.3978e-03, 9.9745e-01, 8.3777e-07, 1.1487e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001523216487839818 tensor([1.0176e-02, 4.4829e-03, 1.5232e-04, 8.2883e-01, 1.5635e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002920148253906518 tensor([1.7671e-03, 1.1918e-03, 2.9201e-04, 6.1783e-01, 3.7892e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002556749386712909 tensor([3.5913e-04, 6.1963e-01, 3.4927e-01, 2.5567e-04, 3.0487e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002229869569418952 tensor([2.2299e-04, 3.9203e-01, 5.5016e-01, 4.3893e-04, 5.7150e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.4651941405172693e-06 tensor([3.4652e-06, 9.0870e-03, 7.9625e-01, 1.7935e-04, 1.9448e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4791300956561457e-11 tensor([9.4371e-01, 5.7878e-04, 2.4791e-11, 5.5711e-02, 3.2784e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.8286332457792014e-05 tensor([4.8286e-05, 2.0517e-03, 1.0081e-02, 9.6575e-03, 9.7816e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [0], [0], [2], [2], [2], [2], [0], [2], [2], [0], [3], [0], [2], [2], [0], [0], [2], [0], [2], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [2], [2], [2], [2], [2], [2], [2], [0], [2], [2], [0], [3], [0], [2], [2], [2], [0], [2], [2], [0], [0], [2], [2], [0], [0], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [2], [3], [0], [2], [2], [2], [3], [2], [2], [2], [2], [3], [0], [2], [2], [2], [0], [0], [2], [0], [0], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [2], [2], [2], [2], [0], [0], [2], [2], [2], [0], [3], [2], [2], [2], [3], [0], [2], [2], [0], [2], [0], [2], [0], [0], [3], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [3], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [2], [0], [3], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [2], [2], [2], [3], [0], [2], [2], [2], [0], [0], [2], [0], [2], [2], [0], [2], [2], [2], [0], [0], [0], [2], [0], [2], [0], [2], [2], [2], [0], [0], [0], [2], [2], [3], [3], [2], [2], [2], [3], [0], [0], [0], [2], [2], [0], [2], [2], [2], [0], [0], [0], [0], [2], [0], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [3], [2], [2], [3], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [3], [0], [0], [2], [0]]\n",
      "[[0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [0], [0], [2], [2], [2], [2], [0], [2], [2], [0], [3], [0], [2], [2], [0], [0], [2], [0], [2], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [2], [2], [2], [2], [2], [2], [2], [0], [2], [2], [0], [3], [0], [2], [2], [2], [0], [2], [2], [0], [0], [2], [2], [0], [0], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [2], [3], [0], [2], [2], [2], [3], [2], [2], [2], [2], [3], [0], [2], [2], [2], [0], [0], [2], [0], [0], [0], [0], [2], [0], [2], [0], [0], [2], [0], [2], [2], [2], [2], [2], [2], [0], [0], [2], [2], [2], [0], [3], [2], [2], [2], [3], [0], [2], [2], [0], [2], [0], [2], [0], [0], [3], [0], [0], [2], [0], [2], [0], [0], [0], [2], [2], [3], [2], [2], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [2], [0], [3], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [2], [2], [2], [3], [0], [2], [2], [2], [0], [0], [2], [0], [2], [2], [0], [2], [2], [2], [0], [0], [0], [2], [0], [2], [0], [2], [2], [2], [0], [0], [0], [2], [2], [3], [3], [2], [2], [2], [3], [0], [0], [0], [2], [2], [0], [2], [2], [2], [0], [0], [0], [0], [2], [0], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [3], [2], [2], [3], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [2], [3], [0], [0], [2], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0044454879760742185  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0044454865455627445  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004445483207702636  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0044454803466796875  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004445475578308105  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004445470333099365  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0044454636573791504  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004445456504821778  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004445449352264404  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004445441246032715  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004445432662963867  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004445423126220703  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004445413589477539  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004445405006408692  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004445394515991211  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004445384502410889  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004445374011993408  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004445362567901612  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004445352077484131  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004445341110229492  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.019747456535696983 tensor([0.0197, 0.4844, 0.0149, 0.0351, 0.4459], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.1600056732704616e-09 tensor([1.6084e-12, 2.0234e-06, 9.9977e-01, 4.1600e-09, 2.2385e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002729063155129552 tensor([0.0007, 0.4102, 0.4345, 0.0027, 0.1519], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.946616521308897e-06 tensor([2.8072e-02, 1.9466e-06, 5.6775e-12, 9.7191e-01, 1.7437e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012060486478731036 tensor([1.4170e-01, 1.2060e-03, 8.8088e-08, 8.5267e-01, 4.4241e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006166661041788757 tensor([9.3188e-01, 4.0153e-02, 6.5996e-08, 2.7355e-02, 6.1667e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.7577613056782866e-06 tensor([9.7208e-01, 1.2496e-03, 7.7203e-11, 2.6663e-02, 3.7578e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02989402785897255 tensor([0.0065, 0.4961, 0.1079, 0.0299, 0.3595], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03266178444027901 tensor([4.7163e-01, 6.5634e-02, 8.8779e-06, 4.3006e-01, 3.2662e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023300747852772474 tensor([7.9053e-02, 2.3301e-03, 1.3693e-06, 9.0088e-01, 1.7737e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019044618238694966 tensor([7.7358e-07, 1.4714e-03, 7.2562e-01, 1.9045e-04, 2.7271e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00025274278596043587 tensor([2.5274e-04, 7.5150e-01, 2.3727e-01, 6.5566e-05, 1.0921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.380584085415194e-08 tensor([1.5080e-10, 6.0713e-05, 9.9969e-01, 2.3806e-08, 2.5378e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00018074075342155993 tensor([6.5039e-01, 9.9471e-04, 1.3954e-09, 3.4844e-01, 1.8074e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05465652421116829 tensor([0.0547, 0.2014, 0.0021, 0.2237, 0.5182], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008004684932529926 tensor([3.4849e-04, 6.2627e-02, 1.5764e-01, 8.0047e-03, 7.7138e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00018142534827347845 tensor([6.8043e-05, 2.1743e-01, 7.6392e-01, 1.8143e-04, 1.8401e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010377945378422737 tensor([4.5095e-01, 3.9878e-01, 8.5676e-05, 1.3981e-01, 1.0378e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002899540588259697 tensor([0.0015, 0.0092, 0.0029, 0.0900, 0.8964], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0061781504191458225 tensor([7.1319e-03, 6.1782e-03, 3.6544e-04, 4.9916e-01, 4.8717e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016864450881257653 tensor([3.0541e-01, 1.6864e-03, 3.4103e-08, 6.8773e-01, 5.1700e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00040885922499001026 tensor([8.5921e-01, 1.3985e-02, 3.6498e-08, 1.2640e-01, 4.0886e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016851897817105055 tensor([1.7332e-05, 5.7022e-02, 9.2087e-01, 1.6852e-04, 2.1919e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.813374547054991e-05 tensor([7.2298e-01, 2.7599e-01, 1.1263e-07, 1.0175e-03, 1.8134e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03085440956056118 tensor([0.0097, 0.5674, 0.0513, 0.0309, 0.3407], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09483632445335388 tensor([3.1677e-01, 2.2037e-01, 1.6202e-04, 3.6787e-01, 9.4836e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005636076093651354 tensor([5.6361e-04, 6.4058e-01, 3.2205e-01, 3.8416e-04, 3.6419e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.9720993552473374e-05 tensor([3.4222e-06, 2.6217e-02, 9.5621e-01, 2.9721e-05, 1.7543e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05454001948237419 tensor([4.7198e-01, 1.1900e-01, 2.1679e-05, 3.5446e-01, 5.4540e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013353809714317322 tensor([1.6614e-04, 1.4562e-02, 7.2064e-02, 1.3354e-02, 8.9985e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013656539842486382 tensor([0.0137, 0.0988, 0.0046, 0.1302, 0.7527], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06154892221093178 tensor([2.1741e-01, 2.0066e-01, 4.4711e-04, 5.1993e-01, 6.1549e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01512943021953106 tensor([0.0471, 0.9133, 0.0064, 0.0181, 0.0151], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027922935783863068 tensor([0.0279, 0.1339, 0.0076, 0.3789, 0.4517], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06271862983703613 tensor([7.9516e-02, 6.2719e-02, 2.5461e-04, 5.9763e-01, 2.5988e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.034015752375125885 tensor([0.0767, 0.7727, 0.0024, 0.0340, 0.1142], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.7371175949374447e-06 tensor([9.6499e-01, 3.4030e-02, 1.0734e-09, 9.7292e-04, 3.7371e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007652820204384625 tensor([2.7575e-06, 2.9945e-03, 7.4832e-01, 7.6528e-04, 2.4791e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0018803960410878062 tensor([2.4316e-03, 1.8804e-03, 3.6540e-04, 6.4782e-01, 3.4750e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01704554259777069 tensor([2.0756e-01, 1.7046e-02, 4.7778e-06, 7.4875e-01, 2.6646e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004440018441528082 tensor([8.0806e-05, 1.5388e-02, 1.5104e-01, 4.4400e-03, 8.2905e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3879917989688693e-06 tensor([2.3880e-06, 1.0726e-01, 8.9121e-01, 1.6103e-06, 1.5178e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.118935249484366e-08 tensor([5.3695e-11, 2.2691e-05, 9.9975e-01, 2.1189e-08, 2.2299e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.7820631228460115e-06 tensor([1.5998e-01, 1.7821e-06, 9.8741e-14, 8.4002e-01, 1.8318e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.038224417716264725 tensor([0.0382, 0.4636, 0.0087, 0.0789, 0.4106], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009406362660229206 tensor([4.3341e-01, 1.4723e-02, 7.4143e-07, 5.4246e-01, 9.4064e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8627049939823337e-05 tensor([5.7704e-07, 5.7021e-03, 9.7729e-01, 1.8627e-05, 1.6991e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04752142354846001 tensor([0.0475, 0.5351, 0.0130, 0.1252, 0.2792], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.6550871805520728e-05 tensor([9.6686e-01, 3.1390e-03, 4.5179e-10, 2.9981e-02, 1.6551e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007558604120276868 tensor([1.1202e-06, 7.5586e-04, 4.0246e-01, 1.2656e-03, 5.9552e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012579594738781452 tensor([5.1284e-04, 2.1080e-02, 1.3988e-02, 1.2580e-02, 9.5184e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03354532644152641 tensor([3.5015e-01, 1.7687e-01, 9.2864e-05, 4.3934e-01, 3.3545e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0033164876513183117 tensor([2.5967e-01, 3.3165e-03, 1.9912e-07, 7.3046e-01, 6.5479e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0039066048339009285 tensor([3.7423e-04, 3.9066e-03, 6.0045e-03, 5.4083e-02, 9.3563e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008802488446235657 tensor([6.2236e-05, 8.8025e-04, 4.6989e-03, 3.1517e-02, 9.6284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002666572807356715 tensor([3.6120e-01, 6.2666e-01, 1.9685e-05, 9.4545e-03, 2.6666e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.019236722961068153 tensor([0.0496, 0.8912, 0.0033, 0.0192, 0.0367], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011640385491773486 tensor([2.6203e-04, 2.4247e-01, 6.2847e-01, 1.1640e-03, 1.2764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00021684817329514772 tensor([4.9612e-01, 7.5659e-04, 2.2758e-09, 5.0291e-01, 2.1685e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0035182125866413116 tensor([1.5546e-02, 3.5182e-03, 3.5094e-05, 8.0511e-01, 1.7579e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002209097845479846 tensor([2.2091e-03, 2.5691e-03, 5.0537e-04, 3.9107e-01, 6.0365e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0584955134618212e-06 tensor([1.3036e-08, 4.8377e-04, 9.9460e-01, 1.0585e-06, 4.9162e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.364742156009015e-07 tensor([9.8460e-08, 1.0141e-02, 9.8916e-01, 3.3647e-07, 7.0232e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003668692661449313 tensor([7.8513e-01, 7.1346e-02, 1.0578e-06, 1.3986e-01, 3.6687e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010197252966463566 tensor([2.7123e-02, 1.0197e-02, 9.7925e-05, 6.1141e-01, 3.5118e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017551528289914131 tensor([2.7809e-01, 1.7552e-03, 4.3245e-08, 7.1602e-01, 4.1292e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.93654942046851e-06 tensor([8.9365e-06, 2.0161e-01, 7.9441e-01, 4.7970e-06, 3.9653e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025414975360035896 tensor([0.0013, 0.1606, 0.2957, 0.0254, 0.5171], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003569624852389097 tensor([6.8760e-02, 3.5696e-03, 3.0527e-06, 8.7158e-01, 5.6091e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030216151848435402 tensor([3.6597e-01, 1.2881e-01, 5.1406e-05, 4.7495e-01, 3.0216e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001255267416127026 tensor([2.1085e-01, 1.2553e-03, 4.2348e-08, 7.8369e-01, 4.2067e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0031221758108586073 tensor([0.0031, 0.7931, 0.1023, 0.0026, 0.0988], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.025594603270292282 tensor([0.0256, 0.4342, 0.0177, 0.1184, 0.4041], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0027723140083253384 tensor([0.0417, 0.9498, 0.0012, 0.0028, 0.0046], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014973223209381104 tensor([0.0414, 0.8772, 0.0045, 0.0150, 0.0620], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019596673548221588 tensor([0.0196, 0.0556, 0.0027, 0.3082, 0.6139], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00030704238452017307 tensor([1.7310e-02, 9.8108e-01, 3.0704e-04, 2.8892e-04, 1.0144e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00835254043340683 tensor([2.1098e-04, 4.8074e-02, 2.7518e-01, 8.3525e-03, 6.6818e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.512147178407758e-05 tensor([6.1747e-01, 5.2841e-04, 3.9350e-10, 3.8192e-01, 8.5121e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10801515728235245 tensor([0.2074, 0.5338, 0.0006, 0.1502, 0.1080], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024575302377343178 tensor([4.6040e-01, 8.4965e-02, 1.5126e-05, 4.3004e-01, 2.4575e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9987697896794998e-07 tensor([1.9491e-08, 2.2418e-03, 9.9680e-01, 1.9988e-07, 9.6001e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3576752166954975e-08 tensor([6.4812e-12, 3.6603e-06, 9.9964e-01, 1.3577e-08, 3.5534e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02882974036037922 tensor([5.2142e-01, 1.8162e-01, 3.2387e-05, 2.6810e-01, 2.8830e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005072519648820162 tensor([0.0021, 0.0140, 0.0051, 0.1551, 0.8237], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00983587559312582 tensor([4.4771e-04, 7.3434e-02, 1.5571e-01, 9.8359e-03, 7.6057e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00021852392819710076 tensor([1.4524e-04, 3.7769e-01, 5.9333e-01, 2.1852e-04, 2.8612e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4699982784804888e-05 tensor([7.1071e-08, 6.6018e-04, 9.6877e-01, 1.4700e-05, 3.0556e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01545415073633194 tensor([0.0155, 0.0167, 0.0006, 0.5564, 0.4108], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02035577781498432 tensor([0.0135, 0.2971, 0.0204, 0.0716, 0.5974], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.145508031186182e-05 tensor([9.7877e-01, 2.0592e-03, 1.3124e-10, 1.9157e-02, 1.1455e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.218071911869629e-07 tensor([1.0377e-09, 4.3188e-05, 9.9122e-01, 8.2181e-07, 8.7332e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0030819487292319536 tensor([0.0012, 0.5397, 0.3868, 0.0031, 0.0693], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0042083072476089 tensor([3.6353e-01, 1.3990e-02, 1.0842e-06, 6.1827e-01, 4.2083e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004979476798325777 tensor([2.3921e-06, 4.9795e-04, 1.8737e-01, 4.8267e-03, 8.0731e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007523312233388424 tensor([8.5904e-01, 1.8242e-02, 7.5314e-08, 1.2197e-01, 7.5233e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007093319436535239 tensor([8.8479e-01, 8.2824e-02, 2.6638e-07, 3.1672e-02, 7.0933e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0519012026488781 tensor([1.5788e-01, 6.3914e-02, 1.5631e-04, 7.2615e-01, 5.1901e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016692429780960083 tensor([1.1884e-01, 1.6692e-03, 2.9090e-07, 8.7367e-01, 5.8270e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.942952601704746e-05 tensor([9.0729e-01, 7.7062e-03, 8.9531e-09, 8.4902e-02, 9.9430e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01385087613016367 tensor([2.7076e-01, 1.3851e-02, 2.4161e-06, 6.8351e-01, 3.1873e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.433784622255189e-07 tensor([5.9935e-08, 4.0108e-03, 9.9493e-01, 5.4338e-07, 1.0615e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.270528031862341e-05 tensor([3.4303e-06, 2.5793e-02, 9.5949e-01, 3.2705e-05, 1.4683e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004363935440778732 tensor([3.6457e-02, 4.3639e-03, 1.3079e-05, 8.7258e-01, 8.6586e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.899312600377016e-05 tensor([6.6809e-03, 5.8993e-05, 1.0169e-07, 9.7938e-01, 1.3883e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.896526322932914e-06 tensor([9.8934e-01, 2.4749e-03, 7.6152e-11, 8.1822e-03, 3.8965e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00322888046503067 tensor([0.0016, 0.6252, 0.2936, 0.0032, 0.0765], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007369892206043005 tensor([0.0074, 0.9133, 0.0437, 0.0041, 0.0315], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00017995972302742302 tensor([5.8215e-03, 1.7996e-04, 1.0801e-06, 9.6240e-01, 3.1596e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019478434696793556 tensor([0.0195, 0.1237, 0.0100, 0.2345, 0.6123], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011276956647634506 tensor([1.3674e-01, 1.1277e-02, 5.3276e-06, 8.1318e-01, 3.8797e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003956806904170662 tensor([6.0313e-04, 9.9293e-01, 6.0629e-03, 8.8254e-06, 3.9568e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.07720326179151e-08 tensor([8.3346e-10, 1.5776e-04, 9.9936e-01, 8.0772e-08, 4.8718e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010017179884016514 tensor([6.7078e-02, 1.0017e-02, 1.9200e-05, 8.1673e-01, 1.0615e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.412535079405643e-06 tensor([9.6926e-01, 6.6704e-04, 2.0557e-11, 3.0067e-02, 3.4125e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.029608791694045067 tensor([0.0077, 0.2250, 0.0296, 0.0588, 0.6789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009969596285372972 tensor([2.6708e-01, 7.2922e-01, 1.1971e-05, 2.6910e-03, 9.9696e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02777886390686035 tensor([0.0219, 0.5036, 0.0278, 0.0607, 0.3860], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017600132850930095 tensor([1.5408e-02, 1.7600e-03, 1.0981e-05, 8.7548e-01, 1.0734e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9607805370469578e-05 tensor([7.8968e-08, 1.9608e-05, 7.6950e-02, 1.4682e-03, 9.2156e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010523836826905608 tensor([8.5991e-06, 4.3315e-03, 2.5453e-01, 1.0524e-03, 7.4008e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009353666100651026 tensor([9.3537e-04, 7.2971e-01, 2.0334e-01, 7.2919e-04, 6.5290e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.223911623237655e-05 tensor([2.2876e-05, 1.1120e-01, 8.8044e-01, 6.2239e-05, 8.2764e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0348074845969677 tensor([0.0014, 0.1452, 0.2084, 0.0348, 0.6102], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12452834844589233 tensor([0.1245, 0.2315, 0.0011, 0.4630, 0.1798], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008578716777265072 tensor([2.1454e-04, 8.5787e-03, 1.9710e-02, 1.8658e-02, 9.5284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02667101100087166 tensor([0.0267, 0.8172, 0.0151, 0.0267, 0.1143], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.477092248744157e-07 tensor([2.6530e-08, 1.0417e-03, 9.9751e-01, 9.4771e-07, 1.4453e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013682861812412739 tensor([1.3104e-04, 1.5851e-02, 1.4216e-01, 1.3683e-02, 8.2818e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003958618617616594 tensor([2.0508e-05, 3.8086e-02, 8.5721e-01, 3.9586e-04, 1.0429e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0018788166344165802 tensor([2.5455e-01, 1.8788e-03, 4.8694e-08, 7.3978e-01, 3.7917e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015983199700713158 tensor([0.0889, 0.8778, 0.0012, 0.0161, 0.0160], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002904279390349984 tensor([2.9043e-04, 8.6602e-01, 1.3146e-01, 3.9148e-05, 2.1871e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009701995179057121 tensor([3.5488e-01, 4.3315e-02, 9.3738e-06, 5.9209e-01, 9.7020e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016088413074612617 tensor([3.2104e-02, 1.6088e-02, 1.6965e-04, 5.9216e-01, 3.5948e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.6792590688273776e-06 tensor([9.9035e-01, 1.7630e-03, 3.8451e-11, 7.8805e-03, 2.6793e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4325250958791003e-05 tensor([4.2654e-06, 4.7175e-02, 9.3875e-01, 1.4325e-05, 1.4061e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11724318563938141 tensor([0.1645, 0.4956, 0.0017, 0.2209, 0.1172], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030704418197274208 tensor([0.0011, 0.1304, 0.2436, 0.0307, 0.5943], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09209155291318893 tensor([0.0921, 0.1360, 0.0011, 0.4497, 0.3210], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.041507285088300705 tensor([3.2494e-01, 5.4553e-01, 3.0249e-04, 8.7721e-02, 4.1507e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00013074545131530613 tensor([2.6063e-07, 7.3861e-04, 7.7032e-01, 1.3075e-04, 2.2881e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.061456367246194e-09 tensor([3.1944e-13, 9.9539e-07, 9.9995e-01, 1.0615e-09, 4.8807e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04747143015265465 tensor([5.7871e-02, 4.7471e-02, 5.2436e-04, 7.7181e-01, 1.2233e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023099957033991814 tensor([0.0315, 0.7547, 0.0101, 0.0231, 0.1806], grad_fn=<SoftmaxBackward0>)\n",
      "4 2.721624809964851e-07 tensor([9.9078e-01, 2.4938e-04, 3.3575e-13, 8.9703e-03, 2.7216e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1249006092839409e-05 tensor([2.8523e-06, 4.2639e-02, 9.4209e-01, 1.1249e-05, 1.5260e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005246567889116704 tensor([5.2466e-04, 9.5354e-01, 4.3230e-02, 3.6479e-05, 2.6730e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04988745227456093 tensor([4.7968e-01, 1.4894e-01, 3.7109e-05, 3.2146e-01, 4.9887e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009900184348225594 tensor([6.1299e-01, 2.2691e-01, 1.4558e-05, 1.5018e-01, 9.9002e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01395079679787159 tensor([0.0031, 0.2409, 0.0633, 0.0140, 0.6788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003277503710705787 tensor([2.8435e-04, 5.3493e-01, 4.3887e-01, 3.2775e-04, 2.5594e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027120433747768402 tensor([0.0199, 0.7994, 0.0271, 0.0359, 0.1177], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006438712123781443 tensor([0.0005, 0.1906, 0.3882, 0.0064, 0.4143], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000819466426037252 tensor([4.2225e-06, 3.6696e-03, 4.0894e-01, 8.1947e-04, 5.8656e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.019889023154973984 tensor([0.0019, 0.0480, 0.0199, 0.0447, 0.8855], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02716849558055401 tensor([0.0010, 0.0272, 0.0777, 0.1100, 0.7841], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000856210885103792 tensor([0.0008, 0.7081, 0.2341, 0.0009, 0.0561], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000883676519151777 tensor([3.1314e-01, 1.4567e-03, 2.2696e-08, 6.8452e-01, 8.8368e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012920092791318893 tensor([0.0129, 0.1267, 0.0076, 0.1224, 0.7304], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03102155216038227 tensor([1.1271e-01, 3.1022e-02, 6.8551e-05, 7.0567e-01, 1.5053e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003817890537902713 tensor([9.3394e-04, 9.9268e-01, 5.9827e-03, 1.8571e-05, 3.8179e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004051422234624624 tensor([6.1991e-05, 3.0641e-02, 6.4837e-01, 4.0514e-03, 3.1688e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00027227558894082904 tensor([6.1236e-04, 2.7228e-04, 1.0228e-04, 4.6662e-01, 5.3240e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0701390728354454 tensor([3.6335e-01, 1.2656e-01, 5.3181e-05, 4.3990e-01, 7.0139e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.028412973508238792 tensor([5.9269e-01, 1.0725e-01, 1.2153e-05, 2.7164e-01, 2.8413e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1939852129216888e-06 tensor([1.0340e-07, 4.3418e-03, 9.8943e-01, 1.1940e-06, 6.2263e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.9838552109140437e-06 tensor([1.1921e-06, 2.8794e-02, 9.6674e-01, 2.9839e-06, 4.4644e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015055072726681828 tensor([2.7906e-02, 1.5055e-03, 3.2553e-06, 8.9998e-01, 7.0609e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006214426248334348 tensor([4.4490e-05, 6.3573e-02, 7.2624e-01, 6.2144e-04, 2.0952e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.3980571768333903e-06 tensor([9.6641e-01, 5.0029e-04, 1.1760e-11, 3.3083e-02, 2.3981e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013906965963542461 tensor([2.4186e-01, 7.2074e-01, 1.6728e-04, 2.3319e-02, 1.3907e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005773820448666811 tensor([7.6921e-05, 2.7319e-02, 4.1584e-01, 5.7738e-03, 5.5099e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.3818935258314013e-05 tensor([9.7427e-02, 1.3819e-05, 2.1016e-11, 9.0251e-01, 5.1603e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1164836905663833e-05 tensor([1.4026e-02, 1.1165e-05, 1.0633e-09, 9.8503e-01, 9.3531e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010164384730160236 tensor([2.7377e-02, 1.0164e-03, 1.9813e-06, 9.3810e-01, 3.3505e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.024217188358306885 tensor([0.0100, 0.5132, 0.0262, 0.0242, 0.4263], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.499651261838153e-05 tensor([2.9873e-06, 3.8936e-02, 9.5207e-01, 1.4997e-05, 8.9799e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00018680338689591736 tensor([3.0586e-07, 1.8680e-04, 3.4630e-01, 1.2273e-03, 6.5228e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.3073315939400345e-05 tensor([9.3555e-01, 3.4343e-03, 1.4301e-09, 6.0961e-02, 5.3073e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0036038104444742203 tensor([1.6438e-04, 3.6038e-03, 1.1924e-02, 3.4628e-02, 9.4968e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17585663497447968 tensor([0.1759, 0.3859, 0.0006, 0.2040, 0.2336], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2637326562980888e-07 tensor([5.9201e-09, 1.0031e-03, 9.9840e-01, 1.2637e-07, 5.9967e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.953045936417766e-05 tensor([2.6400e-02, 4.9530e-05, 5.2825e-09, 9.7266e-01, 8.8964e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00015995404100976884 tensor([6.8049e-04, 1.5995e-04, 4.4454e-05, 5.7765e-01, 4.2147e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008135105017572641 tensor([3.0442e-01, 8.1351e-04, 8.8203e-09, 6.9367e-01, 1.0938e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01946846768260002 tensor([0.0012, 0.1646, 0.2560, 0.0195, 0.5587], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02414688840508461 tensor([1.7621e-04, 2.4846e-02, 3.1020e-01, 2.4147e-02, 6.4064e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.758724106883164e-06 tensor([3.4933e-07, 5.0243e-03, 9.7926e-01, 8.7587e-06, 1.5707e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1994476153631695e-05 tensor([9.8297e-01, 9.3775e-03, 1.0310e-09, 7.6433e-03, 1.1994e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.024357294663786888 tensor([0.0498, 0.7586, 0.0037, 0.0244, 0.1635], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.081822357373312e-05 tensor([6.0818e-05, 5.8974e-01, 4.0662e-01, 1.4145e-05, 3.5664e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0017866156995296478 tensor([1.7866e-03, 9.3548e-01, 5.7946e-02, 3.8652e-04, 4.4048e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.20829676638823e-05 tensor([6.2383e-01, 2.5350e-04, 9.1908e-11, 3.7589e-01, 3.2083e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.166840542689897e-05 tensor([9.2548e-01, 1.2676e-03, 1.8934e-10, 7.3218e-02, 3.1668e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.021372923394665e-05 tensor([9.3287e-01, 5.4951e-03, 3.2971e-09, 6.1557e-02, 8.0214e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010595925414236262 tensor([1.0596e-04, 7.3426e-01, 2.6035e-01, 1.5159e-05, 5.2663e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012163221836090088 tensor([3.0444e-04, 2.6633e-01, 6.1092e-01, 1.2163e-03, 1.2122e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00107634998857975 tensor([4.2340e-05, 3.8141e-02, 5.7540e-01, 1.0763e-03, 3.8534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03307819366455078 tensor([0.0010, 0.0331, 0.0403, 0.0429, 0.8828], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07704734057188034 tensor([2.9193e-01, 5.4487e-01, 2.9964e-04, 8.5855e-02, 7.7047e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008137195371091366 tensor([0.0481, 0.9198, 0.0015, 0.0081, 0.0225], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024851448833942413 tensor([0.0173, 0.5139, 0.0249, 0.0512, 0.3927], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003381653514225036 tensor([6.0916e-01, 1.7672e-03, 4.7244e-09, 3.8874e-01, 3.3817e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008878844790160656 tensor([5.0694e-01, 1.3307e-02, 5.0479e-07, 4.7087e-01, 8.8788e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006116305477917194 tensor([8.8626e-03, 6.1163e-03, 2.0087e-04, 2.9728e-01, 6.8754e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.266399772883233e-08 tensor([4.1215e-10, 1.3861e-04, 9.9937e-01, 3.2664e-08, 4.8735e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.995952617216972e-07 tensor([2.1664e-08, 1.1065e-03, 9.9761e-01, 6.9960e-07, 1.2789e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0024829250760376453 tensor([2.3001e-05, 1.1436e-02, 4.7301e-01, 2.4829e-03, 5.1305e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.897583719161048e-07 tensor([2.4031e-08, 1.5047e-03, 9.9594e-01, 5.8976e-07, 2.5522e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03774242848157883 tensor([0.0377, 0.4390, 0.0127, 0.1189, 0.3916], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.163075376098277e-05 tensor([4.6106e-06, 5.3501e-02, 9.3807e-01, 1.1631e-05, 8.4084e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011066172271966934 tensor([5.6694e-01, 7.0306e-02, 6.0536e-06, 3.5168e-01, 1.1066e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005450258613564074 tensor([2.5760e-03, 5.4503e-04, 4.0294e-05, 7.1733e-01, 2.7951e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001058067791745998 tensor([1.2356e-02, 1.0581e-04, 1.1361e-07, 9.6982e-01, 1.7718e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030820177868008614 tensor([0.0311, 0.7701, 0.0074, 0.0308, 0.1605], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03905639424920082 tensor([1.2699e-01, 3.9056e-02, 5.6278e-05, 5.8528e-01, 2.4862e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04530793055891991 tensor([0.0453, 0.4693, 0.0138, 0.1552, 0.3165], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00013514357851818204 tensor([2.3361e-01, 1.3514e-04, 4.1294e-10, 7.6610e-01, 1.4758e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016882336931303144 tensor([0.0017, 0.0025, 0.0012, 0.4869, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1890612691640854 tensor([0.1891, 0.2845, 0.0005, 0.2410, 0.2849], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030602818354964256 tensor([0.0316, 0.8145, 0.0158, 0.0306, 0.1075], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.486050627543591e-05 tensor([1.2148e-06, 8.6999e-03, 9.7352e-01, 3.4861e-05, 1.7749e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0001351843384327367 tensor([9.3018e-01, 2.2739e-02, 2.8622e-08, 4.6949e-02, 1.3518e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.1430306737311184e-05 tensor([9.3888e-01, 1.4926e-03, 2.3953e-10, 5.9587e-02, 4.1430e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.595020517532248e-06 tensor([8.3159e-01, 2.0827e-04, 1.5105e-11, 1.6819e-01, 8.5950e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016634708270430565 tensor([0.0142, 0.2439, 0.0166, 0.0760, 0.6493], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.7440171404814464e-06 tensor([1.3676e-07, 4.6938e-03, 9.9303e-01, 1.7440e-06, 2.2704e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019841031171381474 tensor([0.0165, 0.9759, 0.0020, 0.0013, 0.0043], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022015064023435116 tensor([2.8188e-03, 2.2015e-03, 2.4518e-04, 3.6744e-01, 6.2729e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014344460796564817 tensor([4.6030e-01, 1.4344e-03, 9.8903e-09, 5.3665e-01, 1.6190e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005135274957865477 tensor([5.1353e-03, 9.7671e-01, 9.1389e-03, 6.1654e-04, 8.4015e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00014505405852105469 tensor([1.0455e-06, 1.4505e-04, 7.9508e-02, 4.1519e-03, 9.1619e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001513254246674478 tensor([2.0388e-02, 1.5133e-04, 7.1662e-08, 9.7699e-01, 2.4671e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.020165355876088142 tensor([3.7390e-04, 2.0165e-02, 7.9690e-02, 4.5718e-02, 8.5405e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0044571273028850555 tensor([3.8350e-02, 4.4571e-03, 1.1703e-05, 8.6762e-01, 8.9563e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008420753292739391 tensor([0.0084, 0.0292, 0.0041, 0.3639, 0.5944], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5176692613749765e-07 tensor([5.4552e-10, 6.1763e-05, 9.9787e-01, 1.5177e-07, 2.0661e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01229969784617424 tensor([0.0123, 0.0320, 0.0018, 0.3878, 0.5661], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0033755190670490265 tensor([1.3344e-04, 3.3755e-03, 1.5002e-02, 2.2757e-02, 9.5873e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019428860396146774 tensor([1.3190e-01, 8.2698e-01, 5.9247e-04, 2.1098e-02, 1.9429e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.859026739723049e-05 tensor([1.1257e-01, 8.8726e-01, 2.7205e-06, 1.2398e-04, 4.8590e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.656700742903922e-07 tensor([2.9984e-08, 1.4510e-03, 9.9738e-01, 8.6567e-07, 1.1706e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0045461771078407764 tensor([1.0457e-02, 4.5462e-03, 1.4886e-04, 8.2996e-01, 1.5489e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012090307427570224 tensor([1.8178e-03, 1.2090e-03, 2.8586e-04, 6.2053e-01, 3.7615e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00037089927354827523 tensor([3.7090e-04, 6.2802e-01, 3.4096e-01, 2.5889e-04, 3.0389e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004477071051951498 tensor([2.3218e-04, 4.0082e-01, 5.4108e-01, 4.4771e-04, 5.7422e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00018478523998055607 tensor([3.6515e-06, 9.4123e-03, 7.9276e-01, 1.8479e-04, 1.9764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.2131176794791827e-06 tensor([9.4471e-01, 5.7646e-04, 2.4086e-11, 5.4707e-02, 3.2131e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002099506324157119 tensor([5.0109e-05, 2.0995e-03, 9.9313e-03, 9.7810e-03, 9.7814e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 0], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 1], [0, 3], [3, 0], [0, 3], [2, 4], [2, 0], [0, 3], [0, 3], [2, 4], [0, 2], [2, 1], [2, 1], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 0], [0, 3], [2, 4], [0, 3], [2, 0], [2, 4], [2, 4], [2, 0], [2, 1], [2, 3], [2, 4], [0, 3], [2, 1], [2, 1], [0, 3], [3, 0], [0, 3], [2, 1], [2, 0], [2, 4], [0, 3], [2, 0], [2, 4], [0, 1], [0, 3], [2, 4], [2, 1], [0, 1], [0, 1], [2, 4], [2, 3], [0, 3], [2, 4], [2, 1], [2, 0], [0, 3], [0, 3], [2, 4], [2, 1], [2, 1], [3, 0], [0, 3], [2, 1], [2, 4], [2, 1], [3, 0], [2, 0], [2, 3], [2, 3], [2, 0], [3, 2], [0, 3], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [0, 3], [0, 3], [0, 3], [2, 0], [0, 2], [2, 4], [0, 3], [0, 3], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 1], [2, 4], [2, 1], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [0, 3], [3, 0], [2, 1], [2, 0], [2, 1], [3, 4], [0, 3], [2, 1], [2, 4], [0, 2], [2, 4], [0, 2], [2, 1], [0, 1], [0, 3], [3, 0], [0, 3], [0, 3], [2, 0], [0, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 4], [3, 0], [2, 4], [2, 1], [2, 4], [0, 3], [2, 4], [0, 3], [2, 0], [2, 4], [0, 3], [0, 3], [2, 1], [2, 3], [2, 4], [0, 3], [3, 0], [2, 4], [2, 4], [0, 3], [0, 3], [0, 2], [0, 3], [0, 3], [0, 2], [0, 1], [0, 3], [2, 4], [2, 0], [2, 1], [3, 4], [0, 3], [2, 1], [2, 4], [2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [2, 4], [2, 4], [0, 3], [2, 1], [2, 1], [2, 1], [0, 3], [0, 3], [0, 1], [2, 4], [0, 1], [2, 0], [0, 3], [2, 1], [2, 1], [2, 1], [0, 3], [0, 3], [0, 3], [2, 4], [2, 3], [3, 0], [3, 0], [2, 4], [2, 4], [2, 4], [3, 0], [0, 3], [0, 3], [0, 1], [2, 4], [2, 3], [0, 2], [2, 4], [2, 4], [2, 1], [0, 3], [0, 3], [0, 3], [0, 3], [2, 0], [0, 3], [2, 4], [2, 1], [2, 1], [2, 3], [2, 1], [2, 0], [2, 1], [2, 0], [2, 0], [2, 3], [0, 3], [2, 4], [2, 4], [2, 4], [0, 2], [0, 3], [3, 2], [2, 1], [2, 1], [3, 0], [0, 1], [2, 1], [0, 1], [2, 1], [2, 0], [0, 3], [2, 0], [0, 1], [2, 4], [2, 4], [0, 3], [2, 1], [2, 1], [3, 0], [0, 3], [0, 3], [2, 4], [0, 1]]\n",
      "[[1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 4], [0, 1, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 3, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 4], [0, 3, 4], [1, 3, 4], [0, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 1, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 0], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 1], [0, 3], [3, 0], [0, 3], [2, 4], [2, 0], [0, 3], [0, 3], [2, 4], [0, 2], [2, 1], [2, 1], [2, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 0], [0, 3], [2, 4], [0, 3], [2, 0], [2, 4], [2, 4], [2, 0], [2, 1], [2, 3], [2, 4], [0, 3], [2, 1], [2, 1], [0, 3], [3, 0], [0, 3], [2, 1], [2, 0], [2, 4], [0, 3], [2, 0], [2, 4], [0, 1], [0, 3], [2, 4], [2, 1], [0, 1], [0, 1], [2, 4], [2, 3], [0, 3], [2, 4], [2, 1], [2, 0], [0, 3], [0, 3], [2, 4], [2, 1], [2, 1], [3, 0], [0, 3], [2, 1], [2, 4], [2, 1], [3, 0], [2, 0], [2, 3], [2, 3], [2, 0], [3, 2], [0, 3], [2, 4], [2, 4], [2, 4], [0, 3], [0, 3], [2, 4], [0, 2], [0, 3], [0, 3], [0, 3], [2, 0], [0, 2], [2, 4], [0, 3], [0, 3], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 1], [2, 4], [2, 1], [0, 3], [0, 3], [2, 1], [2, 1], [2, 4], [0, 3], [3, 0], [2, 1], [2, 0], [2, 1], [3, 4], [0, 3], [2, 1], [2, 4], [0, 2], [2, 4], [0, 2], [2, 1], [0, 1], [0, 3], [3, 0], [0, 3], [0, 3], [2, 0], [0, 1], [2, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 4], [3, 0], [2, 4], [2, 1], [2, 4], [0, 3], [2, 4], [0, 3], [2, 0], [2, 4], [0, 3], [0, 3], [2, 1], [2, 3], [2, 4], [0, 3], [3, 0], [2, 4], [2, 4], [0, 3], [0, 3], [0, 2], [0, 3], [0, 3], [0, 2], [0, 1], [0, 3], [2, 4], [2, 0], [2, 1], [3, 4], [0, 3], [2, 1], [2, 4], [2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [2, 4], [2, 4], [0, 3], [2, 1], [2, 1], [2, 1], [0, 3], [0, 3], [0, 1], [2, 4], [0, 1], [2, 0], [0, 3], [2, 1], [2, 1], [2, 1], [0, 3], [0, 3], [0, 3], [2, 4], [2, 3], [3, 0], [3, 0], [2, 4], [2, 4], [2, 4], [3, 0], [0, 3], [0, 3], [0, 1], [2, 4], [2, 3], [0, 2], [2, 4], [2, 4], [2, 1], [0, 3], [0, 3], [0, 3], [0, 3], [2, 0], [0, 3], [2, 4], [2, 1], [2, 1], [2, 3], [2, 1], [2, 0], [2, 1], [2, 0], [2, 0], [2, 3], [0, 3], [2, 4], [2, 4], [2, 4], [0, 2], [0, 3], [3, 2], [2, 1], [2, 1], [3, 0], [0, 1], [2, 1], [0, 1], [2, 1], [2, 0], [0, 3], [2, 0], [0, 1], [2, 4], [2, 4], [0, 3], [2, 1], [2, 1], [3, 0], [0, 3], [0, 3], [2, 4], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.00449569845199585  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004495694160461426  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004495686531066895  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0044956750869750975  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0044956631660461426  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004495645999908447  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004495627880096435  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004495607376098633  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0044955859184265135  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004495562076568604  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004495536804199219  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004495511054992676  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.0044954829216003415  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004495454788208008  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004495425701141358  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004495396137237549  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004495366096496582  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.0044953346252441405  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0044953031539917  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004495272636413574  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.03413445129990578 tensor([0.0194, 0.4927, 0.0153, 0.0341, 0.4385], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.032256134043564e-06 tensor([1.5842e-12, 2.0323e-06, 9.9978e-01, 4.0799e-09, 2.1993e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14707458019256592 tensor([0.0007, 0.4098, 0.4398, 0.0026, 0.1471], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.751998206600547e-05 tensor([2.8230e-02, 2.0070e-06, 5.8812e-12, 9.7175e-01, 1.7520e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004473426844924688 tensor([1.4243e-01, 1.2546e-03, 9.2577e-08, 8.5184e-01, 4.4734e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02716875448822975 tensor([9.3052e-01, 4.1687e-02, 6.9388e-08, 2.7169e-02, 6.2007e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012880525318905711 tensor([9.7216e-01, 1.2881e-03, 8.0516e-11, 2.6552e-02, 3.7865e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11076926440000534 tensor([0.0063, 0.5023, 0.1108, 0.0288, 0.3518], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0681953951716423 tensor([4.7189e-01, 6.8195e-02, 9.3173e-06, 4.2711e-01, 3.2800e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017948567867279053 tensor([7.9611e-02, 2.4412e-03, 1.4509e-06, 9.0000e-01, 1.7949e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001471971976570785 tensor([7.4649e-07, 1.4720e-03, 7.3448e-01, 1.8234e-04, 2.6386e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010679805651307106 tensor([2.4350e-04, 7.4837e-01, 2.4065e-01, 6.3316e-05, 1.0680e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.098363155615516e-05 tensor([1.4847e-10, 6.0984e-05, 9.9969e-01, 2.3347e-08, 2.4948e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010238358518108726 tensor([6.5252e-01, 1.0238e-03, 1.4391e-09, 3.4628e-01, 1.8066e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20692504942417145 tensor([0.0538, 0.2069, 0.0022, 0.2195, 0.5176], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06409341841936111 tensor([3.4295e-04, 6.4093e-02, 1.6386e-01, 7.8282e-03, 7.6387e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017852945253252983 tensor([6.5317e-05, 2.1535e-01, 7.6656e-01, 1.7386e-04, 1.7853e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1364678591489792 tensor([4.4387e-01, 4.0929e-01, 8.9087e-05, 1.3647e-01, 1.0286e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009529167786240578 tensor([0.0015, 0.0095, 0.0030, 0.0888, 0.8971], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007158077321946621 tensor([7.1581e-03, 6.4371e-03, 3.8392e-04, 4.9620e-01, 4.8982e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005190822761505842 tensor([3.0725e-01, 1.7427e-03, 3.5332e-08, 6.8581e-01, 5.1908e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014597676694393158 tensor([8.5901e-01, 1.4598e-02, 3.8805e-08, 1.2597e-01, 4.1407e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.021156983450055122 tensor([1.6681e-05, 5.6607e-02, 9.2206e-01, 1.6103e-04, 2.1157e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001006946200504899 tensor([7.1612e-01, 2.8286e-01, 1.1681e-07, 1.0069e-03, 1.8137e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05252279341220856 tensor([0.0095, 0.5752, 0.0525, 0.0298, 0.3330], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22753430902957916 tensor([3.1586e-01, 2.2753e-01, 1.6775e-04, 3.6232e-01, 9.4117e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03535807877779007 tensor([5.4292e-04, 6.3845e-01, 3.2528e-01, 3.6906e-04, 3.5358e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016891224309802055 tensor([3.2768e-06, 2.5927e-02, 9.5715e-01, 2.8323e-05, 1.6891e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12330208718776703 tensor([4.7159e-01, 1.2330e-01, 2.2605e-05, 3.5063e-01, 5.4447e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015014738775789738 tensor([1.6501e-04, 1.5015e-02, 7.5231e-02, 1.3156e-02, 8.9643e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10187669843435287 tensor([0.0135, 0.1019, 0.0048, 0.1279, 0.7519], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20791588723659515 tensor([2.1646e-01, 2.0792e-01, 4.6900e-04, 5.1350e-01, 6.1658e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017412297427654266 tensor([0.0455, 0.9158, 0.0065, 0.0174, 0.0148], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13864079117774963 tensor([0.0278, 0.1386, 0.0080, 0.3735, 0.4521], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07922744005918503 tensor([7.9227e-02, 6.5055e-02, 2.6857e-04, 5.9345e-01, 2.6200e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07422143965959549 tensor([0.0742, 0.7795, 0.0025, 0.0326, 0.1111], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000970447261352092 tensor([9.6379e-01, 3.5232e-02, 1.1270e-09, 9.7045e-04, 3.7722e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003005106933414936 tensor([2.6773e-06, 3.0051e-03, 7.5603e-01, 7.3564e-04, 2.4023e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0024309444706887007 tensor([2.4309e-03, 1.9619e-03, 3.8781e-04, 6.4382e-01, 3.5140e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027009351179003716 tensor([2.0815e-01, 1.7838e-02, 5.0820e-06, 7.4699e-01, 2.7009e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015841512009501457 tensor([8.0143e-05, 1.5842e-02, 1.5708e-01, 4.3602e-03, 8.2264e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014811719302088022 tensor([2.3002e-06, 1.0609e-01, 8.9242e-01, 1.5576e-06, 1.4812e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.2790216462453827e-05 tensor([5.2868e-11, 2.2790e-05, 9.9976e-01, 2.0775e-08, 2.1933e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.8331732007936807e-06 tensor([1.6051e-01, 1.8274e-06, 1.0158e-13, 8.3948e-01, 1.8332e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.076625756919384 tensor([0.0373, 0.4713, 0.0090, 0.0766, 0.4059], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01536485180258751 tensor([4.3500e-01, 1.5365e-02, 7.8176e-07, 5.4016e-01, 9.4791e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005660427734255791 tensor([5.5410e-07, 5.6604e-03, 9.7800e-01, 1.7734e-05, 1.6322e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12125042825937271 tensor([0.0463, 0.5439, 0.0134, 0.1213, 0.2752], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003270450746640563 tensor([9.6685e-01, 3.2705e-03, 4.7869e-10, 2.9863e-02, 1.6726e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012264070101082325 tensor([1.0934e-06, 7.6641e-04, 4.1392e-01, 1.2264e-03, 5.8408e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014655089937150478 tensor([5.0955e-04, 2.1792e-02, 1.4655e-02, 1.2409e-02, 9.5063e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18310876190662384 tensor([3.4860e-01, 1.8311e-01, 9.7358e-05, 4.3456e-01, 3.3633e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006611417513340712 tensor([2.6079e-01, 3.4464e-03, 2.0895e-07, 7.2915e-01, 6.6114e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006278578657656908 tensor([3.7321e-04, 4.0460e-03, 6.2786e-03, 5.3434e-02, 9.3587e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004922461695969105 tensor([6.1888e-05, 9.1038e-04, 4.9225e-03, 3.1089e-02, 9.6302e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009171465411782265 tensor([3.5309e-01, 6.3511e-01, 2.0110e-05, 9.1715e-03, 2.6116e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.035710085183382034 tensor([0.0477, 0.8949, 0.0033, 0.0184, 0.0357], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12330424785614014 tensor([2.5299e-04, 2.4223e-01, 6.3310e-01, 1.1142e-03, 1.2330e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007813266129232943 tensor([4.9763e-01, 7.8133e-04, 2.3702e-09, 5.0137e-01, 2.1837e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015579501166939735 tensor([1.5580e-02, 3.6811e-03, 3.7350e-05, 8.0249e-01, 1.7821e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002669326728209853 tensor([2.2071e-03, 2.6693e-03, 5.3162e-04, 3.8789e-01, 6.0670e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004775100969709456 tensor([1.2428e-08, 4.7751e-04, 9.9481e-01, 1.0030e-06, 4.7114e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006853769300505519 tensor([9.4981e-08, 1.0038e-02, 9.8928e-01, 3.2550e-07, 6.8538e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07407374680042267 tensor([7.8360e-01, 7.4074e-02, 1.1111e-06, 1.3864e-01, 3.6813e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027132518589496613 tensor([2.7133e-02, 1.0614e-02, 1.0335e-04, 6.0799e-01, 3.5416e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004160360898822546 tensor([2.7925e-01, 1.8185e-03, 4.5145e-08, 7.1477e-01, 4.1604e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0038744881749153137 tensor([8.6858e-06, 2.0068e-01, 7.9543e-01, 4.6631e-06, 3.8745e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1631435602903366 tensor([0.0013, 0.1631, 0.3039, 0.0246, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05683541297912598 tensor([6.9192e-02, 3.7427e-03, 3.2422e-06, 8.7023e-01, 5.6835e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1338479071855545 tensor([3.6535e-01, 1.3385e-01, 5.4052e-05, 4.7042e-01, 3.0327e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004243406467139721 tensor([2.1177e-01, 1.2980e-03, 4.4140e-08, 7.8268e-01, 4.2434e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09601268917322159 tensor([0.0030, 0.7945, 0.1039, 0.0025, 0.0960], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11502765119075775 tensor([0.0250, 0.4424, 0.0184, 0.1150, 0.3992], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004443457815796137 tensor([0.0404, 0.9513, 0.0012, 0.0027, 0.0044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03991724178195 tensor([0.0399, 0.8810, 0.0046, 0.0143, 0.0601], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05776779726147652 tensor([0.0196, 0.0578, 0.0028, 0.3051, 0.6147], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009910473600029945 tensor([1.6674e-02, 9.8174e-01, 3.1330e-04, 2.7863e-04, 9.9105e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0491466298699379 tensor([2.0754e-04, 4.9147e-02, 2.8435e-01, 8.1337e-03, 6.5816e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005437076324597001 tensor([6.1867e-01, 5.4371e-04, 4.0785e-10, 3.8070e-01, 8.5522e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1457567662000656 tensor([0.2039, 0.5440, 0.0006, 0.1458, 0.1057], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08817680180072784 tensor([4.6111e-01, 8.8177e-02, 1.5777e-05, 4.2613e-01, 2.4559e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009392524953000247 tensor([1.8990e-08, 2.2357e-03, 9.9682e-01, 1.9472e-07, 9.3925e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.672808361443458e-06 tensor([6.3623e-12, 3.6728e-06, 9.9965e-01, 1.3263e-08, 3.4808e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1878501921892166 tensor([5.1893e-01, 1.8785e-01, 3.3812e-05, 2.6442e-01, 2.8761e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014553522691130638 tensor([0.0021, 0.0146, 0.0053, 0.1536, 0.8245], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07516074925661087 tensor([4.4018e-04, 7.5161e-02, 1.6176e-01, 9.6068e-03, 7.5303e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02776985801756382 tensor([1.3933e-04, 3.7434e-01, 5.9754e-01, 2.0953e-04, 2.7770e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006567229283973575 tensor([6.8354e-08, 6.5672e-04, 9.6999e-01, 1.3989e-05, 2.9339e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.017433365806937218 tensor([0.0154, 0.0174, 0.0006, 0.5522, 0.4143], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06996358931064606 tensor([0.0133, 0.3043, 0.0211, 0.0700, 0.5913], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002113224007189274 tensor([9.7887e-01, 2.1132e-03, 1.3522e-10, 1.9005e-02, 1.1441e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.304643516661599e-05 tensor([1.0017e-09, 4.3046e-05, 9.9155e-01, 7.8558e-07, 8.4097e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06694374978542328 tensor([0.0011, 0.5395, 0.3895, 0.0030, 0.0669], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01464685145765543 tensor([3.6544e-01, 1.4647e-02, 1.1470e-06, 6.1566e-01, 4.2467e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004715881310403347 tensor([2.3585e-06, 5.1059e-04, 1.9469e-01, 4.7159e-03, 8.0008e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01898532174527645 tensor([8.5907e-01, 1.8985e-02, 7.9367e-08, 1.2118e-01, 7.5684e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.031435053795576096 tensor([8.8173e-01, 8.6117e-02, 2.8170e-07, 3.1435e-02, 7.1493e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06663739681243896 tensor([1.5866e-01, 6.6637e-02, 1.6421e-04, 7.2233e-01, 5.2211e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005885135382413864 tensor([1.1976e-01, 1.7479e-03, 3.0737e-07, 8.7261e-01, 5.8851e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00803948100656271 tensor([9.0715e-01, 8.0395e-03, 9.5256e-09, 8.4709e-02, 1.0091e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.032181378453969955 tensor([2.7193e-01, 1.4467e-02, 2.5521e-06, 6.8142e-01, 3.2181e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001035907189361751 tensor([5.8021e-08, 3.9793e-03, 9.9498e-01, 5.2670e-07, 1.0359e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014140118844807148 tensor([3.2815e-06, 2.5487e-02, 9.6034e-01, 3.1146e-05, 1.4140e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03662804886698723 tensor([3.6628e-02, 4.5707e-03, 1.3902e-05, 8.7100e-01, 8.7788e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006696164608001709 tensor([6.6962e-03, 6.1349e-05, 1.0751e-07, 9.7914e-01, 1.4098e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0025481870397925377 tensor([9.8933e-01, 2.5482e-03, 7.8771e-11, 8.1185e-03, 3.8969e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07416664063930511 tensor([0.0015, 0.6234, 0.2979, 0.0031, 0.0742], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03071708045899868 tensor([0.0071, 0.9137, 0.0446, 0.0039, 0.0307], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005847134627401829 tensor([5.8471e-03, 1.8673e-04, 1.1329e-06, 9.6202e-01, 3.1943e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1274639070034027 tensor([0.0193, 0.1275, 0.0104, 0.2308, 0.6120], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.039173342287540436 tensor([1.3758e-01, 1.1797e-02, 5.6298e-06, 8.1145e-01, 3.9173e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005844158586114645 tensor([5.8442e-04, 9.9285e-01, 6.1656e-03, 8.5840e-06, 3.8929e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00015815033111721277 tensor([8.1824e-10, 1.5815e-04, 9.9936e-01, 7.9088e-08, 4.7845e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06749043613672256 tensor([6.7490e-02, 1.0499e-02, 2.0360e-05, 8.1463e-01, 1.0736e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006846493342891335 tensor([9.6937e-01, 6.8465e-04, 2.1294e-11, 2.9940e-02, 3.4296e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05753477290272713 tensor([0.0075, 0.2304, 0.0307, 0.0575, 0.6738], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002609813120216131 tensor([2.5938e-01, 7.3702e-01, 1.2314e-05, 2.6098e-03, 9.8118e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.058923326432704926 tensor([0.0214, 0.5113, 0.0285, 0.0589, 0.3798], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015508201904594898 tensor([1.5508e-02, 1.8468e-03, 1.1671e-05, 8.7391e-01, 1.0873e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014442516257986426 tensor([7.8211e-08, 2.0169e-05, 8.0315e-02, 1.4443e-03, 9.1822e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004430707078427076 tensor([8.4691e-06, 4.4307e-03, 2.6355e-01, 1.0278e-03, 7.3098e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06342918425798416 tensor([8.9910e-04, 7.2852e-01, 2.0645e-01, 6.9955e-04, 6.3429e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008036859333515167 tensor([2.1995e-05, 1.1010e-01, 8.8179e-01, 5.9713e-05, 8.0369e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14757783710956573 tensor([0.0014, 0.1476, 0.2155, 0.0337, 0.6018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1809987723827362 tensor([0.1228, 0.2391, 0.0012, 0.4559, 0.1810], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018404068425297737 tensor([2.1414e-04, 8.9026e-03, 2.0618e-02, 1.8404e-02, 9.5186e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025639090687036514 tensor([0.0257, 0.8217, 0.0154, 0.0256, 0.1116], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010403438936918974 tensor([2.5879e-08, 1.0403e-03, 9.9755e-01, 9.1954e-07, 1.4107e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01630938984453678 tensor([1.2984e-04, 1.6309e-02, 1.4785e-01, 1.3422e-02, 8.2228e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0377085916697979 tensor([1.9552e-05, 3.7709e-02, 8.6178e-01, 3.7525e-04, 1.0011e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00381825165823102 tensor([2.5584e-01, 1.9496e-03, 5.0872e-08, 7.3839e-01, 3.8183e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015434881672263145 tensor([0.0858, 0.8819, 0.0012, 0.0154, 0.0156], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002152198227122426 tensor([2.8074e-04, 8.6399e-01, 1.3354e-01, 3.8005e-05, 2.1522e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04522175341844559 tensor([3.5570e-01, 4.5222e-02, 9.9172e-06, 5.8927e-01, 9.7979e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03216797113418579 tensor([3.2168e-02, 1.6792e-02, 1.7929e-04, 5.8844e-01, 3.6242e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001809083390980959 tensor([9.9035e-01, 1.8091e-03, 3.9697e-11, 7.8404e-03, 2.6871e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013599689118564129 tensor([4.0964e-06, 4.6671e-02, 9.3971e-01, 1.3730e-05, 1.3600e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16247621178627014 tensor([0.1625, 0.5061, 0.0017, 0.2149, 0.1147], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13273945450782776 tensor([0.0010, 0.1327, 0.2517, 0.0297, 0.5849], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14062215387821198 tensor([0.0910, 0.1406, 0.0012, 0.4436, 0.3236], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08526191860437393 tensor([3.1878e-01, 5.5489e-01, 3.1001e-04, 8.5262e-02, 4.0757e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007347462815232575 tensor([2.5003e-07, 7.3475e-04, 7.7806e-01, 1.2484e-04, 2.2108e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.0004305295296945e-06 tensor([3.1577e-13, 1.0004e-06, 9.9995e-01, 1.0464e-09, 4.8177e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.058006737381219864 tensor([5.8007e-02, 4.9308e-02, 5.5043e-04, 7.6887e-01, 1.2326e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0305453110486269 tensor([0.0305, 0.7610, 0.0104, 0.0223, 0.1759], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002559088170528412 tensor([9.9081e-01, 2.5591e-04, 3.4662e-13, 8.9323e-03, 2.7308e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01469202060252428 tensor([2.7128e-06, 4.1950e-02, 9.4334e-01, 1.0696e-05, 1.4692e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0026128250174224377 tensor([5.0850e-04, 9.5311e-01, 4.3730e-02, 3.5301e-05, 2.6128e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15413880348205566 tensor([4.7846e-01, 1.5414e-01, 3.8694e-05, 3.1758e-01, 4.9788e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1479920893907547 tensor([6.0761e-01, 2.3448e-01, 1.5264e-05, 1.4799e-01, 9.8994e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0655936747789383 tensor([0.0030, 0.2466, 0.0656, 0.0136, 0.6711], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02490648441016674 tensor([2.7114e-04, 5.2965e-01, 4.4486e-01, 3.1357e-04, 2.4906e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03445174917578697 tensor([0.0192, 0.8041, 0.0276, 0.0345, 0.1146], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19224956631660461 tensor([0.0005, 0.1922, 0.3969, 0.0062, 0.4042], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003732300829142332 tensor([4.1397e-06, 3.7323e-03, 4.2005e-01, 7.9569e-04, 5.7542e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.043963056057691574 tensor([0.0019, 0.0495, 0.0208, 0.0440, 0.8839], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08103304356336594 tensor([0.0010, 0.0280, 0.0810, 0.1080, 0.7820], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.054533496499061584 tensor([0.0008, 0.7063, 0.2376, 0.0008, 0.0545], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015167902456596494 tensor([3.1433e-01, 1.5168e-03, 2.3918e-08, 6.8326e-01, 8.9326e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12009809911251068 tensor([0.0128, 0.1305, 0.0080, 0.1201, 0.7286], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1131075844168663 tensor([1.1311e-01, 3.2324e-02, 7.2171e-05, 7.0283e-01, 1.5166e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009039246360771358 tensor([9.0392e-04, 9.9262e-01, 6.0819e-03, 1.8003e-05, 3.7487e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030797038227319717 tensor([6.0183e-05, 3.0797e-02, 6.5731e-01, 3.8976e-03, 3.0794e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006106258952058852 tensor([6.1063e-04, 2.8298e-04, 1.0802e-04, 4.6267e-01, 5.3633e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13142959773540497 tensor([3.6334e-01, 1.3143e-01, 5.5597e-05, 4.3509e-01, 7.0090e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11096074432134628 tensor([5.9171e-01, 1.1096e-01, 1.2669e-05, 2.6893e-01, 2.8387e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0043223039247095585 tensor([1.0034e-07, 4.3223e-03, 9.8963e-01, 1.1542e-06, 6.0478e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004353334195911884 tensor([1.1580e-06, 2.8648e-02, 9.6699e-01, 2.8968e-06, 4.3533e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02806209959089756 tensor([2.8062e-02, 1.5761e-03, 3.4531e-06, 8.9881e-01, 7.1552e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06357353180646896 tensor([4.2947e-05, 6.3574e-02, 7.3329e-01, 5.9490e-04, 2.0250e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005127428448759019 tensor([9.6660e-01, 5.1274e-04, 1.2114e-11, 3.2889e-02, 2.4019e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022517003118991852 tensor([2.3529e-01, 7.2842e-01, 1.7092e-04, 2.2517e-02, 1.3595e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02765820175409317 tensor([7.4977e-05, 2.7658e-02, 4.2692e-01, 5.5875e-03, 5.3975e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.188567592995241e-05 tensor([9.7961e-02, 1.4261e-05, 2.1802e-11, 9.0197e-01, 5.1886e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009446885669603944 tensor([1.4044e-02, 1.1485e-05, 1.1073e-09, 9.8500e-01, 9.4469e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02759670466184616 tensor([2.7597e-02, 1.0662e-03, 2.1017e-06, 9.3741e-01, 3.3921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027070757001638412 tensor([0.0097, 0.5207, 0.0271, 0.0234, 0.4190], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008653217926621437 tensor([2.8510e-06, 3.8401e-02, 9.5293e-01, 1.4281e-05, 8.6532e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011961192358285189 tensor([3.0113e-07, 1.9056e-04, 3.5647e-01, 1.1961e-03, 6.4214e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0035650781355798244 tensor([9.3587e-01, 3.5651e-03, 1.4985e-09, 6.0514e-02, 5.3178e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012474373914301395 tensor([1.6344e-04, 3.7259e-03, 1.2474e-02, 3.4147e-02, 9.4949e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1990499347448349 tensor([0.1719, 0.3954, 0.0007, 0.1990, 0.2330], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005873676855117083 tensor([5.7899e-09, 1.0031e-03, 9.9841e-01, 1.2337e-07, 5.8737e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008982034050859511 tensor([2.6576e-02, 5.1335e-05, 5.5182e-09, 9.7247e-01, 8.9820e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006825230666436255 tensor([6.8252e-04, 1.6669e-04, 4.6823e-05, 5.7448e-01, 4.2463e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010980158112943172 tensor([3.0645e-01, 8.4166e-04, 9.1480e-09, 6.9161e-01, 1.0980e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16695839166641235 tensor([0.0012, 0.1670, 0.2637, 0.0189, 0.5493], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02531464770436287 tensor([1.7227e-04, 2.5315e-02, 3.2018e-01, 2.3377e-02, 6.3096e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0049848491325974464 tensor([3.3460e-07, 4.9848e-03, 9.7997e-01, 8.3100e-06, 1.5040e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00760849891230464 tensor([9.8264e-01, 9.7430e-03, 1.0853e-09, 7.6085e-03, 1.2095e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0482051856815815 tensor([0.0482, 0.7650, 0.0038, 0.0234, 0.1595], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003490298520773649 tensor([5.8880e-05, 5.8682e-01, 4.0961e-01, 1.3720e-05, 3.4903e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0043190643191337585 tensor([1.7283e-03, 9.3479e-01, 5.8790e-02, 3.7376e-04, 4.3191e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002615497214719653 tensor([6.2519e-01, 2.6155e-04, 9.5671e-11, 3.7452e-01, 3.2288e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0013054355513304472 tensor([9.2584e-01, 1.3054e-03, 1.9673e-10, 7.2825e-02, 3.1806e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00570934871211648 tensor([9.3313e-01, 5.7093e-03, 3.4571e-09, 6.1085e-02, 8.0391e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005161278881132603 tensor([1.0267e-04, 7.3169e-01, 2.6303e-01, 1.4720e-05, 5.1613e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1169765517115593 tensor([2.9226e-04, 2.6539e-01, 6.1618e-01, 1.1599e-03, 1.1698e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03841481730341911 tensor([4.1121e-05, 3.8415e-02, 5.8562e-01, 1.0366e-03, 3.7488e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.042016081511974335 tensor([0.0010, 0.0340, 0.0420, 0.0421, 0.8809], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08320026844739914 tensor([2.8701e-01, 5.5435e-01, 3.0490e-04, 8.3200e-02, 7.5131e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02185111492872238 tensor([0.0463, 0.9225, 0.0015, 0.0078, 0.0219], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.049695611000061035 tensor([0.0169, 0.5224, 0.0255, 0.0497, 0.3855], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0018290417501702905 tensor([6.0996e-01, 1.8290e-03, 4.9472e-09, 3.8787e-01, 3.4129e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013829680159687996 tensor([5.0845e-01, 1.3830e-02, 5.2885e-07, 4.6880e-01, 8.9245e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008877124637365341 tensor([8.8771e-03, 6.3507e-03, 2.1011e-04, 2.9500e-01, 6.8956e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00013894090079702437 tensor([4.0458e-10, 1.3894e-04, 9.9938e-01, 3.1992e-08, 4.7854e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010979089420288801 tensor([2.0914e-08, 1.0979e-03, 9.9766e-01, 6.7385e-07, 1.2433e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011567654088139534 tensor([2.2389e-05, 1.1568e-02, 4.8475e-01, 2.3966e-03, 5.0126e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001495371456257999 tensor([2.3252e-08, 1.4954e-03, 9.9602e-01, 5.6941e-07, 2.4797e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1154930368065834 tensor([0.0369, 0.4474, 0.0132, 0.1155, 0.3871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008172966539859772 tensor([4.4579e-06, 5.3102e-02, 9.3871e-01, 1.1225e-05, 8.1730e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07299357652664185 tensor([5.6694e-01, 7.2994e-02, 6.3453e-06, 3.4896e-01, 1.1100e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002577483421191573 tensor([2.5775e-03, 5.6857e-04, 4.2752e-05, 7.1399e-01, 2.8282e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012416994199156761 tensor([1.2417e-02, 1.0971e-04, 1.1888e-07, 9.6958e-01, 1.7895e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03012717515230179 tensor([0.0301, 0.7765, 0.0076, 0.0296, 0.1563], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1272623986005783 tensor([1.2726e-01, 4.0635e-02, 5.9128e-05, 5.8206e-01, 2.4999e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1508881151676178 tensor([0.0444, 0.4783, 0.0142, 0.1509, 0.3122], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00014875322813168168 tensor([2.3490e-01, 1.3977e-04, 4.3017e-10, 7.6482e-01, 1.4875e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0026419449131935835 tensor([0.0017, 0.0026, 0.0013, 0.4834, 0.5109], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2361217588186264 tensor([0.1855, 0.2925, 0.0005, 0.2361, 0.2854], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03053748607635498 tensor([0.0305, 0.8192, 0.0162, 0.0294, 0.1047], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008609252981841564 tensor([1.1598e-06, 8.6093e-03, 9.7435e-01, 3.3021e-05, 1.7003e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02366362325847149 tensor([9.2956e-01, 2.3664e-02, 3.0197e-08, 4.6639e-02, 1.3622e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015369078610092402 tensor([9.3911e-01, 1.5369e-03, 2.4904e-10, 5.9307e-02, 4.1653e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021343323169276118 tensor([8.3310e-01, 2.1343e-04, 1.5448e-11, 1.6668e-01, 8.5408e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07461102306842804 tensor([0.0140, 0.2506, 0.0172, 0.0746, 0.6435], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0022043411154299974 tensor([1.3157e-07, 4.6438e-03, 9.9315e-01, 1.6774e-06, 2.2043e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004178183153271675 tensor([0.0159, 0.9766, 0.0020, 0.0013, 0.0042], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002819770947098732 tensor([2.8198e-03, 2.2897e-03, 2.5781e-04, 3.6448e-01, 6.3016e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0016284368466585875 tensor([4.6216e-01, 1.4815e-03, 1.0276e-08, 5.3473e-01, 1.6284e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008201329968869686 tensor([4.9369e-03, 9.7693e-01, 9.3341e-03, 5.9319e-04, 8.2013e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004088180605322123 tensor([1.0408e-06, 1.4990e-04, 8.2936e-02, 4.0882e-03, 9.1283e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0024962860625237226 tensor([2.0491e-02, 1.5731e-04, 7.5313e-08, 9.7686e-01, 2.4963e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04472802206873894 tensor([3.6824e-04, 2.0720e-02, 8.3236e-02, 4.4728e-02, 8.5095e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03847791627049446 tensor([3.8478e-02, 4.6578e-03, 1.2422e-05, 8.6607e-01, 9.0781e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03029758296906948 tensor([0.0084, 0.0303, 0.0043, 0.3604, 0.5967], grad_fn=<SoftmaxBackward0>)\n",
      "1 6.172027497086674e-05 tensor([5.3191e-10, 6.1720e-05, 9.9792e-01, 1.4755e-07, 2.0172e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.033277858048677444 tensor([0.0122, 0.0333, 0.0019, 0.3834, 0.5692], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01568170078098774 tensor([1.3288e-04, 3.4915e-03, 1.5682e-02, 2.2454e-02, 9.5824e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02023591846227646 tensor([1.2784e-01, 8.3248e-01, 6.0100e-04, 2.0236e-02, 1.8849e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001203558495035395 tensor([1.0888e-01, 8.9095e-01, 2.7870e-06, 1.2036e-04, 4.7785e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011397394118830562 tensor([2.9038e-08, 1.4424e-03, 9.9742e-01, 8.3580e-07, 1.1397e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010516177862882614 tensor([1.0516e-02, 4.7633e-03, 1.5798e-04, 8.2770e-01, 1.5686e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001815551775507629 tensor([1.8156e-03, 1.2592e-03, 3.0287e-04, 6.1652e-01, 3.8011e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.029548266902565956 tensor([3.5809e-04, 6.2609e-01, 3.4375e-01, 2.4928e-04, 2.9548e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05542099103331566 tensor([2.2237e-04, 3.9829e-01, 5.4564e-01, 4.2710e-04, 5.5421e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00939544290304184 tensor([3.5176e-06, 9.3954e-03, 7.9965e-01, 1.7660e-04, 1.9077e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005926874582655728 tensor([9.4489e-01, 5.9269e-04, 2.5026e-11, 5.4517e-02, 3.2351e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009648594073951244 tensor([4.9696e-05, 2.1655e-03, 1.0411e-02, 9.6486e-03, 9.7773e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 0, 3], [0, 3, 1], [0, 3, 4], [2, 1, 4], [2, 1, 4], [2, 4, 3], [2, 4, 1], [0, 3, 2], [2, 4, 1], [2, 1, 4], [0, 3, 1], [3, 0, 4], [0, 3, 1], [2, 4, 1], [2, 0], [0, 3, 1], [0, 3, 4], [2, 4, 3], [0, 2, 1], [2, 1, 0], [2, 1, 4], [2, 4, 1], [0, 3, 4], [2, 4, 3], [0, 3, 2], [2, 4], [3, 0, 4], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 0, 1], [2, 4], [2, 4, 3], [2, 0, 1], [2, 1, 0], [2, 3, 0], [2, 4, 3], [0, 3, 1], [2, 1, 0], [2, 1, 4], [0, 3, 1], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 0, 3], [2, 4, 1], [0, 3, 1], [2, 0, 3], [2, 4, 1], [0, 1, 3], [0, 3, 2], [2, 4, 1], [2, 1, 4], [0, 1, 2], [0, 1, 2], [2, 4, 3], [2, 3, 4], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 0, 1], [0, 3, 1], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 4, 1], [2, 1, 4], [3, 0, 4], [2, 0, 3], [2, 3, 4], [2, 3, 0], [2, 0, 1], [3, 2, 4], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 4, 1], [0, 3, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [0, 3, 4], [0, 3, 1], [2, 0, 1], [0, 2, 3], [2, 4, 1], [0, 3, 1], [0, 3, 4], [2, 4, 1], [0, 1, 3], [2, 4, 1], [2, 4, 3], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 1, 4], [0, 3, 4], [0, 3, 4], [2, 1, 0], [2, 1, 0], [2, 4, 1], [0, 3, 4], [3, 0, 4], [2, 1, 0], [2, 0, 1], [2, 1, 4], [3, 4, 0], [0, 3, 1], [2, 1, 0], [2, 4, 1], [0, 2, 3], [2, 4, 3], [0, 2, 3], [2, 1, 0], [0, 1, 3], [0, 3, 1], [3, 0, 4], [0, 3, 4], [0, 3, 1], [2, 0, 4], [0, 1, 3], [2, 0, 3], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 1, 4], [2, 4, 3], [3, 0, 4], [2, 4, 1], [2, 1, 0], [2, 4, 1], [0, 3, 4], [2, 4, 0], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [2, 1, 0], [2, 3, 0], [2, 4, 1], [0, 3, 4], [3, 0, 4], [2, 4, 1], [2, 4, 3], [0, 3, 2], [0, 3, 4], [0, 2, 3], [0, 3, 1], [0, 3, 1], [0, 2, 3], [0, 1, 2], [0, 3, 4], [2, 4, 1], [2, 0, 3], [2, 1, 0], [3, 4, 0], [0, 3, 1], [2, 1, 0], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 4], [2, 1, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 1, 0], [0, 3, 2], [0, 3, 4], [0, 1, 3], [2, 4, 1], [0, 1, 2], [2, 0, 3], [0, 3, 4], [2, 1, 4], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 3, 0], [3, 0, 4], [3, 0, 4], [2, 4, 1], [2, 4, 1], [2, 4, 1], [3, 0, 4], [0, 3, 4], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 3, 4], [0, 2, 3], [2, 4, 1], [2, 4, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 0, 3], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 1, 0], [2, 3, 0], [2, 1, 0], [2, 0, 3], [2, 1, 4], [2, 0, 1], [2, 0], [2, 3, 0], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 2, 3], [0, 3, 4], [3, 2, 4], [2, 1, 0], [2, 1, 4], [3, 0, 4], [0, 1, 3], [2, 1, 4], [0, 1, 3], [2, 1, 0], [2, 0, 1], [0, 3, 1], [2, 0, 1], [0, 1, 2], [2, 4, 3], [2, 4, 3], [0, 3, 4], [2, 1, 0], [2, 1, 0], [3, 0, 4], [0, 3, 4], [0, 3, 1], [2, 4, 1], [0, 1, 3]]\n",
      "[[1, 4], [2, 4], [1, 2], [0, 3], [0, 3], [0, 1], [0, 3], [1, 4], [0, 3], [0, 3], [2, 4], [1, 2], [2, 4], [0, 3], [1, 3, 4], [2, 4], [1, 2], [0, 1], [3, 4], [3, 4], [0, 3], [0, 3], [1, 2], [0, 1], [1, 4], [0, 1, 3], [1, 2], [1, 2], [0, 3], [2, 4], [3, 4], [0, 1, 3], [0, 1], [3, 4], [3, 4], [1, 4], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [1, 2], [2, 4], [0, 3], [1, 4], [0, 3], [2, 4], [1, 4], [0, 3], [2, 4], [1, 4], [0, 3], [0, 3], [3, 4], [3, 4], [0, 1], [0, 1], [1, 2], [0, 3], [3, 4], [3, 4], [2, 4], [1, 2], [0, 3], [3, 4], [0, 3], [1, 2], [2, 4], [0, 3], [0, 3], [0, 3], [1, 2], [1, 4], [0, 1], [1, 4], [3, 4], [0, 1], [2, 4], [0, 3], [0, 1], [0, 3], [1, 2], [2, 4], [0, 3], [3, 4], [2, 4], [1, 2], [2, 4], [3, 4], [1, 4], [0, 3], [2, 4], [1, 2], [0, 3], [2, 4], [0, 3], [0, 1], [0, 3], [0, 3], [0, 3], [0, 3], [1, 2], [1, 2], [3, 4], [3, 4], [0, 3], [1, 2], [1, 2], [3, 4], [3, 4], [0, 3], [1, 2], [2, 4], [3, 4], [0, 3], [1, 4], [0, 1], [1, 4], [3, 4], [2, 4], [2, 4], [1, 2], [1, 2], [2, 4], [1, 3], [2, 4], [1, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 1], [1, 2], [0, 3], [3, 4], [0, 3], [1, 2], [1, 3], [2, 4], [3, 4], [0, 1], [2, 4], [2, 4], [3, 4], [1, 4], [0, 3], [1, 2], [1, 2], [0, 3], [0, 1], [1, 4], [1, 2], [1, 4], [2, 4], [2, 4], [1, 4], [3, 4], [1, 2], [0, 3], [1, 4], [3, 4], [1, 2], [2, 4], [3, 4], [0, 3], [0, 3], [2, 4], [1, 2], [3, 4], [2, 4], [0, 3], [0, 1], [2, 4], [0, 3], [0, 3], [3, 4], [1, 4], [1, 2], [2, 4], [0, 3], [3, 4], [1, 4], [1, 2], [0, 3], [3, 4], [0, 3], [2, 4], [2, 4], [2, 4], [0, 1], [1, 4], [1, 2], [1, 2], [0, 3], [0, 3], [0, 3], [1, 2], [1, 2], [2, 4], [3, 4], [0, 1], [0, 1], [1, 4], [0, 3], [0, 3], [3, 4], [2, 4], [2, 4], [2, 4], [2, 4], [1, 4], [1, 2], [0, 3], [3, 4], [3, 4], [1, 4], [3, 4], [1, 4], [0, 3], [3, 4], [1, 3, 4], [1, 4], [2, 4], [0, 3], [0, 3], [0, 3], [1, 4], [1, 2], [0, 1], [3, 4], [0, 3], [1, 2], [2, 4], [0, 3], [2, 4], [3, 4], [3, 4], [2, 4], [3, 4], [3, 4], [0, 1], [0, 1], [1, 2], [3, 4], [3, 4], [1, 2], [1, 2], [2, 4], [0, 3], [2, 4]]\n",
      "NL_pred of 2th iteration [[2, 0, 3], [0, 3, 1], [0, 3, 4], [2, 1, 4], [2, 1, 4], [2, 4, 3], [2, 4, 1], [0, 3, 2], [2, 4, 1], [2, 1, 4], [0, 3, 1], [3, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [0, 3, 4], [2, 4, 3], [0, 2, 1], [2, 1, 0], [2, 1, 4], [2, 4, 1], [0, 3, 4], [2, 4, 3], [0, 3, 2], [3, 0, 4], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 0, 1], [2, 1, 0], [2, 3, 0], [2, 4, 3], [0, 3, 1], [2, 1, 0], [2, 1, 4], [0, 3, 1], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 0, 3], [2, 4, 1], [0, 3, 1], [2, 0, 3], [2, 4, 1], [0, 1, 3], [0, 3, 2], [2, 4, 1], [2, 1, 4], [0, 1, 2], [0, 1, 2], [2, 4, 3], [2, 3, 4], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 0, 1], [0, 3, 1], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 4, 1], [2, 1, 4], [3, 0, 4], [2, 0, 3], [2, 3, 4], [2, 3, 0], [2, 0, 1], [3, 2, 4], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 4, 1], [0, 3, 4], [0, 3, 1], [2, 4, 1], [0, 2, 1], [0, 3, 1], [0, 3, 4], [0, 3, 1], [2, 0, 1], [0, 2, 3], [2, 4, 1], [0, 3, 1], [0, 3, 4], [2, 4, 1], [0, 1, 3], [2, 4, 1], [2, 4, 3], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 1, 4], [0, 3, 4], [0, 3, 4], [2, 1, 0], [2, 1, 0], [2, 4, 1], [0, 3, 4], [3, 0, 4], [2, 1, 0], [2, 0, 1], [2, 1, 4], [3, 4, 0], [0, 3, 1], [2, 1, 0], [2, 4, 1], [0, 2, 3], [2, 4, 3], [0, 2, 3], [2, 1, 0], [0, 1, 3], [0, 3, 1], [3, 0, 4], [0, 3, 4], [0, 3, 1], [2, 0, 4], [0, 1, 3], [2, 0, 3], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 1, 4], [2, 4, 3], [3, 0, 4], [2, 4, 1], [2, 1, 0], [2, 4, 1], [0, 3, 4], [2, 4, 0], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [2, 1, 0], [2, 3, 0], [2, 4, 1], [0, 3, 4], [3, 0, 4], [2, 4, 1], [2, 4, 3], [0, 3, 2], [0, 3, 4], [0, 2, 3], [0, 3, 1], [0, 3, 1], [0, 2, 3], [0, 1, 2], [0, 3, 4], [2, 4, 1], [2, 0, 3], [2, 1, 0], [3, 4, 0], [0, 3, 1], [2, 1, 0], [2, 4, 1], [2, 4, 1], [0, 3, 1], [0, 3, 4], [2, 1, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 1, 0], [0, 3, 2], [0, 3, 4], [0, 1, 3], [2, 4, 1], [0, 1, 2], [2, 0, 3], [0, 3, 4], [2, 1, 4], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 3, 0], [3, 0, 4], [3, 0, 4], [2, 4, 1], [2, 4, 1], [2, 4, 1], [3, 0, 4], [0, 3, 4], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 3, 4], [0, 2, 3], [2, 4, 1], [2, 4, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 0, 3], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 1, 0], [2, 3, 0], [2, 1, 0], [2, 0, 3], [2, 1, 4], [2, 0, 1], [2, 3, 0], [0, 3, 1], [2, 4, 1], [2, 4, 1], [2, 4, 1], [0, 2, 3], [0, 3, 4], [3, 2, 4], [2, 1, 0], [2, 1, 4], [3, 0, 4], [0, 1, 3], [2, 1, 4], [0, 1, 3], [2, 1, 0], [2, 0, 1], [0, 3, 1], [2, 0, 1], [0, 1, 2], [2, 4, 3], [2, 4, 3], [0, 3, 4], [2, 1, 0], [2, 1, 0], [3, 0, 4], [0, 3, 4], [0, 3, 1], [2, 4, 1], [0, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004676960832704373  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004676887659522576  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004676747612836884  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004676550384459457  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004676305181611844  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004676015396428302  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004675692174492813  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004675340361711456  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0046749652885809175  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004674572770188495  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.0046741661986684415  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004673751389108053  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004673331733641586  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004672909655222078  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004672487576802571  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00467206889051732  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0046716555347287555  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004671248963208702  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004670851114319592  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004670462472652032  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.43266746401786804 tensor([0.0152, 0.4327, 0.0192, 0.0325, 0.5005], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00021708570420742035 tensor([1.2874e-12, 1.7296e-06, 9.9978e-01, 3.7165e-09, 2.1709e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.33379504084587097 tensor([4.9506e-04, 3.3380e-01, 5.0966e-01, 2.2774e-03, 1.5377e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.023612631484866142 tensor([2.3613e-02, 1.8003e-06, 7.2328e-12, 9.7636e-01, 2.0732e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11961979418992996 tensor([1.1962e-01, 1.1732e-03, 1.2274e-07, 8.7366e-01, 5.5486e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04565175995230675 tensor([9.2101e-01, 4.5652e-02, 1.0548e-07, 3.2456e-02, 8.8402e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02883468009531498 tensor([9.6983e-01, 1.3353e-03, 1.0230e-10, 2.8835e-02, 4.5281e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3967767357826233 tensor([0.0049, 0.4350, 0.1365, 0.0269, 0.3968], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.42279258370399475 tensor([4.2279e-01, 6.8020e-02, 1.3104e-05, 4.6607e-01, 4.3101e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06639166921377182 tensor([6.6392e-02, 2.2799e-03, 1.9110e-06, 9.0941e-01, 2.1918e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24496106803417206 tensor([5.0148e-07, 1.0937e-03, 7.5380e-01, 1.4625e-04, 2.4496e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.30793529748916626 tensor([1.9634e-04, 6.7960e-01, 3.0794e-01, 6.0808e-05, 1.2204e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00023995377705432475 tensor([1.1028e-10, 4.8431e-05, 9.9971e-01, 2.0421e-08, 2.3995e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.39158183336257935 tensor([6.0714e-01, 1.0369e-03, 2.0433e-09, 3.9158e-01, 2.4454e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17586271464824677 tensor([0.0408, 0.1759, 0.0027, 0.2021, 0.5786], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17987599968910217 tensor([2.4185e-04, 4.9743e-02, 1.7988e-01, 6.6913e-03, 7.6345e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16461318731307983 tensor([4.4651e-05, 1.6461e-01, 8.1819e-01, 1.4111e-04, 1.7011e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.40695518255233765 tensor([4.0696e-01, 4.2760e-01, 1.3348e-04, 1.5130e-01, 1.4006e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0740840807557106 tensor([0.0010, 0.0073, 0.0033, 0.0741, 0.9142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.44807419180870056 tensor([5.2975e-03, 5.3662e-03, 4.5931e-04, 4.4807e-01, 5.4080e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2693866491317749 tensor([2.6939e-01, 1.6696e-03, 4.6577e-08, 7.2243e-01, 6.5124e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14895445108413696 tensor([8.3460e-01, 1.5852e-02, 5.9568e-08, 1.4895e-01, 5.9103e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04096834361553192 tensor([1.0889e-05, 4.0968e-02, 9.3953e-01, 1.2624e-04, 1.9368e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2997129261493683 tensor([6.9920e-01, 2.9971e-01, 1.5384e-07, 1.0678e-03, 2.1687e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3859472870826721 tensor([0.0075, 0.5112, 0.0668, 0.0286, 0.3859], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2218429148197174 tensor([2.7490e-01, 2.2184e-01, 2.3212e-04, 3.8251e-01, 1.2052e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40147337317466736 tensor([4.1929e-04, 5.5870e-01, 4.0147e-01, 3.4091e-04, 3.9069e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01839069277048111 tensor([2.0750e-06, 1.8391e-02, 9.6642e-01, 2.1555e-05, 1.5164e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38343551754951477 tensor([4.2194e-01, 1.2265e-01, 3.1967e-05, 3.8344e-01, 7.1949e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08318784087896347 tensor([1.1511e-04, 1.1622e-02, 8.3188e-02, 1.1138e-02, 8.9394e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11094534397125244 tensor([0.0096, 0.0809, 0.0055, 0.1109, 0.7930], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18612666428089142 tensor([0.1861, 0.2013, 0.0006, 0.5341, 0.0778], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03997093066573143 tensor([0.0400, 0.9145, 0.0090, 0.0181, 0.0185], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34581223130226135 tensor([0.0209, 0.1167, 0.0096, 0.3458, 0.5069], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30937883257865906 tensor([6.2346e-02, 5.7999e-02, 3.4588e-04, 5.6993e-01, 3.0938e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14206300675868988 tensor([0.0637, 0.7566, 0.0035, 0.0342, 0.1421], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03749217838048935 tensor([9.6147e-01, 3.7492e-02, 1.4665e-09, 1.0340e-03, 4.5049e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22058799862861633 tensor([1.7407e-06, 2.1882e-03, 7.7665e-01, 5.7124e-04, 2.2059e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.398391455411911 tensor([1.8733e-03, 1.7021e-03, 4.7547e-04, 5.9756e-01, 3.9839e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1765066385269165 tensor([1.7651e-01, 1.6932e-02, 6.8554e-06, 7.7271e-01, 3.3842e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1709052324295044 tensor([5.5706e-05, 1.2143e-02, 1.7091e-01, 3.6804e-03, 8.1322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07953231781721115 tensor([1.5605e-06, 7.9532e-02, 9.1909e-01, 1.2543e-06, 1.3747e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002126395993400365 tensor([3.9991e-11, 1.8305e-05, 9.9977e-01, 1.8410e-08, 2.1264e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14098286628723145 tensor([1.4098e-01, 1.6666e-06, 1.2179e-13, 8.5901e-01, 2.1430e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.41699734330177307 tensor([0.0291, 0.4170, 0.0114, 0.0725, 0.4701], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3867892324924469 tensor([3.8679e-01, 1.5216e-02, 1.0927e-06, 5.8562e-01, 1.2377e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01477221492677927 tensor([3.5834e-07, 4.0512e-03, 9.8116e-01, 1.3760e-05, 1.4772e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3274097144603729 tensor([0.0374, 0.5000, 0.0176, 0.1177, 0.3274], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0359891876578331 tensor([9.6039e-01, 3.6003e-03, 7.4025e-10, 3.5989e-02, 2.3999e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4398953318595886 tensor([7.5354e-07, 5.8657e-04, 4.3990e-01, 1.0084e-03, 5.5851e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016738327220082283 tensor([3.5192e-04, 1.6738e-02, 1.6044e-02, 1.0486e-02, 9.5638e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30782222747802734 tensor([3.0782e-01, 1.8088e-01, 1.3675e-04, 4.6740e-01, 4.3767e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22443106770515442 tensor([2.2443e-01, 3.2997e-03, 2.8192e-07, 7.6392e-01, 8.3445e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04409485682845116 tensor([2.5569e-04, 3.1312e-03, 6.8732e-03, 4.4095e-02, 9.4565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025571079924702644 tensor([4.2392e-05, 7.0312e-04, 5.4151e-03, 2.5571e-02, 9.6827e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32586899399757385 tensor([3.2587e-01, 6.6038e-01, 2.9607e-05, 1.0192e-02, 3.5331e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.041776515543460846 tensor([0.0418, 0.8879, 0.0047, 0.0194, 0.0461], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18585000932216644 tensor([1.7273e-04, 1.8585e-01, 6.9287e-01, 9.1689e-04, 1.2019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.45056653022766113 tensor([4.5057e-01, 7.7181e-04, 3.2578e-09, 5.4838e-01, 2.8452e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21059666574001312 tensor([1.2334e-02, 3.2896e-03, 4.7989e-05, 7.7373e-01, 2.1060e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34244289994239807 tensor([1.6194e-03, 2.1993e-03, 6.1678e-04, 3.4244e-01, 6.5312e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004205507226288319 tensor([7.9408e-09, 3.4022e-04, 9.9545e-01, 7.7149e-07, 4.2055e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007285531144589186 tensor([6.2389e-08, 7.2855e-03, 9.9210e-01, 2.5610e-07, 6.1823e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16194775700569153 tensor([7.5336e-01, 7.9509e-02, 1.6805e-06, 1.6195e-01, 5.1802e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4024898111820221 tensor([2.0770e-02, 9.1109e-03, 1.2677e-04, 5.6750e-01, 4.0249e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24095754325389862 tensor([2.4096e-01, 1.7308e-03, 6.0313e-08, 7.5206e-01, 5.2489e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15408870577812195 tensor([6.0241e-06, 1.5409e-01, 8.4221e-01, 3.8430e-06, 3.6898e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33873337507247925 tensor([0.0009, 0.1265, 0.3387, 0.0204, 0.5135], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.056786514818668365 tensor([5.6787e-02, 3.4311e-03, 4.2365e-06, 8.7072e-01, 6.9059e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3218180537223816 tensor([3.2182e-01, 1.3238e-01, 7.6501e-05, 5.0605e-01, 3.9679e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18095503747463226 tensor([1.8096e-01, 1.2170e-03, 5.7730e-08, 8.1257e-01, 5.2621e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1374322474002838 tensor([0.0025, 0.7425, 0.1374, 0.0025, 0.1150], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3895903527736664 tensor([0.0193, 0.3896, 0.0233, 0.1076, 0.4602], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03610219061374664 tensor([0.0361, 0.9539, 0.0016, 0.0028, 0.0056], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07725966721773148 tensor([0.0345, 0.8666, 0.0065, 0.0151, 0.0773], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27153298258781433 tensor([0.0143, 0.0472, 0.0033, 0.2715, 0.6636], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014737425372004509 tensor([1.4737e-02, 9.8327e-01, 4.4377e-04, 2.9423e-04, 1.2534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3100980222225189 tensor([1.4062e-04, 3.7236e-02, 3.1010e-01, 6.7120e-03, 6.4581e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42525961995124817 tensor([5.7409e-01, 5.4194e-04, 5.5713e-10, 4.2526e-01, 1.1235e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17703700065612793 tensor([0.1770, 0.5344, 0.0009, 0.1525, 0.1352], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.41576340794563293 tensor([4.1576e-01, 8.8184e-02, 2.2083e-05, 4.6394e-01, 3.2096e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017088899621739984 tensor([1.3502e-08, 1.7089e-03, 9.9741e-01, 1.6460e-07, 8.8467e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003403332084417343 tensor([4.9810e-12, 3.0576e-06, 9.9966e-01, 1.1829e-08, 3.4033e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2961636483669281 tensor([4.7381e-01, 1.9099e-01, 4.9132e-05, 2.9616e-01, 3.8990e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12945125997066498 tensor([0.0015, 0.0114, 0.0060, 0.1295, 0.8517], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17903947830200195 tensor([3.0858e-04, 5.8344e-02, 1.7904e-01, 8.1850e-03, 7.5412e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29937034845352173 tensor([9.9599e-05, 2.9937e-01, 6.7256e-01, 1.7830e-04, 2.7794e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026358522474765778 tensor([4.3918e-08, 4.6893e-04, 9.7316e-01, 1.0850e-05, 2.6359e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4644434452056885 tensor([0.0117, 0.0148, 0.0007, 0.5083, 0.4644], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25530388951301575 tensor([0.0098, 0.2553, 0.0254, 0.0621, 0.6474], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021925460547208786 tensor([9.7585e-01, 2.2078e-03, 1.8379e-10, 2.1925e-02, 1.5241e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00795623380690813 tensor([7.1558e-10, 3.3408e-05, 9.9201e-01, 6.6205e-07, 7.9562e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4548092484474182 tensor([0.0008, 0.4548, 0.4696, 0.0026, 0.0721], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3193373680114746 tensor([3.1934e-01, 1.4315e-02, 1.5997e-06, 6.6084e-01, 5.5101e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21207483112812042 tensor([1.6223e-06, 3.9343e-04, 2.1207e-01, 3.8698e-03, 7.8366e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1436499059200287 tensor([8.3472e-01, 2.0551e-02, 1.2135e-07, 1.4365e-01, 1.0795e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0956212505698204 tensor([8.6608e-01, 9.5621e-02, 4.4266e-07, 3.7269e-02, 1.0318e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13461317121982574 tensor([1.3461e-01, 6.3635e-02, 2.2081e-04, 7.3697e-01, 6.4563e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10002569854259491 tensor([1.0003e-01, 1.6283e-03, 4.0577e-07, 8.9110e-01, 7.2450e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10046501457691193 tensor([8.9062e-01, 8.7771e-03, 1.4514e-08, 1.0047e-01, 1.4246e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23327083885669708 tensor([2.3327e-01, 1.3718e-02, 3.3961e-06, 7.1270e-01, 4.0304e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0029365941882133484 tensor([3.9015e-08, 2.9366e-03, 9.9612e-01, 4.2107e-07, 9.4200e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.018160495907068253 tensor([2.1021e-06, 1.8160e-02, 9.6902e-01, 2.4046e-05, 1.2792e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10576795041561127 tensor([2.9658e-02, 4.1546e-03, 1.8110e-05, 8.6040e-01, 1.0577e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017184505239129066 tensor([5.5122e-03, 5.6530e-05, 1.4027e-07, 9.7725e-01, 1.7185e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008838952518999577 tensor([9.8853e-01, 2.6245e-03, 9.8207e-11, 8.8390e-03, 4.6744e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3680689036846161 tensor([0.0012, 0.5451, 0.3681, 0.0029, 0.0828], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06102428212761879 tensor([0.0061, 0.8911, 0.0610, 0.0041, 0.0376], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03890550136566162 tensor([4.8132e-03, 1.7233e-04, 1.4817e-06, 9.5611e-01, 3.8906e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20702867209911346 tensor([0.0142, 0.1047, 0.0121, 0.2070, 0.6619], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11485186964273453 tensor([1.1485e-01, 1.1032e-02, 7.5045e-06, 8.2566e-01, 4.8452e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008604135364294052 tensor([5.1451e-04, 9.9039e-01, 8.6041e-03, 8.9836e-06, 4.8481e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00046021767775528133 tensor([6.0068e-10, 1.2387e-04, 9.9942e-01, 6.8743e-08, 4.6022e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1299280971288681 tensor([5.4291e-02, 9.4886e-03, 2.6623e-05, 8.0627e-01, 1.2993e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03312774375081062 tensor([9.6616e-01, 7.0674e-04, 2.7384e-11, 3.3128e-02, 4.2820e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1875179260969162 tensor([0.0054, 0.1875, 0.0357, 0.0505, 0.7209], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23486046493053436 tensor([2.3486e-01, 7.6103e-01, 1.7951e-05, 2.8100e-03, 1.2856e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4371846318244934 tensor([0.0168, 0.4546, 0.0360, 0.0554, 0.4372], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13026933372020721 tensor([1.2369e-02, 1.6540e-03, 1.5086e-05, 8.5569e-01, 1.3027e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0878569558262825 tensor([5.6753e-08, 1.6224e-05, 8.7857e-02, 1.2280e-03, 9.1090e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2842474579811096 tensor([5.8742e-06, 3.3954e-03, 2.8425e-01, 8.6430e-04, 7.1149e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2671213150024414 tensor([0.0007, 0.6578, 0.2671, 0.0007, 0.0737], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0812121108174324 tensor([1.4399e-05, 8.1212e-02, 9.1137e-01, 4.6384e-05, 7.3579e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24038174748420715 tensor([0.0010, 0.1146, 0.2404, 0.0283, 0.6157], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22278760373592377 tensor([0.1015, 0.2228, 0.0016, 0.4545, 0.2197], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02251361683011055 tensor([1.4653e-04, 6.8019e-03, 2.2514e-02, 1.5327e-02, 9.5521e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13954339921474457 tensor([0.0219, 0.7913, 0.0211, 0.0262, 0.1395], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0012657152255997062 tensor([1.6927e-08, 7.5445e-04, 9.9798e-01, 7.1728e-07, 1.2657e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16123446822166443 tensor([9.0146e-05, 1.2515e-02, 1.6123e-01, 1.1283e-02, 8.1488e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0915461853146553 tensor([1.2637e-05, 2.7171e-02, 8.8098e-01, 2.9208e-04, 9.1546e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2200298309326172 tensor([2.2003e-01, 1.8589e-03, 6.8302e-08, 7.7329e-01, 4.8162e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07600943744182587 tensor([0.0760, 0.8858, 0.0018, 0.0163, 0.0201], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17818164825439453 tensor([2.3393e-04, 8.1899e-01, 1.7818e-01, 3.7791e-05, 2.5534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3116517663002014 tensor([3.1165e-01, 4.4413e-02, 1.3848e-05, 6.3124e-01, 1.2685e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41171470284461975 tensor([2.4601e-02, 1.4406e-02, 2.1998e-04, 5.4906e-01, 4.1171e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008443971164524555 tensor([9.8971e-01, 1.8409e-03, 4.8287e-11, 8.4440e-03, 3.1509e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03372667357325554 tensor([2.6515e-06, 3.3727e-02, 9.5395e-01, 1.0598e-05, 1.2309e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22047877311706543 tensor([0.1379, 0.4934, 0.0025, 0.2205, 0.1458], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2789619266986847 tensor([0.0007, 0.1017, 0.2790, 0.0247, 0.5939], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3756127953529358 tensor([0.0720, 0.1239, 0.0015, 0.4270, 0.3756], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28868383169174194 tensor([2.8868e-01, 5.6326e-01, 4.4523e-04, 9.3486e-02, 5.4121e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20466789603233337 tensor([1.6878e-07, 5.4698e-04, 7.9468e-01, 1.0020e-04, 2.0467e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.740924487123266e-05 tensor([2.6118e-13, 8.6868e-07, 9.9995e-01, 9.4895e-10, 4.7409e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14726479351520538 tensor([4.7016e-02, 4.4595e-02, 7.0772e-04, 7.6042e-01, 1.4726e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21738597750663757 tensor([0.0257, 0.7200, 0.0141, 0.0229, 0.2174], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009619082324206829 tensor([9.9012e-01, 2.5935e-04, 4.2077e-13, 9.6191e-03, 3.1985e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03018409200012684 tensor([1.7495e-06, 3.0184e-02, 9.5653e-01, 8.2270e-06, 1.3271e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06067708134651184 tensor([4.3868e-04, 9.3563e-01, 6.0677e-02, 3.6365e-05, 3.2180e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3477494716644287 tensor([4.3113e-01, 1.5509e-01, 5.5203e-05, 3.4775e-01, 6.5980e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24773293733596802 tensor([5.7012e-01, 2.4773e-01, 2.2857e-05, 1.6844e-01, 1.3679e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20068970322608948 tensor([0.0022, 0.2007, 0.0751, 0.0121, 0.7098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.44469863176345825 tensor([2.0118e-04, 4.4470e-01, 5.2830e-01, 2.7898e-04, 2.6522e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14304718375205994 tensor([0.0162, 0.7679, 0.0379, 0.0350, 0.1430], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.40698912739753723 tensor([3.0874e-04, 1.4709e-01, 4.4039e-01, 5.2231e-03, 4.0699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4455685615539551 tensor([2.8202e-06, 2.8080e-03, 4.4557e-01, 6.5840e-04, 5.5096e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03834147751331329 tensor([0.0013, 0.0383, 0.0231, 0.0365, 0.9008], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08973104506731033 tensor([6.9366e-04, 2.2114e-02, 9.0597e-02, 8.9731e-02, 7.9686e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.301631361246109 tensor([6.2946e-04, 6.3470e-01, 3.0163e-01, 7.8348e-04, 6.2255e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2733035087585449 tensor([2.7330e-01, 1.4607e-03, 3.2421e-08, 7.2410e-01, 1.1363e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10536522418260574 tensor([0.0090, 0.1054, 0.0093, 0.1033, 0.7730], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18267367780208588 tensor([9.1736e-02, 2.9354e-02, 9.3632e-05, 6.9614e-01, 1.8267e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008486837148666382 tensor([7.9650e-04, 9.9023e-01, 8.4868e-03, 1.8779e-05, 4.6318e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28767240047454834 tensor([3.9261e-05, 2.2520e-02, 6.8670e-01, 3.0724e-03, 2.8767e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41252371668815613 tensor([4.5003e-04, 2.3511e-04, 1.2766e-04, 4.1252e-01, 5.8666e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31898948550224304 tensor([3.1899e-01, 1.2905e-01, 7.7156e-05, 4.6171e-01, 9.0171e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3021705746650696 tensor([5.4581e-01, 1.1358e-01, 1.8344e-05, 3.0217e-01, 3.8423e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005509816110134125 tensor([6.6038e-08, 3.1292e-03, 9.9136e-01, 9.1656e-07, 5.5098e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021191954612731934 tensor([7.7718e-07, 2.1192e-02, 9.7484e-01, 2.3100e-06, 3.9679e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08632657676935196 tensor([2.2734e-02, 1.4321e-03, 4.4986e-06, 8.8950e-01, 8.6327e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1895354837179184 tensor([2.7965e-05, 4.6086e-02, 7.6388e-01, 4.7199e-04, 1.8954e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03626365214586258 tensor([9.6321e-01, 5.2787e-04, 1.5432e-11, 3.6264e-02, 2.9461e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21416129171848297 tensor([2.1416e-01, 7.4264e-01, 2.4651e-04, 2.4820e-02, 1.8133e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4560664892196655 tensor([5.0433e-05, 2.0669e-02, 4.5607e-01, 4.5607e-03, 5.1865e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08231059461832047 tensor([8.2311e-02, 1.2921e-05, 2.7445e-11, 9.1761e-01, 6.2634e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011843386106193066 tensor([1.1843e-02, 1.0539e-05, 1.3899e-09, 9.8702e-01, 1.1303e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04117865115404129 tensor([2.2512e-02, 9.7325e-04, 2.7465e-06, 9.3533e-01, 4.1179e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4565286934375763 tensor([0.0076, 0.4565, 0.0341, 0.0223, 0.4795], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02755836956202984 tensor([1.8311e-06, 2.7558e-02, 9.6465e-01, 1.0935e-05, 7.7759e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.37969696521759033 tensor([2.0846e-07, 1.4603e-04, 3.7970e-01, 9.9111e-04, 6.1917e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07264629751443863 tensor([9.2337e-01, 3.9060e-03, 2.3103e-09, 7.2646e-02, 7.6525e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02824784629046917 tensor([1.1240e-04, 2.8761e-03, 1.3526e-02, 2.8248e-02, 9.5524e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2877991497516632 tensor([0.1414, 0.3708, 0.0009, 0.1990, 0.2878], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007670627092011273 tensor([4.1178e-09, 7.6706e-04, 9.9868e-01, 1.0400e-07, 5.5169e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021952977403998375 tensor([2.1953e-02, 4.6559e-05, 7.0288e-09, 9.7692e-01, 1.0812e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4740586578845978 tensor([5.2293e-04, 1.4400e-04, 5.6659e-05, 5.2522e-01, 4.7406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26604214310646057 tensor([2.6604e-01, 8.0190e-04, 1.2249e-08, 7.3176e-01, 1.3935e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2937435805797577 tensor([0.0008, 0.1301, 0.2937, 0.0160, 0.5593], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3460313677787781 tensor([1.1380e-04, 1.9158e-02, 3.4603e-01, 1.8524e-02, 6.1617e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013494839891791344 tensor([2.1252e-07, 3.5269e-03, 9.8297e-01, 6.4070e-06, 1.3495e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010503298602998257 tensor([9.8100e-01, 1.0503e-02, 1.4842e-09, 8.4784e-03, 1.5288e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1990790069103241 tensor([0.0409, 0.7307, 0.0052, 0.0242, 0.1991], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.48825013637542725 tensor([4.5827e-05, 5.0798e-01, 4.8825e-01, 1.2572e-05, 3.7071e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08086726069450378 tensor([1.4828e-03, 9.1204e-01, 8.0867e-02, 3.8181e-04, 5.2250e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42075279355049133 tensor([5.7894e-01, 2.6329e-04, 1.3492e-10, 4.2075e-01, 4.3394e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08696053922176361 tensor([9.1159e-01, 1.4011e-03, 2.9624e-10, 8.6961e-02, 4.5582e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07293269038200378 tensor([9.2074e-01, 6.2151e-03, 5.2337e-09, 7.2933e-02, 1.1430e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3309943377971649 tensor([8.3815e-05, 6.6308e-01, 3.3099e-01, 1.4256e-05, 5.8318e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20364773273468018 tensor([1.9831e-04, 2.0365e-01, 6.7928e-01, 9.5788e-04, 1.1592e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35511377453804016 tensor([2.7214e-05, 2.8232e-02, 6.1579e-01, 8.3409e-04, 3.5511e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03510691598057747 tensor([6.6725e-04, 2.6397e-02, 4.6171e-02, 3.5107e-02, 8.9166e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.255115807056427 tensor([2.5512e-01, 5.5613e-01, 4.3580e-04, 8.9576e-02, 9.8741e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04080650582909584 tensor([0.0408, 0.9202, 0.0021, 0.0083, 0.0285], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44585829973220825 tensor([0.0132, 0.4618, 0.0322, 0.0469, 0.4459], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4359135925769806 tensor([5.6177e-01, 1.8549e-03, 7.0482e-09, 4.3591e-01, 4.5892e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4621516466140747 tensor([4.6215e-01, 1.3913e-02, 7.3557e-07, 5.1231e-01, 1.1622e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2587512135505676 tensor([6.4022e-03, 5.0949e-03, 2.3815e-04, 2.5875e-01, 7.2951e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00046089262468740344 tensor([2.9930e-10, 1.0987e-04, 9.9943e-01, 2.7982e-08, 4.6089e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011147188488394022 tensor([1.3611e-08, 7.9294e-04, 9.9809e-01, 5.2426e-07, 1.1147e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.47843077778816223 tensor([1.4977e-05, 8.5549e-03, 5.1105e-01, 1.9505e-03, 4.7843e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002238979795947671 tensor([1.5146e-08, 1.0765e-03, 9.9668e-01, 4.4741e-07, 2.2390e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3959679901599884 tensor([0.0289, 0.3960, 0.0166, 0.1098, 0.4486], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03881506621837616 tensor([2.9409e-06, 3.8815e-02, 9.5371e-01, 8.8022e-06, 7.4647e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3883664309978485 tensor([5.2131e-01, 7.5368e-02, 9.2363e-06, 3.8837e-01, 1.4943e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3236408233642578 tensor([2.0228e-03, 4.9794e-04, 5.2416e-05, 6.7379e-01, 3.2364e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.021623356267809868 tensor([1.0293e-02, 1.0102e-04, 1.5253e-07, 9.6798e-01, 2.1623e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1953216791152954 tensor([0.0254, 0.7387, 0.0103, 0.0303, 0.1953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2977951169013977 tensor([1.0047e-01, 3.6124e-02, 7.6163e-05, 5.6553e-01, 2.9780e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3677266836166382 tensor([0.0352, 0.4343, 0.0185, 0.1442, 0.3677], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20264920592308044 tensor([2.0265e-01, 1.3072e-04, 5.5872e-10, 7.9704e-01, 1.8450e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.43521609902381897 tensor([0.0013, 0.0022, 0.0016, 0.4352, 0.5597], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23492330312728882 tensor([0.1524, 0.2684, 0.0007, 0.2349, 0.3436], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13066641986370087 tensor([0.0261, 0.7912, 0.0221, 0.0301, 0.1307], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01522072684019804 tensor([7.3765e-07, 6.1173e-03, 9.7864e-01, 2.5179e-05, 1.5221e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.055753275752067566 tensor([9.1793e-01, 2.6125e-02, 4.6786e-08, 5.5753e-02, 1.9504e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0710655003786087 tensor([9.2722e-01, 1.6513e-03, 3.7483e-10, 7.1066e-02, 5.9746e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18964067101478577 tensor([8.1013e-01, 2.1645e-04, 2.0545e-11, 1.8964e-01, 1.1129e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2077011615037918 tensor([0.0102, 0.2077, 0.0204, 0.0657, 0.6960], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0033371273893862963 tensor([8.4924e-08, 3.3371e-03, 9.9469e-01, 1.2923e-06, 1.9723e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013960444368422031 tensor([0.0140, 0.9765, 0.0029, 0.0014, 0.0053], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31946510076522827 tensor([2.0568e-03, 1.8835e-03, 2.9940e-04, 3.1947e-01, 6.7630e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4145469665527344 tensor([4.1455e-01, 1.4474e-03, 1.3966e-08, 5.8189e-01, 2.1154e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013060105964541435 tensor([4.3566e-03, 9.7163e-01, 1.3060e-02, 6.2530e-04, 1.0331e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09117397665977478 tensor([7.3668e-07, 1.1842e-04, 9.1174e-02, 3.4473e-03, 9.0526e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016710948199033737 tensor([1.6711e-02, 1.4235e-04, 9.7081e-08, 9.8013e-01, 3.0196e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09211243689060211 tensor([2.4715e-04, 1.5878e-02, 9.2112e-02, 3.6493e-02, 8.5527e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10822473466396332 tensor([3.1133e-02, 4.1786e-03, 1.5815e-05, 8.5645e-01, 1.0822e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3207426369190216 tensor([0.0062, 0.0250, 0.0050, 0.3207, 0.6431], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0019558824133127928 tensor([3.9270e-10, 4.8630e-05, 9.9800e-01, 1.2991e-07, 1.9559e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3407042920589447 tensor([0.0089, 0.0274, 0.0022, 0.3407, 0.6208], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018523026257753372 tensor([9.1328e-05, 2.7063e-03, 1.7229e-02, 1.8523e-02, 9.6145e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11432057619094849 tensor([0.1143, 0.8381, 0.0009, 0.0219, 0.0248], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09837812185287476 tensor([9.8378e-02, 9.0144e-01, 3.7471e-06, 1.2376e-04, 5.7128e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010442916536703706 tensor([1.8965e-08, 1.0443e-03, 9.9793e-01, 6.5150e-07, 1.0228e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18403375148773193 tensor([8.3678e-03, 4.2243e-03, 1.9913e-04, 8.0318e-01, 1.8403e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4294847846031189 tensor([1.3956e-03, 1.0964e-03, 3.7250e-04, 5.6765e-01, 4.2948e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.42259150743484497 tensor([2.7536e-04, 5.4430e-01, 4.2259e-01, 2.3004e-04, 3.2600e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3201497793197632 tensor([1.5951e-04, 3.2015e-01, 6.2293e-01, 3.6730e-04, 5.6392e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1752183884382248 tensor([2.3014e-06, 6.8164e-03, 8.1782e-01, 1.3992e-04, 1.7522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0628664642572403 tensor([9.3651e-01, 6.1992e-04, 3.4495e-11, 6.2866e-02, 4.2388e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011470669880509377 tensor([3.4994e-05, 1.6883e-03, 1.1471e-02, 8.1631e-03, 9.7864e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 0, 3], [0, 3, 1, 4], [0, 3, 4], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 2], [2, 4, 1], [2, 1, 4, 0], [0, 3, 1], [3, 0, 4], [0, 3, 1, 4], [2, 4, 1], [2, 0, 1], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 3], [0, 2, 1, 3], [2, 1, 0], [2, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 3], [0, 3, 2], [2, 4], [3, 0, 4], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [2, 0, 1], [2, 1, 0], [2, 3, 0, 4], [2, 4, 3, 1], [0, 3, 1], [2, 1, 0], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3], [2, 4, 1], [0, 3, 1, 4], [2, 0, 3], [2, 4, 1, 3], [0, 1, 3], [0, 3, 2, 1], [2, 4, 1], [2, 1, 4], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3], [2, 3, 4, 0], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0], [2, 0, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 0], [2, 1, 4], [3, 0, 4, 1], [0, 3, 1], [2, 1, 4, 0], [2, 4, 1], [2, 1, 4, 0], [3, 0, 4, 2], [2, 0, 3], [2, 3, 4, 0], [2, 3, 0, 4], [2, 0, 1], [3, 2, 4, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3, 0], [2, 4, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1], [0, 2, 1, 3], [0, 3, 1, 2], [0, 3, 4], [0, 3, 1, 4], [2, 0, 1], [0, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 4], [2, 4, 1], [0, 1, 3], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 3], [2, 1, 4], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4], [3, 0, 4, 2], [2, 1, 0, 4], [2, 0, 1], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 2, 3, 1], [2, 4, 3], [0, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1], [3, 0, 4], [0, 3, 4, 1], [0, 3, 1], [2, 0, 4], [0, 1, 3, 2], [2, 0, 3, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4], [2, 4, 3, 0], [3, 0, 4, 2], [2, 4, 1], [2, 1, 0], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 0], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 0], [2, 4, 1, 3], [0, 3, 4, 1], [3, 0, 4, 2], [2, 4, 1], [2, 4, 3], [0, 3, 2], [0, 3, 4], [0, 2, 3, 4], [0, 3, 1], [0, 3, 1], [0, 2, 3, 1], [0, 1, 2, 3], [0, 3, 4], [2, 4, 1], [2, 0, 3, 1], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1], [2, 1, 0], [2, 4, 1], [2, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 1, 0, 4], [0, 3, 2], [0, 3, 4, 1], [0, 1, 3], [2, 4, 1, 3], [0, 1, 2, 3], [2, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1, 4], [2, 4, 3, 1], [2, 3, 0, 4], [3, 0, 4], [3, 0, 4, 2], [2, 4, 1], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4], [0, 3, 4], [0, 3, 1], [0, 1, 2, 3], [2, 4, 3], [2, 3, 4, 0], [0, 2, 3], [2, 4, 1], [2, 4, 1], [2, 1, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1], [0, 3, 1, 4], [2, 0, 3], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0], [2, 1, 0, 4], [2, 3, 0, 4], [2, 1, 0], [2, 0, 3], [2, 1, 4], [2, 0, 1], [2, 0], [2, 3, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 3], [0, 3, 4, 1], [3, 2, 4, 0], [2, 1, 0], [2, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 1, 0, 4], [2, 0, 1], [0, 3, 1, 4], [2, 0, 1], [0, 1, 2, 3], [2, 4, 3, 0], [2, 4, 3, 0], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0], [3, 0, 4], [0, 3, 4], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 3, 2]]\n",
      "[[1, 4], [2], [1, 2], [3], [3], [0], [0], [1, 4], [0, 3], [3], [2, 4], [1, 2], [2], [0, 3], [3, 4], [4], [2], [0, 1], [4], [3, 4], [0, 3], [0], [2], [0, 1], [1, 4], [0, 1, 3], [1, 2], [2], [0, 3], [4], [4], [1, 3], [1], [3, 4], [3, 4], [1], [0], [2, 4], [3, 4], [3], [4], [2], [2], [3], [1, 4], [0, 3], [2], [1, 4], [0], [2, 4], [4], [0, 3], [0, 3], [4], [4], [0, 1], [1], [2], [0, 3], [3, 4], [3, 4], [2], [2], [0], [3, 4], [0, 3], [2], [2, 4], [3], [0, 3], [3], [1], [1, 4], [1], [1], [3, 4], [1], [2, 4], [0, 3], [1], [0, 3], [2], [2], [0, 3], [4], [4], [1, 2], [2], [3, 4], [1, 4], [0], [2], [1, 2], [0, 3], [2, 4], [0], [0], [3], [3], [0], [0, 3], [2], [2], [3], [3], [0], [1, 2], [1], [3], [3, 4], [3], [1], [2], [3], [0], [4], [0, 1], [1, 4], [3], [4], [2, 4], [1, 2], [2], [2, 4], [1, 3], [4], [1], [2], [4], [2], [0, 3], [1], [1], [0, 3], [3, 4], [0], [2], [1, 3], [2, 4], [3, 4], [0, 1], [2, 4], [2], [3], [1, 4], [0], [2], [1], [0, 3], [0, 1], [1, 4], [1, 2], [1], [2, 4], [2, 4], [4], [4], [1, 2], [0, 3], [4], [3], [1], [2, 4], [3, 4], [0, 3], [0, 3], [2], [2], [3], [2], [0], [0, 1], [2, 4], [3], [3], [3], [1, 4], [2], [2, 4], [0], [4], [1, 4], [2], [3], [3, 4], [0, 3], [2, 4], [2, 4], [2], [0], [1], [1, 2], [1], [0, 3], [0], [0], [1, 2], [1, 2], [2, 4], [4], [0, 1], [1], [1, 4], [0, 3], [0, 3], [3, 4], [2], [2], [2, 4], [2], [1, 4], [2], [0, 3], [3, 4], [3], [1], [3, 4], [1, 4], [0, 3], [3, 4], [1, 3, 4], [1], [2], [0], [0], [0], [1, 4], [2], [1], [3, 4], [0, 3], [1], [4], [3], [4], [3], [3, 4], [2], [3, 4], [4], [1], [1], [2], [3], [3, 4], [1, 2], [1, 2], [2], [0], [4]]\n",
      "NL_pred of 3th iteration [[0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [2, 0, 1], [0, 3, 1, 2], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 0], [2, 4, 3, 0], [2, 3, 0, 4], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 2, 1], [0, 1, 2, 3], [0, 1, 2, 3], [2, 3, 4, 0], [0, 3, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [3, 0, 4, 2], [2, 3, 4, 0], [2, 3, 0, 4], [3, 2, 4, 0], [2, 4, 3, 0], [0, 3, 4, 1], [0, 3, 1, 4], [0, 2, 1, 3], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [2, 1, 0, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 2, 3, 1], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 0, 3, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 3, 0], [3, 0, 4, 2], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4, 1], [3, 0, 4, 2], [0, 2, 3, 4], [0, 2, 3, 1], [0, 1, 2, 3], [2, 0, 3, 1], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 1, 4, 0], [2, 1, 4, 0], [2, 1, 0, 4], [0, 3, 4, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 3, 1], [2, 3, 0, 4], [3, 0, 4, 2], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 2, 3], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 1, 0, 4], [2, 3, 0, 4], [2, 3, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 4, 1], [3, 2, 4, 0], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 1, 0, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 3, 0], [2, 4, 3, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.008653245383886982  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.008653141611771617  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.008652946074231923  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.008652669920338144  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.008652323441539737  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.008651916929286161  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.008651458102164509  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.008650958109245026  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.008650421238631653  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.008649853493669908  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.008649265165809246  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.008648655397428883  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.008648029334253545  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.008647391264387173  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.008646742045450554  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.008646084250306054  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00864541959419525  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.008644748077118139  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.008644071414316301  Accuracy on Support set:0.0\n",
      "torch.Size([139, 2048]) torch.Size([139])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.008643388748168945  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.4499703347682953 tensor([0.0163, 0.4500, 0.0181, 0.0340, 0.4817], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.35818299651145935 tensor([0.0006, 0.3582, 0.4866, 0.0025, 0.1522], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3816467523574829 tensor([0.0054, 0.4571, 0.1272, 0.0287, 0.3816], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4329928159713745 tensor([4.3299e-01, 6.5466e-02, 1.1135e-05, 4.6358e-01, 3.7952e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2566283941268921 tensor([5.8565e-07, 1.2082e-03, 7.4200e-01, 1.6655e-04, 2.5663e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2927522361278534 tensor([2.0433e-04, 6.9528e-01, 2.9275e-01, 6.1849e-05, 1.1706e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38619232177734375 tensor([6.1260e-01, 9.9239e-04, 1.7346e-09, 3.8619e-01, 2.1353e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22118157148361206 tensor([0.0448, 0.1777, 0.0024, 0.2212, 0.5539], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.416966050863266 tensor([4.1961e-01, 4.1697e-01, 1.1510e-04, 1.5088e-01, 1.2434e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4801792502403259 tensor([5.7652e-03, 5.4122e-03, 4.0811e-04, 4.8018e-01, 5.0824e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2735884487628937 tensor([2.7359e-01, 1.6005e-03, 3.9690e-08, 7.1907e-01, 5.7382e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2910441756248474 tensor([7.0786e-01, 2.9104e-01, 1.3743e-07, 1.0715e-03, 1.9978e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3716743588447571 tensor([0.0081, 0.5278, 0.0623, 0.0302, 0.3717], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2179894596338272 tensor([2.8883e-01, 2.1799e-01, 1.9794e-04, 3.8607e-01, 1.0691e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.37968090176582336 tensor([4.5149e-04, 5.8190e-01, 3.7968e-01, 3.5464e-04, 3.7617e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38632357120513916 tensor([4.3259e-01, 1.1727e-01, 2.7071e-05, 3.8632e-01, 6.3794e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19664759933948517 tensor([0.1904, 0.1966, 0.0006, 0.5410, 0.0713], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.37329939007759094 tensor([0.0230, 0.1181, 0.0085, 0.3733, 0.4771], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.284917950630188 tensor([6.5947e-02, 5.7835e-02, 3.0726e-04, 5.9099e-01, 2.8492e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2237975299358368 tensor([1.9305e-06, 2.3691e-03, 7.7321e-01, 6.2163e-04, 2.2380e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36676186323165894 tensor([2.0019e-03, 1.6857e-03, 4.1486e-04, 6.2914e-01, 3.6676e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4303355813026428 tensor([0.0318, 0.4303, 0.0105, 0.0776, 0.4498], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.39389440417289734 tensor([3.9389e-01, 1.4292e-02, 8.9385e-07, 5.8111e-01, 1.0703e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30880364775657654 tensor([0.0404, 0.5106, 0.0160, 0.1241, 0.3088], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4315257668495178 tensor([8.2790e-07, 6.2434e-04, 4.3153e-01, 1.0959e-03, 5.6675e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31688031554222107 tensor([3.1688e-01, 1.7721e-01, 1.1938e-04, 4.6669e-01, 3.9101e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22807595133781433 tensor([2.2808e-01, 3.1749e-03, 2.4299e-07, 7.6134e-01, 7.4107e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3402029573917389 tensor([3.4020e-01, 6.4653e-01, 2.5097e-05, 1.0134e-02, 3.1035e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.454784095287323 tensor([4.5478e-01, 7.4055e-04, 2.7992e-09, 5.4422e-01, 2.5156e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19255302846431732 tensor([1.2710e-02, 3.2127e-03, 4.2389e-05, 7.9148e-01, 1.9255e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.37351495027542114 tensor([1.7895e-03, 2.2436e-03, 5.5328e-04, 3.7351e-01, 6.2190e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3703511357307434 tensor([2.2221e-02, 8.9883e-03, 1.0971e-04, 5.9833e-01, 3.7035e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24749799072742462 tensor([2.4750e-01, 1.6443e-03, 4.9354e-08, 7.4635e-01, 4.5122e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33000290393829346 tensor([0.0010, 0.1359, 0.3300, 0.0223, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3298598527908325 tensor([3.2986e-01, 1.2787e-01, 6.5730e-05, 5.0687e-01, 3.5334e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4035418927669525 tensor([0.0210, 0.4035, 0.0217, 0.1145, 0.4393], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29441189765930176 tensor([0.0158, 0.0489, 0.0030, 0.2944, 0.6378], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3016170263290405 tensor([1.5846e-04, 4.0363e-02, 3.0162e-01, 7.3520e-03, 6.5051e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4194905161857605 tensor([5.7989e-01, 5.1967e-04, 4.7346e-10, 4.1949e-01, 9.8074e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4250487685203552 tensor([4.2505e-01, 8.3893e-02, 1.8439e-05, 4.6290e-01, 2.8139e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2975200116634369 tensor([4.8419e-01, 1.8349e-01, 4.2157e-05, 2.9752e-01, 3.4755e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3167465925216675 tensor([1.0872e-04, 3.1675e-01, 6.5505e-01, 1.9144e-04, 2.7904e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4353104531764984 tensor([0.0125, 0.0150, 0.0007, 0.5365, 0.4353], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26782163977622986 tensor([0.0107, 0.2678, 0.0242, 0.0667, 0.6306], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4506814181804657 tensor([0.0009, 0.4749, 0.4507, 0.0028, 0.0707], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32677409052848816 tensor([3.2677e-01, 1.3506e-02, 1.3068e-06, 6.5497e-01, 4.7514e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20487278699874878 tensor([1.8033e-06, 4.1851e-04, 2.0487e-01, 4.2683e-03, 7.9044e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2393762320280075 tensor([2.3938e-01, 1.2939e-02, 2.7795e-06, 7.1268e-01, 3.5006e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3542064428329468 tensor([0.0012, 0.5616, 0.3542, 0.0030, 0.0800], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2241688221693039 tensor([0.0156, 0.1083, 0.0113, 0.2242, 0.6406], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24354372918605804 tensor([2.4354e-01, 7.5247e-01, 1.5802e-05, 2.8057e-03, 1.1610e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41826117038726807 tensor([0.0182, 0.4713, 0.0337, 0.0586, 0.4183], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2745573818683624 tensor([6.6461e-06, 3.6748e-03, 2.7456e-01, 9.4733e-04, 7.2081e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2491171956062317 tensor([0.0008, 0.6789, 0.2491, 0.0007, 0.0705], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23051628470420837 tensor([0.0011, 0.1224, 0.2305, 0.0316, 0.6144], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2220066487789154 tensor([0.1059, 0.2220, 0.0014, 0.4672, 0.2035], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2249574512243271 tensor([2.2496e-01, 1.7745e-03, 5.7027e-08, 7.6907e-01, 4.1953e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31851211190223694 tensor([3.1851e-01, 4.2312e-02, 1.1608e-05, 6.2805e-01, 1.1113e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3813891112804413 tensor([2.6327e-02, 1.4371e-02, 1.9402e-04, 5.7772e-01, 3.8139e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22497346997261047 tensor([0.1434, 0.4939, 0.0022, 0.2250, 0.1354], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.265834778547287 tensor([0.0008, 0.1105, 0.2658, 0.0281, 0.5947], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3502081632614136 tensor([0.0768, 0.1246, 0.0014, 0.4470, 0.3502], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3015907108783722 tensor([3.0159e-01, 5.5343e-01, 3.8967e-04, 9.5410e-02, 4.9175e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21353918313980103 tensor([1.9148e-07, 5.9200e-04, 7.8576e-01, 1.1268e-04, 2.1354e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2040291279554367 tensor([0.0271, 0.7326, 0.0129, 0.0234, 0.2040], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3483116328716278 tensor([4.4349e-01, 1.4966e-01, 4.7044e-05, 3.4831e-01, 5.8499e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23742352426052094 tensor([5.8277e-01, 2.3742e-01, 1.9396e-05, 1.6773e-01, 1.2062e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2149815559387207 tensor([0.0025, 0.2150, 0.0704, 0.0133, 0.6988], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4664649963378906 tensor([2.1794e-04, 4.6646e-01, 5.0708e-01, 2.9412e-04, 2.5943e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41103407740592957 tensor([3.5053e-04, 1.5855e-01, 4.2427e-01, 5.8003e-03, 4.1103e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.43418800830841064 tensor([3.1645e-06, 3.0221e-03, 4.3419e-01, 7.2371e-04, 5.6206e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.28358861804008484 tensor([0.0007, 0.6551, 0.2836, 0.0008, 0.0598], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2774757444858551 tensor([2.7748e-01, 1.3796e-03, 2.6932e-08, 7.2016e-01, 9.8891e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2930586636066437 tensor([4.3456e-05, 2.4182e-02, 6.7935e-01, 3.3615e-03, 2.9306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4461684823036194 tensor([4.9431e-04, 2.3902e-04, 1.1392e-04, 4.4617e-01, 5.5298e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32897549867630005 tensor([3.2898e-01, 1.2580e-01, 6.6875e-05, 4.6428e-01, 8.0874e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3012401759624481 tensor([5.5559e-01, 1.0907e-01, 1.5730e-05, 3.0124e-01, 3.4089e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2230023294687271 tensor([2.2300e-01, 7.3550e-01, 2.1695e-04, 2.4930e-02, 1.6347e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4466738998889923 tensor([5.6294e-05, 2.2265e-02, 4.4667e-01, 5.0024e-03, 5.2600e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.45968344807624817 tensor([0.0084, 0.4769, 0.0315, 0.0236, 0.4597], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36637750267982483 tensor([2.3412e-07, 1.5571e-04, 3.6638e-01, 1.1046e-03, 6.3236e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2655061185359955 tensor([0.1522, 0.3733, 0.0008, 0.2081, 0.2655], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4468405842781067 tensor([5.5049e-04, 1.4361e-04, 5.1428e-05, 5.5241e-01, 4.4684e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2710982859134674 tensor([2.7110e-01, 7.7115e-04, 1.0401e-08, 7.2691e-01, 1.2218e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2858200967311859 tensor([0.0009, 0.1381, 0.2858, 0.0173, 0.5578], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33875715732574463 tensor([1.2953e-04, 2.0782e-02, 3.3876e-01, 2.0678e-02, 6.1965e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4716801643371582 tensor([4.8063e-05, 5.2464e-01, 4.7168e-01, 1.2926e-05, 3.6209e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41828370094299316 tensor([5.8143e-01, 2.5074e-04, 1.1515e-10, 4.1828e-01, 3.8197e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3151768743991852 tensor([8.7324e-05, 6.7911e-01, 3.1518e-01, 1.4507e-05, 5.6123e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22232668101787567 tensor([2.2694e-04, 2.2233e-01, 6.5966e-01, 1.0550e-03, 1.1674e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3627762794494629 tensor([3.0622e-05, 3.0589e-02, 6.0568e-01, 9.1933e-04, 3.6278e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27022191882133484 tensor([2.7022e-01, 5.4876e-01, 3.7503e-04, 9.1583e-02, 8.9057e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42806825041770935 tensor([0.0143, 0.4776, 0.0302, 0.0499, 0.4281], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4291284680366516 tensor([5.6870e-01, 1.7704e-03, 5.9384e-09, 4.2913e-01, 3.9957e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.46751657128334045 tensor([4.6752e-01, 1.3267e-02, 6.2817e-07, 5.0892e-01, 1.0292e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2858995199203491 tensor([7.1754e-03, 5.2436e-03, 2.1447e-04, 2.8590e-01, 7.0147e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48873624205589294 tensor([1.7009e-05, 9.3111e-03, 4.9977e-01, 2.1616e-03, 4.8874e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4060022234916687 tensor([0.0313, 0.4060, 0.0154, 0.1173, 0.4300], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38549384474754333 tensor([5.2924e-01, 7.2054e-02, 7.8787e-06, 3.8549e-01, 1.3205e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2966926693916321 tensor([2.1244e-03, 4.8845e-04, 4.5795e-05, 7.0065e-01, 2.9669e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2719239592552185 tensor([1.0593e-01, 3.5539e-02, 6.6364e-05, 5.8654e-01, 2.7192e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3483126163482666 tensor([0.0379, 0.4449, 0.0171, 0.1518, 0.3483], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20692279934883118 tensor([2.0692e-01, 1.2733e-04, 4.8302e-10, 7.9279e-01, 1.6320e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4647882878780365 tensor([0.0014, 0.0023, 0.0014, 0.4648, 0.5302], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24634906649589539 tensor([0.1633, 0.2698, 0.0006, 0.2463, 0.3199], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21964521706104279 tensor([0.0113, 0.2196, 0.0196, 0.0707, 0.6789], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34605616331100464 tensor([2.2630e-03, 1.9466e-03, 2.7568e-04, 3.4606e-01, 6.4946e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.41935446858406067 tensor([4.1935e-01, 1.3769e-03, 1.1759e-08, 5.7742e-01, 1.8484e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34282737970352173 tensor([0.0067, 0.0257, 0.0047, 0.3428, 0.6201], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3699212372303009 tensor([0.0098, 0.0279, 0.0020, 0.3699, 0.5904], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4035886526107788 tensor([1.4677e-03, 1.0969e-03, 3.3931e-04, 5.9351e-01, 4.0359e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4034678041934967 tensor([2.9365e-04, 5.6442e-01, 4.0347e-01, 2.3919e-04, 3.1577e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3386330306529999 tensor([1.7318e-04, 3.3863e-01, 6.0480e-01, 3.8977e-04, 5.6008e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 0, 3], [0, 3, 1, 4], [0, 3, 4], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 2], [2, 4, 1], [2, 1, 4, 0], [0, 3, 1], [3, 0, 4], [0, 3, 1, 4], [2, 4, 1], [2, 0, 1], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 3], [0, 2, 1, 3], [2, 1, 0], [2, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 3], [0, 3, 2], [2, 4], [3, 0, 4], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 0, 1], [2, 4, 3, 0], [2, 0, 1], [2, 1, 0], [2, 3, 0, 4], [2, 4, 3, 1], [0, 3, 1], [2, 1, 0], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3], [2, 4, 1], [0, 3, 1, 4], [2, 0, 3], [2, 4, 1, 3], [0, 1, 3], [0, 3, 2, 1], [2, 4, 1], [2, 1, 4], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3], [2, 3, 4, 0], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0, 4], [2, 0, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 0], [2, 1, 4], [3, 0, 4, 1], [0, 3, 1], [2, 1, 4, 0], [2, 4, 1], [2, 1, 4, 0], [3, 0, 4, 2], [2, 0, 3], [2, 3, 4, 0], [2, 3, 0, 4], [2, 0, 1], [3, 2, 4, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3, 0], [2, 4, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1], [0, 2, 1, 3], [0, 3, 1, 2], [0, 3, 4], [0, 3, 1, 4], [2, 0, 1], [0, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 4], [2, 4, 1], [0, 1, 3], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 3], [2, 1, 4], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4], [3, 0, 4, 2], [2, 1, 0, 4], [2, 0, 1], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 2, 3, 1], [2, 4, 3], [0, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1], [3, 0, 4], [0, 3, 4, 1], [0, 3, 1], [2, 0, 4], [0, 1, 3, 2], [2, 0, 3, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4], [2, 4, 3, 0], [3, 0, 4, 2], [2, 4, 1], [2, 1, 0], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 0], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 0], [2, 4, 1, 3], [0, 3, 4, 1], [3, 0, 4, 2], [2, 4, 1], [2, 4, 3], [0, 3, 2], [0, 3, 4], [0, 2, 3, 4], [0, 3, 1], [0, 3, 1], [0, 2, 3, 1], [0, 1, 2, 3], [0, 3, 4], [2, 4, 1], [2, 0, 3, 1], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1], [2, 1, 0], [2, 4, 1], [2, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 1, 0, 4], [0, 3, 2], [0, 3, 4, 1], [0, 1, 3], [2, 4, 1, 3], [0, 1, 2, 3], [2, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1, 4], [2, 4, 3, 1], [2, 3, 0, 4], [3, 0, 4], [3, 0, 4, 2], [2, 4, 1], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4], [0, 3, 4], [0, 3, 1], [0, 1, 2, 3], [2, 4, 3], [2, 3, 4, 0], [0, 2, 3], [2, 4, 1], [2, 4, 1], [2, 1, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1], [0, 3, 1, 4], [2, 0, 3], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0], [2, 1, 0, 4], [2, 3, 0, 4], [2, 1, 0], [2, 0, 3], [2, 1, 4], [2, 0, 1], [2, 0], [2, 3, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 3], [0, 3, 4, 1], [3, 2, 4, 0], [2, 1, 0], [2, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 1, 0, 4], [2, 0, 1], [0, 3, 1, 4], [2, 0, 1], [0, 1, 2, 3], [2, 4, 3, 0], [2, 4, 3, 0], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0], [3, 0, 4], [0, 3, 4], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 3, 2]]\n",
      "[[1, 4], [2], [1, 2], [3], [3], [0], [0], [1, 4], [0, 3], [3], [2, 4], [1, 2], [2], [0, 3], [3, 4], [4], [2], [0, 1], [4], [3, 4], [0, 3], [0], [2], [0, 1], [1, 4], [0, 1, 3], [1, 2], [2], [0, 3], [4], [4], [3], [1], [3, 4], [3, 4], [1], [0], [2, 4], [3, 4], [3], [4], [2], [2], [3], [1, 4], [0, 3], [2], [1, 4], [0], [2, 4], [4], [0, 3], [0, 3], [4], [4], [0, 1], [1], [2], [0, 3], [3], [3, 4], [2], [2], [0], [3, 4], [0, 3], [2], [2, 4], [3], [0, 3], [3], [1], [1, 4], [1], [1], [3, 4], [1], [2, 4], [0, 3], [1], [0, 3], [2], [2], [0, 3], [4], [4], [1, 2], [2], [3, 4], [1, 4], [0], [2], [1, 2], [0, 3], [2, 4], [0], [0], [3], [3], [0], [0, 3], [2], [2], [3], [3], [0], [1, 2], [1], [3], [3, 4], [3], [1], [2], [3], [0], [4], [0, 1], [1, 4], [3], [4], [2, 4], [1, 2], [2], [2, 4], [1, 3], [4], [1], [2], [4], [2], [0, 3], [1], [1], [0, 3], [3, 4], [0], [2], [1, 3], [2, 4], [3, 4], [0, 1], [2, 4], [2], [3], [1, 4], [0], [2], [1], [0, 3], [0, 1], [1, 4], [1, 2], [1], [2, 4], [2, 4], [4], [4], [1, 2], [0, 3], [4], [3], [1], [2, 4], [3, 4], [0, 3], [0, 3], [2], [2], [3], [2], [0], [0, 1], [2, 4], [3], [3], [3], [1, 4], [2], [2, 4], [0], [4], [1, 4], [2], [3], [3, 4], [0, 3], [2, 4], [2, 4], [2], [0], [1], [1, 2], [1], [0, 3], [0], [0], [1, 2], [1, 2], [2, 4], [4], [0, 1], [1], [1, 4], [0, 3], [0, 3], [3, 4], [2], [2], [2, 4], [2], [1, 4], [2], [0, 3], [3, 4], [3], [1], [3, 4], [1, 4], [0, 3], [3, 4], [1, 3, 4], [1], [2], [0], [0], [0], [1, 4], [2], [1], [3, 4], [0, 3], [1], [4], [3], [4], [3], [3, 4], [2], [3, 4], [4], [1], [1], [2], [3], [3, 4], [1, 2], [1, 2], [2], [0], [4]]\n",
      "NL_pred of 4th iteration [[2, 4, 0, 1], [2, 1, 0, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.6566352844238281  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.6508059501647949  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.6407549381256104  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.6285295486450195  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.6160454750061035  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.6046762466430664  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.595118522644043  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.5875194072723389  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.5816938877105713  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.577325701713562  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.574084997177124  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.5717206001281738  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.5699703097343445  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.5686511993408203  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.5676448345184326  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.5668660402297974  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.5662533044815063  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.5657622814178467  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.5653613209724426  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.5650267004966736  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.2048814296722412 tensor([0.1379, 0.2049, 0.0009, 0.3526, 0.3038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06971582025289536 tensor([0.0138, 0.5282, 0.0697, 0.0673, 0.3210], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21266712248325348 tensor([0.0596, 0.2127, 0.0057, 0.4537, 0.2684], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3971882164478302 tensor([3.9719e-01, 3.2094e-03, 6.8564e-08, 5.9691e-01, 2.6928e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15179449319839478 tensor([1.1305e-05, 1.9625e-03, 1.5179e-01, 3.6739e-03, 8.4256e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.032874878495931625 tensor([3.7873e-03, 9.4574e-01, 3.2875e-02, 8.6428e-04, 1.6731e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4677658975124359 tensor([5.3212e-01, 9.4955e-05, 2.6374e-11, 4.6777e-01, 1.6494e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09091418981552124 tensor([1.1285e-01, 2.0886e-02, 2.8766e-05, 7.7532e-01, 9.0914e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026253318414092064 tensor([6.4627e-01, 2.6253e-02, 7.1878e-07, 3.2629e-01, 1.1793e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.046299729496240616 tensor([8.3061e-03, 3.6608e-04, 2.7771e-06, 9.4503e-01, 4.6300e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2060786485671997 tensor([2.0608e-01, 1.3930e-04, 6.4250e-10, 7.9334e-01, 4.4450e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.029130568727850914 tensor([9.6843e-01, 2.9131e-02, 2.6139e-09, 2.4387e-03, 3.9257e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2269253432750702 tensor([0.0902, 0.2431, 0.0025, 0.4373, 0.2269], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012570435181260109 tensor([3.3750e-01, 1.2570e-02, 1.3054e-06, 6.4080e-01, 9.1243e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.056358009576797485 tensor([0.0088, 0.8568, 0.0564, 0.0060, 0.0721], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4355989694595337 tensor([4.3560e-01, 6.0120e-03, 1.6799e-07, 5.5354e-01, 4.8447e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.054298628121614456 tensor([3.8490e-02, 9.1686e-03, 6.9824e-05, 8.9797e-01, 5.4299e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01671404391527176 tensor([7.1987e-02, 2.3657e-03, 1.0965e-06, 9.0893e-01, 1.6714e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16357086598873138 tensor([6.3439e-05, 5.1606e-03, 1.6357e-01, 2.5368e-02, 8.0584e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027470842003822327 tensor([2.2626e-03, 9.2863e-05, 2.3632e-06, 9.7017e-01, 2.7471e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10293437540531158 tensor([1.7826e-01, 1.0293e-01, 2.2759e-04, 5.7320e-01, 1.4537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33432480692863464 tensor([3.3432e-01, 7.7276e-04, 6.2906e-09, 6.6423e-01, 6.6850e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08031634986400604 tensor([1.5899e-01, 9.4893e-02, 3.0399e-04, 6.6550e-01, 8.0316e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04011935740709305 tensor([1.4226e-05, 6.4894e-04, 4.0119e-02, 2.3570e-02, 9.3565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31899750232696533 tensor([3.1900e-01, 7.5991e-03, 5.3474e-07, 6.7084e-01, 2.5604e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18631570041179657 tensor([1.8632e-01, 2.2451e-04, 2.4337e-09, 8.1299e-01, 4.6557e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08441555500030518 tensor([8.8127e-01, 8.4416e-02, 3.7162e-07, 3.3733e-02, 5.8162e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.37393975257873535 tensor([3.7394e-01, 7.0857e-05, 4.5544e-11, 6.2597e-01, 2.0191e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06979219615459442 tensor([3.2381e-03, 1.8457e-04, 4.4400e-06, 9.2678e-01, 6.9792e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026736728847026825 tensor([2.5151e-02, 5.0801e-04, 6.5643e-07, 9.4760e-01, 2.6737e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19523057341575623 tensor([1.9523e-01, 1.2714e-04, 5.7614e-10, 8.0435e-01, 2.8869e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021494124084711075 tensor([0.0130, 0.0863, 0.0215, 0.4122, 0.4671], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3116111159324646 tensor([3.1161e-01, 4.7041e-03, 2.3432e-07, 6.8169e-01, 1.9924e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07247022539377213 tensor([9.5642e-02, 7.2470e-02, 3.4043e-04, 7.2047e-01, 1.1108e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08945140242576599 tensor([3.2993e-02, 4.7601e-03, 3.0586e-05, 8.7276e-01, 8.9451e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02324090339243412 tensor([0.0026, 0.0325, 0.0232, 0.1584, 0.7832], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4893895089626312 tensor([4.8939e-01, 6.2062e-05, 1.2114e-11, 5.1054e-01, 1.0607e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3902832269668579 tensor([3.9028e-01, 3.8775e-03, 1.0328e-07, 6.0387e-01, 1.9660e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.45380303263664246 tensor([5.3526e-01, 8.5625e-03, 1.9766e-07, 4.5380e-01, 2.3792e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14546161890029907 tensor([0.0037, 0.7584, 0.1455, 0.0059, 0.0865], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03454474359750748 tensor([1.6271e-02, 8.8508e-04, 3.8442e-06, 9.4830e-01, 3.4545e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07463069260120392 tensor([7.7360e-02, 7.4631e-02, 5.4124e-04, 6.2004e-01, 2.2742e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06416235864162445 tensor([0.0237, 0.6744, 0.0642, 0.0901, 0.1476], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2626613974571228 tensor([2.6266e-01, 5.5718e-04, 6.2780e-09, 7.3651e-01, 2.6947e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01260000467300415 tensor([2.5795e-05, 2.8159e-04, 1.2600e-02, 9.1077e-02, 8.9602e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19343766570091248 tensor([1.9344e-01, 7.1736e-04, 2.1755e-08, 8.0356e-01, 2.2838e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0396999828517437 tensor([0.0317, 0.6781, 0.0397, 0.0970, 0.1535], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10830523073673248 tensor([4.0773e-02, 1.2458e-02, 1.2924e-04, 8.3833e-01, 1.0831e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1256941854953766 tensor([8.6246e-01, 1.2569e-01, 2.6340e-07, 1.1583e-02, 2.6517e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14491036534309387 tensor([0.1206, 0.1449, 0.0011, 0.5448, 0.1886], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0221694502979517 tensor([6.1604e-05, 2.6688e-03, 2.2169e-02, 9.7838e-03, 9.6532e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026176132261753082 tensor([0.0151, 0.8267, 0.0262, 0.0127, 0.1194], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012622891925275326 tensor([0.0114, 0.0627, 0.0126, 0.4594, 0.4539], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010247796773910522 tensor([1.3764e-01, 1.0248e-02, 5.4366e-06, 8.3869e-01, 1.3418e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1814347207546234 tensor([1.8143e-01, 1.2391e-04, 5.4515e-10, 8.1820e-01, 2.4344e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26561248302459717 tensor([2.6561e-01, 1.6542e-03, 5.1217e-08, 7.3209e-01, 6.4717e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02509515918791294 tensor([3.0955e-02, 7.0975e-04, 8.7956e-07, 9.4324e-01, 2.5095e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04034620150923729 tensor([2.9899e-01, 4.0346e-02, 1.6150e-05, 6.4489e-01, 1.5758e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01297687366604805 tensor([0.0098, 0.0568, 0.0130, 0.4773, 0.4431], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02799334190785885 tensor([1.0139e-01, 6.5874e-03, 6.7689e-06, 8.6402e-01, 2.7993e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05386325344443321 tensor([6.4533e-01, 5.3863e-02, 4.0947e-06, 2.9340e-01, 7.4006e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17081831395626068 tensor([4.6632e-06, 1.1036e-03, 1.7082e-01, 3.2487e-03, 8.2482e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11930835992097855 tensor([0.2784, 0.3200, 0.0005, 0.2818, 0.1193], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.47932642698287964 tensor([4.7933e-01, 8.1310e-03, 2.8588e-07, 5.0809e-01, 4.4555e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012304951436817646 tensor([7.0619e-01, 1.2305e-02, 1.0352e-07, 2.8059e-01, 9.1925e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1394369900226593 tensor([0.0267, 0.1394, 0.0051, 0.1601, 0.6687], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0818801075220108 tensor([0.0063, 0.8395, 0.0819, 0.0080, 0.0644], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.043939538300037384 tensor([0.0071, 0.1706, 0.0439, 0.1461, 0.6324], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.038863398134708405 tensor([4.3043e-05, 2.8285e-03, 3.8863e-02, 1.1122e-02, 9.4714e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03286689147353172 tensor([0.0134, 0.8321, 0.0329, 0.0156, 0.1060], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22527483105659485 tensor([2.2527e-01, 1.0714e-04, 3.1567e-10, 7.7455e-01, 6.9485e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1136927530169487 tensor([0.0013, 0.0401, 0.1137, 0.1353, 0.7096], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05294547602534294 tensor([8.5303e-04, 2.3166e-05, 1.0681e-06, 9.4618e-01, 5.2945e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.334579199552536 tensor([3.3458e-01, 4.7947e-03, 2.2849e-07, 6.5611e-01, 4.5146e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4275651276111603 tensor([5.6473e-01, 5.2819e-03, 8.5639e-08, 4.2757e-01, 2.4212e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11474042385816574 tensor([7.6426e-01, 1.1474e-01, 3.5427e-06, 1.1721e-01, 3.7877e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.037150364369153976 tensor([0.0012, 0.0216, 0.0372, 0.1495, 0.7905], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25059956312179565 tensor([0.0919, 0.2506, 0.0016, 0.3229, 0.3330], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03418290615081787 tensor([3.1238e-06, 1.3227e-04, 3.4183e-02, 2.0732e-02, 9.4495e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03218130022287369 tensor([3.2085e-01, 3.2181e-02, 6.5380e-06, 6.1398e-01, 3.2977e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03266020119190216 tensor([8.5830e-04, 1.3268e-05, 4.1519e-07, 9.6647e-01, 3.2660e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20748206973075867 tensor([2.0748e-01, 6.2782e-05, 1.3525e-10, 7.9237e-01, 8.4069e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01712646521627903 tensor([0.0134, 0.0866, 0.0171, 0.3487, 0.5342], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018727155402302742 tensor([0.0020, 0.0132, 0.0187, 0.4227, 0.5434], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07045843452215195 tensor([9.9256e-04, 9.2276e-01, 7.0458e-02, 1.8421e-04, 5.6039e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4870205223560333 tensor([4.8702e-01, 2.8520e-05, 2.5668e-12, 5.1295e-01, 3.8266e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04171665385365486 tensor([1.2442e-03, 9.4982e-01, 4.1717e-02, 1.4484e-04, 7.0780e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1271861046552658 tensor([0.0078, 0.4601, 0.1272, 0.0370, 0.3679], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0890577882528305 tensor([5.5749e-04, 4.2508e-02, 8.9058e-02, 1.8115e-02, 8.4976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06487088650465012 tensor([6.3069e-01, 6.4871e-02, 4.9091e-06, 2.8938e-01, 1.5053e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16662093997001648 tensor([0.1109, 0.1666, 0.0010, 0.5176, 0.2039], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4818631708621979 tensor([4.8186e-01, 1.5646e-04, 8.4542e-11, 5.1795e-01, 3.0687e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3976157605648041 tensor([3.9762e-01, 8.7959e-04, 6.3326e-09, 6.0075e-01, 7.5923e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10713917016983032 tensor([1.5752e-02, 6.3118e-04, 2.9476e-06, 8.7647e-01, 1.0714e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.052385251969099045 tensor([3.4315e-04, 1.0942e-02, 5.2385e-02, 5.1665e-02, 8.8466e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07401169091463089 tensor([1.2711e-01, 7.4012e-02, 2.8738e-04, 6.8257e-01, 1.1603e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.49540695548057556 tensor([4.9541e-01, 2.9895e-03, 3.4790e-08, 5.0080e-01, 8.0840e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020016085356473923 tensor([2.4619e-03, 3.5520e-05, 3.4292e-07, 9.7749e-01, 2.0016e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016351833939552307 tensor([1.1666e-01, 1.7266e-03, 3.0160e-07, 8.6526e-01, 1.6352e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06689822673797607 tensor([1.2933e-01, 6.6898e-02, 2.5231e-04, 7.2792e-01, 7.5600e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15748658776283264 tensor([1.5749e-01, 1.2184e-05, 8.4624e-12, 8.4249e-01, 1.3132e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.050578001886606216 tensor([2.0285e-03, 1.5450e-04, 9.5893e-06, 9.4723e-01, 5.0578e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021158061921596527 tensor([2.9593e-01, 2.1158e-02, 4.9698e-06, 6.4575e-01, 3.7154e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06722155958414078 tensor([0.0678, 0.0672, 0.0006, 0.5772, 0.2871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07698275148868561 tensor([4.6213e-03, 1.9549e-04, 2.6584e-06, 9.1820e-01, 7.6983e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33328813314437866 tensor([3.3329e-01, 1.2370e-04, 1.8013e-10, 6.6645e-01, 1.4124e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08096303790807724 tensor([1.1918e-02, 2.2062e-03, 4.4638e-05, 9.0487e-01, 8.0963e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06467413902282715 tensor([1.7531e-02, 2.2186e-03, 1.5386e-05, 9.1556e-01, 6.4674e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02451486326754093 tensor([1.9644e-03, 5.9544e-05, 1.3986e-06, 9.7346e-01, 2.4515e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05374705791473389 tensor([0.0067, 0.8754, 0.0537, 0.0046, 0.0595], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10817907750606537 tensor([0.0061, 0.6931, 0.1082, 0.0140, 0.1787], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 0, 3], [0, 3, 1, 4], [0, 3, 4, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 2], [2, 4, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 1, 4], [2, 4, 1], [2, 0, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 3, 1], [0, 2, 1, 3], [2, 1, 0, 4], [2, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 3, 1], [0, 3, 2], [2, 4, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 0, 1], [2, 4, 3, 0], [2, 0, 1, 4], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [2, 1, 0, 4], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3, 1], [2, 4, 1], [0, 3, 1, 4], [2, 0, 3, 4], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 2, 1], [2, 4, 1], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0, 4], [2, 0, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4, 1], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1], [2, 1, 4, 0], [3, 0, 4, 2], [2, 0, 3, 1], [2, 3, 4, 0], [2, 3, 0, 4], [2, 0, 1, 4], [3, 2, 4, 0], [0, 3, 1, 2], [2, 4, 1], [2, 4, 3, 0], [2, 4, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1], [0, 2, 1, 3], [0, 3, 1, 2], [0, 3, 4, 2], [0, 3, 1, 4], [2, 0, 1, 4], [0, 2, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 4, 2], [2, 4, 1], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4, 2], [3, 0, 4, 2], [2, 1, 0, 4], [2, 0, 1, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 2, 3, 1], [2, 4, 3, 1], [0, 2, 3, 1], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 4, 1], [0, 1, 3, 2], [2, 0, 3, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 0], [3, 0, 4, 2], [2, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 0, 1, 4], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 1, 3], [0, 3, 4, 1], [3, 0, 4, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 2, 1], [0, 3, 4, 2], [0, 2, 3, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 2, 3, 1], [0, 1, 2, 3], [0, 3, 4, 2], [2, 4, 1], [2, 0, 3, 1], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 2], [2, 1, 0, 4], [2, 4, 1], [2, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 1, 0, 4], [0, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 0, 3, 1], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 0, 4], [2, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 3, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 1], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 4, 2], [0, 3, 1, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 3, 4, 0], [0, 2, 3, 1], [2, 4, 1], [2, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 0, 3, 1], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 3, 0, 4], [2, 1, 0, 4], [2, 0, 3, 1], [2, 1, 4, 0], [2, 0, 1, 4], [2, 0, 1], [2, 3, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 4, 1], [3, 2, 4, 0], [2, 1, 0, 4], [2, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 1, 0, 4], [2, 0, 1, 4], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 2, 3], [2, 4, 3, 0], [2, 4, 3, 0], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3, 4, 2], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 3, 2]]\n",
      "[[1, 4], [2], [1], [3], [3], [0], [0], [1, 4], [0, 3], [3], [4], [1], [2], [0, 3], [3], [4], [2], [0], [4], [3], [0, 3], [0], [2], [0], [1, 4], [0, 3], [1], [2], [0, 3], [4], [4], [3], [1], [3], [3], [1], [0], [4], [3], [3], [4], [2], [2], [3], [4], [0, 3], [2], [1], [0], [4], [4], [0, 3], [3], [4], [4], [0], [1], [2], [0, 3], [3], [3], [2], [2], [0], [3], [3], [2], [4], [3], [0, 3], [3], [1], [4], [1], [1], [3], [1], [4], [0, 3], [1], [0, 3], [2], [2], [0, 3], [4], [4], [1], [2], [3], [4], [0], [2], [1], [0, 3], [4], [0], [0], [3], [3], [0], [3], [2], [2], [3], [3], [0], [1], [1], [3], [3], [3], [1], [2], [3], [0], [4], [0], [4], [3], [4], [4], [1], [2], [4], [3], [4], [1], [2], [4], [2], [3], [1], [1], [0, 3], [3], [0], [2], [3], [4], [3], [0], [4], [2], [3], [1], [0], [2], [1], [0, 3], [0], [4], [1], [1], [4], [4], [4], [4], [1], [0, 3], [4], [3], [1], [4], [3], [0, 3], [0, 3], [2], [2], [3], [2], [0], [0], [4], [3], [3], [3], [1, 4], [2], [4], [0], [4], [4], [2], [3], [3], [0, 3], [4], [4], [2], [0], [1], [1], [1], [0, 3], [0], [0], [1], [1], [4], [4], [0], [1], [4], [0, 3], [0, 3], [3], [2], [2], [4], [2], [4], [2], [0, 3], [3], [3], [1], [3], [4], [3], [3], [3, 4], [1], [2], [0], [0], [0], [4], [2], [1], [3], [0, 3], [1], [4], [3], [4], [3], [3], [2], [3], [4], [1], [1], [2], [3], [3], [1], [1], [2], [0], [4]]\n",
      "NL_pred of 5th iteration [[0, 3, 4, 2], [0, 3, 1, 2], [3, 0, 4, 2], [2, 0, 1, 4], [2, 4, 3, 1], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 1], [3, 0, 4, 2], [2, 0, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [2, 1, 0, 4], [2, 0, 3, 1], [2, 0, 3, 4], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 3, 1], [2, 0, 1, 4], [2, 1, 0, 4], [2, 1, 4, 0], [0, 3, 1, 2], [2, 0, 3, 1], [2, 0, 1, 4], [0, 3, 1, 2], [0, 3, 4, 2], [2, 0, 1, 4], [0, 2, 3, 1], [0, 3, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 3, 4, 2], [2, 0, 1, 4], [2, 4, 3, 1], [0, 2, 3, 1], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 1, 2], [2, 0, 4, 1], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 0, 1], [0, 3, 1, 2], [2, 0, 1, 4], [2, 4, 3, 1], [0, 3, 1, 2], [2, 3, 0, 4], [2, 4, 3, 1], [0, 3, 2, 1], [0, 3, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 4, 2], [0, 3, 1, 2], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [0, 1, 3, 2], [2, 0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 1, 2], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 4, 2], [0, 3, 1, 2], [2, 4, 3, 1], [0, 2, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 0, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 0, 3, 1], [2, 1, 4, 0], [2, 0, 1, 4], [2, 0, 1], [0, 2, 3, 1], [2, 1, 0, 4], [2, 0, 1, 4], [2, 0, 1, 4], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3, 4, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.014127075672149658  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.01411494045030503  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.01409256032535008  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.014062120800926572  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.014025862727846419  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.01398590916679019  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.013944144759859358  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.013902069557280768  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.01386087281363351  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.013821427311216081  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.013784275168464297  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.013749737115133377  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01371791674977257  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.013688833940596808  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.013662417729695639  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.013638509171349662  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.013616940804890223  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.013597501175744193  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.013579994440078735  Accuracy on Support set:0.0\n",
      "torch.Size([84, 2048]) torch.Size([84])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.013564227592377435  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 0, 3], [0, 3, 1, 4], [0, 3, 4, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 2], [2, 4, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 1, 4], [2, 4, 1], [2, 0, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 3, 1], [0, 2, 1, 3], [2, 1, 0, 4], [2, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 3, 1], [0, 3, 2], [2, 4, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 0, 1], [2, 4, 3, 0], [2, 0, 1, 4], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [2, 1, 0, 4], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 3, 1], [2, 4, 1], [0, 3, 1, 4], [2, 0, 3, 4], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 2, 1], [2, 4, 1], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0, 4], [2, 0, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4, 1], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1], [2, 1, 4, 0], [3, 0, 4, 2], [2, 0, 3, 1], [2, 3, 4, 0], [2, 3, 0, 4], [2, 0, 1, 4], [3, 2, 4, 0], [0, 3, 1, 2], [2, 4, 1], [2, 4, 3, 0], [2, 4, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1], [0, 2, 1, 3], [0, 3, 1, 2], [0, 3, 4, 2], [0, 3, 1, 4], [2, 0, 1, 4], [0, 2, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 4, 2], [2, 4, 1], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 1, 0], [2, 1, 4, 0], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4, 2], [3, 0, 4, 2], [2, 1, 0, 4], [2, 0, 1, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 2, 3, 1], [2, 4, 3, 1], [0, 2, 3, 1], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 0, 4, 1], [0, 1, 3, 2], [2, 0, 3, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 0], [3, 0, 4, 2], [2, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 4, 0, 1], [0, 3, 1, 2], [2, 0, 1, 4], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 0, 4], [2, 4, 1, 3], [0, 3, 4, 1], [3, 0, 4, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 2, 1], [0, 3, 4, 2], [0, 2, 3, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 2, 3, 1], [0, 1, 2, 3], [0, 3, 4, 2], [2, 4, 1], [2, 0, 3, 1], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 2], [2, 1, 0, 4], [2, 4, 1], [2, 4, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 1, 0, 4], [0, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 0, 3, 1], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 0, 4], [2, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 3, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 1], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 4, 2], [0, 3, 1, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 3, 4, 0], [0, 2, 3, 1], [2, 4, 1], [2, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 0, 3, 1], [0, 3, 4, 1], [2, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 3, 0, 4], [2, 1, 0, 4], [2, 0, 3, 1], [2, 1, 4, 0], [2, 0, 1, 4], [2, 0, 1], [2, 3, 0, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 4, 1], [3, 2, 4, 0], [2, 1, 0, 4], [2, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 1, 0, 4], [2, 0, 1, 4], [0, 3, 1, 4], [2, 0, 1, 4], [0, 1, 2, 3], [2, 4, 3, 0], [2, 4, 3, 0], [0, 3, 4, 1], [2, 1, 0, 4], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3, 4, 2], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 3, 2]]\n",
      "POSITION :  [[1, 4], [2], [1], [3], [3], [0], [0], [1, 4], [0, 3], [3], [4], [1], [2], [0, 3], [3], [4], [2], [0], [4], [3], [0, 3], [0], [2], [0], [1, 4], [0, 3], [1], [2], [0, 3], [4], [4], [3], [1], [3], [3], [1], [0], [4], [3], [3], [4], [2], [2], [3], [4], [0, 3], [2], [1], [0], [4], [4], [0, 3], [3], [4], [4], [0], [1], [2], [0, 3], [3], [3], [2], [2], [0], [3], [3], [2], [4], [3], [0, 3], [3], [1], [4], [1], [1], [3], [1], [4], [0, 3], [1], [0, 3], [2], [2], [0, 3], [4], [4], [1], [2], [3], [4], [0], [2], [1], [0, 3], [4], [0], [0], [3], [3], [0], [3], [2], [2], [3], [3], [0], [1], [1], [3], [3], [3], [1], [2], [3], [0], [4], [0], [4], [3], [4], [4], [1], [2], [4], [3], [4], [1], [2], [4], [2], [3], [1], [1], [0, 3], [3], [0], [2], [3], [4], [3], [0], [4], [2], [3], [1], [0], [2], [1], [0, 3], [0], [4], [1], [1], [4], [4], [4], [4], [1], [0, 3], [4], [3], [1], [4], [3], [0, 3], [0, 3], [2], [2], [3], [2], [0], [0], [4], [3], [3], [3], [1, 4], [2], [4], [0], [4], [4], [2], [3], [3], [0, 3], [4], [4], [2], [0], [1], [1], [1], [0, 3], [0], [0], [1], [1], [4], [4], [0], [1], [4], [0, 3], [0, 3], [3], [2], [2], [4], [2], [4], [2], [0, 3], [3], [3], [1], [3], [4], [3], [3], [3, 4], [1], [2], [0], [0], [0], [4], [2], [1], [3], [0, 3], [1], [4], [3], [4], [3], [3], [2], [3], [4], [1], [1], [2], [3], [3], [1], [1], [2], [0], [4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.44\n",
      "tensor([2, 1, 3, 3, 0, 0, 3, 4, 1, 2, 3, 4, 2, 0, 4, 3, 0, 2, 0, 1, 2, 4, 4, 3,\n",
      "        1, 3, 3, 1, 0, 4, 3, 3, 4, 2, 2, 3, 4, 2, 1, 0, 4, 4, 3, 4, 4, 0, 1, 2,\n",
      "        3, 3, 2, 2, 0, 3, 3, 2, 4, 3, 3, 1, 4, 1, 1, 3, 1, 4, 1, 2, 2, 4, 4, 1,\n",
      "        2, 3, 4, 0, 2, 1, 4, 0, 0, 3, 3, 0, 3, 2, 2, 3, 3, 0, 1, 1, 3, 3, 3, 1,\n",
      "        2, 3, 0, 4, 0, 4, 3, 4, 4, 1, 2, 4, 3, 4, 1, 2, 4, 2, 3, 1, 1, 3, 0, 2,\n",
      "        3, 4, 3, 0, 4, 2, 3, 1, 0, 2, 1, 0, 4, 1, 1, 4, 4, 4, 4, 1, 4, 3, 1, 4,\n",
      "        3, 2, 2, 3, 2, 0, 0, 4, 3, 3, 3, 2, 4, 0, 4, 4, 2, 3, 3, 4, 4, 2, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 4, 4, 0, 1, 4, 3, 2, 2, 4, 2, 4, 2, 3, 3, 1, 3, 4, 3,\n",
      "        3, 1, 2, 0, 0, 0, 4, 2, 1, 3, 1, 4, 3, 4, 3, 3, 2, 3, 4, 1, 1, 2, 3, 3,\n",
      "        1, 1, 2, 0, 4])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 87.33031674208145\n",
      "Epoch: 1  Loss: 93.66515837104072\n",
      "Epoch: 2  Loss: 92.3076923076923\n",
      "Epoch: 3  Loss: 95.47511312217195\n",
      "Epoch: 4  Loss: 98.64253393665159\n",
      "Epoch: 5  Loss: 98.64253393665159\n",
      "Epoch: 6  Loss: 99.5475113122172\n",
      "Epoch: 7  Loss: 100.0\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 9/15 [07:11<05:10, 51.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  44.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 5.881854108925909  Accuracy on Support set:32.0\n",
      "Train_Epoch: 1  Train_Loss: 3.8027260949555783  Accuracy on Support set:44.0\n",
      "Train_Epoch: 2  Train_Loss: 2.0784831514209507  Accuracy on Support set:72.0\n",
      "Train_Epoch: 3  Train_Loss: 0.95774218223989  Accuracy on Support set:84.0\n",
      "Train_Epoch: 4  Train_Loss: 0.5846771774068474  Accuracy on Support set:92.0\n",
      "Train_Epoch: 5  Train_Loss: 0.37637693122029303  Accuracy on Support set:96.0\n",
      "Train_Epoch: 6  Train_Loss: 0.23479886636137962  Accuracy on Support set:96.0\n",
      "Train_Epoch: 7  Train_Loss: 0.1717324995994568  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.13053199172019958  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.10574497997760773  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.09062820836901665  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.07998287916183472  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.07208779409527778  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.06584957018494605  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.06084160253405571  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.056610285341739654  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.05301611796021462  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.04989034816622734  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.0471478071808815  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.044729886054992674  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.042561628818511964  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.040613706186413766  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.03884958185255528  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.03723257906734943  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.0357514151930809  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.03439027544111013  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.03313520528376102  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.03197000682353973  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.030885490253567696  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.029876066632568837  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.028931345045566558  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.028044547811150553  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.027212434150278568  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.026428976133465766  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.025691288858652114  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.024997256733477115  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.024340023025870322  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.023715623393654822  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.023119955472648144  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.02255844824016094  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.022020371928811074  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.02150986693799496  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.0210228019580245  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.020553606487810613  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.020112027749419212  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.019685189165174963  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.019277392067015172  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.01888703439384699  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.0185126893222332  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.018154579140245916  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  39.33333333333333\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.00011050085595343262 tensor([7.5677e-01, 8.5187e-02, 1.1050e-04, 1.4102e-01, 1.6913e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002294104779139161 tensor([0.0023, 0.1734, 0.1468, 0.0033, 0.6742], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014293131418526173 tensor([0.0143, 0.0976, 0.0216, 0.0310, 0.8355], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.484581176000262e-11 tensor([3.9330e-03, 1.7862e-07, 9.4846e-11, 9.9571e-01, 3.5187e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018366549629718065 tensor([0.0018, 0.1517, 0.2098, 0.0038, 0.6329], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013448799028992653 tensor([0.0361, 0.4157, 0.0276, 0.0134, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0547756573942024e-05 tensor([1.3118e-04, 4.0067e-01, 5.7992e-01, 1.0548e-05, 1.9266e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.10877488990991e-08 tensor([9.1088e-08, 1.2541e-03, 9.5852e-01, 1.7653e-06, 4.0221e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7948192976291466e-07 tensor([1.8042e-02, 4.0069e-05, 1.7948e-07, 9.7001e-01, 1.1903e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.07118709644783e-08 tensor([8.0712e-08, 4.3836e-04, 8.1584e-01, 1.4170e-05, 1.8371e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.34048633906059e-05 tensor([4.3405e-05, 2.3228e-02, 3.9379e-01, 2.2415e-04, 5.8271e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2876578693976626e-05 tensor([1.2877e-05, 2.4044e-02, 7.5527e-01, 3.9778e-05, 2.2063e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.491478987096343e-06 tensor([5.4915e-06, 2.4650e-02, 9.0708e-01, 1.0234e-05, 6.8253e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.226601568735333e-10 tensor([9.3750e-01, 1.6378e-04, 4.2266e-10, 6.2320e-02, 1.3603e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.704329169020639e-07 tensor([5.7043e-07, 1.2397e-02, 9.8366e-01, 5.9901e-07, 3.9461e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.215745997906197e-07 tensor([7.6161e-01, 3.9435e-03, 6.2157e-07, 2.2910e-01, 5.3498e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00026580790290609 tensor([2.3297e-03, 9.2217e-04, 2.6581e-04, 1.0695e-01, 8.8953e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.41491192002286e-07 tensor([7.4149e-07, 1.2626e-02, 9.7920e-01, 1.0178e-06, 8.1703e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.682282377961201e-08 tensor([9.1991e-02, 8.1367e-05, 5.6823e-08, 9.0515e-01, 2.7823e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005992446676827967 tensor([5.9924e-04, 1.0889e-01, 2.4778e-01, 8.9685e-04, 6.4184e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.8810067053418607e-05 tensor([7.6818e-01, 4.5881e-02, 3.8810e-05, 1.6721e-01, 1.8695e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8088273101568575e-09 tensor([8.4482e-01, 1.8670e-04, 1.8088e-09, 1.5484e-01, 1.4888e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3596908502222504e-05 tensor([7.2785e-04, 8.4467e-01, 1.4937e-01, 1.3597e-05, 5.2182e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008517177193425596 tensor([8.5172e-04, 2.3954e-02, 3.3427e-02, 6.0351e-03, 9.3573e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0362658940721303e-05 tensor([1.0363e-05, 1.6126e-03, 1.2917e-01, 1.3832e-03, 8.6783e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02194414660334587 tensor([0.0219, 0.2211, 0.0299, 0.0264, 0.7006], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000502184615470469 tensor([9.7031e-02, 8.9842e-01, 1.1668e-03, 5.0218e-04, 2.8833e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.143003985111136e-05 tensor([1.6960e-04, 3.5186e-01, 6.1498e-01, 2.1430e-05, 3.2970e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7570405574929282e-08 tensor([1.0112e-01, 5.3774e-05, 1.7570e-08, 8.9506e-01, 3.7636e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001549129723571241 tensor([0.0444, 0.0308, 0.0015, 0.1300, 0.7931], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.953855189640308e-06 tensor([9.5238e-01, 2.6035e-02, 1.9539e-06, 2.1052e-02, 5.3449e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.932346371802396e-08 tensor([1.9998e-01, 9.2670e-05, 1.9323e-08, 7.9875e-01, 1.1714e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0024306857958436012 tensor([0.0024, 0.1870, 0.1939, 0.0041, 0.6126], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3389303294152342e-07 tensor([2.3389e-07, 2.4496e-06, 9.9015e-04, 3.1877e-03, 9.9582e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.4909901636126506e-09 tensor([3.4910e-09, 3.3662e-04, 9.9238e-01, 8.4358e-08, 7.2813e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3816797705956674e-10 tensor([9.8884e-01, 1.7466e-04, 1.3817e-10, 1.0985e-02, 3.8757e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000523684429936111 tensor([2.7822e-02, 9.4743e-01, 1.2724e-02, 5.2368e-04, 1.1500e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.172703367861686e-06 tensor([7.7149e-04, 9.2946e-01, 6.8554e-02, 5.1727e-06, 1.2096e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.471680555198418e-08 tensor([1.3465e-02, 7.1107e-06, 1.4717e-08, 9.8332e-01, 3.2067e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9711904315045103e-05 tensor([6.0925e-01, 1.8840e-02, 1.9712e-05, 3.3887e-01, 3.3024e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.959126063246003e-08 tensor([4.2593e-02, 5.2214e-05, 8.9591e-08, 9.4645e-01, 1.0902e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4792802858210052e-06 tensor([2.7484e-01, 1.5112e-03, 1.4793e-06, 7.1090e-01, 1.2750e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.262189122528071e-06 tensor([8.1466e-06, 5.3306e-02, 9.1441e-01, 4.2622e-06, 3.2269e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.590120609310964e-12 tensor([6.2705e-02, 2.8638e-07, 2.5901e-12, 9.3728e-01, 1.4700e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.446481322403997e-05 tensor([2.5551e-04, 3.0005e-01, 6.3090e-01, 7.4465e-05, 6.8720e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.896069291746244e-05 tensor([7.8961e-05, 5.0017e-04, 3.2567e-03, 1.1738e-02, 9.8443e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6521343754877238e-12 tensor([9.9510e-01, 2.5633e-05, 1.6521e-12, 4.8784e-03, 1.0220e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005262205377221107 tensor([0.4850, 0.4915, 0.0005, 0.0121, 0.0108], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.149301963072503e-07 tensor([3.4203e-07, 2.1493e-07, 3.1269e-05, 2.0254e-02, 9.7971e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00019157851056661457 tensor([1.9158e-04, 2.0871e-02, 1.8642e-01, 2.2611e-03, 7.9025e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.032538943235295e-09 tensor([8.4513e-01, 1.5672e-04, 1.0325e-09, 1.5466e-01, 5.5909e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.524101531293127e-07 tensor([9.5124e-01, 4.6090e-02, 8.5241e-07, 2.4334e-03, 2.3364e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2519895790319424e-05 tensor([8.1084e-04, 8.3631e-01, 1.5932e-01, 1.2520e-05, 3.5434e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3482463802793063e-05 tensor([8.8063e-01, 7.7997e-02, 2.3482e-05, 3.3076e-02, 8.2723e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01556574646383524 tensor([0.1524, 0.6638, 0.0156, 0.0266, 0.1416], grad_fn=<SoftmaxBackward0>)\n",
      "0 9.880406715012668e-09 tensor([9.8804e-09, 3.5419e-04, 9.7692e-01, 4.3424e-07, 2.2721e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.007757979707094e-06 tensor([4.0078e-06, 1.2322e-02, 8.9234e-01, 2.4999e-05, 9.5311e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.0346888049971312e-06 tensor([2.9327e-05, 2.4724e-01, 7.4581e-01, 2.0347e-06, 6.9150e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.972156387670566e-08 tensor([1.2391e-01, 8.6108e-05, 2.9722e-08, 8.7357e-01, 2.4314e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010582865215837955 tensor([0.0011, 0.0104, 0.0094, 0.0102, 0.9690], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.080172619884252e-08 tensor([9.8044e-01, 4.9405e-03, 8.0802e-08, 1.4513e-02, 1.0256e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.565426570479758e-05 tensor([5.5679e-01, 3.1680e-02, 4.5654e-05, 2.2486e-01, 1.8662e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5364281352958642e-05 tensor([8.8260e-01, 4.9231e-02, 1.5364e-05, 5.5917e-02, 1.2233e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.951041880005505e-06 tensor([8.9770e-01, 4.6798e-02, 7.9510e-06, 4.4175e-02, 1.1323e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.96133808983268e-10 tensor([5.9613e-10, 1.4388e-05, 7.6094e-01, 1.5914e-06, 2.3904e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.7100774926204783e-13 tensor([6.4875e-01, 1.2344e-06, 3.7101e-13, 3.5125e-01, 9.5847e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.941905328654684e-05 tensor([1.8375e-01, 8.2055e-03, 5.9419e-05, 5.4303e-01, 2.6495e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.178272527293302e-05 tensor([1.9544e-04, 3.1996e-01, 6.3783e-01, 3.1783e-05, 4.1981e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.181124749346736e-10 tensor([5.8012e-01, 6.8497e-05, 9.1811e-10, 4.1975e-01, 6.4318e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8305519600136932e-10 tensor([1.8306e-10, 1.8646e-05, 9.5848e-01, 1.5336e-07, 4.1498e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015800242545083165 tensor([0.0016, 0.0028, 0.0022, 0.0628, 0.9306], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002982150996103883 tensor([0.0298, 0.8087, 0.0354, 0.0030, 0.1232], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000372054724721238 tensor([1.3421e-01, 8.6325e-01, 6.7808e-04, 3.7205e-04, 1.4867e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4242781176676544e-08 tensor([8.6172e-01, 6.2371e-04, 1.4243e-08, 1.3743e-01, 2.2556e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00044192728819325566 tensor([4.4193e-04, 3.1713e-02, 8.3858e-02, 1.8134e-03, 8.8217e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.198167294409359e-06 tensor([9.5421e-01, 3.6041e-02, 2.1982e-06, 9.3521e-03, 3.9242e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0021545460913330317 tensor([0.2860, 0.6604, 0.0022, 0.0099, 0.0416], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.926888742100125e-12 tensor([8.9269e-12, 1.9591e-05, 9.9925e-01, 8.8758e-10, 7.2731e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002511083730496466 tensor([2.5111e-04, 7.3796e-02, 4.1848e-01, 1.0303e-03, 5.0644e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8604476281325333e-05 tensor([4.1493e-01, 9.4823e-03, 1.8604e-05, 4.7893e-01, 9.6643e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007042055949568748 tensor([7.0421e-04, 2.3723e-03, 4.6444e-03, 4.5425e-02, 9.4685e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008286837837658823 tensor([8.2868e-04, 1.0169e-02, 1.6671e-02, 1.5922e-02, 9.5641e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0027886193711310625 tensor([0.1539, 0.8069, 0.0028, 0.0036, 0.0328], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.202625761099478e-13 tensor([2.3483e-01, 4.9050e-07, 6.2026e-13, 7.6517e-01, 1.7967e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.6992778404965065e-06 tensor([3.6993e-06, 1.6204e-02, 9.3496e-01, 8.7525e-06, 4.8827e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.723123882082291e-05 tensor([6.9349e-01, 3.0342e-01, 3.7231e-05, 2.3424e-03, 7.0512e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.298293424653821e-05 tensor([5.2230e-01, 4.7642e-01, 4.2983e-05, 6.6724e-04, 5.7310e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0849337741092313e-05 tensor([2.5034e-04, 5.7285e-01, 4.1896e-01, 1.0849e-05, 7.9341e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6579261910010246e-06 tensor([1.6579e-06, 4.5937e-06, 3.7587e-04, 1.1767e-02, 9.8785e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013370588421821594 tensor([0.5127, 0.2114, 0.0013, 0.1572, 0.1175], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0035449902061372995 tensor([0.0054, 0.4812, 0.2757, 0.0035, 0.2341], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5163478939438377e-11 tensor([8.6618e-01, 2.3913e-05, 2.5163e-11, 1.3379e-01, 4.3910e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.344765617744997e-05 tensor([5.0115e-02, 9.4855e-01, 8.3202e-04, 7.3448e-05, 4.2546e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5048132457072825e-08 tensor([2.0422e-03, 1.8613e-06, 2.5048e-08, 9.9281e-01, 5.1477e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009485080954618752 tensor([9.4851e-04, 9.6128e-03, 8.8553e-03, 1.0132e-02, 9.7045e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.436923846806167e-07 tensor([9.4369e-07, 4.7069e-04, 2.1649e-01, 3.1279e-04, 7.8272e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2171421985840425e-05 tensor([5.9522e-01, 1.3475e-02, 1.2171e-05, 3.5472e-01, 3.6565e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.145426934584975e-05 tensor([2.0754e-02, 9.7288e-01, 4.3898e-03, 7.1454e-05, 1.9094e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.752393690741428e-08 tensor([8.2828e-04, 8.9744e-07, 2.7524e-08, 9.8300e-01, 1.6175e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005254528019577265 tensor([0.1624, 0.2505, 0.0053, 0.0849, 0.4969], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015983206685632467 tensor([0.1050, 0.0546, 0.0016, 0.2649, 0.5739], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.857802883227123e-06 tensor([9.3948e-01, 2.3625e-02, 2.8578e-06, 3.3359e-02, 3.5351e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.1110796498978743e-06 tensor([3.3318e-06, 2.7747e-02, 9.4070e-01, 3.1111e-06, 3.1551e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.378273186337992e-09 tensor([9.9427e-01, 4.5669e-03, 8.3783e-09, 1.1561e-03, 5.6085e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1495112062220869e-07 tensor([1.1495e-07, 9.4745e-05, 3.1028e-01, 1.9588e-04, 6.8943e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6372446225432213e-06 tensor([9.1893e-01, 7.9061e-02, 1.6372e-06, 1.7935e-03, 2.1024e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00011913156049558893 tensor([3.5583e-01, 6.4230e-01, 1.1913e-04, 6.8108e-04, 1.0715e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005075572989881039 tensor([1.3382e-02, 8.5650e-01, 3.4153e-02, 5.0756e-04, 9.5460e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0572391790096844e-08 tensor([3.3780e-02, 2.3494e-05, 2.0572e-08, 9.5835e-01, 7.8471e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.400235908974537e-08 tensor([4.4002e-08, 6.0997e-04, 9.4745e-01, 2.1866e-06, 5.1939e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6535044778720476e-05 tensor([1.6535e-05, 4.6160e-04, 1.1128e-02, 2.3678e-03, 9.8603e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00044548549340106547 tensor([1.3452e-02, 3.1157e-03, 4.4549e-04, 4.2576e-01, 5.5723e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001278875395655632 tensor([0.1418, 0.8400, 0.0014, 0.0013, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1716034720166135e-07 tensor([9.3356e-01, 3.7697e-03, 1.1716e-07, 6.2358e-02, 3.0889e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002761776850093156 tensor([4.2082e-02, 9.5139e-01, 3.4911e-03, 2.7618e-04, 2.7578e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7151795645986567e-06 tensor([1.0121e-01, 6.4337e-04, 1.7152e-06, 8.8025e-01, 1.7896e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00041881296783685684 tensor([0.3169, 0.0551, 0.0004, 0.3772, 0.2504], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.321749511708276e-09 tensor([8.3217e-09, 7.8113e-04, 9.9550e-01, 6.9062e-08, 3.7179e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6937834468322421e-09 tensor([5.1914e-03, 1.0877e-06, 1.6938e-09, 9.9235e-01, 2.4615e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009685353143140674 tensor([0.3344, 0.1668, 0.0010, 0.1063, 0.3916], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.914383427452208e-10 tensor([8.1372e-01, 9.4125e-05, 5.9144e-10, 1.8615e-01, 3.8583e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4753960719815495e-08 tensor([9.9314e-01, 3.7364e-03, 1.4754e-08, 3.0982e-03, 2.8067e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1897274816874415e-05 tensor([8.9011e-01, 9.2509e-02, 1.1897e-05, 1.2770e-02, 4.5987e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00020553189096972346 tensor([5.3935e-03, 1.1915e-03, 2.0553e-04, 2.6709e-01, 7.2612e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.864527363679372e-05 tensor([1.9800e-04, 2.8396e-01, 5.9760e-01, 4.8645e-05, 1.1819e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002488931058906019 tensor([2.4889e-04, 2.4231e-02, 1.6957e-01, 3.2925e-03, 8.0266e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.763508387550246e-06 tensor([7.9973e-01, 1.9807e-01, 9.7635e-06, 1.8949e-03, 2.9166e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.4693042156040974e-08 tensor([4.4693e-08, 1.6299e-03, 9.8797e-01, 3.1816e-07, 1.0398e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00018904510943684727 tensor([1.7014e-02, 2.2484e-03, 1.8905e-04, 6.0898e-01, 3.7157e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012049757642671466 tensor([0.0054, 0.0032, 0.0012, 0.2365, 0.7536], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005083125084638596 tensor([0.0309, 0.0439, 0.0051, 0.1355, 0.7845], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0031062320340424776 tensor([0.0031, 0.0444, 0.0354, 0.0179, 0.8991], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.751060714625055e-06 tensor([5.7261e-06, 3.1689e-02, 9.1809e-01, 4.7511e-06, 5.0214e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.418150579607369e-11 tensor([9.3277e-01, 2.4931e-05, 1.4182e-11, 6.7200e-02, 2.2188e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6383385315066334e-08 tensor([1.6383e-08, 8.1610e-04, 9.9115e-01, 2.2323e-07, 8.0325e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005363639444112778 tensor([0.0357, 0.0604, 0.0054, 0.0765, 0.8220], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.555702297206603e-14 tensor([9.3147e-01, 1.3823e-06, 6.5557e-14, 6.8527e-02, 9.1077e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00028914009453728795 tensor([5.3204e-04, 2.3160e-01, 5.0591e-01, 2.8914e-04, 2.6167e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.776003121291893e-11 tensor([5.8407e-01, 1.2099e-05, 3.7760e-11, 4.1590e-01, 2.1722e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.33788806933444e-05 tensor([5.3379e-05, 3.9869e-03, 5.1639e-02, 1.6325e-03, 9.4269e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012709904462099075 tensor([0.1459, 0.2342, 0.0127, 0.2190, 0.3882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008155064424499869 tensor([1.9897e-03, 1.1972e-03, 8.1551e-04, 1.6249e-01, 8.3350e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.1993300514111525e-09 tensor([2.1993e-09, 3.5422e-04, 9.9634e-01, 3.6241e-08, 3.3100e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00030744881951250136 tensor([3.0745e-04, 3.6379e-02, 1.3177e-01, 1.7862e-03, 8.2976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6564526106321864e-07 tensor([1.6565e-07, 7.3151e-03, 9.8972e-01, 1.6745e-07, 2.9661e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004632955533452332 tensor([4.6330e-04, 6.4684e-02, 2.2382e-01, 1.5606e-03, 7.0947e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2679720334272133e-06 tensor([9.2071e-01, 7.4435e-02, 3.2680e-06, 4.4968e-03, 3.5880e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.048696832294809e-08 tensor([6.0487e-08, 3.8383e-03, 9.9300e-01, 1.1733e-07, 3.1615e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003865794977173209 tensor([0.0039, 0.0234, 0.0116, 0.0262, 0.9350], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016751645307522267 tensor([7.1102e-01, 1.0105e-01, 1.6752e-04, 1.5664e-01, 3.1121e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011482546105980873 tensor([0.0212, 0.0081, 0.0011, 0.4011, 0.5685], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008644542656838894 tensor([0.1277, 0.7108, 0.0086, 0.0115, 0.1413], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002311557065695524 tensor([0.3010, 0.3259, 0.0023, 0.0612, 0.3096], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.52564527222421e-05 tensor([1.5544e-01, 7.3396e-03, 4.5256e-05, 6.4729e-01, 1.8989e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.733811177435541e-09 tensor([2.7338e-09, 5.2424e-04, 9.9459e-01, 2.7780e-08, 4.8835e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.163259851949988e-06 tensor([1.6069e-02, 3.0289e-04, 7.1633e-06, 8.4141e-01, 1.4221e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002626661444082856 tensor([0.2238, 0.1680, 0.0026, 0.1633, 0.4423], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003352142812218517 tensor([2.3922e-01, 7.5788e-01, 3.3521e-04, 7.3932e-04, 1.8263e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.181785920489347e-06 tensor([6.1818e-06, 9.9372e-06, 1.5551e-04, 1.3591e-02, 9.8624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01060330681502819 tensor([0.0106, 0.1179, 0.0243, 0.0142, 0.8330], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009392816573381424 tensor([0.0073, 0.0071, 0.0009, 0.0506, 0.9340], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003799107391387224 tensor([0.2027, 0.1820, 0.0038, 0.1663, 0.4452], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005340143688954413 tensor([1.8806e-01, 8.0832e-01, 5.3401e-04, 5.3920e-04, 2.5443e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.114458407566417e-06 tensor([1.7332e-01, 3.0290e-03, 9.1145e-06, 7.2205e-01, 1.0160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3368304482573876e-06 tensor([2.3368e-06, 1.3398e-02, 9.2302e-01, 7.0232e-06, 6.3568e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.773776761199613e-11 tensor([9.5523e-01, 7.3603e-05, 7.7738e-11, 4.4693e-02, 5.4509e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.02531544050089e-09 tensor([9.9104e-01, 8.6167e-03, 7.0253e-09, 3.3657e-04, 4.4552e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3693183120722097e-07 tensor([9.7403e-01, 2.5037e-02, 1.3693e-07, 9.1937e-04, 1.1105e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.357205746899126e-06 tensor([1.3572e-06, 4.9694e-06, 4.3039e-04, 1.0705e-02, 9.8886e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014485213905572891 tensor([0.0014, 0.0032, 0.0029, 0.0563, 0.9362], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003968336619436741 tensor([0.3341, 0.3555, 0.0040, 0.0923, 0.2142], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5442551557498518e-06 tensor([9.2959e-01, 6.7784e-02, 1.5443e-06, 2.2520e-03, 3.6859e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00012873433297500014 tensor([1.0936e-02, 9.6884e-01, 1.6951e-02, 1.2873e-04, 3.1445e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.029529575040215e-07 tensor([1.3070e-05, 5.0295e-07, 3.2976e-06, 3.5138e-01, 6.4861e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.1270668336655945e-05 tensor([5.1271e-05, 1.8929e-02, 3.0627e-01, 3.7105e-04, 6.7438e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.931883111363277e-05 tensor([3.7212e-04, 4.7005e-01, 5.0301e-01, 3.9319e-05, 2.6528e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002738102339208126 tensor([0.0463, 0.8949, 0.0239, 0.0027, 0.0321], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.763418850941889e-07 tensor([1.7634e-07, 7.4281e-03, 9.8833e-01, 1.9117e-07, 4.2434e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.025842852382419e-14 tensor([7.2555e-01, 6.6590e-07, 7.0258e-14, 2.7445e-01, 2.0921e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.84529039113113e-07 tensor([4.8453e-07, 2.0283e-03, 8.6092e-01, 2.0273e-05, 1.3703e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.583667306297002e-09 tensor([8.3116e-01, 3.1825e-04, 5.5837e-09, 1.6841e-01, 1.1118e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004660505801439285 tensor([0.0047, 0.2521, 0.1650, 0.0051, 0.5731], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.461746721062809e-05 tensor([7.4173e-04, 6.4127e-01, 3.3020e-01, 4.4617e-05, 2.7742e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.728316830935e-08 tensor([1.2790e-02, 2.0973e-05, 7.7283e-08, 9.6923e-01, 1.7963e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010612836922518909 tensor([3.5975e-03, 8.7879e-01, 9.3952e-02, 1.0613e-04, 2.3554e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3235705864644842e-07 tensor([2.4193e-02, 4.0212e-05, 1.3236e-07, 9.6261e-01, 1.3155e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.508522621904376e-08 tensor([9.8486e-01, 4.3461e-03, 4.5085e-08, 1.0642e-02, 1.4733e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.2117862903978676e-06 tensor([7.1586e-01, 2.8387e-01, 4.2118e-06, 2.2160e-04, 4.0955e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.463724866139493e-12 tensor([3.3990e-02, 1.1836e-07, 1.4637e-12, 9.6600e-01, 8.6665e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.125294023575975e-09 tensor([7.1648e-01, 1.9044e-04, 3.1253e-09, 2.8293e-01, 3.9247e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.754080772632733e-05 tensor([1.0069e-03, 6.4316e-01, 3.3649e-01, 8.7541e-05, 1.9259e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000192641033208929 tensor([1.9264e-04, 1.8303e-02, 1.0728e-01, 2.1154e-03, 8.7211e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.7731714453693712e-06 tensor([1.1765e-05, 9.7810e-02, 8.9411e-01, 2.7732e-06, 8.0605e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.270774530188646e-06 tensor([4.9649e-01, 4.8484e-03, 2.2708e-06, 4.7708e-01, 2.1582e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3689288607565686e-05 tensor([2.2230e-05, 8.2942e-02, 8.8440e-01, 1.3689e-05, 3.2624e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00034326655440963805 tensor([3.4327e-04, 8.2618e-03, 3.1265e-02, 7.6799e-03, 9.5245e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.884391154926561e-07 tensor([6.8844e-07, 1.1839e-02, 9.7652e-01, 9.8140e-07, 1.1641e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.168001195235192e-11 tensor([2.1680e-11, 2.2707e-05, 9.9872e-01, 2.9198e-09, 1.2583e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.707068871987133e-10 tensor([6.8744e-03, 8.3226e-07, 7.7071e-10, 9.9263e-01, 4.9415e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002613920019939542 tensor([0.0027, 0.0036, 0.0026, 0.1172, 0.8739], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1338339156452548e-08 tensor([7.7031e-01, 4.1359e-04, 1.1338e-08, 2.2892e-01, 3.4819e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.982600027541139e-09 tensor([9.7903e-01, 8.4344e-04, 3.9826e-09, 2.0106e-02, 2.3339e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.092686165473424e-05 tensor([3.0927e-05, 5.9374e-02, 8.0710e-01, 3.9847e-05, 1.3346e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3809730248226515e-08 tensor([1.1347e-02, 6.2063e-06, 1.3810e-08, 9.8319e-01, 5.4560e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.32814917480573e-05 tensor([3.3575e-01, 1.8544e-02, 6.3281e-05, 5.1683e-01, 1.2882e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.84056789926035e-07 tensor([1.3531e-02, 4.3628e-05, 3.8406e-07, 9.6650e-01, 1.9925e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.930307917064056e-06 tensor([6.5668e-01, 5.6527e-03, 1.9303e-06, 3.2858e-01, 9.0871e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.10796084604226e-05 tensor([8.0152e-01, 1.5281e-01, 7.1080e-05, 3.0972e-02, 1.4630e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.737960009355447e-07 tensor([3.6692e-05, 5.7380e-07, 1.3239e-06, 6.1372e-01, 3.8624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00023349835828412324 tensor([3.1187e-01, 3.8463e-02, 2.3350e-04, 5.1335e-01, 1.3609e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009624685626477003 tensor([0.0026, 0.0023, 0.0010, 0.0998, 0.8943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012584996409714222 tensor([0.0142, 0.0500, 0.0126, 0.0610, 0.8622], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.4100426543327558e-08 tensor([8.3010e-08, 1.2698e-02, 9.8633e-01, 2.4100e-08, 9.7302e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.220111466362141e-05 tensor([4.2575e-01, 1.4221e-02, 3.2201e-05, 4.7767e-01, 8.2326e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1675483913009543e-09 tensor([8.9744e-01, 2.1313e-04, 1.1675e-09, 1.0231e-01, 4.3319e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0878162356675602e-05 tensor([4.7221e-05, 1.8481e-01, 7.9372e-01, 1.0878e-05, 2.1412e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00028086858219467103 tensor([6.4718e-01, 1.7428e-01, 2.8087e-04, 8.1690e-02, 9.6569e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.611970547965029e-06 tensor([5.6651e-03, 9.8859e-01, 5.3680e-03, 6.6120e-06, 3.7114e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.642067184406357e-14 tensor([9.7679e-01, 1.8076e-06, 3.6421e-14, 2.3210e-02, 2.0193e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.699809894897044e-05 tensor([5.0073e-04, 5.8473e-01, 3.9248e-01, 3.6998e-05, 2.2250e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0029928558506071568 tensor([0.0030, 0.0078, 0.0035, 0.0391, 0.9467], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01372473407536745 tensor([0.0434, 0.4375, 0.0270, 0.0137, 0.4783], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.5809346172754886e-06 tensor([3.5809e-06, 1.4828e-02, 9.3176e-01, 1.2532e-05, 5.3395e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1769631100833067e-06 tensor([3.1904e-05, 1.1770e-06, 3.7161e-06, 3.8523e-01, 6.1473e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012698270380496979 tensor([0.0202, 0.1534, 0.0136, 0.0127, 0.8001], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009148361161351204 tensor([0.0825, 0.1934, 0.0091, 0.0806, 0.6344], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008341104257851839 tensor([9.5724e-03, 8.4286e-01, 7.2766e-02, 8.3411e-04, 7.3970e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00483194924890995 tensor([0.0121, 0.4028, 0.0764, 0.0048, 0.5038], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014929583994671702 tensor([5.7896e-01, 4.1304e-01, 1.4930e-04, 4.5333e-03, 3.3179e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6226404113695025e-06 tensor([1.6226e-06, 6.7796e-03, 8.7668e-01, 1.2368e-05, 1.1653e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3506847196254057e-08 tensor([9.5363e-01, 1.0685e-03, 1.3507e-08, 4.5105e-02, 1.9287e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1178967468671317e-09 tensor([8.3233e-01, 2.0250e-04, 2.1179e-09, 1.6732e-01, 1.4984e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031300445552915335 tensor([0.0402, 0.8313, 0.0323, 0.0031, 0.0931], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00013098155613988638 tensor([5.6740e-04, 1.3098e-04, 1.3279e-04, 3.1397e-01, 6.8520e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.39109826402273e-05 tensor([6.0268e-05, 4.3911e-05, 2.2903e-04, 4.6384e-02, 9.5328e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.188214681264071e-07 tensor([5.9401e-06, 1.2667e-01, 8.6900e-01, 7.1882e-07, 4.3283e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.3825599504998536e-07 tensor([9.1989e-01, 7.9749e-02, 4.3826e-07, 3.4355e-04, 1.7375e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.41960206546355e-06 tensor([9.4196e-06, 2.9610e-02, 8.9953e-01, 1.4847e-05, 7.0834e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1147146400380947e-10 tensor([7.1370e-01, 2.7296e-05, 1.1147e-10, 2.8626e-01, 1.3176e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3520025038692438e-08 tensor([2.3520e-08, 8.1322e-04, 9.9178e-01, 4.0854e-07, 7.4066e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.7798381274333224e-05 tensor([1.4467e-02, 7.9910e-04, 4.7798e-05, 6.2832e-01, 3.5636e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014146286412142217 tensor([6.2158e-01, 6.7769e-02, 1.4146e-04, 2.3130e-01, 7.9209e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00254494184628129 tensor([0.1794, 0.7733, 0.0025, 0.0035, 0.0413], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002805005991831422 tensor([0.2493, 0.1816, 0.0028, 0.1737, 0.3927], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9657842130982317e-05 tensor([6.2381e-01, 1.7091e-02, 1.9658e-05, 3.0285e-01, 5.6226e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000542172696441412 tensor([8.6861e-02, 2.2180e-02, 5.4217e-04, 3.1380e-01, 5.7662e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9372115022852086e-05 tensor([2.9372e-05, 3.4018e-02, 7.1867e-01, 8.7221e-05, 2.4719e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2586350806031987e-07 tensor([1.3331e-07, 7.3302e-03, 9.8768e-01, 1.2586e-07, 4.9886e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1921314896644617e-07 tensor([6.8979e-03, 1.8600e-05, 2.1921e-07, 9.6416e-01, 2.8920e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.343614191602228e-08 tensor([8.9741e-01, 1.1010e-03, 2.3436e-08, 1.0099e-01, 4.9308e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [0], [0], [2], [0], [3], [3], [0], [2], [0], [0], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [3], [0], [0], [0], [3], [3], [2], [2], [2], [2], [0], [0], [0], [2], [3], [3], [2], [2], [2], [2], [3], [2], [3], [0], [2], [2], [1], [0], [2], [2], [3], [2], [2], [0], [0], [3], [2], [0], [2], [2], [2], [2], [0], [2], [2], [3], [2], [0], [0], [3], [3], [2], [0], [2], [2], [0], [0], [2], [0], [0], [2], [2], [0], [2], [2], [3], [0], [2], [3], [2], [3], [2], [0], [0], [2], [3], [2], [2], [2], [2], [3], [2], [0], [2], [2], [3], [2], [0], [0], [2], [3], [2], [3], [2], [2], [0], [2], [2], [2], [2], [2], [2], [3], [0], [2], [0], [2], [2], [2], [0], [3], [2], [0], [2], [2], [3], [2], [0], [2], [2], [0], [0], [0], [0], [2], [0], [0], [2], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [3], [1], [0], [3], [3], [0], [2], [0], [2], [0], [3], [2], [3], [2], [2], [2], [2], [2], [3], [0], [3], [2], [3], [0], [0], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [2], [1], [2], [2], [2], [3], [2], [2], [3], [2], [3], [2], [3], [0], [3], [0], [1], [3], [2], [3], [3], [2], [0], [2], [2], [3], [1], [1], [3], [2], [0], [2], [0], [2], [2], [2], [2], [2], [2], [0], [3], [2], [2]]\n",
      "[[0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [0], [0], [2], [0], [3], [3], [0], [2], [0], [0], [0], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [3], [0], [0], [0], [3], [3], [2], [2], [2], [2], [0], [0], [0], [2], [3], [3], [2], [2], [2], [2], [3], [2], [3], [0], [2], [2], [1], [0], [2], [2], [3], [2], [2], [0], [0], [3], [2], [0], [2], [2], [2], [2], [0], [2], [2], [3], [2], [0], [0], [3], [3], [2], [0], [2], [2], [0], [0], [2], [0], [0], [2], [2], [0], [2], [2], [3], [0], [2], [3], [2], [3], [2], [0], [0], [2], [3], [2], [2], [2], [2], [3], [2], [0], [2], [2], [3], [2], [0], [0], [2], [3], [2], [3], [2], [2], [0], [2], [2], [2], [2], [2], [2], [3], [0], [2], [0], [2], [2], [2], [0], [3], [2], [0], [2], [2], [3], [2], [0], [2], [2], [0], [0], [0], [0], [2], [0], [0], [2], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [3], [1], [0], [3], [3], [0], [2], [0], [2], [0], [3], [2], [3], [2], [2], [2], [2], [2], [3], [0], [3], [2], [3], [0], [0], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [2], [1], [2], [2], [2], [3], [2], [2], [3], [2], [3], [2], [3], [0], [3], [0], [1], [3], [2], [3], [3], [2], [0], [2], [2], [3], [1], [1], [3], [2], [0], [2], [0], [2], [2], [2], [2], [2], [2], [0], [3], [2], [2]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0044602642059326176  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004460262298583984  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004460259437561035  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004460254669189453  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004460248470306396  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004460241794586182  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004460233211517334  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004460225582122803  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004460216045379639  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004460205554962158  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004460193634033203  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004460183143615723  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004460171222686767  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004460158824920654  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0044601459503173825  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004460132122039795  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0044601187705993655  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004460105895996094  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004460091114044189  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004460077285766601  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.01652861200273037 tensor([7.5746e-01, 8.3096e-02, 1.0565e-04, 1.4281e-01, 1.6529e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0034598263446241617 tensor([0.0024, 0.1740, 0.1438, 0.0035, 0.6764], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02111080102622509 tensor([0.0147, 0.0975, 0.0211, 0.0320, 0.8347], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.7516659056582284e-07 tensor([3.9033e-03, 1.7517e-07, 9.1424e-11, 9.9575e-01, 3.4168e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003929187543690205 tensor([0.0019, 0.1525, 0.2061, 0.0039, 0.6356], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02690460905432701 tensor([0.0370, 0.4157, 0.0269, 0.0139, 0.5065], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00013617727381642908 tensor([1.3618e-04, 4.0668e-01, 5.7369e-01, 1.0950e-05, 1.9478e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8683770122152055e-06 tensor([9.6305e-08, 1.2885e-03, 9.5730e-01, 1.8684e-06, 4.1405e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.905731500708498e-05 tensor([1.7926e-02, 3.9057e-05, 1.7119e-07, 9.7054e-01, 1.1495e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4983722394390497e-05 tensor([8.5198e-08, 4.4933e-04, 8.1115e-01, 1.4984e-05, 1.8839e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00023209147911984473 tensor([4.4746e-05, 2.3410e-02, 3.8786e-01, 2.3209e-04, 5.8845e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.178145172772929e-05 tensor([1.3473e-05, 2.4502e-02, 7.4983e-01, 4.1781e-05, 2.2561e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0783573088701814e-05 tensor([5.7739e-06, 2.5255e-02, 9.0468e-01, 1.0784e-05, 7.0044e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3350158042158e-05 tensor([9.3674e-01, 1.6117e-04, 4.0985e-10, 6.3088e-02, 1.3350e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.231299494174891e-07 tensor([5.9467e-07, 1.2686e-02, 9.8330e-01, 6.2313e-07, 4.0151e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0038458153139799833 tensor([7.5959e-01, 3.8458e-03, 5.9536e-07, 2.3135e-01, 5.2166e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009220468928106129 tensor([2.4016e-03, 9.2205e-04, 2.5855e-04, 1.1106e-01, 8.8536e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0605687066345126e-06 tensor([7.7425e-07, 1.2925e-02, 9.7875e-01, 1.0606e-06, 8.3270e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.9250821727328e-05 tensor([9.1307e-02, 7.9251e-05, 5.4265e-08, 9.0592e-01, 2.6908e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009299490484409034 tensor([6.1956e-04, 1.0965e-01, 2.4282e-01, 9.2995e-04, 6.4598e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01829715445637703 tensor([7.6765e-01, 4.4803e-02, 3.7238e-05, 1.6921e-01, 1.8297e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0001463604421587661 tensor([8.4288e-01, 1.8364e-04, 1.7575e-09, 1.5679e-01, 1.4636e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007466194219887257 tensor([7.4662e-04, 8.4765e-01, 1.4637e-01, 1.3964e-05, 5.2180e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006226740311831236 tensor([8.7449e-04, 2.3976e-02, 3.2630e-02, 6.2267e-03, 9.3629e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001430896925739944 tensor([1.0700e-05, 1.6225e-03, 1.2622e-01, 1.4309e-03, 8.7072e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02732686698436737 tensor([0.0225, 0.2208, 0.0292, 0.0273, 0.7002], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011349825654178858 tensor([9.9346e-02, 8.9612e-01, 1.1350e-03, 5.1654e-04, 2.8783e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017659338482189924 tensor([1.7659e-04, 3.5771e-01, 6.0871e-01, 2.2302e-05, 3.3386e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.245388820185326e-05 tensor([1.0036e-01, 5.2454e-05, 1.6813e-08, 8.9594e-01, 3.6413e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03071536310017109 tensor([0.0457, 0.0307, 0.0015, 0.1346, 0.7875], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005216384306550026 tensor([9.5281e-01, 2.5367e-02, 1.8651e-06, 2.1305e-02, 5.2164e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.049258369486779e-05 tensor([1.9854e-01, 9.0493e-05, 1.8530e-08, 8.0024e-01, 1.1363e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004258080385625362 tensor([0.0025, 0.1878, 0.1897, 0.0043, 0.6157], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.458880771882832e-06 tensor([2.4093e-07, 2.4589e-06, 9.6820e-04, 3.2985e-03, 9.9573e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.680022745011229e-08 tensor([3.5918e-09, 3.4137e-04, 9.9229e-01, 8.6800e-08, 7.3703e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.821033260464901e-06 tensor([9.8867e-01, 1.7224e-04, 1.3469e-10, 1.1157e-02, 3.8210e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011507181450724602 tensor([2.8436e-02, 9.4707e-01, 1.2445e-02, 5.3777e-04, 1.1507e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000787844997830689 tensor([7.8784e-04, 9.3083e-01, 6.7173e-02, 5.2877e-06, 1.2066e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.941258106962778e-06 tensor([1.3354e-02, 6.9413e-06, 1.4105e-08, 9.8353e-01, 3.1059e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01830551214516163 tensor([6.0750e-01, 1.8306e-02, 1.8764e-05, 3.4205e-01, 3.2124e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.088951002107933e-05 tensor([4.2267e-02, 5.0890e-05, 8.5556e-08, 9.4715e-01, 1.0535e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014658193103969097 tensor([2.7320e-01, 1.4658e-03, 1.4033e-06, 7.1300e-01, 1.2329e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.515790796082001e-06 tensor([8.5158e-06, 5.4532e-02, 9.1253e-01, 4.4501e-06, 3.2922e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.8087123382647405e-07 tensor([6.2112e-02, 2.8087e-07, 2.5041e-12, 9.3787e-01, 1.4311e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002671848051249981 tensor([2.6718e-04, 3.0549e-01, 6.2432e-01, 7.7938e-05, 6.9846e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005011243047192693 tensor([8.1248e-05, 5.0112e-04, 3.1845e-03, 1.2160e-02, 9.8407e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.0151640594813216e-07 tensor([9.9501e-01, 2.5391e-05, 1.6240e-12, 4.9678e-03, 1.0152e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010724729858338833 tensor([0.4914, 0.4850, 0.0005, 0.0124, 0.0107], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.512326429699897e-07 tensor([3.5123e-07, 2.1576e-07, 3.0664e-05, 2.0920e-02, 9.7905e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0023386154789477587 tensor([1.9764e-04, 2.0997e-02, 1.8266e-01, 2.3386e-03, 7.9380e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.487701128004119e-05 tensor([8.4344e-01, 1.5423e-04, 1.0024e-09, 1.5636e-01, 5.4877e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00022908326354809105 tensor([9.5226e-01, 4.5052e-02, 8.1785e-07, 2.4615e-03, 2.2908e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008303109789267182 tensor([8.3031e-04, 8.3933e-01, 1.5629e-01, 1.2819e-05, 3.5385e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008095009252429008 tensor([8.8221e-01, 7.6140e-02, 2.2470e-05, 3.3528e-02, 8.0950e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.027472136542201042 tensor([0.1564, 0.6601, 0.0151, 0.0275, 0.1410], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.4938965970686695e-07 tensor([1.0223e-08, 3.5984e-04, 9.7657e-01, 4.4939e-07, 2.3073e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.637691613927018e-05 tensor([4.2057e-06, 1.2591e-02, 8.8953e-01, 2.6377e-05, 9.7847e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.0469365810859017e-05 tensor([3.0469e-05, 2.5188e-01, 7.4108e-01, 2.1092e-06, 7.0009e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.394571341341361e-05 tensor([1.2297e-01, 8.3946e-05, 2.8425e-08, 8.7459e-01, 2.3530e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009170892648398876 tensor([0.0011, 0.0104, 0.0092, 0.0105, 0.9688], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00010042097710538656 tensor([9.8036e-01, 4.8150e-03, 7.7383e-08, 1.4729e-02, 1.0042e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030987871810793877 tensor([5.5785e-01, 3.0988e-02, 4.3860e-05, 2.2813e-01, 1.8299e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011977472342550755 tensor([8.8330e-01, 4.8096e-02, 1.4732e-05, 5.6614e-02, 1.1977e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01106351613998413 tensor([8.9853e-01, 4.5665e-02, 7.6028e-06, 4.4737e-02, 1.1064e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.6541503100597765e-06 tensor([6.1692e-10, 1.4572e-05, 7.5706e-01, 1.6542e-06, 2.4293e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.456193765799981e-07 tensor([6.4515e-01, 1.2192e-06, 3.6341e-13, 3.5484e-01, 9.4562e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008020053617656231 tensor([1.8393e-01, 8.0201e-03, 5.6926e-05, 5.4928e-01, 2.5871e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002040079125436023 tensor([2.0401e-04, 3.2553e-01, 6.3159e-01, 3.3191e-05, 4.2643e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.290857709245756e-05 tensor([5.7729e-01, 6.7260e-05, 8.8883e-10, 4.2258e-01, 6.2909e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5862050872783584e-07 tensor([1.8893e-10, 1.8908e-05, 9.5785e-01, 1.5862e-07, 4.2129e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002174613531678915 tensor([0.0016, 0.0028, 0.0022, 0.0650, 0.9284], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03053107112646103 tensor([0.0305, 0.8082, 0.0346, 0.0031, 0.1236], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006609082338400185 tensor([1.3702e-01, 8.6046e-01, 6.6091e-04, 3.8184e-04, 1.4845e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002208993973908946 tensor([8.6015e-01, 6.1157e-04, 1.3753e-08, 1.3902e-01, 2.2090e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018718844512477517 tensor([4.5437e-04, 3.1787e-02, 8.1914e-02, 1.8719e-03, 8.8397e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003835202078334987 tensor([9.5501e-01, 3.5140e-02, 2.1008e-06, 9.4640e-03, 3.8352e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01015433855354786 tensor([0.2917, 0.6548, 0.0021, 0.0102, 0.0413], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.092231412211049e-10 tensor([9.1511e-12, 1.9857e-05, 9.9925e-01, 9.0922e-10, 7.3389e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001072510494850576 tensor([2.6038e-04, 7.4588e-02, 4.1222e-01, 1.0725e-03, 5.1186e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009229420684278011 tensor([4.1416e-01, 9.2294e-03, 1.7717e-05, 4.8274e-01, 9.3861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023841916117817163 tensor([7.2870e-04, 2.3842e-03, 4.5374e-03, 4.7199e-02, 9.4515e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01016763411462307 tensor([8.5431e-04, 1.0168e-02, 1.6247e-02, 1.6552e-02, 9.5618e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003696369705721736 tensor([0.1574, 0.8034, 0.0027, 0.0037, 0.0328], grad_fn=<SoftmaxBackward0>)\n",
      "1 4.817986223315529e-07 tensor([2.3298e-01, 4.8180e-07, 6.0051e-13, 7.6702e-01, 1.7530e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.217400474881288e-06 tensor([3.8865e-06, 1.6599e-02, 9.3327e-01, 9.2174e-06, 5.0115e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006936848512850702 tensor([6.9890e-01, 2.9799e-01, 3.5800e-05, 2.3819e-03, 6.9368e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005662338808178902 tensor([5.2836e-01, 4.7035e-01, 4.1526e-05, 6.7942e-04, 5.6623e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00025790577637962997 tensor([2.5791e-04, 5.7832e-01, 4.1343e-01, 1.1177e-05, 7.9725e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.606973107001977e-06 tensor([1.7047e-06, 4.6070e-06, 3.6781e-04, 1.2170e-02, 9.8746e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11511585861444473 tensor([0.5167, 0.2069, 0.0013, 0.1600, 0.1151], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00558060547336936 tensor([0.0056, 0.4842, 0.2708, 0.0037, 0.2357], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.324626843299484e-06 tensor([8.6423e-01, 2.3534e-05, 2.4488e-11, 1.3575e-01, 4.3246e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00042464720900170505 tensor([5.1282e-02, 9.4741e-01, 8.1105e-04, 7.5422e-05, 4.2465e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.8189730326412246e-06 tensor([2.0298e-03, 1.8190e-06, 2.3978e-08, 9.9299e-01, 4.9809e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008643467910587788 tensor([0.0010, 0.0096, 0.0086, 0.0105, 0.9703], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003229081048630178 tensor([9.7243e-07, 4.7407e-04, 2.1248e-01, 3.2291e-04, 7.8673e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013090994209051132 tensor([5.9312e-01, 1.3091e-02, 1.1601e-05, 3.5817e-01, 3.5600e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0019109993008896708 tensor([2.1258e-02, 9.7247e-01, 4.2851e-03, 7.3502e-05, 1.9110e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.774568982516939e-07 tensor([8.2341e-04, 8.7746e-07, 2.6361e-08, 9.8353e-01, 1.5650e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.087549589574337 tensor([0.1666, 0.2487, 0.0051, 0.0875, 0.4922], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05403224751353264 tensor([0.1067, 0.0540, 0.0015, 0.2715, 0.5662], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0034569655545055866 tensor([9.3971e-01, 2.3047e-02, 2.7354e-06, 3.3787e-02, 3.4570e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.491403731459286e-06 tensor([3.4914e-06, 2.8404e-02, 9.3929e-01, 3.2605e-06, 3.2299e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.527033408725401e-06 tensor([9.9434e-01, 4.4782e-03, 8.1075e-09, 1.1732e-03, 5.5270e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.574777504894882e-05 tensor([1.1901e-07, 9.5748e-05, 3.0513e-01, 2.0268e-04, 6.9458e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00020644301548600197 tensor([9.2051e-01, 7.7471e-02, 1.5759e-06, 1.8140e-03, 2.0644e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006967054796405137 tensor([3.6150e-01, 6.3663e-01, 1.1556e-04, 6.9671e-04, 1.0635e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013738016597926617 tensor([1.3738e-02, 8.5686e-01, 3.3307e-02, 5.2327e-04, 9.5574e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.2909480321686715e-05 tensor([3.3512e-02, 2.2909e-05, 1.9676e-08, 9.5888e-01, 7.5855e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3106765638658544e-06 tensor([4.6431e-08, 6.2576e-04, 9.4601e-01, 2.3107e-06, 5.3365e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004613811324816197 tensor([1.6930e-05, 4.6138e-04, 1.0847e-02, 2.4361e-03, 9.8624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0030757037457078695 tensor([1.3641e-02, 3.0757e-03, 4.2908e-04, 4.3497e-01, 5.4789e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013877409510314465 tensor([0.1450, 0.8369, 0.0014, 0.0013, 0.0154], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003014214162249118 tensor([9.3294e-01, 3.6735e-03, 1.1195e-07, 6.3087e-02, 3.0142e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002757288282737136 tensor([4.3077e-02, 9.5048e-01, 3.4050e-03, 2.8392e-04, 2.7573e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006239538197405636 tensor([1.0050e-01, 6.2395e-04, 1.6270e-06, 8.8158e-01, 1.7291e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0538598969578743 tensor([0.3180, 0.0539, 0.0004, 0.3827, 0.2450], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.100621957079056e-08 tensor([8.5666e-09, 7.9269e-04, 9.9545e-01, 7.1006e-08, 3.7607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.063865852302115e-06 tensor([5.1502e-03, 1.0639e-06, 1.6268e-09, 9.9246e-01, 2.3849e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10882066190242767 tensor([0.3393, 0.1646, 0.0009, 0.1088, 0.3864], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.777232632273808e-05 tensor([8.1188e-01, 9.2509e-05, 5.7240e-10, 1.8799e-01, 3.7772e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.7595275241765194e-05 tensor([9.9317e-01, 3.6629e-03, 1.4255e-08, 3.1414e-03, 2.7595e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004493925254791975 tensor([8.9227e-01, 9.0297e-02, 1.1357e-05, 1.2923e-02, 4.4939e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011856276541948318 tensor([5.5132e-03, 1.1856e-03, 1.9930e-04, 2.7478e-01, 7.1832e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000206560202059336 tensor([2.0656e-04, 2.8880e-01, 5.9086e-01, 5.0837e-05, 1.2008e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0033993504475802183 tensor([2.5614e-04, 2.4361e-02, 1.6638e-01, 3.3994e-03, 8.0560e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00028739628032781184 tensor([8.0304e-01, 1.9474e-01, 9.4431e-06, 1.9227e-03, 2.8740e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.2732430099713383e-07 tensor([4.6003e-08, 1.6533e-03, 9.8782e-01, 3.2732e-07, 1.0524e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002209034515544772 tensor([1.7141e-02, 2.2090e-03, 1.8126e-04, 6.1740e-01, 3.6307e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003218831494450569 tensor([0.0056, 0.0032, 0.0012, 0.2437, 0.7463], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03172414004802704 tensor([0.0317, 0.0437, 0.0049, 0.1401, 0.7796], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018521200865507126 tensor([0.0032, 0.0445, 0.0347, 0.0185, 0.8992], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.986356882203836e-06 tensor([5.9864e-06, 3.2385e-02, 9.1630e-01, 4.9627e-06, 5.1306e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.1858975287614157e-06 tensor([9.3176e-01, 2.4572e-05, 1.3813e-11, 6.8209e-02, 2.1859e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3023133621791203e-07 tensor([1.6905e-08, 8.2878e-04, 9.9103e-01, 2.3023e-07, 8.1386e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03665292635560036 tensor([0.0367, 0.0602, 0.0052, 0.0791, 0.8188], grad_fn=<SoftmaxBackward0>)\n",
      "4 9.053971439243469e-08 tensor([9.3035e-01, 1.3728e-06, 6.4702e-14, 6.9649e-02, 9.0540e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005547254113480449 tensor([5.5473e-04, 2.3490e-01, 4.9883e-01, 3.0205e-04, 2.6541e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.186670851893723e-05 tensor([5.8078e-01, 1.1867e-05, 3.6515e-11, 4.1918e-01, 2.1236e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001683841459453106 tensor([5.4809e-05, 3.9908e-03, 5.0337e-02, 1.6838e-03, 9.4393e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14851777255535126 tensor([0.1485, 0.2311, 0.0122, 0.2251, 0.3831], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011941164266318083 tensor([2.0406e-03, 1.1941e-03, 7.9324e-04, 1.6785e-01, 8.2812e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.730855624439755e-08 tensor([2.2690e-09, 3.5997e-04, 9.9629e-01, 3.7309e-08, 3.3497e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018452435033395886 tensor([3.1617e-04, 3.6490e-02, 1.2891e-01, 1.8452e-03, 8.3244e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.7180343547806842e-07 tensor([1.6999e-07, 7.4092e-03, 9.8959e-01, 1.7180e-07, 2.9969e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0016123317182064056 tensor([4.7714e-04, 6.5099e-02, 2.1984e-01, 1.6123e-03, 7.1297e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00035229968489147723 tensor([9.2220e-01, 7.2890e-02, 3.1463e-06, 4.5567e-03, 3.5230e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.202219124252224e-07 tensor([6.1919e-08, 3.8828e-03, 9.9292e-01, 1.2022e-07, 3.1937e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011290785856544971 tensor([0.0040, 0.0234, 0.0113, 0.0271, 0.9342], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030436333268880844 tensor([7.1202e-01, 9.8579e-02, 1.6019e-04, 1.5881e-01, 3.0436e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007962256669998169 tensor([0.0216, 0.0080, 0.0011, 0.4103, 0.5591], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011870414949953556 tensor([0.1308, 0.7077, 0.0084, 0.0119, 0.1413], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06284027546644211 tensor([0.3060, 0.3223, 0.0022, 0.0628, 0.3066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007153137121349573 tensor([1.5542e-01, 7.1531e-03, 4.3095e-05, 6.5290e-01, 1.8448e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.8562929799136327e-08 tensor([2.8145e-09, 5.3222e-04, 9.9453e-01, 2.8563e-08, 4.9397e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00029533402994275093 tensor([1.6026e-02, 2.9533e-04, 6.8333e-06, 8.4572e-01, 1.3795e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16585558652877808 tensor([0.2275, 0.1659, 0.0025, 0.1676, 0.4365], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007595725473947823 tensor([2.4435e-01, 7.5275e-01, 3.2481e-04, 7.5957e-04, 1.8162e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.972044608730357e-06 tensor([6.3618e-06, 9.9720e-06, 1.5220e-04, 1.4084e-02, 9.8575e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014706053771078587 tensor([0.0109, 0.1180, 0.0237, 0.0147, 0.8327], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007133137434720993 tensor([7.5215e-03, 7.1331e-03, 9.1620e-04, 5.2404e-02, 9.3203e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17081904411315918 tensor([0.2063, 0.1797, 0.0037, 0.1708, 0.4396], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005531056667678058 tensor([1.9201e-01, 8.0439e-01, 5.1857e-04, 5.5311e-04, 2.5344e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002938651479780674 tensor([1.7250e-01, 2.9387e-03, 8.6496e-06, 7.2616e-01, 9.8391e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.414304036501562e-06 tensor([2.4618e-06, 1.3742e-02, 9.2093e-01, 7.4143e-06, 6.5315e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.368419351725606e-06 tensor([9.5462e-01, 7.2624e-05, 7.5777e-11, 4.5298e-02, 5.3684e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.380024620331824e-06 tensor([9.9122e-01, 8.4390e-03, 6.7766e-09, 3.4128e-04, 4.3800e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.0905184353759978e-05 tensor([9.7456e-01, 2.4496e-02, 1.3179e-07, 9.3145e-04, 1.0905e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.996852567273891e-06 tensor([1.3988e-06, 4.9969e-06, 4.2168e-04, 1.1089e-02, 9.8848e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0028387114871293306 tensor([0.0015, 0.0032, 0.0028, 0.0586, 0.9339], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09464270621538162 tensor([0.3392, 0.3509, 0.0038, 0.0946, 0.2115], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003611752763390541 tensor([9.3109e-01, 6.6266e-02, 1.4804e-06, 2.2775e-03, 3.6118e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0031430362723767757 tensor([1.1203e-02, 9.6898e-01, 1.6545e-02, 1.3229e-04, 3.1430e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2119100978889037e-06 tensor([1.3350e-05, 5.0268e-07, 3.2119e-06, 3.5993e-01, 6.4005e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003843131416942924 tensor([5.2917e-05, 1.9063e-02, 3.0070e-01, 3.8431e-04, 6.7980e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00038663603481836617 tensor([3.8664e-04, 4.7635e-01, 4.9642e-01, 4.0888e-05, 2.6809e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.023350728675723076 tensor([0.0475, 0.8942, 0.0234, 0.0028, 0.0321], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9649229443530203e-07 tensor([1.8138e-07, 7.5331e-03, 9.8817e-01, 1.9649e-07, 4.2925e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.0687375013039855e-07 tensor([7.2228e-01, 6.5878e-07, 6.8978e-14, 2.7771e-01, 2.0687e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.141920595022384e-05 tensor([5.1085e-07, 2.0795e-03, 8.5734e-01, 2.1419e-05, 1.4056e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00010917021427303553 tensor([8.2926e-01, 3.1294e-04, 5.4196e-09, 1.7032e-01, 1.0917e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005244622007012367 tensor([0.0048, 0.2529, 0.1618, 0.0052, 0.5753], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007669933838769794 tensor([7.6699e-04, 6.4691e-01, 3.2440e-01, 4.6137e-05, 2.7882e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.0462193788262084e-05 tensor([1.2699e-02, 2.0462e-05, 7.3894e-08, 9.6991e-01, 1.7368e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0036924928426742554 tensor([3.6925e-03, 8.8065e-01, 9.1936e-02, 1.0930e-04, 2.3612e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.9166676288004965e-05 tensor([2.4003e-02, 3.9167e-05, 1.2634e-07, 9.6324e-01, 1.2713e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00014461897080764174 tensor([9.8481e-01, 4.2553e-03, 4.3454e-08, 1.0790e-02, 1.4462e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.043150329380296e-05 tensor([7.2026e-01, 2.7947e-01, 4.0748e-06, 2.2502e-04, 4.0432e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1618301698490541e-07 tensor([3.3696e-02, 1.1618e-07, 1.4148e-12, 9.6630e-01, 8.4354e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00018686852126847953 tensor([7.1407e-01, 1.8687e-04, 3.0181e-09, 2.8536e-01, 3.8341e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010379788000136614 tensor([1.0380e-03, 6.4825e-01, 3.3125e-01, 9.0440e-05, 1.9378e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0021821551490575075 tensor([1.9797e-04, 1.8367e-02, 1.0502e-01, 2.1822e-03, 8.7423e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2242399861861486e-05 tensor([1.2242e-05, 9.9905e-02, 8.9190e-01, 2.8800e-06, 8.1843e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0047127800062298775 tensor([4.9411e-01, 4.7128e-03, 2.1653e-06, 4.8020e-01, 2.0972e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.331922951270826e-05 tensor([2.3319e-05, 8.4980e-02, 8.8168e-01, 1.4355e-05, 3.3304e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007941951043903828 tensor([3.5432e-04, 8.3049e-03, 3.0566e-02, 7.9420e-03, 9.5283e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0202895737165818e-06 tensor([7.1691e-07, 1.2103e-02, 9.7605e-01, 1.0203e-06, 1.1848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.99559577143782e-09 tensor([2.2271e-11, 2.3035e-05, 9.9871e-01, 2.9956e-09, 1.2705e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.148093115778465e-07 tensor([6.8190e-03, 8.1481e-07, 7.4183e-10, 9.9270e-01, 4.7984e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002761856187134981 tensor([0.0028, 0.0036, 0.0025, 0.1215, 0.8695], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00034058716846629977 tensor([7.6822e-01, 4.0605e-04, 1.0962e-08, 2.3103e-01, 3.4059e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.293593752256129e-05 tensor([9.7876e-01, 8.2743e-04, 3.8498e-09, 2.0393e-02, 2.2936e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.199624891043641e-05 tensor([3.2523e-05, 6.0757e-02, 8.0250e-01, 4.1996e-05, 1.3667e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.061586191208335e-06 tensor([1.1267e-02, 6.0616e-06, 1.3227e-08, 9.8345e-01, 5.2782e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01804056018590927 tensor([3.3578e-01, 1.8041e-02, 6.0044e-05, 5.2123e-01, 1.2489e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.2543520976323634e-05 tensor([1.3433e-02, 4.2544e-05, 3.6719e-07, 9.6725e-01, 1.9276e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005490884650498629 tensor([6.5391e-01, 5.4909e-03, 1.8420e-06, 3.3174e-01, 8.8540e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014334327541291714 tensor([8.0473e-01, 1.4943e-01, 6.8061e-05, 3.1430e-02, 1.4334e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2803185427401331e-06 tensor([3.7006e-05, 5.6777e-07, 1.2803e-06, 6.2209e-01, 3.7787e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.037479743361473083 tensor([3.1154e-01, 3.7480e-02, 2.2283e-04, 5.1820e-01, 1.3256e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002293073572218418 tensor([0.0026, 0.0023, 0.0009, 0.1034, 0.8907], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014562554657459259 tensor([0.0146, 0.0499, 0.0122, 0.0632, 0.8601], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.514776084211917e-08 tensor([8.5148e-08, 1.2870e-02, 9.8615e-01, 2.4696e-08, 9.8200e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013832218945026398 tensor([4.2474e-01, 1.3832e-02, 3.0652e-05, 4.8141e-01, 7.9990e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.244820956955664e-05 tensor([8.9626e-01, 2.0952e-04, 1.1303e-09, 1.0348e-01, 4.2448e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.930501017952338e-05 tensor([4.9305e-05, 1.8877e-01, 7.8941e-01, 1.1351e-05, 2.1763e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08313510566949844 tensor([6.5109e-01, 1.7071e-01, 2.6949e-04, 8.3135e-02, 9.4797e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00037046559737063944 tensor([5.7840e-03, 9.8859e-01, 5.2495e-03, 6.7666e-06, 3.7047e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.0120140220569738e-08 tensor([9.7636e-01, 1.7962e-06, 3.5990e-14, 2.3640e-02, 2.0120e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000517232867423445 tensor([5.1723e-04, 5.9035e-01, 3.8670e-01, 3.8247e-05, 2.2397e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0034084906801581383 tensor([0.0031, 0.0078, 0.0034, 0.0405, 0.9453], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026340797543525696 tensor([0.0445, 0.4369, 0.0263, 0.0142, 0.4781], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3254520126793068e-05 tensor([3.7792e-06, 1.5223e-02, 9.2984e-01, 1.3255e-05, 5.4923e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.6174030810798286e-06 tensor([3.2523e-05, 1.1748e-06, 3.6174e-06, 3.9396e-01, 6.0600e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01321596559137106 tensor([0.0208, 0.1536, 0.0132, 0.0131, 0.7993], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08324400335550308 tensor([0.0846, 0.1925, 0.0089, 0.0832, 0.6308], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009822165593504906 tensor([0.0098, 0.8439, 0.0712, 0.0009, 0.0742], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012412814423441887 tensor([0.0124, 0.4029, 0.0746, 0.0050, 0.5051], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032753695268183947 tensor([5.8492e-01, 4.0704e-01, 1.4412e-04, 4.6204e-03, 3.2754e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.304076340602478e-05 tensor([1.7072e-06, 6.9452e-03, 8.7349e-01, 1.3041e-05, 1.1955e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00018873868975788355 tensor([9.5308e-01, 1.0453e-03, 1.2989e-08, 4.5689e-02, 1.8874e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0001470085553592071 tensor([8.3053e-01, 1.9925e-04, 2.0558e-09, 1.6912e-01, 1.4701e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03146813437342644 tensor([0.0413, 0.8309, 0.0315, 0.0032, 0.0932], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012853211956098676 tensor([5.7962e-04, 1.3021e-04, 1.2853e-04, 3.2253e-01, 6.7663e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.227731500985101e-05 tensor([6.2277e-05, 4.4097e-05, 2.2363e-04, 4.8175e-02, 9.5149e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.181526714499341e-06 tensor([6.1815e-06, 1.2934e-01, 8.6626e-01, 7.4691e-07, 4.3941e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.7078840755857527e-05 tensor([9.2148e-01, 7.8158e-02, 4.2211e-07, 3.4758e-04, 1.7079e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5660627468605526e-05 tensor([9.9184e-06, 3.0356e-02, 8.9689e-01, 1.5661e-05, 7.2725e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.2945101843797602e-05 tensor([7.1099e-01, 2.6869e-05, 1.0841e-10, 2.8897e-01, 1.2945e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.250720735399227e-07 tensor([2.4447e-08, 8.2947e-04, 9.9163e-01, 4.2507e-07, 7.5365e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007851390982978046 tensor([1.4542e-02, 7.8514e-04, 4.5944e-05, 6.3619e-01, 3.4844e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06615152209997177 tensor([6.2188e-01, 6.6152e-02, 1.3550e-04, 2.3434e-01, 7.7487e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003623374504968524 tensor([0.1834, 0.7693, 0.0025, 0.0036, 0.0411], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17797935009002686 tensor([0.2530, 0.1791, 0.0027, 0.1780, 0.3871], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01664114184677601 tensor([6.2268e-01, 1.6641e-02, 1.8762e-05, 3.0588e-01, 5.4783e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021955274045467377 tensor([8.8098e-02, 2.1955e-02, 5.2482e-04, 3.2076e-01, 5.6867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.160750778391957e-05 tensor([3.0708e-05, 3.4620e-02, 7.1259e-01, 9.1608e-05, 2.5267e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3721185609938402e-07 tensor([1.3721e-07, 7.4379e-03, 9.8752e-01, 1.2941e-07, 5.0460e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.816137410060037e-05 tensor([6.8525e-03, 1.8161e-05, 2.0986e-07, 9.6513e-01, 2.8001e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004808518569916487 tensor([8.9620e-01, 1.0735e-03, 2.2418e-08, 1.0225e-01, 4.8085e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [0, 3], [0, 2], [2, 1], [0, 3], [3, 2], [3, 0], [0, 3], [2, 1], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [0, 3], [2, 1], [0, 3], [2, 4], [2, 4], [3, 0], [0, 3], [0, 3], [0, 3], [3, 2], [3, 0], [2, 1], [2, 1], [2, 4], [2, 1], [0, 3], [0, 1], [0, 3], [2, 4], [3, 4], [3, 0], [2, 1], [2, 1], [2, 1], [2, 1], [3, 0], [2, 1], [3, 0], [0, 1], [2, 4], [2, 4], [1, 0], [0, 3], [2, 4], [2, 4], [3, 0], [2, 4], [2, 3], [0, 3], [0, 3], [3, 0], [2, 1], [0, 2], [2, 4], [2, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 1], [3, 0], [2, 4], [0, 3], [0, 2], [3, 0], [3, 2], [2, 4], [0, 3], [2, 4], [2, 3], [0, 3], [0, 3], [2, 1], [0, 1], [0, 1], [2, 3], [2, 1], [0, 3], [2, 4], [2, 4], [3, 0], [0, 1], [2, 4], [3, 0], [2, 4], [3, 4], [2, 1], [0, 2], [0, 3], [2, 1], [3, 4], [2, 1], [2, 3], [2, 1], [2, 4], [3, 0], [2, 4], [0, 1], [2, 4], [2, 3], [3, 0], [2, 1], [0, 3], [0, 1], [2, 1], [3, 2], [2, 4], [3, 4], [2, 1], [2, 1], [0, 3], [2, 1], [2, 3], [2, 4], [2, 4], [2, 4], [2, 1], [3, 0], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [2, 0], [0, 3], [3, 0], [2, 4], [0, 3], [2, 0], [2, 4], [3, 0], [2, 1], [0, 3], [2, 0], [2, 1], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [0, 2], [2, 4], [2, 1], [2, 3], [2, 3], [2, 1], [0, 3], [2, 1], [2, 1], [2, 3], [0, 1], [0, 3], [2, 1], [2, 3], [2, 3], [2, 1], [0, 3], [2, 4], [2, 4], [2, 4], [0, 1], [0, 2], [2, 3], [2, 4], [3, 4], [1, 2], [0, 3], [3, 0], [3, 2], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [3, 0], [2, 1], [3, 0], [2, 1], [2, 4], [2, 4], [2, 1], [2, 1], [3, 0], [0, 3], [3, 0], [2, 1], [3, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 0], [2, 4], [2, 4], [0, 3], [2, 1], [2, 1], [2, 1], [2, 1], [2, 4], [1, 2], [2, 1], [2, 1], [2, 0], [3, 0], [2, 1], [2, 4], [3, 0], [2, 3], [3, 4], [2, 4], [3, 0], [0, 2], [3, 2], [0, 3], [1, 2], [3, 2], [2, 3], [3, 0], [3, 0], [2, 4], [0, 3], [2, 4], [2, 4], [3, 2], [1, 2], [1, 0], [3, 0], [2, 4], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [2, 3], [2, 3], [2, 1], [2, 1], [0, 3], [3, 0], [2, 1], [2, 4]]\n",
      "[[0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 2], [0, 3, 4], [1, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [0, 1, 2], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 4], [2, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 3, 4], [0, 1, 4], [0, 1, 3], [0, 1, 2], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 4], [0, 1, 2], [0, 1, 3], [1, 2, 4], [1, 3, 4], [0, 1, 4], [1, 2, 4], [0, 3, 4], [0, 1, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 4], [0, 1, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3]]\n",
      "NL_pred of 1th iteration [[2, 4], [0, 3], [0, 2], [2, 1], [0, 3], [3, 2], [3, 0], [0, 3], [2, 1], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [0, 3], [2, 1], [0, 3], [2, 4], [2, 4], [3, 0], [0, 3], [0, 3], [0, 3], [3, 2], [3, 0], [2, 1], [2, 1], [2, 4], [2, 1], [0, 3], [0, 1], [0, 3], [2, 4], [3, 4], [3, 0], [2, 1], [2, 1], [2, 1], [2, 1], [3, 0], [2, 1], [3, 0], [0, 1], [2, 4], [2, 4], [1, 0], [0, 3], [2, 4], [2, 4], [3, 0], [2, 4], [2, 3], [0, 3], [0, 3], [3, 0], [2, 1], [0, 2], [2, 4], [2, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 1], [3, 0], [2, 4], [0, 3], [0, 2], [3, 0], [3, 2], [2, 4], [0, 3], [2, 4], [2, 3], [0, 3], [0, 3], [2, 1], [0, 1], [0, 1], [2, 3], [2, 1], [0, 3], [2, 4], [2, 4], [3, 0], [0, 1], [2, 4], [3, 0], [2, 4], [3, 4], [2, 1], [0, 2], [0, 3], [2, 1], [3, 4], [2, 1], [2, 3], [2, 1], [2, 4], [3, 0], [2, 4], [0, 1], [2, 4], [2, 3], [3, 0], [2, 1], [0, 3], [0, 1], [2, 1], [3, 2], [2, 4], [3, 4], [2, 1], [2, 1], [0, 3], [2, 1], [2, 3], [2, 4], [2, 4], [2, 4], [2, 1], [3, 0], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [2, 0], [0, 3], [3, 0], [2, 4], [0, 3], [2, 0], [2, 4], [3, 0], [2, 1], [0, 3], [2, 0], [2, 1], [0, 3], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [0, 2], [2, 4], [2, 1], [2, 3], [2, 3], [2, 1], [0, 3], [2, 1], [2, 1], [2, 3], [0, 1], [0, 3], [2, 1], [2, 3], [2, 3], [2, 1], [0, 3], [2, 4], [2, 4], [2, 4], [0, 1], [0, 2], [2, 3], [2, 4], [3, 4], [1, 2], [0, 3], [3, 0], [3, 2], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [3, 0], [2, 1], [3, 0], [2, 1], [2, 4], [2, 4], [2, 1], [2, 1], [3, 0], [0, 3], [3, 0], [2, 1], [3, 0], [0, 3], [0, 3], [0, 3], [2, 1], [2, 0], [2, 4], [2, 4], [0, 3], [2, 1], [2, 1], [2, 1], [2, 1], [2, 4], [1, 2], [2, 1], [2, 1], [2, 0], [3, 0], [2, 1], [2, 4], [3, 0], [2, 3], [3, 4], [2, 4], [3, 0], [0, 2], [3, 2], [0, 3], [1, 2], [3, 2], [2, 3], [3, 0], [3, 0], [2, 4], [0, 3], [2, 4], [2, 4], [3, 2], [1, 2], [1, 0], [3, 0], [2, 4], [0, 3], [2, 4], [0, 3], [2, 1], [2, 1], [2, 3], [2, 3], [2, 1], [2, 1], [0, 3], [3, 0], [2, 1], [2, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004498807430267334  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004498804569244385  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004498798847198487  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0044987907409667965  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004498779773712158  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004498766422271728  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004498752117156982  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0044987363815307616  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004498717784881592  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004498699188232422  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004498678684234619  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004498658180236817  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004498635768890381  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004498612880706787  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004498589992523194  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0044985651969909664  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00449854040145874  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004498515129089355  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004498489856719971  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004498463630676269  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.08483254909515381 tensor([7.5812e-01, 8.4833e-02, 1.0786e-04, 1.4023e-01, 1.6716e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14390221238136292 tensor([0.0024, 0.1749, 0.1439, 0.0034, 0.6755], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03149198740720749 tensor([0.0147, 0.0985, 0.0211, 0.0315, 0.8342], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000348537607351318 tensor([3.9527e-03, 1.7891e-07, 9.3213e-11, 9.9570e-01, 3.4854e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1537788212299347 tensor([0.0019, 0.1538, 0.2066, 0.0038, 0.6339], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.036934323608875275 tensor([0.0369, 0.4178, 0.0268, 0.0136, 0.5048], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019472725689411163 tensor([1.3601e-04, 4.0814e-01, 5.7224e-01, 1.0803e-05, 1.9473e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0013001500628888607 tensor([9.6579e-08, 1.3002e-03, 9.5736e-01, 1.8419e-06, 4.1333e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011732266284525394 tensor([1.8223e-02, 4.0093e-05, 1.7491e-07, 9.7000e-01, 1.1732e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00045504706213250756 tensor([8.6035e-08, 4.5505e-04, 8.1092e-01, 1.4837e-05, 1.8861e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02362867072224617 tensor([4.4807e-05, 2.3629e-02, 3.8873e-01, 2.2844e-04, 5.8737e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024711424484848976 tensor([1.3548e-05, 2.4711e-02, 7.4931e-01, 4.1399e-05, 2.2592e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02542598359286785 tensor([5.7787e-06, 2.5426e-02, 9.0459e-01, 1.0635e-05, 6.9969e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00016326626064255834 tensor([9.3752e-01, 1.6327e-04, 4.1508e-10, 6.2306e-02, 1.3502e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004022178705781698 tensor([5.9668e-07, 1.2785e-02, 9.8319e-01, 6.1692e-07, 4.0222e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005304266232997179 tensor([7.6163e-01, 3.9149e-03, 6.0742e-07, 2.2915e-01, 5.3043e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0023965805303305387 tensor([2.3966e-03, 9.2855e-04, 2.5908e-04, 1.0907e-01, 8.8735e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008356976322829723 tensor([7.8120e-07, 1.3066e-02, 9.7857e-01, 1.0544e-06, 8.3570e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0027470067143440247 tensor([9.2746e-02, 8.1423e-05, 5.5529e-08, 9.0443e-01, 2.7470e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11084352433681488 tensor([6.2069e-04, 1.1084e-01, 2.4344e-01, 9.1363e-04, 6.4419e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.045671798288822174 tensor([7.6933e-01, 4.5672e-02, 3.7902e-05, 1.6648e-01, 1.8488e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00018588794046081603 tensor([8.4441e-01, 1.8589e-04, 1.7786e-09, 1.5526e-01, 1.4812e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005207309033721685 tensor([7.4047e-04, 8.4794e-01, 1.4610e-01, 1.3703e-05, 5.2073e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0242412481456995 tensor([8.7453e-04, 2.4241e-02, 3.2802e-02, 6.1116e-03, 9.3597e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016395882703363895 tensor([1.0751e-05, 1.6396e-03, 1.2641e-01, 1.4114e-03, 8.7053e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.029216021299362183 tensor([0.0224, 0.2220, 0.0292, 0.0267, 0.6997], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0028767320327460766 tensor([9.8934e-02, 8.9655e-01, 1.1305e-03, 5.0929e-04, 2.8767e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03328489139676094 tensor([1.7672e-04, 3.5986e-01, 6.0665e-01, 2.1961e-05, 3.3285e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0037121204659342766 tensor([1.0187e-01, 5.3774e-05, 1.7157e-08, 8.9436e-01, 3.7121e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.045707400888204575 tensor([0.0457, 0.0310, 0.0015, 0.1322, 0.7895], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020890967920422554 tensor([9.5287e-01, 2.5710e-02, 1.8796e-06, 2.0891e-02, 5.2326e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011615820694714785 tensor([2.0049e-01, 9.2563e-05, 1.8982e-08, 7.9826e-01, 1.1616e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1890077441930771 tensor([0.0025, 0.1890, 0.1903, 0.0042, 0.6140], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009674983448348939 tensor([2.4161e-07, 2.4784e-06, 9.6750e-04, 3.2518e-03, 9.9578e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00034619393409229815 tensor([3.6435e-09, 3.4619e-04, 9.9224e-01, 8.6604e-08, 7.4091e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00017491001926828176 tensor([9.8883e-01, 1.7491e-04, 1.3690e-10, 1.0994e-02, 3.8676e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012449112720787525 tensor([2.8135e-02, 9.4742e-01, 1.2449e-02, 5.2643e-04, 1.1472e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0012068122159689665 tensor([7.8234e-04, 9.3096e-01, 6.7046e-02, 5.2051e-06, 1.2068e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0031659393571317196 tensor([1.3535e-02, 7.0929e-06, 1.4366e-08, 9.8329e-01, 3.1659e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0325641930103302 tensor([6.1035e-01, 1.8640e-02, 1.9082e-05, 3.3843e-01, 3.2564e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010758917778730392 tensor([4.2912e-02, 5.2207e-05, 8.7480e-08, 9.4628e-01, 1.0759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012507634237408638 tensor([2.7609e-01, 1.4936e-03, 1.4231e-06, 7.0991e-01, 1.2508e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03303976356983185 tensor([8.6027e-06, 5.5138e-02, 9.1181e-01, 4.4296e-06, 3.3040e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.4596412256651092e-05 tensor([6.2927e-02, 2.8723e-07, 2.5551e-12, 9.3706e-01, 1.4596e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06949767470359802 tensor([2.6696e-04, 3.0739e-01, 6.2277e-01, 7.6519e-05, 6.9498e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0031891169492155313 tensor([8.1318e-05, 5.0585e-04, 3.1891e-03, 1.1940e-02, 9.8428e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.571107143012341e-05 tensor([9.9507e-01, 2.5711e-05, 1.6459e-12, 4.9045e-03, 1.0259e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01217565219849348 tensor([0.4893, 0.4873, 0.0005, 0.0122, 0.0107], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.069377999054268e-05 tensor([3.5130e-07, 2.1723e-07, 3.0694e-05, 2.0601e-02, 9.7937e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021172957494854927 tensor([1.9808e-04, 2.1173e-02, 1.8279e-01, 2.3057e-03, 7.9353e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001563916594022885 tensor([8.4491e-01, 1.5639e-04, 1.0180e-09, 1.5487e-01, 5.5664e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0024373154155910015 tensor([9.5171e-01, 4.5622e-02, 8.2999e-07, 2.4373e-03, 2.3206e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0035292976535856724 tensor([8.2704e-04, 8.4016e-01, 1.5547e-01, 1.2621e-05, 3.5293e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03288106247782707 tensor([8.8163e-01, 7.7326e-02, 2.2757e-05, 3.2881e-02, 8.1410e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14007620513439178 tensor([0.1548, 0.6634, 0.0151, 0.0267, 0.1401], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00036448132595978677 tensor([1.0323e-08, 3.6448e-04, 9.7652e-01, 4.4579e-07, 2.3119e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012650694698095322 tensor([4.1979e-06, 1.2651e-02, 8.8952e-01, 2.5982e-05, 9.7802e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007014312315732241 tensor([3.0682e-05, 2.5398e-01, 7.3897e-01, 2.0935e-06, 7.0143e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0024018895346671343 tensor([1.2444e-01, 8.5859e-05, 2.9029e-08, 8.7308e-01, 2.4019e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010363692417740822 tensor([0.0011, 0.0105, 0.0092, 0.0104, 0.9689], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00489196227863431 tensor([9.8049e-01, 4.8920e-03, 7.8645e-08, 1.4516e-02, 1.0157e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18484117090702057 tensor([5.5826e-01, 3.1347e-02, 4.4323e-05, 2.2551e-01, 1.8484e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.048673707991838455 tensor([8.8368e-01, 4.8674e-02, 1.4825e-05, 5.5620e-02, 1.2013e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04402131587266922 tensor([8.9874e-01, 4.6122e-02, 7.6399e-06, 4.4021e-02, 1.1109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.472249914513668e-05 tensor([6.2093e-10, 1.4722e-05, 7.5699e-01, 1.6365e-06, 2.4299e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2348103837211966e-06 tensor([6.4756e-01, 1.2348e-06, 3.6812e-13, 3.5244e-01, 9.5799e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1849403828382492 tensor([1.8494e-01, 8.1735e-03, 5.7976e-05, 5.4418e-01, 2.6265e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.042539723217487335 tensor([2.0434e-04, 3.2760e-01, 6.2962e-01, 3.2727e-05, 4.2540e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.838982517365366e-05 tensor([5.8078e-01, 6.8390e-05, 9.0189e-10, 4.1909e-01, 6.3754e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9122255253023468e-05 tensor([1.9046e-10, 1.9122e-05, 9.5776e-01, 1.5733e-07, 4.2217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027998508885502815 tensor([0.0016, 0.0028, 0.0022, 0.0639, 0.9295], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.034706976264715195 tensor([0.0301, 0.8088, 0.0347, 0.0030, 0.1234], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014817685587331653 tensor([1.3625e-01, 8.6124e-01, 6.5903e-04, 3.7577e-04, 1.4818e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006195268360897899 tensor([8.6186e-01, 6.1953e-04, 1.3899e-08, 1.3729e-01, 2.2292e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03217672184109688 tensor([4.5864e-04, 3.2177e-02, 8.1824e-02, 1.8520e-03, 8.8369e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009310044348239899 tensor([9.5463e-01, 3.5673e-02, 2.1298e-06, 9.3100e-03, 3.8671e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.041179586201906204 tensor([0.2891, 0.6578, 0.0021, 0.0099, 0.0412], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.013607809203677e-05 tensor([9.2804e-12, 2.0136e-05, 9.9924e-01, 9.0704e-10, 7.3828e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07524708658456802 tensor([2.5965e-04, 7.5247e-02, 4.1352e-01, 1.0491e-03, 5.0993e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09534605592489243 tensor([4.1678e-01, 9.4182e-03, 1.8066e-05, 4.7843e-01, 9.5346e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004540535621345043 tensor([7.2782e-04, 2.4018e-03, 4.5405e-03, 4.6336e-02, 9.4599e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016168972477316856 tensor([8.4711e-04, 1.0206e-02, 1.6273e-02, 1.6169e-02, 9.5650e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.032597221434116364 tensor([0.1558, 0.8053, 0.0027, 0.0036, 0.0326], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.7806992218538653e-06 tensor([2.3555e-01, 4.9120e-07, 6.0999e-13, 7.6444e-01, 1.7807e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01673540472984314 tensor([3.9064e-06, 1.6735e-02, 9.3304e-01, 9.1389e-06, 5.0213e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0023426972329616547 tensor([6.9590e-01, 3.0102e-01, 3.6181e-05, 2.3427e-03, 6.9867e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006708353757858276 tensor([5.2634e-01, 4.7238e-01, 4.1642e-05, 6.7084e-04, 5.6874e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007955196313560009 tensor([2.5744e-04, 5.8013e-01, 4.1164e-01, 1.1011e-05, 7.9552e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00036822690162807703 tensor([1.7057e-06, 4.6435e-06, 3.6823e-04, 1.1970e-02, 9.8766e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15652485191822052 tensor([0.5184, 0.2093, 0.0013, 0.1565, 0.1145], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23434589803218842 tensor([0.0055, 0.4859, 0.2706, 0.0036, 0.2343], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.3844693714636378e-05 tensor([8.6552e-01, 2.3845e-05, 2.4844e-11, 1.3445e-01, 4.3850e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008078836253844202 tensor([5.0988e-02, 9.4771e-01, 8.0788e-04, 7.4210e-05, 4.2375e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0020578154362738132 tensor([2.0578e-03, 1.8606e-06, 2.4464e-08, 9.9286e-01, 5.0815e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009726443327963352 tensor([0.0010, 0.0097, 0.0087, 0.0103, 0.9703], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00047786394134163857 tensor([9.7236e-07, 4.7786e-04, 2.1312e-01, 3.1781e-04, 7.8608e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03616052120923996 tensor([5.9541e-01, 1.3312e-02, 1.1804e-05, 3.5511e-01, 3.6161e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004278484266251326 tensor([2.1072e-02, 9.7267e-01, 4.2785e-03, 7.2101e-05, 1.9054e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000834316189866513 tensor([8.3432e-04, 8.9805e-07, 2.6945e-08, 9.8318e-01, 1.5989e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16650953888893127 tensor([0.1665, 0.2516, 0.0051, 0.0856, 0.4912], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10681506991386414 tensor([0.1068, 0.0546, 0.0016, 0.2676, 0.5695], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02333918772637844 tensor([9.3995e-01, 2.3339e-02, 2.7595e-06, 3.3233e-02, 3.4762e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028644798323512077 tensor([3.5066e-06, 2.8645e-02, 9.3901e-01, 3.2290e-06, 3.2338e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001161152496933937 tensor([9.9428e-01, 4.5494e-03, 8.2673e-09, 1.1612e-03, 5.6101e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002006596769206226 tensor([1.1985e-07, 9.6720e-05, 3.0502e-01, 2.0066e-04, 6.9468e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001789542962796986 tensor([9.1924e-01, 7.8763e-02, 1.6074e-06, 1.7895e-03, 2.0912e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010688534239307046 tensor([3.6012e-01, 6.3801e-01, 1.1563e-04, 6.8910e-04, 1.0689e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0332450345158577 tensor([1.3614e-02, 8.5755e-01, 3.3245e-02, 5.1177e-04, 9.5078e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00775177450850606 tensor([3.4095e-02, 2.3563e-05, 2.0143e-08, 9.5813e-01, 7.7518e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006333795608952641 tensor([4.6829e-08, 6.3338e-04, 9.4594e-01, 2.2874e-06, 5.3423e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002398452255874872 tensor([1.6970e-05, 4.6643e-04, 1.0897e-02, 2.3985e-03, 9.8622e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013652868568897247 tensor([1.3653e-02, 3.1006e-03, 4.3128e-04, 4.3028e-01, 5.5253e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01532234437763691 tensor([0.1441, 0.8379, 0.0014, 0.0013, 0.0153], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003735111327841878 tensor([9.3395e-01, 3.7351e-03, 1.1353e-07, 6.2007e-02, 3.0394e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003384152427315712 tensor([4.2878e-02, 9.5071e-01, 3.3842e-03, 2.7905e-04, 2.7444e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017682699486613274 tensor([1.0188e-01, 6.4119e-04, 1.6700e-06, 8.7980e-01, 1.7683e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24761483073234558 tensor([0.3186, 0.0545, 0.0004, 0.3789, 0.2476], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008032529731281102 tensor([8.6806e-09, 8.0325e-04, 9.9542e-01, 7.0847e-08, 3.7812e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00243619317188859 tensor([5.2240e-03, 1.0898e-06, 1.6629e-09, 9.9234e-01, 2.4362e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16680964827537537 tensor([0.3382, 0.1668, 0.0009, 0.1064, 0.3877], grad_fn=<SoftmaxBackward0>)\n",
      "1 9.372072963742539e-05 tensor([8.1347e-01, 9.3721e-05, 5.8067e-10, 1.8640e-01, 3.8308e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031074003782123327 tensor([9.9315e-01, 3.7105e-03, 1.4456e-08, 3.1074e-03, 2.7917e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01269962452352047 tensor([8.9172e-01, 9.1074e-02, 1.1382e-05, 1.2700e-02, 4.4974e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00552777387201786 tensor([5.5278e-03, 1.2035e-03, 2.0132e-04, 2.7029e-01, 7.2278e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11975647509098053 tensor([2.0691e-04, 2.9066e-01, 5.8932e-01, 5.0098e-05, 1.1976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02455419860780239 tensor([2.5494e-04, 2.4554e-02, 1.6730e-01, 3.3289e-03, 8.0456e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018925275653600693 tensor([8.0041e-01, 1.9740e-01, 9.5914e-06, 1.8925e-03, 2.9044e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016740375431254506 tensor([4.6525e-08, 1.6740e-03, 9.8776e-01, 3.2591e-07, 1.0567e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017300210893154144 tensor([1.7300e-02, 2.2496e-03, 1.8355e-04, 6.1275e-01, 3.6752e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0055694496259093285 tensor([0.0056, 0.0032, 0.0012, 0.2402, 0.7498], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04424808546900749 tensor([0.0316, 0.0442, 0.0050, 0.1370, 0.7822], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03477035462856293 tensor([0.0032, 0.0447, 0.0348, 0.0181, 0.8992], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03282590210437775 tensor([6.0689e-06, 3.2826e-02, 9.1565e-01, 4.9498e-06, 5.1514e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.4890554414014332e-05 tensor([9.3261e-01, 2.4891e-05, 1.3988e-11, 6.7367e-02, 2.2091e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008393518510274589 tensor([1.7104e-08, 8.3935e-04, 9.9099e-01, 2.2925e-07, 8.1744e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06077064573764801 tensor([0.0364, 0.0608, 0.0053, 0.0773, 0.8203], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.388827172377205e-06 tensor([9.3097e-01, 1.3888e-06, 6.5632e-14, 6.9025e-02, 9.1744e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2363334596157074 tensor([5.5581e-04, 2.3633e-01, 4.9770e-01, 2.9827e-04, 2.6511e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.1567924704868346e-05 tensor([5.8396e-01, 1.2068e-05, 3.7126e-11, 4.1601e-01, 2.1568e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004038050305098295 tensor([5.4862e-05, 4.0381e-03, 5.0664e-02, 1.6538e-03, 9.4359e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2202761471271515 tensor([0.1468, 0.2331, 0.0124, 0.2203, 0.3875], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0020363095682114363 tensor([2.0363e-03, 1.2053e-03, 7.9777e-04, 1.6477e-01, 8.3119e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00036469625774770975 tensor([2.2974e-09, 3.6470e-04, 9.9627e-01, 3.7189e-08, 3.3677e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.036906078457832336 tensor([3.1729e-04, 3.6906e-02, 1.2924e-01, 1.8160e-03, 8.3172e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003009939333423972 tensor([1.7198e-07, 7.5024e-03, 9.8949e-01, 1.7120e-07, 3.0099e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06566821783781052 tensor([4.7708e-04, 6.5668e-02, 2.2039e-01, 1.5844e-03, 7.1188e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004490769933909178 tensor([9.2115e-01, 7.3997e-02, 3.1979e-06, 4.4908e-03, 3.5635e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032101795077323914 tensor([6.2590e-08, 3.9280e-03, 9.9286e-01, 1.1983e-07, 3.2102e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.023675840348005295 tensor([0.0040, 0.0237, 0.0113, 0.0266, 0.9345], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09964344650506973 tensor([7.1282e-01, 9.9643e-02, 1.6130e-04, 1.5672e-01, 3.0650e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021609365940093994 tensor([0.0216, 0.0081, 0.0011, 0.4049, 0.5643], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12983015179634094 tensor([0.1298, 0.7091, 0.0084, 0.0116, 0.1411], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30425962805747986 tensor([0.3043, 0.3259, 0.0023, 0.0612, 0.3064], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15707743167877197 tensor([1.5708e-01, 7.3182e-03, 4.3910e-05, 6.4814e-01, 1.8742e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005393793690018356 tensor([2.8498e-09, 5.3938e-04, 9.9450e-01, 2.8456e-08, 4.9626e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01623172126710415 tensor([1.6232e-02, 3.0184e-04, 6.9524e-06, 8.4315e-01, 1.4030e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16569529473781586 tensor([0.2269, 0.1660, 0.0025, 0.1657, 0.4389], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001810566522181034 tensor([2.4262e-01, 7.5450e-01, 3.2418e-04, 7.4440e-04, 1.8106e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00015240984794218093 tensor([6.3662e-06, 1.0058e-05, 1.5241e-04, 1.3836e-02, 9.8600e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02367972396314144 tensor([0.0109, 0.1191, 0.0237, 0.0144, 0.8318], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007508513052016497 tensor([7.5085e-03, 7.1954e-03, 9.1904e-04, 5.1413e-02, 9.3296e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1825488656759262 tensor([0.2047, 0.1825, 0.0037, 0.1663, 0.4428], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0025298118125647306 tensor([1.9089e-01, 8.0552e-01, 5.1745e-04, 5.4393e-04, 2.5298e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10029052197933197 tensor([1.7468e-01, 3.0180e-03, 8.8568e-06, 7.2200e-01, 1.0029e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013897735625505447 tensor([2.4833e-06, 1.3898e-02, 9.2067e-01, 7.3541e-06, 6.5422e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.362654287135229e-05 tensor([9.5515e-01, 7.3627e-05, 7.6947e-11, 4.4772e-02, 5.4392e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003381500137038529 tensor([9.9112e-01, 8.5345e-03, 6.8604e-09, 3.3815e-04, 4.4313e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009186071692965925 tensor([9.7420e-01, 2.4873e-02, 1.3389e-07, 9.1861e-04, 1.1028e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004220420669298619 tensor([1.3993e-06, 5.0354e-06, 4.2204e-04, 1.0898e-02, 9.8867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003225082764402032 tensor([0.0015, 0.0032, 0.0028, 0.0575, 0.9349], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21165411174297333 tensor([0.3370, 0.3554, 0.0039, 0.0920, 0.2117], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002251827158033848 tensor([9.3038e-01, 6.7004e-02, 1.4971e-06, 2.2518e-03, 3.6482e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01111992821097374 tensor([1.1120e-02, 9.6916e-01, 1.6465e-02, 1.2958e-04, 3.1242e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3443854186334647e-05 tensor([1.3444e-05, 5.0836e-07, 3.2209e-06, 3.5651e-01, 6.4347e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019258247688412666 tensor([5.3013e-05, 1.9258e-02, 3.0154e-01, 3.7808e-04, 6.7877e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026691844686865807 tensor([3.8465e-04, 4.7782e-01, 4.9507e-01, 4.0085e-05, 2.6692e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03201425448060036 tensor([0.0471, 0.8948, 0.0233, 0.0028, 0.0320], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004315659869462252 tensor([1.8344e-07, 7.6216e-03, 9.8806e-01, 1.9596e-07, 4.3157e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.674667929473799e-07 tensor([7.2444e-01, 6.6747e-07, 6.9944e-14, 2.7555e-01, 2.0966e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0021065673790872097 tensor([5.1681e-07, 2.1066e-03, 8.5688e-01, 2.1268e-05, 1.4099e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00031746336026117206 tensor([8.3140e-01, 3.1746e-04, 5.4863e-09, 1.6817e-01, 1.1027e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1619037389755249 tensor([0.0048, 0.2545, 0.1619, 0.0051, 0.5737], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027792874723672867 tensor([7.6659e-04, 6.4893e-01, 3.2247e-01, 4.5467e-05, 2.7793e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012916740961372852 tensor([1.2917e-02, 2.1034e-05, 7.5624e-08, 9.6931e-01, 1.7750e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0234779454767704 tensor([3.6706e-03, 8.8129e-01, 9.1456e-02, 1.0719e-04, 2.3478e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012977906502783298 tensor([2.4341e-02, 4.0106e-05, 1.2902e-07, 9.6264e-01, 1.2978e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004310441669076681 tensor([9.8489e-01, 4.3104e-03, 4.3996e-08, 1.0651e-02, 1.4604e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00022199060185812414 tensor([7.1757e-01, 2.8216e-01, 4.1186e-06, 2.2199e-04, 4.0761e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.591458936280105e-06 tensor([3.4123e-02, 1.1857e-07, 1.4398e-12, 9.6587e-01, 8.5915e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00038717742427252233 tensor([7.1702e-01, 1.8935e-04, 3.0473e-09, 2.8240e-01, 3.8718e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019310614094138145 tensor([1.0287e-03, 6.4873e-01, 3.3084e-01, 8.8583e-05, 1.9311e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.018537484109401703 tensor([1.9748e-04, 1.8537e-02, 1.0563e-01, 2.1398e-03, 8.7349e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008191355504095554 tensor([1.2290e-05, 1.0072e-01, 8.9107e-01, 2.8493e-06, 8.1914e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.021291252225637436 tensor([4.9858e-01, 4.8289e-03, 2.2114e-06, 4.7530e-01, 2.1291e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03329882025718689 tensor([2.3382e-05, 8.5636e-02, 8.8103e-01, 1.4175e-05, 3.3299e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008371074683964252 tensor([3.5543e-04, 8.3711e-03, 3.0527e-02, 7.8349e-03, 9.5291e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01186112780123949 tensor([7.1505e-07, 1.2146e-02, 9.7599e-01, 1.0075e-06, 1.1861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.3365499146166258e-05 tensor([2.2565e-11, 2.3365e-05, 9.9870e-01, 2.9824e-09, 1.2763e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004902649088762701 tensor([6.9213e-03, 8.3590e-07, 7.5954e-10, 9.9259e-01, 4.9026e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003675879444926977 tensor([0.0028, 0.0037, 0.0025, 0.1197, 0.8713], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00041257194243371487 tensor([7.7048e-01, 4.1257e-04, 1.1149e-08, 2.2877e-01, 3.4563e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00083848339272663 tensor([9.7900e-01, 8.3848e-04, 3.9028e-09, 2.0140e-02, 2.3193e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06121426075696945 tensor([3.2595e-05, 6.1214e-02, 8.0218e-01, 4.1441e-05, 1.3653e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0053843045607209206 tensor([1.1414e-02, 6.1959e-06, 1.3490e-08, 9.8320e-01, 5.3843e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12615205347537994 tensor([3.3884e-01, 1.8363e-02, 6.0684e-05, 5.1659e-01, 1.2615e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013606604188680649 tensor([1.3607e-02, 4.3518e-05, 3.7491e-07, 9.6667e-01, 1.9684e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008956893347203732 tensor([6.5635e-01, 5.5578e-03, 1.8599e-06, 3.2913e-01, 8.9569e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030859043821692467 tensor([8.0375e-01, 1.5095e-01, 6.8411e-05, 3.0859e-02, 1.4371e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.729220406967215e-05 tensor([3.7292e-05, 5.7736e-07, 1.2970e-06, 6.1729e-01, 3.8268e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13421940803527832 tensor([3.1385e-01, 3.8157e-02, 2.2585e-04, 5.1354e-01, 1.3422e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002632073825225234 tensor([0.0026, 0.0023, 0.0009, 0.1015, 0.8927], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05018758028745651 tensor([0.0144, 0.0502, 0.0123, 0.0617, 0.8614], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009873448871076107 tensor([8.5962e-08, 1.3008e-02, 9.8600e-01, 2.4602e-08, 9.8734e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08097614347934723 tensor([4.2813e-01, 1.4105e-02, 3.1114e-05, 4.7676e-01, 8.0976e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021196667512413114 tensor([8.9770e-01, 2.1197e-04, 1.1393e-09, 1.0205e-01, 4.2750e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02176428772509098 tensor([4.9409e-05, 1.8999e-01, 7.8818e-01, 1.1212e-05, 2.1764e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09503750503063202 tensor([6.5056e-01, 1.7260e-01, 2.7092e-04, 8.1528e-02, 9.5038e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005221574567258358 tensor([5.7658e-03, 9.8864e-01, 5.2216e-03, 6.6839e-06, 3.7022e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.8170433122577379e-06 tensor([9.7663e-01, 1.8170e-06, 3.6451e-14, 2.3371e-02, 2.0339e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02236904762685299 tensor([5.1505e-04, 5.9148e-01, 3.8560e-01, 3.7659e-05, 2.2369e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007854588329792023 tensor([0.0031, 0.0079, 0.0034, 0.0395, 0.9461], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04418902471661568 tensor([0.0442, 0.4389, 0.0264, 0.0138, 0.4767], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01538866851478815 tensor([3.8062e-06, 1.5389e-02, 9.2966e-01, 1.3116e-05, 5.4933e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.273509719292633e-05 tensor([3.2735e-05, 1.1899e-06, 3.6378e-06, 3.8989e-01, 6.1008e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020854901522397995 tensor([0.0209, 0.1552, 0.0132, 0.0129, 0.7978], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08407429605722427 tensor([0.0841, 0.1951, 0.0090, 0.0809, 0.6309], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07113219797611237 tensor([0.0098, 0.8440, 0.0711, 0.0008, 0.0743], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07477683573961258 tensor([0.0123, 0.4047, 0.0748, 0.0049, 0.5033], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004527095705270767 tensor([5.8171e-01, 4.1033e-01, 1.4500e-04, 4.5271e-03, 3.2810e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007021182216703892 tensor([1.7192e-06, 7.0212e-03, 8.7343e-01, 1.2902e-05, 1.1954e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010578202782198787 tensor([9.5383e-01, 1.0578e-03, 1.3075e-08, 4.4924e-02, 1.8946e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00020239809236954898 tensor([8.3232e-01, 2.0240e-04, 2.0916e-09, 1.6733e-01, 1.4922e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04094313085079193 tensor([0.0409, 0.8321, 0.0314, 0.0032, 0.0925], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005804281099699438 tensor([5.8043e-04, 1.3183e-04, 1.2969e-04, 3.1775e-01, 6.8141e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00022428983356803656 tensor([6.2071e-05, 4.4395e-05, 2.2429e-04, 4.7248e-02, 9.5242e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004414185415953398 tensor([6.2275e-06, 1.3045e-01, 8.6513e-01, 7.4272e-07, 4.4142e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00034423175384290516 tensor([9.2044e-01, 7.9194e-02, 4.2906e-07, 3.4423e-04, 1.7306e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03059304505586624 tensor([9.9475e-06, 3.0593e-02, 8.9667e-01, 1.5474e-05, 7.2709e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.7303380193188787e-05 tensor([7.1368e-01, 2.7303e-05, 1.1021e-10, 2.8628e-01, 1.3125e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008384400280192494 tensor([2.4658e-08, 8.3844e-04, 9.9160e-01, 4.2202e-07, 7.5621e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014636009000241756 tensor([1.4636e-02, 8.0016e-04, 4.6764e-05, 6.3084e-01, 3.5367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07833448797464371 tensor([6.2274e-01, 6.6882e-02, 1.3681e-04, 2.3191e-01, 7.8334e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04088611528277397 tensor([0.1824, 0.7707, 0.0025, 0.0035, 0.0409], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1828264594078064 tensor([0.2515, 0.1828, 0.0028, 0.1729, 0.3901], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05524371191859245 tensor([6.2566e-01, 1.6893e-02, 1.8949e-05, 3.0218e-01, 5.5244e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08805851638317108 tensor([8.8059e-02, 2.2170e-02, 5.2885e-04, 3.1636e-01, 5.7289e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.034907639026641846 tensor([3.0848e-05, 3.4908e-02, 7.1221e-01, 9.0629e-05, 2.5276e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005072070751339197 tensor([1.3870e-07, 7.5244e-03, 9.8740e-01, 1.2898e-07, 5.0721e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006959134712815285 tensor([6.9591e-03, 1.8615e-05, 2.1416e-07, 9.6446e-01, 2.8557e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010907875839620829 tensor([8.9790e-01, 1.0908e-03, 2.2691e-08, 1.0052e-01, 4.8428e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [0, 3, 2], [0, 2, 3], [2, 1, 4], [0, 3, 1], [3, 2, 0], [3, 0, 4], [0, 3, 1], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 3, 4], [2, 1, 4], [2, 1, 0], [0, 3, 4], [2, 1, 4], [0, 3, 1], [2, 4, 1], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 3, 1], [0, 3, 2], [3, 2, 4], [3, 0, 4], [2, 1, 4], [2, 1, 0], [2, 4, 3], [2, 1, 4], [0, 3, 1], [0, 1, 2], [0, 3, 1], [2, 4, 1], [3, 4, 2], [3, 0, 4], [2, 1, 4], [2, 1, 4], [2, 1, 4], [2, 1, 4], [3, 0, 4], [2, 1, 4], [3, 0, 4], [0, 1, 2], [2, 4, 1], [2, 4, 3], [1, 0, 2], [0, 3, 1], [2, 4, 1], [2, 4, 3], [3, 0, 4], [2, 4, 3], [2, 3, 4], [0, 3, 1], [0, 3, 1], [3, 0, 4], [2, 1, 4], [0, 2, 3], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [2, 1, 0], [3, 0, 4], [2, 4, 1], [0, 3, 1], [0, 2, 1], [3, 0, 2], [3, 2, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3], [2, 3, 4], [0, 3, 1], [0, 3, 1], [2, 1, 4], [0, 1, 2], [0, 1, 3], [2, 3, 4], [2, 1, 4], [0, 3, 1], [2, 4, 3], [2, 4, 3], [3, 0, 4], [0, 1, 2], [2, 4, 3], [3, 0], [2, 4, 1], [3, 4, 2], [2, 1, 0], [0, 2, 1], [0, 3, 1], [2, 1, 4], [3, 4, 2], [2, 1, 0], [2, 3, 0], [2, 1, 0], [2, 4, 1], [3, 0, 1], [2, 4, 3], [0, 1, 3], [2, 4, 3], [2, 3, 4], [3, 0, 2], [2, 1, 4], [0, 3, 1], [0, 1, 3], [2, 1, 0], [3, 2, 4], [2, 4, 1], [3, 4, 2], [2, 1, 4], [2, 1], [0, 3, 1], [2, 1, 4], [2, 3, 1], [2, 4, 1], [2, 4, 3], [2, 4, 3], [2, 1, 0], [3, 0, 4], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [2, 1, 0], [2, 0, 1], [0, 3, 2], [3, 0, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1], [2, 4, 1], [3, 0], [2, 1, 4], [0, 3, 1], [2, 0], [2, 1, 0], [0, 3, 1], [0, 3, 1], [0, 3, 4], [0, 3, 1], [2, 4, 3], [0, 3, 4], [0, 2, 1], [2, 4, 1], [2, 1, 0], [2, 3, 0], [2, 3], [2, 1, 0], [0, 3, 1], [2, 1, 0], [2, 1, 3], [2, 3, 4], [0, 1, 2], [0, 3, 2], [2, 1, 0], [2, 3, 1], [2, 3, 4], [2, 1, 4], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 4, 3], [0, 1, 2], [0, 2, 1], [2, 3], [2, 4, 3], [3, 4, 0], [1, 2, 0], [0, 3, 1], [3, 0, 4], [3, 2, 4], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 2], [3, 0, 4], [2, 1, 0], [3, 0, 4], [2, 1, 4], [2, 4, 1], [2, 4, 3], [2, 1, 4], [2, 1, 4], [3, 0, 4], [0, 3, 1], [3, 0, 4], [2, 1, 4], [3, 0, 4], [0, 3, 1], [0, 3, 4], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 1, 0], [2, 1, 4], [2, 4, 3], [1, 2, 0], [2, 1, 4], [2, 1, 0], [2, 0, 1], [3, 0, 4], [2, 1, 4], [2, 4, 1], [3, 0, 4], [2, 3, 4], [3, 4, 2], [2, 4, 1], [3, 0, 4], [0, 2, 1], [3, 2, 0], [0, 3, 1], [1, 2, 0], [3, 2, 0], [2, 3, 0], [3, 0, 2], [3, 0, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [2, 4, 1], [3, 2, 0], [1, 2, 0], [1, 0, 2], [3, 0, 4], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 1, 0], [2, 1, 4], [2, 3, 4], [2, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [3, 0, 4], [2, 1, 0], [2, 4, 1]]\n",
      "[[0, 3], [1, 4], [1, 4], [0, 3], [2, 4], [1, 4], [1, 2], [2, 4], [0, 3], [2, 4], [2, 4], [2, 4], [2, 4], [0, 3], [1, 2], [0, 3], [3, 4], [1, 2], [0, 3], [2, 4], [0, 3], [0, 3], [1, 2], [2, 4], [2, 4], [1, 4], [0, 1], [1, 2], [0, 3], [3, 4], [0, 1], [0, 3], [2, 4], [3, 4], [2, 4], [0, 3], [0, 1], [1, 2], [0, 3], [0, 3], [0, 3], [0, 3], [1, 2], [0, 3], [1, 2], [3, 4], [0, 3], [0, 1], [3, 4], [2, 4], [0, 3], [0, 1], [1, 2], [0, 1], [0, 1], [2, 4], [2, 4], [1, 2], [0, 3], [1, 4], [0, 3], [0, 3], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [1, 2], [0, 3], [2, 4], [3, 4], [1, 4], [0, 1], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [0, 3], [3, 4], [2, 4], [0, 1], [0, 3], [2, 4], [0, 1], [0, 1], [1, 2], [3, 4], [0, 1], [1, 2, 4], [0, 3], [0, 1], [3, 4], [3, 4], [2, 4], [0, 3], [0, 1], [3, 4], [1, 4], [3, 4], [0, 3], [2, 4], [0, 1], [2, 4], [0, 1], [0, 1], [1, 4], [0, 3], [2, 4], [2, 4], [3, 4], [0, 1], [0, 3], [0, 1], [0, 3], [0, 3, 4], [2, 4], [0, 3], [0, 4], [0, 3], [0, 1], [0, 1], [3, 4], [1, 2], [2, 4], [0, 1], [2, 4], [3, 4], [3, 4], [3, 4], [1, 4], [2, 4], [0, 3], [2, 4], [3, 4], [0, 3], [1, 2, 4], [0, 3], [2, 4], [1, 3, 4], [3, 4], [2, 4], [2, 4], [1, 2], [2, 4], [0, 1], [1, 2], [3, 4], [0, 3], [3, 4], [1, 4], [0, 1, 4], [3, 4], [2, 4], [3, 4], [0, 4], [0, 1], [3, 4], [1, 4], [3, 4], [0, 4], [0, 1], [0, 3], [2, 4], [0, 3], [0, 1], [0, 1], [3, 4], [3, 4], [0, 1, 4], [0, 1], [1, 2], [3, 4], [2, 4], [1, 2], [0, 1], [1, 2], [0, 3], [2, 4], [0, 3], [1, 4], [1, 2], [3, 4], [1, 2], [0, 3], [0, 3], [0, 1], [0, 3], [0, 3], [1, 2], [2, 4], [1, 2], [0, 3], [1, 2], [2, 4], [1, 2], [2, 4], [0, 3], [3, 4], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [3, 4], [0, 3], [0, 1], [3, 4], [0, 3], [3, 4], [3, 4], [1, 2], [0, 3], [0, 3], [1, 2], [0, 1], [0, 1], [0, 3], [1, 2], [3, 4], [1, 4], [2, 4], [3, 4], [1, 4], [1, 4], [1, 4], [1, 4], [0, 1], [2, 4], [0, 3], [0, 3], [1, 4], [3, 4], [3, 4], [1, 2], [0, 1], [2, 4], [0, 3], [2, 4], [3, 4], [0, 3], [0, 1], [0, 4], [0, 3], [3, 4], [2, 4], [1, 2], [3, 4], [0, 3]]\n",
      "NL_pred of 2th iteration [[2, 4, 1], [0, 3, 2], [0, 2, 3], [2, 1, 4], [0, 3, 1], [3, 2, 0], [3, 0, 4], [0, 3, 1], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [0, 3, 4], [2, 1, 4], [2, 1, 0], [0, 3, 4], [2, 1, 4], [0, 3, 1], [2, 4, 1], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 3, 1], [0, 3, 2], [3, 2, 4], [3, 0, 4], [2, 1, 4], [2, 1, 0], [2, 4, 3], [2, 1, 4], [0, 3, 1], [0, 1, 2], [0, 3, 1], [2, 4, 1], [3, 4, 2], [3, 0, 4], [2, 1, 4], [2, 1, 4], [2, 1, 4], [2, 1, 4], [3, 0, 4], [2, 1, 4], [3, 0, 4], [0, 1, 2], [2, 4, 1], [2, 4, 3], [1, 0, 2], [0, 3, 1], [2, 4, 1], [2, 4, 3], [3, 0, 4], [2, 4, 3], [2, 3, 4], [0, 3, 1], [0, 3, 1], [3, 0, 4], [2, 1, 4], [0, 2, 3], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [2, 1, 0], [3, 0, 4], [2, 4, 1], [0, 3, 1], [0, 2, 1], [3, 0, 2], [3, 2, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3], [2, 3, 4], [0, 3, 1], [0, 3, 1], [2, 1, 4], [0, 1, 2], [0, 1, 3], [2, 3, 4], [2, 1, 4], [0, 3, 1], [2, 4, 3], [2, 4, 3], [3, 0, 4], [0, 1, 2], [2, 4, 3], [2, 4, 1], [3, 4, 2], [2, 1, 0], [0, 2, 1], [0, 3, 1], [2, 1, 4], [3, 4, 2], [2, 1, 0], [2, 3, 0], [2, 1, 0], [2, 4, 1], [3, 0, 1], [2, 4, 3], [0, 1, 3], [2, 4, 3], [2, 3, 4], [3, 0, 2], [2, 1, 4], [0, 3, 1], [0, 1, 3], [2, 1, 0], [3, 2, 4], [2, 4, 1], [3, 4, 2], [2, 1, 4], [0, 3, 1], [2, 1, 4], [2, 3, 1], [2, 4, 1], [2, 4, 3], [2, 4, 3], [2, 1, 0], [3, 0, 4], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [2, 1, 0], [2, 0, 1], [0, 3, 2], [3, 0, 1], [2, 4, 1], [0, 3, 1], [2, 0, 1], [2, 4, 1], [2, 1, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [0, 3, 4], [0, 3, 1], [2, 4, 3], [0, 3, 4], [0, 2, 1], [2, 4, 1], [2, 1, 0], [2, 3, 0], [2, 1, 0], [0, 3, 1], [2, 1, 0], [2, 1, 3], [2, 3, 4], [0, 1, 2], [0, 3, 2], [2, 1, 0], [2, 3, 1], [2, 3, 4], [2, 1, 4], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 4, 3], [0, 1, 2], [0, 2, 1], [2, 4, 3], [3, 4, 0], [1, 2, 0], [0, 3, 1], [3, 0, 4], [3, 2, 4], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 4, 1], [0, 3, 2], [3, 0, 4], [2, 1, 0], [3, 0, 4], [2, 1, 4], [2, 4, 1], [2, 4, 3], [2, 1, 4], [2, 1, 4], [3, 0, 4], [0, 3, 1], [3, 0, 4], [2, 1, 4], [3, 0, 4], [0, 3, 1], [0, 3, 4], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 4, 1], [2, 4, 1], [0, 3, 1], [2, 1, 4], [2, 1, 4], [2, 1, 0], [2, 1, 4], [2, 4, 3], [1, 2, 0], [2, 1, 4], [2, 1, 0], [2, 0, 1], [3, 0, 4], [2, 1, 4], [2, 4, 1], [3, 0, 4], [2, 3, 4], [3, 4, 2], [2, 4, 1], [3, 0, 4], [0, 2, 1], [3, 2, 0], [0, 3, 1], [1, 2, 0], [3, 2, 0], [2, 3, 0], [3, 0, 2], [3, 0, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [2, 4, 1], [3, 2, 0], [1, 2, 0], [1, 0, 2], [3, 0, 4], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 1, 0], [2, 1, 4], [2, 3, 4], [2, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [3, 0, 4], [2, 1, 0], [2, 4, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004693504728254725  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004693450986361894  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004693348876765517  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004693203285092213  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004693020562656591  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004692805106522607  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004692561802316884  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004692295047103382  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004692007283695409  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004691704374844911  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004691387786239874  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004691061914944258  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00469072871520871  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004690389652721217  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004690047658857752  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004689705176431624  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00468936318256816  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004689021677267356  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004688682614779863  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004688348437918991  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.1654539853334427 tensor([7.4840e-01, 6.9433e-02, 8.8768e-05, 1.6545e-01, 1.6622e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1522119641304016 tensor([0.0025, 0.1522, 0.1235, 0.0041, 0.7177], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08242003619670868 tensor([0.0148, 0.0824, 0.0177, 0.0371, 0.8481], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003448646981269121 tensor([3.4486e-03, 1.3954e-07, 7.3782e-11, 9.9625e-01, 3.0552e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18213750422000885 tensor([0.0019, 0.1343, 0.1821, 0.0046, 0.6770], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37334269285202026 tensor([0.0386, 0.3733, 0.0243, 0.0165, 0.5473], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4097454845905304 tensor([1.5388e-04, 4.0975e-01, 5.6804e-01, 1.3344e-05, 2.2044e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04935900866985321 tensor([1.0964e-07, 1.2785e-03, 9.4936e-01, 2.3959e-06, 4.9359e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015671702101826668 tensor([1.5672e-02, 3.0197e-05, 1.3394e-07, 9.7416e-01, 1.0141e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22002971172332764 tensor([9.7075e-08, 4.3862e-04, 7.7951e-01, 1.9271e-05, 2.2003e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3469412624835968 tensor([4.6023e-05, 2.0959e-02, 3.4694e-01, 2.6949e-04, 6.3178e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26111099123954773 tensor([1.5165e-05, 2.3777e-02, 7.1504e-01, 5.2649e-05, 2.6111e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08260507136583328 tensor([6.6040e-06, 2.5202e-02, 8.9217e-01, 1.3696e-05, 8.2605e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07301883399486542 tensor([9.2683e-01, 1.4153e-04, 3.6896e-10, 7.3019e-02, 1.3608e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012675181962549686 tensor([6.4777e-07, 1.2675e-02, 9.8288e-01, 7.2341e-07, 4.4393e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2624354958534241 tensor([7.2923e-01, 3.1688e-03, 4.9999e-07, 2.6244e-01, 5.1633e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12679646909236908 tensor([2.3634e-03, 7.6330e-04, 2.1275e-04, 1.2680e-01, 8.6986e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01289602741599083 tensor([8.4526e-07, 1.2896e-02, 9.7785e-01, 1.2382e-06, 9.2478e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08045244961977005 tensor([8.0452e-02, 6.1775e-05, 4.2878e-08, 9.1709e-01, 2.3988e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2148926854133606 tensor([6.3401e-04, 9.7389e-02, 2.1489e-01, 1.0726e-03, 6.8601e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19469784200191498 tensor([7.5014e-01, 3.6901e-02, 3.0867e-05, 1.9470e-01, 1.8227e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17656280100345612 tensor([8.2313e-01, 1.5992e-04, 1.5569e-09, 1.7656e-01, 1.4556e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14491674304008484 tensor([8.2917e-04, 8.4843e-01, 1.4492e-01, 1.6691e-05, 5.8037e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026948662474751472 tensor([8.5727e-04, 1.9964e-02, 2.6949e-02, 7.0027e-03, 9.4523e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10695463418960571 tensor([1.0656e-05, 1.3930e-03, 1.0695e-01, 1.6118e-03, 8.9003e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18875209987163544 tensor([0.0229, 0.1888, 0.0250, 0.0324, 0.7309], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11317560821771622 tensor([1.1318e-01, 8.8189e-01, 1.0803e-03, 6.3986e-04, 3.2157e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3592950701713562 tensor([1.9904e-04, 3.5930e-01, 6.0266e-01, 2.7091e-05, 3.7817e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08817265927791595 tensor([8.8173e-02, 4.0476e-05, 1.3102e-08, 9.0858e-01, 3.2117e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15260864794254303 tensor([0.0446, 0.0253, 0.0012, 0.1526, 0.7762], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021374573931097984 tensor([9.5344e-01, 2.1375e-02, 1.5598e-06, 2.4668e-02, 5.2072e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17660994827747345 tensor([1.7661e-01, 7.2345e-05, 1.5207e-08, 8.2227e-01, 1.0435e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16742640733718872 tensor([0.0026, 0.1644, 0.1674, 0.0050, 0.6606], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0038038792554289103 tensor([2.4439e-07, 2.1009e-06, 8.0733e-04, 3.8039e-03, 9.9539e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008076637983322144 tensor([3.7683e-09, 3.3116e-04, 9.9159e-01, 9.8481e-08, 8.0766e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012681419961154461 tensor([9.8716e-01, 1.5276e-04, 1.1991e-10, 1.2681e-02, 3.8057e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0326952189207077 tensor([3.2695e-02, 9.4130e-01, 1.2059e-02, 6.7916e-04, 1.3265e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0656547024846077 tensor([8.6870e-04, 9.3218e-01, 6.5655e-02, 6.1051e-06, 1.2943e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01182633452117443 tensor([1.1826e-02, 5.4730e-06, 1.1140e-08, 9.8542e-01, 2.7435e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3797856271266937 tensor([5.7506e-01, 1.4530e-02, 1.4885e-05, 3.7979e-01, 3.0609e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03687676414847374 tensor([3.6877e-02, 3.9362e-05, 6.7238e-08, 9.5376e-01, 9.3267e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24563361704349518 tensor([2.4563e-01, 1.1170e-03, 1.0656e-06, 7.4217e-01, 1.1080e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.054889753460884094 tensor([9.6203e-06, 5.4890e-02, 9.0740e-01, 5.4145e-06, 3.7700e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0547243170440197 tensor([5.4724e-02, 2.2510e-07, 2.0633e-12, 9.4526e-01, 1.3010e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3051811158657074 tensor([3.0322e-04, 3.0518e-01, 6.1398e-01, 9.6724e-05, 8.0435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013987076468765736 tensor([8.0807e-05, 4.2081e-04, 2.6586e-03, 1.3987e-02, 9.8285e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005631685256958008 tensor([9.9435e-01, 2.2669e-05, 1.4594e-12, 5.6317e-03, 1.0122e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.44291749596595764 tensor([5.2955e-01, 4.4292e-01, 4.6235e-04, 1.5419e-02, 1.1655e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02381153404712677 tensor([3.6042e-07, 1.9066e-07, 2.6308e-05, 2.3812e-02, 9.7616e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15733660757541656 tensor([2.0096e-04, 1.8386e-02, 1.5734e-01, 2.6659e-03, 8.2141e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1778838038444519 tensor([8.2193e-01, 1.3511e-04, 9.0842e-10, 1.7788e-01, 5.5825e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038362763822078705 tensor([9.5864e-01, 3.8363e-02, 6.7939e-07, 2.7734e-03, 2.2545e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.154282808303833 tensor([9.1407e-04, 8.4094e-01, 1.5428e-01, 1.4938e-05, 3.8520e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06475931406021118 tensor([8.8797e-01, 6.4759e-02, 1.9052e-05, 3.9047e-02, 8.2036e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17337025701999664 tensor([0.1734, 0.6217, 0.0141, 0.0346, 0.1562], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025916118174791336 tensor([1.0770e-08, 3.4512e-04, 9.7374e-01, 5.2490e-07, 2.5916e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11744454503059387 tensor([4.8788e-06, 1.2446e-02, 8.7007e-01, 3.4915e-05, 1.1744e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2513365149497986 tensor([3.3041e-05, 2.5134e-01, 7.4091e-01, 2.4372e-06, 7.7132e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10839676111936569 tensor([1.0840e-01, 6.5303e-05, 2.2406e-08, 8.8944e-01, 2.0948e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008792998269200325 tensor([0.0011, 0.0088, 0.0076, 0.0119, 0.9706], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017114734277129173 tensor([9.7874e-01, 4.0465e-03, 6.4701e-08, 1.7115e-02, 1.0005e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25657472014427185 tensor([5.4409e-01, 2.4969e-02, 3.4552e-05, 2.5657e-01, 1.7433e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06530582904815674 tensor([8.8238e-01, 4.0362e-02, 1.2253e-05, 6.5306e-02, 1.1940e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03837209567427635 tensor([8.9888e-01, 3.8372e-02, 6.3378e-06, 5.1703e-02, 1.1042e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2746763527393341 tensor([6.7491e-10, 1.3942e-05, 7.2531e-01, 2.0161e-06, 2.7468e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3875306248664856 tensor([6.1247e-01, 1.0533e-06, 3.2375e-13, 3.8753e-01, 9.3348e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23975245654582977 tensor([1.6668e-01, 6.1609e-03, 4.4245e-05, 5.8736e-01, 2.3975e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3266826868057251 tensor([2.3259e-04, 3.2668e-01, 6.2383e-01, 4.1174e-05, 4.9217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4581109881401062 tensor([5.4177e-01, 5.6067e-05, 7.5239e-10, 4.5811e-01, 6.0002e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04768441617488861 tensor([2.0412e-10, 1.8355e-05, 9.5230e-01, 1.8840e-07, 4.7684e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0747988373041153 tensor([0.0016, 0.0023, 0.0018, 0.0748, 0.9195], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14434179663658142 tensor([0.0346, 0.7839, 0.0332, 0.0040, 0.1443], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15591488778591156 tensor([1.5591e-01, 8.4140e-01, 6.1435e-04, 4.6294e-04, 1.6064e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1586800515651703 tensor([8.4058e-01, 5.2213e-04, 1.1911e-08, 1.5868e-01, 2.1957e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06958496570587158 tensor([4.5185e-04, 2.7298e-02, 6.9585e-02, 2.0959e-03, 9.0057e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.029921934008598328 tensor([9.5895e-01, 2.9922e-02, 1.7583e-06, 1.0748e-02, 3.7811e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3239896297454834 tensor([0.3240, 0.6151, 0.0019, 0.0128, 0.0462], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007796758436597884 tensor([9.2715e-12, 1.9093e-05, 9.9920e-01, 9.8500e-10, 7.7968e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36975306272506714 tensor([2.7532e-04, 6.7235e-02, 3.6975e-01, 1.3029e-03, 5.6143e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3870881497859955 tensor([3.8709e-01, 7.3177e-03, 1.3919e-05, 5.1863e-01, 8.6953e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05373075231909752 tensor([7.1535e-04, 1.9891e-03, 3.7941e-03, 5.3731e-02, 9.3977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013460003770887852 tensor([8.4060e-04, 8.4065e-03, 1.3460e-02, 1.9030e-02, 9.5826e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17826764285564423 tensor([0.1783, 0.7768, 0.0026, 0.0047, 0.0376], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2092394083738327 tensor([2.0924e-01, 3.9334e-07, 5.0492e-13, 7.9076e-01, 1.6320e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.059278346598148346 tensor([4.4553e-06, 1.6619e-02, 9.2409e-01, 1.1729e-05, 5.9278e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2654113173484802 tensor([7.3111e-01, 2.6541e-01, 3.0925e-05, 2.7496e-03, 7.0063e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4338214099407196 tensor([5.6478e-01, 4.3382e-01, 3.6867e-05, 7.8301e-04, 5.8260e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4102385342121124 tensor([2.8505e-04, 5.8069e-01, 4.1024e-01, 1.3168e-05, 8.7746e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013892874121665955 tensor([1.7205e-06, 3.9657e-06, 3.1090e-04, 1.3893e-02, 9.8579e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17635706067085266 tensor([0.5209, 0.1764, 0.0011, 0.1857, 0.1160], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2570047676563263 tensor([0.0062, 0.4634, 0.2570, 0.0046, 0.2688], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15532293915748596 tensor([8.4465e-01, 2.0502e-05, 2.1948e-11, 1.5532e-01, 4.3768e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.057744842022657394 tensor([5.7745e-02, 9.4094e-01, 7.7107e-04, 8.9446e-05, 4.5647e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004390160087496042 tensor([1.8038e-03, 1.4340e-06, 1.8877e-08, 9.9380e-01, 4.3902e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011694601736962795 tensor([9.4954e-04, 7.9966e-03, 7.1799e-03, 1.1695e-02, 9.7218e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18303781747817993 tensor([9.7536e-07, 4.1234e-04, 1.8304e-01, 3.6567e-04, 8.1618e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.39741024374961853 tensor([5.5829e-01, 1.0362e-02, 9.2278e-06, 3.9741e-01, 3.3932e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.024057939648628235 tensor([2.4058e-02, 9.6961e-01, 4.1293e-03, 8.8826e-05, 2.1187e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013653810136020184 tensor([7.3032e-04, 6.8310e-07, 2.0374e-08, 9.8562e-01, 1.3654e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.217754527926445 tensor([0.1690, 0.2178, 0.0044, 0.1010, 0.5077], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3034670054912567 tensor([0.1008, 0.0432, 0.0013, 0.3035, 0.5513], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03919782117009163 tensor([9.3799e-01, 1.9361e-02, 2.2902e-06, 3.9198e-02, 3.4533e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03729183226823807 tensor([3.9286e-06, 2.8400e-02, 9.3430e-01, 3.9885e-06, 3.7292e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003832141635939479 tensor([9.9486e-01, 3.8321e-03, 6.7618e-09, 1.3061e-03, 5.3254e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.269980788230896 tensor([1.2322e-07, 8.6427e-05, 2.6998e-01, 2.3234e-04, 7.2970e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06676861643791199 tensor([9.3100e-01, 6.6769e-02, 1.3200e-06, 2.0282e-03, 2.0362e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3985409438610077 tensor([3.9854e-01, 5.9942e-01, 1.0364e-04, 8.2470e-04, 1.1123e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1097593829035759 tensor([1.5531e-02, 8.4195e-01, 3.2102e-02, 6.5279e-04, 1.0976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0292141642421484 tensor([2.9214e-02, 1.7666e-05, 1.5422e-08, 9.6409e-01, 6.6786e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06346230208873749 tensor([5.3567e-08, 6.2356e-04, 9.3591e-01, 2.9954e-06, 6.3462e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008836882188916206 tensor([1.6446e-05, 3.8054e-04, 8.8369e-03, 2.7180e-03, 9.8805e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4723640978336334 tensor([1.2756e-02, 2.4193e-03, 3.3533e-04, 4.7236e-01, 5.1213e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16302959620952606 tensor([0.1630, 0.8165, 0.0013, 0.0016, 0.0175], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07284079492092133 tensor([9.2382e-01, 3.0449e-03, 9.2286e-08, 7.2841e-02, 2.9788e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04925037547945976 tensor([4.9250e-02, 9.4407e-01, 3.2480e-03, 3.4965e-04, 3.0815e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08767441660165787 tensor([8.7674e-02, 4.6465e-04, 1.2203e-06, 8.9653e-01, 1.5331e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23296573758125305 tensor([3.0038e-01, 4.2625e-02, 3.1706e-04, 4.2371e-01, 2.3297e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004063652362674475 tensor([8.8792e-09, 7.6788e-04, 9.9517e-01, 7.8944e-08, 4.0637e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004526407457888126 tensor([4.5264e-03, 8.3487e-07, 1.2893e-09, 9.9336e-01, 2.1097e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3416270911693573 tensor([0.3416, 0.1386, 0.0008, 0.1270, 0.3920], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2119552493095398 tensor([7.8793e-01, 8.0746e-05, 5.1481e-10, 2.1196e-01, 3.8059e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003168460913002491 tensor([9.9328e-01, 3.1685e-03, 1.2128e-08, 3.5218e-03, 2.7050e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0774243026971817 tensor([9.0306e-01, 7.7424e-02, 9.6506e-06, 1.4941e-02, 4.5687e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3051736056804657 tensor([5.2840e-03, 9.6016e-04, 1.6067e-04, 3.0517e-01, 6.8842e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2866882085800171 tensor([2.3451e-04, 2.8669e-01, 5.7453e-01, 6.3348e-05, 1.3848e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1423889696598053 tensor([2.5264e-04, 2.0724e-02, 1.4239e-01, 3.8538e-03, 8.3278e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17041422426700592 tensor([8.2712e-01, 1.7041e-01, 7.9767e-06, 2.1778e-03, 2.8225e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011507904157042503 tensor([4.7553e-08, 1.5905e-03, 9.8690e-01, 3.6808e-07, 1.1508e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3312751054763794 tensor([1.5572e-02, 1.6978e-03, 1.3903e-04, 6.5132e-01, 3.3128e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2717030644416809 tensor([0.0054, 0.0026, 0.0009, 0.2717, 0.7193], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15938334167003632 tensor([0.0307, 0.0356, 0.0040, 0.1594, 0.7703], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03724240884184837 tensor([0.0032, 0.0372, 0.0290, 0.0213, 0.9093], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.058933768421411514 tensor([6.7371e-06, 3.2435e-02, 9.0862e-01, 6.0340e-06, 5.8934e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07843334227800369 tensor([9.2154e-01, 2.1742e-05, 1.2541e-11, 7.8433e-02, 2.2286e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008964312262833118 tensor([1.7646e-08, 7.9986e-04, 9.9024e-01, 2.6259e-07, 8.9643e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08972309529781342 tensor([0.0356, 0.0496, 0.0043, 0.0897, 0.8207], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07765596359968185 tensor([9.2234e-01, 1.2316e-06, 5.8569e-14, 7.7656e-02, 9.0112e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22640740871429443 tensor([6.1751e-04, 2.2641e-01, 4.7145e-01, 3.7245e-04, 3.0115e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4561704099178314 tensor([5.4380e-01, 9.9473e-06, 3.1427e-11, 4.5617e-01, 2.0515e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04189765080809593 tensor([5.3717e-05, 3.3569e-03, 4.1898e-02, 1.8764e-03, 9.5282e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19331231713294983 tensor([0.1473, 0.1933, 0.0103, 0.2619, 0.3872], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18903514742851257 tensor([1.9814e-03, 9.8067e-04, 6.4975e-04, 1.8904e-01, 8.0735e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0036255393642932177 tensor([2.3325e-09, 3.4651e-04, 9.9603e-01, 4.1403e-08, 3.6255e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10889552533626556 tensor([3.1712e-04, 3.1181e-02, 1.0890e-01, 2.1112e-03, 8.5749e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007208275143057108 tensor([1.7602e-07, 7.2083e-03, 9.8958e-01, 1.8939e-07, 3.2126e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1919132024049759 tensor([4.7919e-04, 5.6621e-02, 1.9191e-01, 1.8494e-03, 7.4914e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.062031470239162445 tensor([9.3243e-01, 6.2031e-02, 2.6094e-06, 5.1874e-03, 3.4617e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003782142186537385 tensor([6.4734e-08, 3.7821e-03, 9.9276e-01, 1.3430e-07, 3.4537e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.031005743891000748 tensor([0.0039, 0.0195, 0.0094, 0.0310, 0.9362], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1836325228214264 tensor([7.0564e-01, 8.0764e-02, 1.2920e-04, 1.8363e-01, 2.9839e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.44714754819869995 tensor([0.0202, 0.0063, 0.0009, 0.4471, 0.5255], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16073015332221985 tensor([0.1449, 0.6714, 0.0080, 0.0150, 0.1607], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2831171452999115 tensor([0.3172, 0.2831, 0.0020, 0.0751, 0.3227], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1674833744764328 tensor([1.3882e-01, 5.3787e-03, 3.2637e-05, 6.8828e-01, 1.6748e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005348970182240009 tensor([2.9304e-09, 5.1701e-04, 9.9413e-01, 3.1830e-08, 5.3490e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12267549335956573 tensor([1.3999e-02, 2.2015e-04, 5.1423e-06, 8.6310e-01, 1.2268e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22560973465442657 tensor([0.2256, 0.1384, 0.0021, 0.1944, 0.4395], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2722961902618408 tensor([2.7230e-01, 7.2453e-01, 3.0164e-04, 9.1417e-04, 1.9590e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016380513086915016 tensor([6.4509e-06, 8.4699e-06, 1.2631e-04, 1.6381e-02, 9.8348e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10060718655586243 tensor([0.0110, 0.1006, 0.0200, 0.0169, 0.8515], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05992714688181877 tensor([7.4161e-03, 5.8978e-03, 7.5352e-04, 5.9927e-02, 9.2601e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20393551886081696 tensor([0.2039, 0.1495, 0.0031, 0.1991, 0.4444], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21505045890808105 tensor([2.1505e-01, 7.8105e-01, 4.8285e-04, 6.6265e-04, 2.7490e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15428802371025085 tensor([1.5429e-01, 2.1786e-03, 6.3158e-06, 7.5700e-01, 8.6527e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07711996883153915 tensor([2.8494e-06, 1.3836e-02, 9.0903e-01, 9.4617e-06, 7.7120e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.052691731601953506 tensor([9.4724e-01, 6.4578e-05, 6.9715e-11, 5.2692e-02, 5.5629e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007259279955178499 tensor([9.9236e-01, 7.2593e-03, 5.6375e-09, 3.7529e-04, 4.1929e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021091438829898834 tensor([9.7788e-01, 2.1091e-02, 1.0931e-07, 1.0171e-03, 1.0320e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012736113741993904 tensor([1.4141e-06, 4.2859e-06, 3.5440e-04, 1.2736e-02, 9.8690e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06686794757843018 tensor([0.0015, 0.0027, 0.0024, 0.0669, 0.9266], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2216104418039322 tensor([0.3520, 0.3098, 0.0034, 0.1132, 0.2216], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05677790194749832 tensor([9.4032e-01, 5.6778e-02, 1.2302e-06, 2.5458e-03, 3.5474e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016182320192456245 tensor([1.2777e-02, 9.6728e-01, 1.6182e-02, 1.6505e-04, 3.5935e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3927382230758667 tensor([1.3021e-05, 4.2673e-07, 2.6843e-06, 3.9274e-01, 6.0725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26357290148735046 tensor([5.4332e-05, 1.6876e-02, 2.6357e-01, 4.4502e-04, 7.1905e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4786307215690613 tensor([4.3862e-04, 4.7863e-01, 4.9035e-01, 5.0310e-05, 3.0529e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05501670762896538 tensor([0.0550, 0.8816, 0.0226, 0.0037, 0.0372], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007324493955820799 tensor([1.8878e-07, 7.3245e-03, 9.8804e-01, 2.1866e-07, 4.6379e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30656492710113525 tensor([6.9343e-01, 5.7546e-07, 6.2132e-14, 3.0656e-01, 2.0632e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16513802111148834 tensor([5.8058e-07, 2.0362e-03, 8.3280e-01, 2.7573e-05, 1.6514e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19347348809242249 tensor([8.0614e-01, 2.7268e-04, 4.8763e-09, 1.9347e-01, 1.1050e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2270301729440689 tensor([0.0051, 0.2270, 0.1425, 0.0063, 0.6191], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31903892755508423 tensor([8.7200e-04, 6.4819e-01, 3.1904e-01, 5.7026e-05, 3.1840e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015234625898301601 tensor([1.1080e-02, 1.5728e-05, 5.7467e-08, 9.7367e-01, 1.5235e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08950960636138916 tensor([4.2240e-03, 8.7906e-01, 8.9510e-02, 1.3622e-04, 2.7074e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020894881337881088 tensor([2.0895e-02, 2.9936e-05, 9.7508e-08, 9.6793e-01, 1.1142e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012461678124964237 tensor([9.8373e-01, 3.6586e-03, 3.7485e-08, 1.2462e-02, 1.4588e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2514857351779938 tensor([7.4822e-01, 2.5149e-01, 3.5125e-06, 2.4913e-04, 3.9684e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.029680194333195686 tensor([2.9680e-02, 9.3518e-08, 1.1711e-12, 9.7031e-01, 7.6798e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.314402312040329 tensor([6.8506e-01, 1.6079e-04, 2.6492e-09, 3.1440e-01, 3.7576e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3276151716709137 tensor([1.1720e-03, 6.4899e-01, 3.2762e-01, 1.1140e-04, 2.2114e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08802039921283722 tensor([1.9513e-04, 1.5466e-02, 8.8020e-02, 2.4679e-03, 8.9385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0995173379778862 tensor([1.3255e-05, 9.9517e-02, 8.9145e-01, 3.3286e-06, 9.0144e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4568077027797699 tensor([4.5681e-01, 3.6688e-03, 1.6901e-06, 5.2000e-01, 1.9519e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08615804463624954 tensor([2.6899e-05, 8.6158e-02, 8.7502e-01, 1.8051e-05, 3.8778e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.025737352669239044 tensor([3.4994e-04, 7.0452e-03, 2.5737e-02, 8.8830e-03, 9.5798e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012033587321639061 tensor([7.7937e-07, 1.2034e-02, 9.7478e-01, 1.1903e-06, 1.3185e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0013601310783997178 tensor([2.2534e-11, 2.2013e-05, 9.9862e-01, 3.2757e-09, 1.3601e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006018990185111761 tensor([6.0190e-03, 6.4946e-07, 6.0028e-10, 9.9355e-01, 4.3097e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13790221512317657 tensor([0.0027, 0.0030, 0.0021, 0.1379, 0.8542], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2586680054664612 tensor([7.4064e-01, 3.5043e-04, 9.7168e-09, 2.5867e-01, 3.3924e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023550866171717644 tensor([9.7572e-01, 7.0912e-04, 3.2919e-09, 2.3551e-02, 2.2886e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15921983122825623 tensor([3.6573e-05, 5.9639e-02, 7.8105e-01, 5.2700e-05, 1.5922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009943736717104912 tensor([9.9437e-03, 4.7347e-06, 1.0361e-08, 9.8542e-01, 4.6353e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31213638186454773 tensor([3.1214e-01, 1.4047e-02, 4.6081e-05, 5.5929e-01, 1.1448e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01710900105535984 tensor([1.1604e-02, 3.2581e-05, 2.8791e-07, 9.7125e-01, 1.7109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3684898018836975 tensor([6.1870e-01, 4.3650e-03, 1.4676e-06, 3.6849e-01, 8.4404e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12831559777259827 tensor([8.1976e-01, 1.2832e-01, 5.8262e-05, 3.7102e-02, 1.4760e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3463626503944397 tensor([3.4595e-05, 4.6664e-07, 1.0425e-06, 6.5360e-01, 3.4636e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28965049982070923 tensor([2.8965e-01, 2.8713e-02, 1.6723e-04, 5.6057e-01, 1.2090e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11751194298267365 tensor([2.5749e-03, 1.8904e-03, 7.7566e-04, 1.1751e-01, 8.7725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0728442594408989 tensor([0.0143, 0.0410, 0.0101, 0.0728, 0.8617], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012462104670703411 tensor([8.6563e-08, 1.2462e-02, 9.8650e-01, 2.6637e-08, 1.0369e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3952721655368805 tensor([3.9527e-01, 1.0732e-02, 2.3468e-05, 5.2037e-01, 7.3597e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11744231730699539 tensor([8.8233e-01, 1.8338e-04, 1.0043e-09, 1.1744e-01, 4.2416e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18919934332370758 tensor([5.4791e-05, 1.8920e-01, 7.8616e-01, 1.3653e-05, 2.4577e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1452150195837021 tensor([6.6148e-01, 1.4522e-01, 2.2659e-04, 9.7364e-02, 9.5712e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00643078563734889 tensor([6.4308e-03, 9.8813e-01, 5.0342e-03, 7.8108e-06, 3.9360e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02660079300403595 tensor([9.7340e-01, 1.6158e-06, 3.2687e-14, 2.6601e-02, 2.0084e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.38077595829963684 tensor([5.8779e-04, 5.9314e-01, 3.8078e-01, 4.7012e-05, 2.5446e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.046230498701334 tensor([0.0030, 0.0064, 0.0028, 0.0462, 0.9415], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3937527537345886 tensor([0.0469, 0.3938, 0.0237, 0.0171, 0.5185], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06527545303106308 tensor([4.3429e-06, 1.5216e-02, 9.1949e-01, 1.7024e-05, 6.5275e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42920780181884766 tensor([3.1587e-05, 9.8800e-07, 2.9886e-06, 4.2921e-01, 5.7076e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1333332061767578 tensor([0.0208, 0.1333, 0.0113, 0.0147, 0.8198], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.164035364985466 tensor([0.0845, 0.1640, 0.0076, 0.0963, 0.6475], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08726766705513 tensor([0.0113, 0.8315, 0.0688, 0.0011, 0.0873], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3646393418312073 tensor([0.0132, 0.3646, 0.0665, 0.0060, 0.5496], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3681245744228363 tensor([6.2271e-01, 3.6812e-01, 1.2837e-04, 5.5571e-03, 3.4777e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13996027410030365 tensor([1.9473e-06, 6.8884e-03, 8.5313e-01, 1.6530e-05, 1.3996e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05309474468231201 tensor([9.4582e-01, 8.9014e-04, 1.1140e-08, 5.3095e-02, 1.9009e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1915293037891388 tensor([8.0815e-01, 1.7333e-04, 1.8372e-09, 1.9153e-01, 1.4779e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10817994922399521 tensor([0.0465, 0.8107, 0.0305, 0.0041, 0.1082], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3569366931915283 tensor([5.5937e-04, 1.0542e-04, 1.0208e-04, 3.5694e-01, 6.4230e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05499239265918732 tensor([6.2061e-05, 3.7268e-05, 1.8675e-04, 5.4992e-02, 9.4472e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13012875616550446 tensor([6.8414e-06, 1.3013e-01, 8.6497e-01, 8.8029e-07, 4.8900e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06833964586257935 tensor([9.3127e-01, 6.8340e-02, 3.5428e-07, 3.7601e-04, 1.6333e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08563105016946793 tensor([1.1317e-05, 3.0253e-02, 8.8408e-01, 1.9843e-05, 8.5631e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31931352615356445 tensor([6.8065e-01, 2.3363e-05, 9.7457e-11, 3.1931e-01, 1.2922e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008480042219161987 tensor([2.6604e-08, 8.1839e-04, 9.9070e-01, 5.0847e-07, 8.4800e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.319863498210907 tensor([1.3018e-02, 6.0179e-04, 3.5681e-05, 6.6648e-01, 3.1986e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26667413115501404 tensor([6.0487e-01, 5.3364e-02, 1.0806e-04, 2.6667e-01, 7.4986e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2057647407054901 tensor([0.2058, 0.7409, 0.0023, 0.0045, 0.0465], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24870264530181885 tensor([0.2487, 0.1495, 0.0023, 0.2063, 0.3931], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3380949795246124 tensor([5.9641e-01, 1.3424e-02, 1.4984e-05, 3.3809e-01, 5.2058e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35209956765174866 tensor([8.2529e-02, 1.7574e-02, 4.2621e-04, 3.5210e-01, 5.4737e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29083290696144104 tensor([3.4169e-05, 3.3189e-02, 6.7583e-01, 1.1502e-04, 2.9083e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007192872930318117 tensor([1.4158e-07, 7.1929e-03, 9.8736e-01, 1.4340e-07, 5.4501e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024652132764458656 tensor([6.0802e-03, 1.4226e-05, 1.6423e-07, 9.6925e-01, 2.4652e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11819753795862198 tensor([8.8042e-01, 8.9960e-04, 1.8952e-08, 1.1820e-01, 4.7909e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [0, 3, 2, 1], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 2, 0], [3, 0, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 1, 4], [2, 1, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2, 1], [3, 2, 4, 0], [3, 0, 4], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 4], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4, 1], [2, 1, 4, 0], [3, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [1, 0, 2, 3], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [3, 0, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 1, 3], [2, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1], [2, 4, 1], [2, 1, 0], [3, 0, 4], [2, 4, 1], [0, 3, 1, 4], [0, 2, 1, 3], [3, 0, 2, 4], [3, 2, 4, 0], [2, 4, 1, 3], [0, 3, 1, 2], [2, 4, 3, 1], [2, 3, 4], [0, 3, 1, 4], [0, 3, 1], [2, 1, 4], [0, 1, 2, 3], [0, 1, 3, 2], [2, 3, 4, 0], [2, 1, 4], [0, 3, 1, 4], [2, 4, 3], [2, 4, 3], [3, 0, 4], [0, 1, 2, 3], [2, 4, 3, 1], [3, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 0, 4], [0, 2, 1, 3], [0, 3, 1, 2], [2, 1, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 3, 0], [2, 1, 0], [2, 4, 1, 3], [3, 0, 1, 4], [2, 4, 3, 1], [0, 1, 3], [2, 4, 3, 1], [2, 3, 4], [3, 0, 2, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 0], [3, 2, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 4, 0], [2, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 1], [2, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 0], [3, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0], [2, 1, 0], [2, 0, 1, 3], [0, 3, 2, 1], [3, 0, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 1, 3], [3, 0], [2, 1, 4], [0, 3, 1, 2], [2, 0, 1], [2, 1, 0, 3], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 1, 0], [2, 3, 0, 4], [2, 3], [2, 1, 0, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 3], [2, 3, 4], [0, 1, 2, 3], [0, 3, 2, 1], [2, 1, 0, 3], [2, 3, 1], [2, 3, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 2, 1, 3], [2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [1, 2, 0], [0, 3, 1], [3, 0, 4], [3, 2, 4, 0], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 2], [3, 0, 4], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1, 2], [3, 0, 4, 1], [2, 1, 4], [3, 0, 4, 1], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4], [2, 1, 0, 4], [2, 1, 4], [2, 4, 3, 1], [1, 2, 0], [2, 1, 4], [2, 1, 0, 3], [2, 0, 1, 3], [3, 0, 4, 1], [2, 1, 4], [2, 4, 1, 3], [3, 0, 4, 1], [2, 3, 4, 1], [3, 4, 2, 0], [2, 4, 1, 3], [3, 0, 4], [0, 2, 1, 3], [3, 2, 0], [0, 3, 1, 4], [1, 2, 0], [3, 2, 0, 1], [2, 3, 0, 1], [3, 0, 2, 4], [3, 0, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 2, 0, 4], [1, 2, 0], [1, 0, 2, 3], [3, 0, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 0], [2, 1, 4], [2, 3, 4], [2, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [3, 0, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "[[0], [4], [4], [3], [4], [1, 4], [1, 2], [2], [3], [2, 4], [2, 4], [2, 4], [2], [0], [2], [0, 3], [4], [2], [3], [2, 4], [0], [0], [1], [4], [4], [4], [1], [1, 2], [3], [4], [0], [3], [4], [4], [2], [0], [1], [1], [3], [0, 3], [3], [0, 3], [2], [3], [1, 2], [4], [0], [0, 1], [4], [4], [0], [0], [1], [0], [1], [2], [2], [1, 2], [3], [4], [0], [0, 3], [0], [0], [2, 4], [0, 3], [3, 4], [1, 2], [0, 3], [2], [4], [1], [1], [0], [4], [0], [0, 1], [2], [2, 4], [0, 3], [4], [4], [1], [0, 3], [2], [0, 1], [0, 1], [1, 2], [4], [0], [1, 2, 4], [0], [1], [3], [4], [4], [0, 3], [1], [3], [1, 4], [3, 4], [0], [2], [0], [2, 4], [0], [0, 1], [1], [3], [2], [4], [3, 4], [1], [0], [1], [3], [0, 3, 4], [2], [3], [0, 4], [0, 3], [0], [0], [3, 4], [1, 2], [4], [0], [2], [3, 4], [3, 4], [4], [4], [2], [0], [2], [4], [0], [1, 2, 4], [0, 3], [4], [3, 4], [4], [2], [4], [2], [4], [0], [2], [4], [0], [3, 4], [1], [0, 1, 4], [3], [2], [3], [0, 4], [0, 1], [4], [4], [4], [0, 4], [0, 1], [3], [2], [0], [0], [0], [4], [4], [0, 1, 4], [0], [1], [3, 4], [2, 4], [1, 2], [1], [2], [0, 3], [2], [0], [1, 4], [1, 2], [3], [1], [3], [0], [0, 1], [3], [0, 3], [1, 2], [4], [2], [0, 3], [2], [4], [2], [2], [3], [4], [0, 3], [0], [2], [3], [0, 3], [3], [0, 3], [0], [3, 4], [0, 3], [4], [4], [2], [0, 3], [0], [2], [0], [1], [0], [1, 2], [4], [1, 4], [2], [3, 4], [4], [4], [1], [1, 4], [0, 1], [2], [0], [0], [1], [3, 4], [4], [2], [0], [2], [0, 3], [2], [3, 4], [0, 3], [0, 1], [0, 4], [0, 3], [3, 4], [2, 4], [2], [3], [0]]\n",
      "NL_pred of 3th iteration [[2, 4, 1, 3], [0, 3, 2, 1], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 1, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2, 1], [3, 2, 4, 0], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 4, 0], [3, 0, 4, 1], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 1, 3], [1, 0, 2, 3], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 4], [0, 2, 1, 3], [3, 0, 2, 4], [3, 2, 4, 0], [2, 4, 1, 3], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 1, 2, 3], [0, 1, 3, 2], [2, 3, 4, 0], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 0, 4], [0, 2, 1, 3], [0, 3, 1, 2], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 1, 4], [2, 4, 3, 1], [2, 4, 3, 1], [3, 0, 2, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [3, 2, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 0, 1, 3], [0, 3, 2, 1], [3, 0, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 1, 3], [0, 3, 1, 2], [2, 0, 1], [2, 1, 0, 3], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 3, 0, 4], [2, 1, 0, 4], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [0, 3, 2, 1], [2, 1, 0, 3], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 3, 1], [3, 4, 0, 2], [3, 2, 4, 0], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 2], [3, 0, 4, 1], [3, 0, 4, 1], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 1, 0, 3], [2, 0, 1, 3], [3, 0, 4, 1], [2, 4, 1, 3], [3, 0, 4, 1], [2, 3, 4, 1], [3, 4, 2, 0], [2, 4, 1, 3], [0, 2, 1, 3], [0, 3, 1, 4], [3, 2, 0, 1], [2, 3, 0, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 2, 0, 4], [1, 0, 2, 3], [3, 0, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 4], [3, 0, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0070593810220908005  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.007059226956283837  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0070589362529286164  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.007058524946023149  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.007058013949477882  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.007057415811639083  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.007056748657895808  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.007056025733724672  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.007055258193211249  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.007054461373223199  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.00705364015367296  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.007052800111603318  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00705195100683915  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.007051094930771499  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.007050234671921758  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00704937720159341  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.007048519034134715  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.007047662958067063  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.007046808973390457  Accuracy on Support set:0.0\n",
      "torch.Size([171, 2048]) torch.Size([171])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0070459577772352434  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.3815775513648987 tensor([0.0355, 0.3816, 0.0260, 0.0139, 0.5430], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.39872464537620544 tensor([1.3838e-04, 3.9872e-01, 5.7993e-01, 1.1309e-05, 2.1191e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20083874464035034 tensor([8.0833e-08, 4.1534e-04, 7.9873e-01, 1.4965e-05, 2.0084e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3655827045440674 tensor([4.3106e-05, 2.1395e-02, 3.6558e-01, 2.3075e-04, 6.1275e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24302361905574799 tensor([1.3390e-05, 2.3185e-02, 7.3374e-01, 4.3013e-05, 2.4302e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2388266623020172 tensor([7.5187e-01, 3.5749e-03, 5.7901e-07, 2.3883e-01, 5.7262e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23212897777557373 tensor([5.7132e-04, 9.8590e-02, 2.3213e-01, 8.9106e-04, 6.6782e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.34880685806274414 tensor([1.7801e-04, 3.4881e-01, 6.1472e-01, 2.2864e-05, 3.6274e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34758108854293823 tensor([6.0037e-01, 1.7112e-02, 1.8285e-05, 3.4758e-01, 3.4918e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2674345076084137 tensor([2.6743e-01, 1.3507e-03, 1.3440e-06, 7.1802e-01, 1.3196e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29351806640625 tensor([2.6319e-04, 2.9352e-01, 6.3033e-01, 7.8820e-05, 7.5809e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.473070353269577 tensor([0.5013, 0.4731, 0.0005, 0.0130, 0.0120], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24437656998634338 tensor([3.0219e-05, 2.4438e-01, 7.4813e-01, 2.1178e-06, 7.4628e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22781801223754883 tensor([5.5367e-01, 2.8580e-02, 4.0748e-05, 2.2782e-01, 1.8989e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26071324944496155 tensor([6.0188e-10, 1.3641e-05, 7.3927e-01, 1.6908e-06, 2.6071e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.355482816696167 tensor([6.4452e-01, 1.1470e-06, 3.5353e-13, 3.5548e-01, 1.0118e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27413231134414673 tensor([1.7539e-01, 7.2706e-03, 5.4200e-05, 5.4315e-01, 2.7413e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.31444746255874634 tensor([2.0217e-04, 3.1445e-01, 6.3904e-01, 3.3694e-05, 4.6281e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42594581842422485 tensor([5.7392e-01, 6.3123e-05, 8.5928e-10, 4.2595e-01, 6.7055e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2988913357257843 tensor([0.2989, 0.6419, 0.0021, 0.0107, 0.0464], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3910816013813019 tensor([2.4707e-04, 6.7307e-02, 3.9108e-01, 1.0711e-03, 5.4029e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.40652623772621155 tensor([4.0653e-01, 8.6414e-03, 1.7274e-05, 4.8373e-01, 1.0108e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23118488490581512 tensor([2.3118e-01, 4.5330e-07, 5.9012e-13, 7.6881e-01, 1.8940e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2874664068222046 tensor([7.0934e-01, 2.8747e-01, 3.5158e-05, 2.4189e-03, 7.4265e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.45872899889945984 tensor([5.3994e-01, 4.5873e-01, 4.1059e-05, 6.8919e-04, 6.0504e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.42497244477272034 tensor([2.5434e-04, 5.6622e-01, 4.2497e-01, 1.1182e-05, 8.5415e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26023638248443604 tensor([0.0056, 0.4609, 0.2695, 0.0038, 0.2602], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.36116525530815125 tensor([5.8863e-01, 1.2222e-02, 1.1111e-05, 3.6117e-01, 3.7974e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2298327088356018 tensor([0.1538, 0.2298, 0.0051, 0.0837, 0.5276], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26385101675987244 tensor([0.0979, 0.0468, 0.0014, 0.2639, 0.5901], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2855290174484253 tensor([1.0986e-07, 8.5076e-05, 2.8553e-01, 1.9697e-04, 7.1419e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3760128617286682 tensor([3.7601e-01, 6.2203e-01, 1.1224e-04, 7.1276e-04, 1.1283e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4218086302280426 tensor([1.2667e-02, 2.6744e-03, 3.8886e-04, 4.2181e-01, 5.6246e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2593443989753723 tensor([3.0962e-01, 4.9444e-02, 3.8014e-04, 3.8121e-01, 2.5934e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32676711678504944 tensor([0.3268, 0.1507, 0.0009, 0.1086, 0.4130], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19178320467472076 tensor([8.0809e-01, 8.8330e-05, 5.7590e-10, 1.9178e-01, 4.1648e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2635139524936676 tensor([5.0396e-03, 1.0134e-03, 1.7833e-04, 2.6351e-01, 7.3025e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27684855461120605 tensor([2.0565e-04, 2.7685e-01, 5.9203e-01, 5.2046e-05, 1.3086e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3784845471382141 tensor([1.5998e-02, 1.9433e-03, 1.6766e-04, 6.0341e-01, 3.7848e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2312336266040802 tensor([0.0051, 0.0028, 0.0011, 0.2312, 0.7598], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22116847336292267 tensor([5.3698e-04, 2.2117e-01, 4.9412e-01, 3.0203e-04, 2.8387e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41966181993484497 tensor([5.8030e-01, 1.1372e-05, 3.6420e-11, 4.1966e-01, 2.3023e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22607478499412537 tensor([0.1414, 0.2087, 0.0117, 0.2261, 0.4122], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3963635265827179 tensor([0.0199, 0.0069, 0.0010, 0.3964, 0.5758], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30001702904701233 tensor([0.3000, 0.3041, 0.0022, 0.0629, 0.3308], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21644635498523712 tensor([0.2164, 0.1509, 0.0024, 0.1654, 0.4648], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.250204473733902 tensor([2.5020e-01, 7.4672e-01, 3.3018e-04, 7.7375e-04, 1.9710e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19742172956466675 tensor([0.1974, 0.1640, 0.0035, 0.1685, 0.4666], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19900141656398773 tensor([1.9900e-01, 7.9715e-01, 5.2172e-04, 5.7030e-04, 2.7557e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23191800713539124 tensor([0.3358, 0.3322, 0.0038, 0.0964, 0.2319], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3439108431339264 tensor([1.2417e-05, 4.4713e-07, 2.9886e-06, 3.4391e-01, 6.5607e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2838158905506134 tensor([4.9509e-05, 1.7168e-02, 2.8382e-01, 3.7260e-04, 6.9859e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.46428412199020386 tensor([3.8538e-04, 4.6428e-01, 5.0607e-01, 4.1630e-05, 2.9217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2782117426395416 tensor([7.2179e-01, 6.2001e-07, 6.7361e-14, 2.7821e-01, 2.2200e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23150217533111572 tensor([0.0047, 0.2315, 0.1516, 0.0053, 0.6069], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33502161502838135 tensor([7.6447e-04, 6.3349e-01, 3.3502e-01, 4.7198e-05, 3.0675e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26992926001548767 tensor([7.2980e-01, 2.6993e-01, 3.9414e-06, 2.2244e-04, 4.1855e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2889237701892853 tensor([7.1047e-01, 1.8013e-04, 3.0740e-09, 2.8892e-01, 4.2383e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.340970516204834 tensor([1.0486e-03, 6.3639e-01, 3.4097e-01, 9.4058e-05, 2.1494e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4850325286388397 tensor([4.8830e-01, 4.3424e-03, 2.0496e-06, 4.8503e-01, 2.2326e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23470915853977203 tensor([7.6453e-01, 3.8670e-04, 1.0900e-08, 2.3471e-01, 3.7105e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3250107169151306 tensor([3.2501e-01, 1.6522e-02, 5.7787e-05, 5.2387e-01, 1.3454e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3351351022720337 tensor([6.5027e-01, 5.1288e-03, 1.7655e-06, 3.3514e-01, 9.4675e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39608249068260193 tensor([3.4965e-05, 5.1676e-07, 1.2247e-06, 6.0388e-01, 3.9608e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30602309107780457 tensor([3.0602e-01, 3.3979e-02, 2.0538e-04, 5.2091e-01, 1.3888e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.41801342368125916 tensor([4.1801e-01, 1.2757e-02, 2.9054e-05, 4.8432e-01, 8.4885e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3959222137928009 tensor([5.1998e-04, 5.7899e-01, 3.9592e-01, 3.9222e-05, 2.4527e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40633857250213623 tensor([0.0430, 0.4063, 0.0257, 0.0141, 0.5108], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3791794180870056 tensor([3.0380e-05, 1.0400e-06, 3.3368e-06, 3.7918e-01, 6.2079e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3755386769771576 tensor([0.0121, 0.3755, 0.0723, 0.0050, 0.5351], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.39425286650657654 tensor([5.9720e-01, 3.9425e-01, 1.4352e-04, 4.7967e-03, 3.6031e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31012803316116333 tensor([5.3600e-04, 1.1256e-04, 1.1540e-04, 3.1013e-01, 6.8911e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29258236289024353 tensor([7.0738e-01, 2.5861e-05, 1.1034e-10, 2.9258e-01, 1.4245e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3623351752758026 tensor([1.3592e-02, 6.8583e-04, 4.2000e-05, 6.2335e-01, 3.6234e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24034181237220764 tensor([6.1513e-01, 6.0852e-02, 1.2877e-04, 2.4034e-01, 8.3546e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1882573813199997 tensor([0.1883, 0.7593, 0.0025, 0.0038, 0.0461], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24076901376247406 tensor([0.2408, 0.1643, 0.0026, 0.1760, 0.4163], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30843910574913025 tensor([6.1713e-01, 1.5582e-02, 1.8123e-05, 3.0844e-01, 5.8832e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31367728114128113 tensor([8.1998e-02, 1.8961e-02, 4.7203e-04, 3.1368e-01, 5.8489e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2692379057407379 tensor([3.0286e-05, 3.2768e-02, 6.9787e-01, 9.2895e-05, 2.6924e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [0, 3, 2, 1], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 2, 0], [3, 0, 4], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1], [0, 3, 1], [0, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 1, 4], [2, 1, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2, 1], [3, 2, 4, 0], [3, 0, 4], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 4], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4, 1], [2, 1, 4, 0], [3, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [1, 0, 2, 3], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [3, 0, 4], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 1, 3], [2, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1], [2, 4, 1], [2, 1, 0], [3, 0, 4], [2, 4, 1], [0, 3, 1, 4], [0, 2, 1, 3], [3, 0, 2, 4], [3, 2, 4, 0], [2, 4, 1, 3], [0, 3, 1, 2], [2, 4, 3, 1], [2, 3, 4], [0, 3, 1, 4], [0, 3, 1], [2, 1, 4], [0, 1, 2, 3], [0, 1, 3, 2], [2, 3, 4, 0], [2, 1, 4], [0, 3, 1, 4], [2, 4, 3], [2, 4, 3], [3, 0, 4], [0, 1, 2, 3], [2, 4, 3, 1], [3, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 0, 4], [0, 2, 1, 3], [0, 3, 1, 2], [2, 1, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 3, 0], [2, 1, 0], [2, 4, 1, 3], [3, 0, 1, 4], [2, 4, 3, 1], [0, 1, 3], [2, 4, 3, 1], [2, 3, 4], [3, 0, 2, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 0], [3, 2, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 4, 0], [2, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 0], [3, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0], [2, 1, 0], [2, 0, 1, 3], [0, 3, 2, 1], [3, 0, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 1, 3], [3, 0], [2, 1, 4], [0, 3, 1, 2], [2, 0, 1], [2, 1, 0, 3], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 1, 0], [2, 3, 0, 4], [2, 3], [2, 1, 0, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 3], [2, 3, 4], [0, 1, 2, 3], [0, 3, 2, 1], [2, 1, 0, 3], [2, 3, 1, 0], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 2, 1, 3], [2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [1, 2, 0], [0, 3, 1], [3, 0, 4], [3, 2, 4, 0], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 2], [3, 0, 4], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1, 2], [3, 0, 4, 1], [2, 1, 4], [3, 0, 4, 1], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4], [2, 1, 0, 4], [2, 1, 4], [2, 4, 3, 1], [1, 2, 0], [2, 1, 4], [2, 1, 0, 3], [2, 0, 1, 3], [3, 0, 4, 1], [2, 1, 4], [2, 4, 1, 3], [3, 0, 4, 1], [2, 3, 4, 1], [3, 4, 2, 0], [2, 4, 1, 3], [3, 0, 4], [0, 2, 1, 3], [3, 2, 0], [0, 3, 1, 4], [1, 2, 0], [3, 2, 0, 1], [2, 3, 0, 1], [3, 0, 2, 4], [3, 0, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 2, 0, 4], [1, 2, 0], [1, 0, 2, 3], [3, 0, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 0], [2, 1, 4], [2, 3, 4, 0], [2, 3, 1], [2, 1, 4], [2, 1, 0], [0, 3, 1], [3, 0, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "[[0], [4], [4], [3], [4], [1, 4], [1, 2], [2], [3], [2, 4], [2, 4], [2, 4], [2], [0], [2], [0, 3], [4], [2], [3], [2, 4], [0], [0], [1], [4], [4], [4], [1], [1, 2], [3], [4], [0], [3], [4], [4], [2], [0], [1], [1], [3], [0, 3], [3], [0, 3], [2], [3], [1, 2], [4], [0], [0, 1], [4], [4], [0], [0], [1], [0], [1], [2], [2], [1, 2], [3], [4], [0], [0, 3], [0], [0], [2, 4], [0, 3], [3, 4], [1, 2], [0, 3], [2], [4], [1], [1], [0], [4], [0], [0, 1], [2], [2, 4], [0, 3], [4], [4], [1], [0, 3], [2], [0, 1], [0, 1], [1, 2], [4], [0], [1, 2, 4], [0], [1], [3], [4], [4], [0, 3], [1], [3], [1, 4], [3, 4], [0], [2], [0], [2, 4], [0], [0, 1], [1], [3], [2], [4], [3, 4], [1], [0], [1], [3], [0, 3, 4], [2], [3], [0, 4], [0], [0], [0], [3, 4], [1, 2], [4], [0], [2], [3, 4], [3, 4], [4], [4], [2], [0], [2], [4], [0], [1, 2, 4], [0, 3], [4], [3, 4], [4], [2], [4], [2], [4], [0], [2], [4], [0], [3, 4], [1], [0, 1, 4], [3], [2], [3], [0, 4], [0, 1], [4], [4], [4], [4], [1], [3], [2], [0], [0], [0], [4], [4], [0, 1, 4], [0], [1], [3, 4], [2, 4], [1, 2], [1], [2], [0, 3], [2], [0], [1, 4], [1, 2], [3], [1], [3], [0], [0, 1], [3], [0, 3], [1, 2], [4], [2], [0, 3], [2], [4], [2], [2], [3], [4], [0, 3], [0], [2], [3], [0, 3], [3], [0, 3], [0], [3, 4], [0, 3], [4], [4], [2], [0, 3], [0], [2], [0], [1], [0], [1, 2], [4], [1, 4], [2], [3, 4], [4], [4], [1], [1, 4], [0, 1], [2], [0], [0], [1], [3, 4], [4], [2], [0], [2], [0, 3], [2], [3, 4], [0, 3], [1], [0, 4], [0, 3], [3, 4], [2, 4], [2], [3], [0]]\n",
      "NL_pred of 4th iteration [[2, 4, 1, 3], [2, 3, 1, 0], [2, 3, 4, 0], [2, 3, 4, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.32942095398902893  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.32754868268966675  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.3242676556110382  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.32017117738723755  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.3158261775970459  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.31168133020401  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.30797290802001953  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.30476513504981995  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.3020400404930115  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.29972898960113525  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.29774928092956543  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.2960318922996521  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.2945139408111572  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.29313963651657104  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.2919011116027832  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.29074960947036743  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.2897033393383026  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.2887599468231201  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.28791099786758423  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.28714847564697266  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.30764058232307434 tensor([0.0019, 0.4383, 0.2515, 0.0007, 0.3076], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09366326034069061 tensor([3.8521e-06, 9.3663e-02, 9.0067e-01, 5.0886e-07, 5.6667e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022390078753232956 tensor([2.0677e-09, 1.2723e-04, 9.7748e-01, 2.8661e-07, 2.2390e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08583929389715195 tensor([8.8340e-07, 7.2655e-03, 9.0689e-01, 4.1234e-06, 8.5839e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025462189689278603 tensor([2.1553e-07, 4.8523e-03, 9.6968e-01, 7.3989e-07, 2.5462e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12280087918043137 tensor([7.1449e-01, 9.0810e-02, 9.6836e-05, 1.2280e-01, 7.1802e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14130498468875885 tensor([1.5289e-05, 4.4017e-02, 8.1464e-01, 2.2159e-05, 1.4130e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0716911032795906 tensor([3.5299e-06, 7.1691e-02, 9.2097e-01, 6.8304e-07, 7.3310e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11413554102182388 tensor([0.3523, 0.2654, 0.0019, 0.1141, 0.2662], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2845189869403839 tensor([2.8452e-01, 3.8496e-02, 2.8564e-04, 4.6425e-01, 2.1245e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06075209751725197 tensor([4.6943e-06, 6.0752e-02, 9.2729e-01, 1.7186e-06, 1.1952e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04095049947500229 tensor([4.0950e-02, 9.3725e-01, 7.9195e-03, 8.6097e-04, 1.3022e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06034282594919205 tensor([1.0743e-06, 6.0343e-02, 9.3771e-01, 1.1609e-07, 1.9455e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.041514523327350616 tensor([0.1353, 0.1675, 0.0018, 0.0415, 0.6539], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04778996482491493 tensor([4.3731e-11, 7.8953e-06, 9.5220e-01, 8.2404e-08, 4.7790e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21530157327651978 tensor([7.8468e-01, 9.6206e-06, 8.9378e-12, 2.1530e-01, 6.2845e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07546408474445343 tensor([0.0363, 0.0384, 0.0021, 0.0755, 0.8477], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.059697527438402176 tensor([3.5280e-06, 5.9698e-02, 9.3170e-01, 8.9669e-07, 8.5933e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30993571877479553 tensor([6.8840e-01, 8.9360e-04, 5.8515e-08, 3.0994e-01, 7.6767e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01960921846330166 tensor([1.9609e-02, 9.1678e-01, 2.5708e-02, 6.7140e-04, 3.7233e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0722246766090393 tensor([4.2374e-06, 2.0458e-02, 9.0730e-01, 1.4740e-05, 7.2225e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12790507078170776 tensor([0.1557, 0.0872, 0.0014, 0.1279, 0.6278], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3416798710823059 tensor([3.4168e-01, 5.4940e-06, 2.6714e-11, 6.5830e-01, 1.8974e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08236822485923767 tensor([8.2368e-02, 9.1480e-01, 1.0378e-03, 2.6655e-04, 1.5227e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06054535135626793 tensor([6.0545e-02, 9.3776e-01, 6.9521e-04, 8.9067e-05, 9.1354e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18081669509410858 tensor([9.8028e-06, 1.8082e-01, 8.1600e-01, 6.8760e-07, 3.1749e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.051608968526124954 tensor([1.0114e-04, 1.6455e-01, 7.8368e-01, 6.5696e-05, 5.1609e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1285448968410492 tensor([0.3182, 0.1924, 0.0015, 0.1285, 0.3594], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.41493433713912964 tensor([0.0121, 0.4149, 0.0709, 0.0053, 0.4968], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01705673709511757 tensor([0.0097, 0.1259, 0.0284, 0.0171, 0.8189], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15789182484149933 tensor([4.3970e-09, 4.6572e-05, 8.4206e-01, 6.3081e-06, 1.5789e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030755488201975822 tensor([3.0755e-02, 9.6641e-01, 1.5122e-03, 6.7272e-05, 1.2595e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.028941985219717026 tensor([0.0013, 0.0078, 0.0089, 0.0289, 0.9530], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04594792053103447 tensor([0.0522, 0.2105, 0.0125, 0.0459, 0.6788], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03207239881157875 tensor([0.0321, 0.3853, 0.0190, 0.0085, 0.5551], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015431893989443779 tensor([4.8438e-04, 2.5254e-03, 3.1419e-03, 1.5432e-02, 9.7842e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0571763850748539 tensor([3.4147e-06, 5.7176e-02, 9.2359e-01, 1.1146e-06, 1.9231e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.061055295169353485 tensor([0.0028, 0.0090, 0.0056, 0.0611, 0.9216], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013118505477905273 tensor([4.1981e-04, 5.8503e-03, 1.7084e-02, 1.3119e-02, 9.6353e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03271046280860901 tensor([6.4475e-06, 4.5574e-02, 9.2170e-01, 4.0379e-06, 3.2710e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30492907762527466 tensor([6.9469e-01, 1.3553e-04, 1.9513e-09, 3.0493e-01, 2.4586e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01003420539200306 tensor([0.0114, 0.4454, 0.1577, 0.0100, 0.3754], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02585800178349018 tensor([0.0022, 0.0221, 0.0237, 0.0259, 0.9261], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.024024169892072678 tensor([0.0240, 0.6034, 0.0359, 0.0041, 0.3326], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019197290763258934 tensor([0.0192, 0.3587, 0.0452, 0.0107, 0.5663], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015290649607777596 tensor([1.5291e-02, 9.7866e-01, 4.2235e-03, 5.4824e-05, 1.7703e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.028192615136504173 tensor([0.0282, 0.6825, 0.0557, 0.0057, 0.2278], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02889593504369259 tensor([1.3856e-06, 7.8339e-07, 3.5689e-05, 2.8896e-02, 9.7107e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12166968733072281 tensor([1.1147e-06, 6.6319e-03, 8.7169e-01, 7.8175e-06, 1.2167e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12581689655780792 tensor([1.0461e-05, 1.2582e-01, 8.6699e-01, 1.5267e-06, 7.1854e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1456795036792755 tensor([8.5432e-01, 4.2227e-06, 1.0975e-12, 1.4568e-01, 9.4844e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13039365410804749 tensor([1.3534e-04, 1.3039e-01, 6.9684e-01, 1.3889e-04, 1.7249e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17515704035758972 tensor([1.7102e-05, 1.7516e-01, 8.1547e-01, 1.7511e-06, 9.3565e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11860903352499008 tensor([1.1861e-01, 8.8115e-01, 1.0183e-04, 3.8543e-05, 9.6436e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2036013901233673 tensor([7.8868e-01, 2.4866e-03, 2.1786e-07, 2.0360e-01, 5.2311e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21390224993228912 tensor([3.2499e-05, 2.1390e-01, 7.7887e-01, 4.0203e-06, 7.1880e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2391735166311264 tensor([4.0859e-01, 9.2166e-02, 2.9880e-04, 2.3917e-01, 2.5977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14083364605903625 tensor([8.4649e-01, 8.0753e-03, 1.2671e-06, 1.4083e-01, 4.6002e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1077122911810875 tensor([0.1078, 0.1407, 0.0035, 0.1077, 0.6403], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17483535408973694 tensor([5.8277e-01, 1.2228e-01, 2.9648e-04, 1.7484e-01, 1.1983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07014518231153488 tensor([5.4814e-06, 1.3327e-06, 2.1227e-05, 7.0145e-02, 9.2983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08719734847545624 tensor([0.0924, 0.2722, 0.0111, 0.0872, 0.5372], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13022775948047638 tensor([0.1813, 0.1471, 0.0025, 0.1302, 0.5389], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17728565633296967 tensor([1.6012e-05, 1.7729e-01, 8.1519e-01, 1.7528e-06, 7.5018e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31125226616859436 tensor([0.0023, 0.4422, 0.2436, 0.0007, 0.3113], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.029705077409744263 tensor([3.4324e-06, 2.2099e-06, 4.9185e-05, 2.9705e-02, 9.7024e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2114524245262146 tensor([3.8402e-04, 2.7331e-01, 5.1470e-01, 1.5841e-04, 2.1145e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06375662237405777 tensor([6.3757e-02, 9.2815e-01, 2.6156e-03, 4.6396e-04, 5.0136e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01864161528646946 tensor([4.9766e-05, 2.6860e-04, 2.0879e-03, 1.8642e-02, 9.7895e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20702773332595825 tensor([7.9264e-01, 2.2506e-04, 3.6061e-09, 2.0703e-01, 1.1018e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07168758660554886 tensor([0.0026, 0.0031, 0.0013, 0.0717, 0.9214], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.044050343334674835 tensor([0.1805, 0.4505, 0.0066, 0.0441, 0.3184], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.022407760843634605 tensor([0.0224, 0.3938, 0.0475, 0.0117, 0.5245], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09542485326528549 tensor([0.2823, 0.1984, 0.0018, 0.0954, 0.4221], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02341528609395027 tensor([0.0096, 0.0571, 0.0100, 0.0234, 0.8999], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02628357708454132 tensor([4.1615e-07, 6.3774e-03, 9.6734e-01, 1.3157e-06, 2.6284e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [0, 3, 2, 1], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 2, 0], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 1, 4, 3], [2, 1, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2, 1], [3, 2, 4, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 4, 3], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4, 1], [2, 1, 4, 0], [3, 0, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [1, 0, 2, 3], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [3, 0, 4, 1], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 1, 3], [2, 1, 4, 3], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [2, 1, 0, 3], [3, 0, 4, 1], [2, 4, 1], [0, 3, 1, 4], [0, 2, 1, 3], [3, 0, 2, 4], [3, 2, 4, 0], [2, 4, 1, 3], [0, 3, 1, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 3], [0, 1, 2, 3], [0, 1, 3, 2], [2, 3, 4, 0], [2, 1, 4], [0, 3, 1, 4], [2, 4, 3, 0], [2, 4, 3, 0], [3, 0, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 0, 4], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 0, 4], [0, 2, 1, 3], [0, 3, 1, 2], [2, 1, 4, 3], [3, 4, 2, 0], [2, 1, 0, 4], [2, 3, 0], [2, 1, 0, 3], [2, 4, 1, 3], [3, 0, 1, 4], [2, 4, 3, 1], [0, 1, 3, 4], [2, 4, 3, 1], [2, 3, 4, 0], [3, 0, 2, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 0, 3], [3, 2, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 4, 0], [2, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 1, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 0, 3], [3, 0, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 3], [2, 1, 0, 3], [2, 0, 1, 3], [0, 3, 2, 1], [3, 0, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 1, 3], [3, 0, 4], [2, 1, 4], [0, 3, 1, 2], [2, 0, 1, 3], [2, 1, 0, 3], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 1, 0, 3], [2, 3, 0, 4], [2, 3, 0], [2, 1, 0, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 3, 0], [2, 3, 4, 0], [0, 1, 2, 3], [0, 3, 2, 1], [2, 1, 0, 3], [2, 3, 1, 0], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 2, 1, 3], [2, 3, 0], [2, 4, 3, 1], [3, 4, 0, 2], [1, 2, 0, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 2, 4, 0], [0, 3, 4, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 2, 1], [3, 0, 4, 1], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1, 2], [3, 0, 4, 1], [2, 1, 4], [3, 0, 4, 1], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4, 3], [2, 1, 0, 4], [2, 1, 4, 3], [2, 4, 3, 1], [1, 2, 0, 3], [2, 1, 4, 3], [2, 1, 0, 3], [2, 0, 1, 3], [3, 0, 4, 1], [2, 1, 4, 3], [2, 4, 1, 3], [3, 0, 4, 1], [2, 3, 4, 1], [3, 4, 2, 0], [2, 4, 1, 3], [3, 0, 4, 1], [0, 2, 1, 3], [3, 2, 0], [0, 3, 1, 4], [1, 2, 0, 3], [3, 2, 0, 1], [2, 3, 0, 1], [3, 0, 2, 4], [3, 0, 2], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 2, 0, 4], [1, 2, 0, 3], [1, 0, 2, 3], [3, 0, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 0, 3], [2, 1, 4, 3], [2, 3, 4, 0], [2, 3, 1, 0], [2, 1, 4, 3], [2, 1, 0, 3], [0, 3, 1, 4], [3, 0, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "[[0], [4], [4], [3], [4], [1, 4], [2], [2], [3], [2], [2], [2], [2], [0], [2], [0], [4], [2], [3], [2], [0], [0], [1], [4], [4], [4], [1], [2], [3], [4], [0], [3], [4], [4], [2], [0], [1], [1], [3], [0], [3], [0, 3], [2], [3], [2], [4], [0], [1], [4], [4], [0], [0], [1], [0], [1], [2], [2], [2], [3], [4], [0], [0], [0], [0], [2], [0, 3], [4], [2], [0, 3], [2], [4], [1], [1], [0], [4], [0], [1], [2], [2], [0], [4], [4], [1], [0, 3], [2], [1], [1], [2], [4], [0], [1, 2], [0], [1], [3], [4], [4], [0], [1], [3], [1, 4], [4], [0], [2], [0], [2], [0], [1], [1], [3], [2], [4], [4], [1], [0], [1], [3], [0, 4], [2], [3], [4], [0], [0], [0], [4], [2], [4], [0], [2], [4], [4], [4], [4], [2], [0], [2], [4], [0], [1, 2], [0, 3], [4], [4], [4], [2], [4], [2], [4], [0], [2], [4], [0], [4], [1], [1, 4], [3], [2], [3], [4], [1], [4], [4], [4], [4], [1], [3], [2], [0], [0], [0], [4], [4], [1, 4], [0], [1], [4], [2], [2], [1], [2], [0], [2], [0], [4], [2], [3], [1], [3], [0], [1], [3], [0, 3], [1, 2], [4], [2], [0, 3], [2], [4], [2], [2], [3], [4], [0], [0], [2], [3], [0], [3], [0], [0], [4], [0], [4], [4], [2], [0], [0], [2], [0], [1], [0], [2], [4], [1, 4], [2], [4], [4], [4], [1], [1, 4], [1], [2], [0], [0], [1], [4], [4], [2], [0], [2], [0, 3], [2], [4], [0], [1], [4], [0], [4], [2], [2], [3], [0]]\n",
      "NL_pred of 5th iteration [[3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 3], [0, 3, 1, 4], [3, 0, 4, 1], [2, 1, 4, 3], [3, 0, 4, 1], [2, 4, 3, 0], [3, 0, 4, 1], [2, 1, 4, 3], [0, 3, 1, 4], [2, 1, 0, 3], [3, 0, 4, 1], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 4, 3], [2, 4, 3, 0], [2, 4, 3, 0], [3, 0, 4, 1], [3, 0, 4], [2, 1, 4, 3], [2, 1, 0, 3], [0, 1, 3, 4], [2, 3, 4, 0], [2, 1, 0, 3], [2, 1, 3], [2, 3, 1, 0], [2, 1, 0, 3], [3, 0, 4, 1], [2, 1, 0, 3], [2, 1, 0, 3], [3, 0, 4], [2, 0, 1, 3], [2, 1, 0, 3], [2, 3, 0], [2, 1, 3, 0], [2, 3, 4, 0], [2, 3, 0], [1, 2, 0, 3], [0, 3, 1, 4], [3, 0, 4, 1], [2, 4, 1, 3], [0, 3, 2, 1], [3, 0, 4, 1], [2, 4, 3, 0], [2, 4, 1, 3], [2, 1, 4, 3], [2, 1, 4, 3], [1, 2, 0, 3], [2, 1, 4, 3], [2, 1, 4, 3], [3, 0, 4, 1], [1, 2, 0, 3], [2, 4, 3, 0], [1, 2, 0, 3], [2, 1, 0, 3], [2, 1, 4, 3], [2, 3, 1, 0], [2, 1, 4, 3], [2, 1, 0, 3], [0, 3, 1, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.018933625448317754  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.018916557705591596  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.018885031579032777  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.01884205946846614  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.018790805150592137  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.018734205336797805  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.01867487317039853  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.01861493360428583  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.018556065029568143  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.01849936492859371  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.018445508820669993  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.01839524412912036  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.018349170684814453  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.018306868416922435  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.018268365708608476  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.01823369094303676  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.018202231043861025  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.018173709748283265  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.018147742937481594  Accuracy on Support set:0.0\n",
      "torch.Size([63, 2048]) torch.Size([63])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.018124184911213224  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 1, 3], [0, 3, 2, 1], [0, 2, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [3, 2, 0], [3, 0, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 1, 4, 3], [2, 1, 0, 3], [0, 3, 4, 1], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2, 1], [3, 2, 4, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 0, 3], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 4, 3], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4, 1], [2, 1, 4, 0], [3, 0, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [1, 0, 2, 3], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [3, 0, 4, 1], [2, 1, 4, 0], [0, 2, 3, 1], [2, 4, 1, 3], [2, 1, 4, 3], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [2, 1, 0, 3], [3, 0, 4, 1], [2, 4, 1], [0, 3, 1, 4], [0, 2, 1, 3], [3, 0, 2, 4], [3, 2, 4, 0], [2, 4, 1, 3], [0, 3, 1, 2], [2, 4, 3, 1], [2, 3, 4, 0], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 4, 3], [0, 1, 2, 3], [0, 1, 3, 2], [2, 3, 4, 0], [2, 1, 4], [0, 3, 1, 4], [2, 4, 3, 0], [2, 4, 3, 0], [3, 0, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 0, 4], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 0, 4], [0, 2, 1, 3], [0, 3, 1, 2], [2, 1, 4, 3], [3, 4, 2, 0], [2, 1, 0, 4], [2, 3, 0], [2, 1, 0, 3], [2, 4, 1, 3], [3, 0, 1, 4], [2, 4, 3, 1], [0, 1, 3, 4], [2, 4, 3, 1], [2, 3, 4, 0], [3, 0, 2, 4], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [2, 1, 0, 3], [3, 2, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [2, 1, 4, 0], [2, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 3, 1, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 0, 3], [3, 0, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 3], [2, 1, 0, 3], [2, 0, 1, 3], [0, 3, 2, 1], [3, 0, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 1, 3], [3, 0, 4], [2, 1, 4], [0, 3, 1, 2], [2, 0, 1, 3], [2, 1, 0, 3], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 2, 1, 3], [2, 4, 1, 3], [2, 1, 0, 3], [2, 3, 0, 4], [2, 3, 0], [2, 1, 0, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 3, 0], [2, 3, 4, 0], [0, 1, 2, 3], [0, 3, 2, 1], [2, 1, 0, 3], [2, 3, 1, 0], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 2, 1, 3], [2, 3, 0], [2, 4, 3, 1], [3, 4, 0, 2], [1, 2, 0, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 2, 4, 0], [0, 3, 4, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 2, 1], [3, 0, 4, 1], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [2, 1, 4, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1, 2], [3, 0, 4, 1], [2, 1, 4], [3, 0, 4, 1], [0, 3, 1, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 4, 3], [2, 1, 0, 4], [2, 1, 4, 3], [2, 4, 3, 1], [1, 2, 0, 3], [2, 1, 4, 3], [2, 1, 0, 3], [2, 0, 1, 3], [3, 0, 4, 1], [2, 1, 4, 3], [2, 4, 1, 3], [3, 0, 4, 1], [2, 3, 4, 1], [3, 4, 2, 0], [2, 4, 1, 3], [3, 0, 4, 1], [0, 2, 1, 3], [3, 2, 0], [0, 3, 1, 4], [1, 2, 0, 3], [3, 2, 0, 1], [2, 3, 0, 1], [3, 0, 2, 4], [3, 0, 2], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 2, 0, 4], [1, 2, 0, 3], [1, 0, 2, 3], [3, 0, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 0, 3], [2, 1, 4, 3], [2, 3, 4, 0], [2, 3, 1, 0], [2, 1, 4, 3], [2, 1, 0, 3], [0, 3, 1, 4], [3, 0, 4, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "POSITION :  [[0], [4], [4], [3], [4], [1, 4], [2], [2], [3], [2], [2], [2], [2], [0], [2], [0], [4], [2], [3], [2], [0], [0], [1], [4], [4], [4], [1], [2], [3], [4], [0], [3], [4], [4], [2], [0], [1], [1], [3], [0], [3], [0, 3], [2], [3], [2], [4], [0], [1], [4], [4], [0], [0], [1], [0], [1], [2], [2], [2], [3], [4], [0], [0], [0], [0], [2], [0, 3], [4], [2], [0, 3], [2], [4], [1], [1], [0], [4], [0], [1], [2], [2], [0], [4], [4], [1], [0, 3], [2], [1], [1], [2], [4], [0], [1, 2], [0], [1], [3], [4], [4], [0], [1], [3], [1, 4], [4], [0], [2], [0], [2], [0], [1], [1], [3], [2], [4], [4], [1], [0], [1], [3], [0, 4], [2], [3], [4], [0], [0], [0], [4], [2], [4], [0], [2], [4], [4], [4], [4], [2], [0], [2], [4], [0], [1, 2], [0, 3], [4], [4], [4], [2], [4], [2], [4], [0], [2], [4], [0], [4], [1], [1, 4], [3], [2], [3], [4], [1], [4], [4], [4], [4], [1], [3], [2], [0], [0], [0], [4], [4], [1, 4], [0], [1], [4], [2], [2], [1], [2], [0], [2], [0], [4], [2], [3], [1], [3], [0], [1], [3], [0, 3], [1, 2], [4], [2], [0, 3], [2], [4], [2], [2], [3], [4], [0], [0], [2], [3], [0], [3], [0], [0], [4], [0], [4], [4], [2], [0], [0], [2], [0], [1], [0], [2], [4], [1, 4], [2], [4], [4], [4], [1], [1, 4], [1], [2], [0], [0], [1], [4], [4], [2], [0], [2], [0, 3], [2], [4], [0], [1], [4], [0], [4], [2], [2], [3], [0]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.356\n",
      "tensor([0, 4, 4, 3, 4, 2, 2, 3, 2, 2, 2, 2, 0, 2, 0, 4, 2, 3, 2, 0, 0, 1, 4, 4,\n",
      "        4, 1, 2, 3, 4, 0, 3, 4, 4, 2, 0, 1, 1, 3, 0, 3, 2, 3, 2, 4, 0, 1, 4, 4,\n",
      "        0, 0, 1, 0, 1, 2, 2, 2, 3, 4, 0, 0, 0, 0, 2, 4, 2, 2, 4, 1, 1, 0, 4, 0,\n",
      "        1, 2, 2, 0, 4, 4, 1, 2, 1, 1, 2, 4, 0, 0, 1, 3, 4, 4, 0, 1, 3, 4, 0, 2,\n",
      "        0, 2, 0, 1, 1, 3, 2, 4, 4, 1, 0, 1, 3, 2, 3, 4, 0, 0, 0, 4, 2, 4, 0, 2,\n",
      "        4, 4, 4, 4, 2, 0, 2, 4, 0, 4, 4, 4, 2, 4, 2, 4, 0, 2, 4, 0, 4, 1, 3, 2,\n",
      "        3, 4, 1, 4, 4, 4, 4, 1, 3, 2, 0, 0, 0, 4, 4, 0, 1, 4, 2, 2, 1, 2, 0, 2,\n",
      "        0, 4, 2, 3, 1, 3, 0, 1, 3, 4, 2, 2, 4, 2, 2, 3, 4, 0, 0, 2, 3, 0, 3, 0,\n",
      "        0, 4, 0, 4, 4, 2, 0, 0, 2, 0, 1, 0, 2, 4, 2, 4, 4, 4, 1, 1, 2, 0, 0, 1,\n",
      "        4, 4, 2, 0, 2, 2, 4, 0, 1, 4, 0, 4, 2, 2, 3, 0])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 96.98275862068965\n",
      "Epoch: 1  Loss: 97.41379310344827\n",
      "Epoch: 2  Loss: 99.13793103448276\n",
      "Epoch: 3  Loss: 99.56896551724138\n",
      "Epoch: 4  Loss: 99.13793103448276\n",
      "Epoch: 5  Loss: 98.70689655172413\n",
      "Epoch: 6  Loss: 99.13793103448276\n",
      "Epoch: 7  Loss: 99.13793103448276\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 10/15 [08:07<04:24, 52.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  36.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 5.74951419185847  Accuracy on Support set:44.0\n",
      "Train_Epoch: 1  Train_Loss: 3.6168374771624805  Accuracy on Support set:52.0\n",
      "Train_Epoch: 2  Train_Loss: 2.319521860331297  Accuracy on Support set:68.0\n",
      "Train_Epoch: 3  Train_Loss: 1.342075191102922  Accuracy on Support set:80.0\n",
      "Train_Epoch: 4  Train_Loss: 0.7438123888149858  Accuracy on Support set:88.0\n",
      "Train_Epoch: 5  Train_Loss: 0.428207730948925  Accuracy on Support set:92.0\n",
      "Train_Epoch: 6  Train_Loss: 0.2589173122495413  Accuracy on Support set:100.0\n",
      "Train_Epoch: 7  Train_Loss: 0.16645254991948605  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.12360755421221256  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.10277663517743349  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.08963294547051191  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.07969858165830374  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.072058260217309  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.06590105559676886  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.06082981117069721  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.056523066125810144  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.05283086933195591  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.04965559884905815  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.046857255585491656  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.04438111957162619  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.04216726738959551  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.040183738842606544  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.038352665565907954  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.03671453818678856  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.03522077634930611  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.033838653843849896  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.032565508186817166  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.03138643538579345  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.030284504555165768  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.029251348692923784  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.02827757239341736  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.02738590892404318  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.026543977297842504  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.02575389511883259  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.025014759935438634  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.02432458143681288  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.023655460197478533  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.023028756361454725  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.02243813330307603  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.021865690276026726  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.021333021055907012  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.02081610605120659  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.020333798825740813  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.019876955542713404  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.0194359396584332  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.019025093130767345  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.018626855835318566  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.018244266863912343  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.017888416685163975  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.017540926057845355  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  30.666666666666664\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 1.7588730770512484e-05 tensor([1.7589e-05, 5.6350e-03, 3.5532e-01, 1.2982e-03, 6.3773e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3824769666825887e-05 tensor([4.5692e-01, 5.4290e-01, 1.9256e-05, 1.5026e-04, 1.3825e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00168466882314533 tensor([0.0856, 0.0256, 0.0017, 0.6394, 0.2477], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.11165417265147e-05 tensor([5.1117e-05, 7.0873e-05, 9.9341e-04, 6.2687e-02, 9.3620e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.793123738025315e-05 tensor([1.7931e-05, 2.0212e-05, 5.3974e-04, 4.7428e-02, 9.5199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00045641331234946847 tensor([3.0232e-01, 6.9562e-01, 4.5641e-04, 1.1373e-03, 4.6126e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00223103747703135 tensor([0.0232, 0.8874, 0.0510, 0.0022, 0.0362], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008146773092448711 tensor([0.3111, 0.4805, 0.0081, 0.0962, 0.1041], grad_fn=<SoftmaxBackward0>)\n",
      "1 9.069451607501833e-07 tensor([2.6529e-06, 9.0695e-07, 5.0441e-05, 4.9826e-02, 9.5012e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.235133688889618e-07 tensor([9.2351e-07, 2.2559e-04, 8.8746e-02, 5.8145e-04, 9.1045e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3791122910333797e-05 tensor([4.1940e-05, 1.8031e-01, 8.0104e-01, 1.3791e-05, 1.8591e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.585601645767383e-08 tensor([9.5856e-08, 5.8943e-03, 9.9126e-01, 2.6438e-07, 2.8494e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.4657066407817183e-06 tensor([9.3292e-01, 1.8583e-02, 3.4657e-06, 4.8047e-02, 4.4645e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1584776226955e-06 tensor([4.1585e-06, 9.5419e-06, 7.0350e-04, 1.7117e-02, 9.8217e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.684572559199296e-06 tensor([4.6846e-06, 5.2549e-06, 2.1605e-04, 2.1685e-02, 9.7809e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.797366960294312e-07 tensor([9.6169e-01, 3.7638e-02, 2.7974e-07, 6.7264e-04, 2.3306e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.44213804055471e-05 tensor([1.7674e-02, 9.8010e-01, 1.9748e-03, 3.4421e-05, 2.1580e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005334132816642523 tensor([0.0129, 0.6912, 0.1503, 0.0053, 0.1403], grad_fn=<SoftmaxBackward0>)\n",
      "1 8.640856208330661e-07 tensor([2.4361e-05, 8.6409e-07, 1.0665e-05, 4.4555e-01, 5.5442e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002077291253954172 tensor([0.0021, 0.0076, 0.0190, 0.1092, 0.8621], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012126310029998422 tensor([0.0065, 0.7671, 0.1781, 0.0012, 0.0470], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7299558496451937e-05 tensor([2.7300e-05, 1.2609e-03, 5.5465e-02, 8.3346e-03, 9.3491e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.089049987499038e-09 tensor([9.3626e-01, 3.0445e-04, 4.0890e-09, 6.3428e-02, 1.0185e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.9444001015122012e-08 tensor([4.8738e-08, 2.9444e-08, 2.1806e-05, 1.0641e-02, 9.8934e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.723708168010489e-08 tensor([1.4920e-01, 9.0146e-05, 6.7237e-08, 8.4915e-01, 1.5549e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.918597162235528e-05 tensor([1.0143e-02, 9.7828e-01, 1.0726e-02, 5.9186e-05, 7.8659e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.1834431410970865e-06 tensor([8.9729e-01, 1.0135e-01, 3.1834e-06, 1.3371e-03, 2.0054e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00016548807616345584 tensor([1.6549e-04, 6.6406e-04, 8.6885e-03, 7.2952e-02, 9.1753e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0018192284042015672 tensor([0.3424, 0.1469, 0.0018, 0.3047, 0.2042], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01735159568488598 tensor([0.1595, 0.7029, 0.0174, 0.0423, 0.0780], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.385374919977039e-05 tensor([1.7339e-03, 8.5924e-01, 1.3508e-01, 7.3854e-05, 3.8686e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.871946993669553e-07 tensor([1.9514e-06, 5.8209e-02, 9.3953e-01, 6.8719e-07, 2.2558e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.917507302413469e-08 tensor([2.9175e-08, 1.5364e-03, 9.9306e-01, 4.0202e-07, 5.4031e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2994105418329127e-06 tensor([9.5022e-01, 1.1306e-02, 1.2994e-06, 3.8012e-02, 4.5736e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.180515903040032e-09 tensor([9.1805e-09, 3.8277e-04, 9.7554e-01, 8.2700e-07, 2.4080e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011447216384112835 tensor([0.4843, 0.4857, 0.0011, 0.0205, 0.0084], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.5991513414046494e-06 tensor([8.9261e-01, 1.0601e-01, 3.5992e-06, 1.3551e-03, 2.1311e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0059086434921483e-06 tensor([7.3853e-01, 2.7162e-03, 1.0059e-06, 2.5811e-01, 6.4948e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.588757793859259e-08 tensor([4.5308e-07, 8.5888e-08, 1.7893e-05, 5.4119e-02, 9.4586e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.62411623832304e-05 tensor([8.9704e-05, 5.6241e-05, 3.3369e-04, 8.2682e-02, 9.1684e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.067474532756023e-05 tensor([2.3841e-03, 8.9547e-01, 9.8517e-02, 6.0675e-05, 3.5664e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3235717233328614e-06 tensor([7.7953e-04, 9.8260e-01, 1.6544e-02, 1.3236e-06, 7.6724e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0821515345215147e-10 tensor([7.2720e-01, 2.2577e-05, 2.0822e-10, 2.7277e-01, 5.4236e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.519620399103587e-08 tensor([1.0988e-05, 6.5196e-08, 5.1359e-07, 7.0687e-01, 2.9312e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.875436131143942e-05 tensor([9.6221e-04, 7.8754e-05, 8.5910e-05, 4.6612e-01, 5.3275e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00373094342648983 tensor([0.0069, 0.5974, 0.2863, 0.0037, 0.1057], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003816701006144285 tensor([0.0584, 0.9061, 0.0188, 0.0038, 0.0129], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017984837177209556 tensor([1.7985e-04, 1.5472e-01, 7.5181e-01, 2.5192e-04, 9.3039e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.5781479173758726e-09 tensor([2.5781e-09, 6.4074e-07, 9.0687e-03, 1.1115e-04, 9.9082e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.318440212751739e-05 tensor([3.9267e-05, 1.0892e-01, 8.3142e-01, 3.3184e-05, 5.9586e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010462389764143154 tensor([1.0462e-04, 1.4636e-01, 7.9955e-01, 1.1767e-04, 5.3867e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.6136224076035433e-05 tensor([1.2462e-03, 9.2989e-01, 6.8309e-02, 1.6136e-05, 5.4345e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.281927638454363e-05 tensor([1.2280e-01, 2.6914e-03, 3.2819e-05, 8.3507e-01, 3.9409e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7908393374076468e-09 tensor([2.4785e-02, 2.9920e-06, 2.7908e-09, 9.7438e-01, 8.3421e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.427176816956262e-10 tensor([7.2113e-01, 5.4356e-05, 9.4272e-10, 2.7878e-01, 2.9070e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8212775785286794e-06 tensor([7.9103e-06, 1.1463e-01, 8.8266e-01, 1.8213e-06, 2.6960e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.865404869429767e-05 tensor([9.7697e-03, 9.8308e-01, 6.7905e-03, 3.8654e-05, 3.2332e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014352282509207726 tensor([0.1755, 0.6120, 0.0144, 0.0504, 0.1478], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.3518911847259e-08 tensor([1.0002e-02, 6.7234e-06, 7.3519e-08, 9.8800e-01, 1.9878e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.1426197200090655e-09 tensor([1.8641e-02, 2.1875e-06, 3.1426e-09, 9.8102e-01, 3.3227e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010559025395195931 tensor([4.0818e-02, 9.5713e-01, 1.7755e-03, 1.0559e-04, 1.6981e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007730282377451658 tensor([1.3943e-02, 9.2893e-01, 4.7218e-02, 7.7303e-04, 9.1380e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.513862444990082e-06 tensor([9.0267e-01, 1.8208e-02, 6.5139e-06, 7.8248e-02, 8.6519e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1609828334258054e-06 tensor([3.5523e-02, 1.1023e-04, 1.1610e-06, 9.5530e-01, 9.0616e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.966986122984963e-07 tensor([5.9670e-07, 4.1227e-03, 9.5438e-01, 1.1935e-05, 4.1481e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2276442475922522e-06 tensor([6.9993e-03, 9.9248e-01, 5.1062e-04, 1.2276e-06, 8.8097e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.593844550981885e-06 tensor([2.0557e-04, 7.2383e-01, 2.7463e-01, 4.5938e-06, 1.3247e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.3561050233620335e-07 tensor([7.5090e-01, 1.8586e-03, 4.3561e-07, 2.4691e-01, 3.3453e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3097620765734064e-09 tensor([3.3246e-03, 3.3141e-07, 1.3098e-09, 9.9406e-01, 2.6176e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6923177099670283e-06 tensor([1.6923e-06, 1.7054e-03, 4.4173e-01, 1.8125e-04, 5.5638e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000190518592717126 tensor([2.1484e-01, 1.3530e-02, 1.9052e-04, 7.3811e-01, 3.3327e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9384064216865227e-05 tensor([8.7990e-01, 1.1078e-01, 1.9384e-05, 8.9178e-03, 3.8136e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00013643621059600264 tensor([1.1928e-03, 7.0671e-01, 2.7050e-01, 1.3644e-04, 2.1460e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.1928186217555776e-05 tensor([6.3476e-03, 2.1852e-04, 3.1928e-05, 6.2427e-01, 3.6913e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9377738524894994e-08 tensor([9.9331e-01, 4.7198e-03, 1.9378e-08, 1.9689e-03, 1.8317e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.819948900258169e-06 tensor([9.8199e-06, 6.1713e-03, 6.8737e-01, 6.1544e-04, 3.0584e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004914498422294855 tensor([5.1298e-03, 8.7215e-01, 1.0851e-01, 4.9145e-04, 1.3723e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0957112550613601e-07 tensor([1.0957e-07, 1.9735e-04, 4.0341e-01, 1.1056e-04, 5.9628e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3667912074643027e-08 tensor([2.3668e-08, 3.1761e-06, 1.1562e-02, 3.8522e-04, 9.8805e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014576235553249717 tensor([0.0015, 0.0203, 0.0387, 0.0230, 0.9165], grad_fn=<SoftmaxBackward0>)\n",
      "4 5.923940261709504e-05 tensor([8.0317e-02, 9.1895e-01, 5.9208e-04, 8.6431e-05, 5.9239e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0719579222495668e-05 tensor([1.3360e-04, 4.7120e-01, 5.2350e-01, 1.0720e-05, 5.1648e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.904481194969776e-08 tensor([9.8825e-01, 2.0355e-03, 1.9045e-08, 9.7121e-03, 5.4311e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.6419965649135975e-09 tensor([1.5016e-01, 1.8220e-05, 4.6420e-09, 8.4955e-01, 2.7711e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.897197984770173e-06 tensor([3.8972e-06, 2.8018e-05, 2.5651e-03, 9.1486e-03, 9.8825e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.200361716921179e-08 tensor([8.1490e-01, 9.6395e-04, 9.2004e-08, 1.8393e-01, 2.0099e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.148091779323295e-05 tensor([4.8552e-04, 6.0109e-01, 3.8584e-01, 6.1481e-05, 1.2521e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01541633065789938 tensor([0.2009, 0.6680, 0.0154, 0.0416, 0.0740], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.5685318405521684e-07 tensor([2.5685e-07, 1.6920e-06, 6.2805e-04, 3.4115e-03, 9.9596e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.253890383803082e-07 tensor([3.2539e-07, 8.9399e-04, 6.2828e-01, 4.9819e-05, 3.7077e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000765580334700644 tensor([0.6438, 0.1773, 0.0008, 0.1663, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015250641154125333 tensor([0.5470, 0.2832, 0.0015, 0.1231, 0.0452], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.43628311536304e-07 tensor([2.0685e-01, 4.6272e-04, 6.4363e-07, 7.9013e-01, 2.5579e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003576244693249464 tensor([0.0053, 0.0048, 0.0036, 0.2371, 0.7493], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.2133506263344316e-06 tensor([3.6995e-05, 3.7291e-01, 6.2422e-01, 2.2134e-06, 2.8321e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00038301892345771194 tensor([1.5278e-03, 5.5596e-01, 3.8278e-01, 3.8302e-04, 5.9348e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5004723536549136e-05 tensor([6.3669e-01, 1.3404e-02, 1.5005e-05, 3.4335e-01, 6.5442e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014908858574926853 tensor([0.0149, 0.0336, 0.0291, 0.2294, 0.6929], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.962977876246441e-05 tensor([1.9233e-03, 5.0993e-05, 1.9630e-05, 8.0291e-01, 1.9509e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00013388381921686232 tensor([2.4792e-01, 1.2412e-02, 1.3388e-04, 6.6707e-01, 7.2460e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.047387029975652695 tensor([0.0474, 0.4671, 0.0794, 0.0481, 0.3581], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8838153437172878e-06 tensor([5.1819e-04, 9.6079e-01, 3.8495e-02, 1.8838e-06, 1.9161e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.4901008802989963e-06 tensor([2.4651e-04, 8.2290e-01, 1.7620e-01, 3.4901e-06, 6.4830e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004343358159530908 tensor([6.3540e-02, 7.2430e-03, 4.3434e-04, 6.3666e-01, 2.9213e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.1154281057533808e-05 tensor([3.3574e-04, 2.1154e-05, 4.5319e-05, 4.7569e-01, 5.2391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005754377925768495 tensor([5.7544e-04, 1.6275e-01, 6.0411e-01, 1.3885e-03, 2.3118e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.306429375195876e-05 tensor([7.2540e-01, 2.7109e-01, 5.3064e-05, 3.3305e-03, 1.2160e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.095607672141341e-06 tensor([6.6201e-01, 2.5581e-03, 1.0956e-06, 3.3414e-01, 1.2944e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.524854713139348e-09 tensor([5.1755e-07, 4.5249e-09, 3.5805e-07, 3.9612e-01, 6.0388e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.282084359554574e-05 tensor([6.2821e-05, 4.4655e-02, 7.1258e-01, 4.0012e-04, 2.4230e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.307833352067973e-06 tensor([7.3212e-04, 9.0083e-01, 9.7687e-02, 9.3078e-06, 7.4521e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9445944417384453e-05 tensor([2.6560e-04, 5.5900e-01, 4.3568e-01, 1.9446e-05, 5.0343e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.493663944529544e-06 tensor([1.8992e-01, 5.6868e-04, 1.4937e-06, 8.0665e-01, 2.8512e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.091892146287137e-08 tensor([1.8828e-06, 6.0919e-08, 3.8281e-06, 3.4241e-01, 6.5759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.333040590364661e-11 tensor([9.3330e-11, 4.9167e-05, 9.9470e-01, 2.3551e-08, 5.2466e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.637217246752698e-05 tensor([5.7644e-05, 1.9833e-01, 7.8081e-01, 1.6372e-05, 2.0789e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.423569965998468e-07 tensor([6.6517e-01, 2.1656e-03, 8.4236e-07, 3.3191e-01, 7.5571e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.826633231568849e-07 tensor([9.8266e-07, 1.0073e-02, 9.6889e-01, 4.6094e-06, 2.1033e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9263895839903853e-07 tensor([3.5164e-01, 4.9162e-04, 2.9264e-07, 6.4665e-01, 1.2098e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.985102946695406e-05 tensor([3.0140e-05, 2.9851e-05, 5.6436e-04, 5.3384e-02, 9.4599e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002483234566170722 tensor([1.5501e-02, 9.6569e-01, 1.6236e-02, 2.4832e-04, 2.3201e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004137176554650068 tensor([0.0099, 0.6604, 0.1459, 0.0041, 0.1797], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.101201842691808e-08 tensor([8.1012e-08, 5.4590e-03, 9.9127e-01, 2.4891e-07, 3.2701e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.125188866055396e-07 tensor([4.5273e-06, 4.1252e-07, 1.6206e-05, 1.6652e-01, 8.3346e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.210369221866131e-05 tensor([7.2104e-05, 9.8819e-03, 1.7766e-01, 2.5985e-03, 8.0978e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011482514441013336 tensor([0.1283, 0.1452, 0.0115, 0.3369, 0.3782], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.7286913943244144e-05 tensor([8.4944e-01, 1.2332e-01, 5.7287e-05, 2.6505e-02, 6.7754e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.303478959907098e-14 tensor([7.8323e-01, 3.0469e-07, 4.3035e-14, 2.1677e-01, 2.7429e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.89738846435273e-11 tensor([2.4341e-01, 3.3791e-06, 9.8974e-11, 7.5657e-01, 1.4543e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.73366628234362e-07 tensor([6.7337e-07, 3.8304e-03, 9.2904e-01, 1.5424e-05, 6.7109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.557367212029931e-07 tensor([7.5574e-07, 1.5423e-02, 9.7386e-01, 1.6502e-06, 1.0710e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0312728448980124e-07 tensor([9.8709e-01, 5.7091e-03, 1.0313e-07, 7.1876e-03, 1.0724e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00030685067758895457 tensor([6.8685e-02, 9.2872e-01, 1.9688e-03, 3.2271e-04, 3.0685e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7174715816192787e-10 tensor([7.2365e-01, 2.1072e-05, 1.7175e-10, 2.7632e-01, 1.0865e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001045806347974576 tensor([1.0295e-03, 1.0458e-04, 1.0683e-04, 4.9863e-01, 5.0013e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0020162102300673723 tensor([0.3287, 0.0974, 0.0020, 0.4877, 0.0842], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.931768019858282e-05 tensor([3.3215e-04, 6.1757e-01, 3.7728e-01, 1.9318e-05, 4.8012e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.382334744448599e-07 tensor([1.2106e-02, 1.3221e-05, 1.3823e-07, 9.8418e-01, 3.7049e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5669720596633852e-06 tensor([1.1527e-02, 5.3483e-05, 1.5670e-06, 9.5149e-01, 3.6924e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.650627569295466e-05 tensor([4.5407e-01, 2.0113e-02, 8.6506e-05, 4.9136e-01, 3.4375e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0016489633126184344 tensor([0.5595, 0.2689, 0.0016, 0.1243, 0.0456], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7671902829974329e-09 tensor([1.7672e-09, 8.3707e-08, 1.3370e-03, 5.2809e-04, 9.9813e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.074795268924845e-08 tensor([9.9098e-01, 2.3448e-03, 2.0748e-08, 6.6752e-03, 2.1461e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001353672705590725 tensor([0.4866, 0.1534, 0.0014, 0.2750, 0.0837], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009396447567269206 tensor([0.3512, 0.6423, 0.0009, 0.0043, 0.0014], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0060281283585937e-06 tensor([8.9603e-01, 1.0362e-01, 1.0060e-06, 3.4487e-04, 3.6957e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.345188019920897e-07 tensor([3.3452e-07, 2.1192e-04, 1.9878e-01, 2.6164e-04, 8.0074e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.1782121379109185e-10 tensor([9.8633e-01, 1.7048e-04, 3.1782e-10, 1.3495e-02, 7.0334e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.443772982336668e-07 tensor([6.2969e-05, 3.4438e-07, 6.0940e-07, 7.4897e-01, 2.5097e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.321103392295299e-08 tensor([3.3211e-08, 1.7331e-03, 9.8490e-01, 4.5608e-07, 1.3362e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002818885841406882 tensor([6.9376e-01, 2.8352e-01, 2.8189e-04, 2.0628e-02, 1.8128e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00023573845101054758 tensor([1.4523e-03, 6.8192e-01, 2.9420e-01, 2.3574e-04, 2.2188e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.808885468170047e-06 tensor([8.8089e-06, 1.6921e-03, 1.8033e-01, 1.6707e-03, 8.1630e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.034476089349482e-06 tensor([1.4981e-03, 1.6267e-05, 5.0345e-06, 8.3717e-01, 1.6132e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0879240335270879e-06 tensor([1.0879e-06, 8.9676e-03, 9.5694e-01, 6.6457e-06, 3.4084e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.6263315905671334e-06 tensor([2.0029e-04, 7.4053e-01, 2.5823e-01, 3.6263e-06, 1.0340e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02647297829389572 tensor([0.0327, 0.4420, 0.0641, 0.0265, 0.4348], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.5131921574939042e-06 tensor([1.1496e-05, 1.3885e-01, 8.5745e-01, 2.5132e-06, 3.6861e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.141843181874719e-07 tensor([1.1289e-04, 8.1418e-07, 1.6245e-06, 8.6620e-01, 1.3368e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.807389129586227e-07 tensor([5.8074e-07, 3.4778e-04, 2.1906e-01, 2.6288e-04, 7.8033e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.58847033669008e-06 tensor([9.3206e-01, 4.1796e-02, 8.5885e-06, 2.5732e-02, 4.0882e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.536208381177858e-06 tensor([6.9685e-01, 3.0267e-01, 9.5362e-06, 4.5051e-04, 1.6292e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005529654445126653 tensor([0.0006, 0.0757, 0.3892, 0.0042, 0.5304], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.543737218756007e-10 tensor([6.5437e-10, 2.3142e-05, 7.9396e-01, 1.8819e-06, 2.0602e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.5083056698349537e-07 tensor([2.5083e-07, 1.6286e-04, 1.4846e-01, 2.0927e-04, 8.5117e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.505022166587878e-06 tensor([8.9725e-01, 9.2623e-03, 2.5050e-06, 9.2986e-02, 4.9758e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.987358923041029e-06 tensor([5.3187e-04, 9.2765e-01, 7.1425e-02, 3.9874e-06, 3.8822e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005336008034646511 tensor([5.0737e-03, 8.3087e-01, 1.4297e-01, 5.3360e-04, 2.0551e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9361805492223993e-09 tensor([3.7549e-05, 7.4718e-09, 1.9362e-09, 9.9408e-01, 5.8834e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.1126807797932088e-09 tensor([3.1127e-09, 7.4818e-06, 1.6415e-01, 3.9936e-05, 8.3580e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007358440780080855 tensor([2.2898e-01, 7.6779e-01, 9.2712e-04, 1.5641e-03, 7.3584e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00040544383227825165 tensor([2.9386e-01, 7.0360e-01, 4.0544e-04, 1.5269e-03, 6.0241e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.674904059560504e-06 tensor([7.3920e-01, 8.5236e-03, 5.6749e-06, 2.4841e-01, 3.8606e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.408103894907981e-05 tensor([3.1070e-01, 1.2641e-02, 8.4081e-05, 6.5171e-01, 2.4864e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.665987919201143e-05 tensor([6.5396e-02, 2.3961e-03, 5.6660e-05, 6.9044e-01, 2.4171e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.3402762963087298e-05 tensor([6.4043e-02, 9.3570e-01, 2.0949e-04, 2.5052e-05, 2.3403e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1266491128480993e-05 tensor([1.1266e-05, 1.8006e-03, 1.0282e-01, 1.4204e-03, 8.9395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.272864941867738e-07 tensor([9.7937e-01, 1.3555e-02, 4.2729e-07, 7.0502e-03, 2.3460e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.931056813399891e-09 tensor([3.9311e-09, 3.8990e-08, 2.1711e-04, 8.9684e-04, 9.9889e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005897435359656811 tensor([0.1791, 0.1594, 0.0059, 0.2310, 0.4246], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016389730444643646 tensor([7.8346e-01, 1.0838e-01, 1.6390e-04, 9.3901e-02, 1.4102e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005059682880528271 tensor([6.4023e-01, 3.0884e-01, 5.0597e-04, 4.1326e-02, 9.0952e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000256006809649989 tensor([2.5601e-04, 8.0615e-03, 9.1355e-02, 1.7535e-02, 8.8279e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027064917609095573 tensor([0.0271, 0.4095, 0.1160, 0.0382, 0.4093], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.158063817769289e-05 tensor([7.1581e-05, 9.4293e-02, 8.0471e-01, 1.2807e-04, 1.0080e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016996977501548827 tensor([2.2934e-03, 8.3440e-01, 1.5563e-01, 1.6997e-04, 7.5074e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.08157300928724e-05 tensor([6.7259e-04, 8.6993e-01, 1.2858e-01, 1.0816e-05, 8.0375e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.779401817562757e-06 tensor([5.7794e-06, 4.7743e-02, 9.4330e-01, 6.1679e-06, 8.9485e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.5630691286314686e-07 tensor([5.8940e-04, 1.6883e-06, 3.5631e-07, 9.6015e-01, 3.9256e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7649444955812896e-09 tensor([2.7649e-09, 1.3175e-05, 3.5680e-01, 2.4139e-05, 6.4316e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001371644320897758 tensor([0.5657, 0.3876, 0.0014, 0.0367, 0.0086], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.846312210953329e-06 tensor([3.8463e-06, 1.1359e-02, 9.1367e-01, 4.2200e-05, 7.4929e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031959000043570995 tensor([0.0046, 0.5312, 0.2695, 0.0032, 0.1915], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6394017166021513e-07 tensor([1.8322e-04, 3.6961e-07, 1.6394e-07, 9.3837e-01, 6.1449e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.891887409845367e-05 tensor([1.9310e-02, 1.0388e-03, 8.8919e-05, 7.8350e-01, 1.9606e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.917375013566925e-06 tensor([2.9174e-06, 4.9243e-03, 7.6420e-01, 1.2657e-04, 2.3075e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5871890354901552e-05 tensor([6.8703e-04, 8.5435e-01, 1.4353e-01, 1.5872e-05, 1.4198e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.3999028801190434e-06 tensor([3.3999e-06, 3.6218e-05, 4.3692e-03, 8.6621e-03, 9.8693e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014217216812539846 tensor([1.3337e-01, 7.5179e-03, 1.4217e-04, 7.5721e-01, 1.0175e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.162450070703926e-07 tensor([3.1625e-07, 7.5799e-07, 2.0465e-04, 7.4035e-03, 9.9239e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.490272476687096e-05 tensor([7.1805e-02, 9.2781e-01, 2.8567e-04, 4.4903e-05, 5.8300e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.415141745539586e-07 tensor([1.1345e-06, 4.1940e-02, 9.5660e-01, 4.4151e-07, 1.4533e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.24852183598598e-13 tensor([3.1454e-02, 2.5273e-08, 4.2485e-13, 9.6854e-01, 1.2747e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00042789310100488365 tensor([1.8667e-01, 1.6936e-02, 4.2789e-04, 6.9954e-01, 9.6430e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3923163733053912e-09 tensor([1.3923e-09, 1.4239e-06, 3.7624e-02, 4.2439e-05, 9.6233e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.077536040218547e-05 tensor([8.0609e-01, 1.7890e-01, 8.0775e-05, 1.4212e-02, 7.1105e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006356536759994924 tensor([5.0107e-02, 9.4190e-01, 5.3664e-03, 6.3565e-04, 1.9884e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0921765351668e-05 tensor([1.0922e-05, 3.7704e-03, 3.2540e-01, 8.3268e-04, 6.6998e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.2384137992048636e-06 tensor([6.4824e-01, 4.4651e-03, 2.2384e-06, 3.4309e-01, 4.2029e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.785787837136013e-07 tensor([4.7858e-07, 4.3707e-05, 2.3348e-02, 9.6240e-04, 9.7565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8075145362672629e-06 tensor([3.6748e-05, 3.8398e-01, 6.1482e-01, 1.8075e-06, 1.1538e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.805287067531026e-06 tensor([9.4218e-01, 1.5707e-02, 2.8053e-06, 4.1862e-02, 2.4569e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004078288038726896 tensor([1.8138e-01, 1.8087e-02, 4.0783e-04, 7.2856e-01, 7.1562e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9437769879004918e-05 tensor([1.1401e-03, 2.8991e-05, 1.9438e-05, 7.9275e-01, 2.0607e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.6544961428953684e-07 tensor([2.7101e-07, 1.4880e-02, 9.8263e-01, 2.6545e-07, 2.4942e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.662864012061618e-05 tensor([8.5299e-01, 1.3176e-01, 4.6629e-05, 1.4700e-02, 5.0255e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001263529120478779 tensor([2.7323e-02, 1.6094e-03, 1.2635e-04, 7.6395e-01, 2.0699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1297313449176727e-06 tensor([4.1209e-03, 1.7132e-05, 1.1297e-06, 9.7275e-01, 2.3112e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012291023507714272 tensor([0.1383, 0.1887, 0.0123, 0.2377, 0.4230], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.074160309144645e-06 tensor([1.0742e-06, 6.0438e-03, 9.2204e-01, 1.3792e-05, 7.1904e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002046138048171997 tensor([0.0020, 0.1343, 0.3806, 0.0117, 0.4712], grad_fn=<SoftmaxBackward0>)\n",
      "3 5.223878542892635e-05 tensor([4.7377e-03, 9.6685e-01, 2.7173e-02, 5.2239e-05, 1.1842e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.717565475573451e-11 tensor([9.8980e-01, 9.3086e-05, 7.7176e-11, 1.0103e-02, 1.5554e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.407207091437158e-07 tensor([6.8638e-05, 2.4072e-07, 3.3999e-07, 8.9440e-01, 1.0553e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.906427321036631e-09 tensor([2.9064e-09, 8.0498e-05, 8.8571e-01, 2.3779e-06, 1.1420e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005431975587271154 tensor([6.1006e-02, 9.3298e-01, 4.5085e-03, 5.4320e-04, 9.6493e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.135204451609752e-06 tensor([3.1352e-06, 4.1703e-03, 7.5766e-01, 1.8478e-04, 2.3798e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.1304830372391734e-06 tensor([3.1305e-06, 7.0192e-03, 8.8268e-01, 6.1115e-05, 1.1023e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.0220085540786386e-05 tensor([2.6643e-01, 8.0744e-03, 5.0220e-05, 6.7272e-01, 5.2725e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015764700947329402 tensor([1.5765e-04, 1.0209e-02, 6.1815e-02, 2.4809e-03, 9.2534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00021543068578466773 tensor([2.1543e-04, 3.4037e-02, 3.4331e-01, 2.9656e-03, 6.1948e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012143589556217194 tensor([0.0012, 0.2719, 0.5032, 0.0019, 0.2218], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.446514599796501e-07 tensor([6.6562e-04, 1.4593e-06, 2.4465e-07, 9.8067e-01, 1.8663e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1306282203804585e-06 tensor([1.1306e-06, 1.6171e-06, 1.7976e-04, 1.1394e-02, 9.8842e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007173125050030649 tensor([0.0007, 0.0851, 0.3807, 0.0039, 0.5296], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011243444896535948 tensor([6.7140e-03, 9.6298e-01, 2.7594e-02, 1.1243e-04, 2.5973e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.142853635537904e-06 tensor([1.4456e-03, 9.7007e-01, 2.8323e-02, 6.1429e-06, 1.5355e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.638788257376291e-05 tensor([8.4445e-01, 4.6161e-02, 3.6388e-05, 1.0393e-01, 5.4252e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2240564501553308e-05 tensor([4.5533e-05, 1.2241e-05, 1.6257e-04, 1.7280e-01, 8.2698e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.991568592847216e-08 tensor([2.9916e-08, 8.6593e-04, 9.6821e-01, 1.3710e-06, 3.0923e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.0885993851370586e-07 tensor([3.4388e-06, 1.4273e-01, 8.5660e-01, 3.0886e-07, 6.6876e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00012344634160399437 tensor([1.7748e-03, 8.0436e-01, 1.8408e-01, 1.2345e-04, 9.6555e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.725071510667476e-07 tensor([2.7251e-07, 4.0642e-05, 4.6705e-02, 8.7823e-04, 9.5238e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.3791919779323507e-07 tensor([3.5530e-03, 8.4857e-06, 4.3792e-07, 9.6948e-01, 2.6953e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.399432652566702e-09 tensor([9.7059e-01, 3.4946e-04, 2.3994e-09, 2.9052e-02, 4.9340e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.404745479609119e-06 tensor([3.9968e-01, 6.0024e-01, 1.2119e-05, 6.1555e-05, 5.4047e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012595194857567549 tensor([0.3014, 0.6916, 0.0013, 0.0042, 0.0015], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.4099128242814913e-05 tensor([2.3858e-01, 7.6115e-01, 8.8550e-05, 1.5119e-04, 3.4099e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.001615032691916e-06 tensor([1.3480e-03, 5.8483e-06, 1.0016e-06, 9.3781e-01, 6.0837e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.0971781345433556e-06 tensor([8.8778e-01, 1.5039e-02, 4.0972e-06, 9.6566e-02, 6.1430e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[0], [4], [2], [0], [0], [2], [3], [2], [1], [0], [3], [0], [2], [0], [0], [2], [3], [3], [1], [0], [3], [0], [2], [1], [2], [3], [2], [0], [2], [2], [3], [3], [0], [2], [0], [2], [2], [2], [1], [1], [3], [3], [2], [1], [1], [3], [3], [0], [0], [3], [0], [3], [2], [2], [2], [3], [3], [2], [2], [2], [3], [3], [2], [2], [0], [3], [3], [2], [2], [0], [2], [2], [3], [2], [2], [0], [3], [0], [0], [0], [4], [3], [2], [2], [0], [2], [3], [2], [0], [0], [2], [2], [2], [2], [3], [3], [2], [0], [2], [2], [0], [3], [3], [2], [1], [0], [2], [2], [1], [0], [3], [3], [2], [1], [0], [3], [2], [0], [2], [1], [3], [3], [0], [1], [0], [2], [2], [2], [2], [0], [0], [2], [4], [2], [1], [2], [3], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [1], [0], [2], [3], [0], [2], [0], [3], [3], [3], [1], [0], [2], [2], [0], [0], [0], [2], [3], [3], [2], [0], [4], [2], [2], [2], [2], [4], [0], [2], [0], [2], [2], [2], [0], [0], [0], [3], [3], [0], [2], [0], [2], [0], [3], [2], [2], [0], [3], [0], [2], [0], [3], [3], [2], [2], [0], [2], [3], [0], [2], [0], [3], [2], [2], [2], [3], [2], [2], [2], [2], [0], [0], [3], [2], [1], [0], [3], [0], [0], [2], [0], [0], [0], [2], [0], [0], [3], [3], [2], [1], [0], [3], [3], [0], [2], [2], [4], [2], [4], [2], [2]]\n",
      "[[1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4]]\n",
      "NL_pred of 0th iteration [[0], [4], [2], [0], [0], [2], [3], [2], [1], [0], [3], [0], [2], [0], [0], [2], [3], [3], [1], [0], [3], [0], [2], [1], [2], [3], [2], [0], [2], [2], [3], [3], [0], [2], [0], [2], [2], [2], [1], [1], [3], [3], [2], [1], [1], [3], [3], [0], [0], [3], [0], [3], [2], [2], [2], [3], [3], [2], [2], [2], [3], [3], [2], [2], [0], [3], [3], [2], [2], [0], [2], [2], [3], [2], [2], [0], [3], [0], [0], [0], [4], [3], [2], [2], [0], [2], [3], [2], [0], [0], [2], [2], [2], [2], [3], [3], [2], [0], [2], [2], [0], [3], [3], [2], [1], [0], [2], [2], [1], [0], [3], [3], [2], [1], [0], [3], [2], [0], [2], [1], [3], [3], [0], [1], [0], [2], [2], [2], [2], [0], [0], [2], [4], [2], [1], [2], [3], [2], [2], [2], [2], [0], [2], [2], [2], [2], [0], [2], [1], [0], [2], [3], [0], [2], [0], [3], [3], [3], [1], [0], [2], [2], [0], [0], [0], [2], [3], [3], [2], [0], [4], [2], [2], [2], [2], [4], [0], [2], [0], [2], [2], [2], [0], [0], [0], [3], [3], [0], [2], [0], [2], [0], [3], [2], [2], [0], [3], [0], [2], [0], [3], [3], [2], [2], [0], [2], [3], [0], [2], [0], [3], [2], [2], [2], [3], [2], [2], [2], [2], [0], [0], [3], [2], [1], [0], [3], [0], [0], [2], [0], [0], [0], [2], [0], [0], [3], [3], [2], [1], [0], [3], [3], [0], [2], [2], [4], [2], [4], [2], [2]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004460514545440674  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00446051025390625  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0044605016708374025  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004460488796234131  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004460473060607911  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004460453987121582  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004460432529449463  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004460409164428711  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00446038293838501  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0044603548049926755  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004460325241088867  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004460294723510742  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004460262775421143  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004460229396820069  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004460195541381836  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004460160732269287  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004460124969482422  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.00446008825302124  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0044600520133972164  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004460015773773193  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.0012107652146369219 tensor([1.5629e-05, 5.2778e-03, 3.6234e-01, 1.2108e-03, 6.3116e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.088812652800698e-05 tensor([4.4411e-01, 5.5570e-01, 2.0888e-05, 1.4997e-04, 1.4564e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.025754429399967194 tensor([0.0802, 0.0258, 0.0018, 0.6281, 0.2640], grad_fn=<SoftmaxBackward0>)\n",
      "1 6.757929077139124e-05 tensor([4.5579e-05, 6.7579e-05, 1.0262e-03, 5.8201e-02, 9.4066e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.927865741890855e-05 tensor([1.6059e-05, 1.9279e-05, 5.5532e-04, 4.4144e-02, 9.5527e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004894619924016297 tensor([2.9018e-01, 7.0770e-01, 4.9818e-04, 1.1325e-03, 4.8946e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021667538210749626 tensor([0.0217, 0.8824, 0.0554, 0.0022, 0.0384], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09634571522474289 tensor([0.2963, 0.4861, 0.0090, 0.0963, 0.1122], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.419186557744979e-06 tensor([2.4192e-06, 8.7724e-07, 5.2272e-05, 4.6834e-02, 9.5311e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021593285782728344 tensor([8.4416e-07, 2.1593e-04, 9.1085e-02, 5.5273e-04, 9.0815e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.7555779272224754e-05 tensor([3.7556e-05, 1.6871e-01, 8.1277e-01, 1.3004e-05, 1.8467e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.5711531748129346e-07 tensor([8.9100e-08, 5.6004e-03, 9.9154e-01, 2.5712e-07, 2.8556e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005044369027018547 tensor([9.2944e-01, 1.9773e-02, 4.0104e-06, 5.0281e-02, 5.0444e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.193109690386336e-06 tensor([3.7749e-06, 9.1931e-06, 7.2757e-04, 1.6039e-02, 9.8322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.06009882883518e-06 tensor([4.2464e-06, 5.0601e-06, 2.2378e-04, 2.0324e-02, 9.7944e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.4913342713261954e-06 tensor([9.5995e-01, 3.9356e-02, 3.0831e-07, 6.8588e-04, 2.4913e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002262756461277604 tensor([1.6749e-02, 9.8086e-01, 2.1334e-03, 3.4031e-05, 2.2628e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011853738687932491 tensor([0.0119, 0.6756, 0.1607, 0.0052, 0.1467], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1278311831119936e-05 tensor([2.2615e-05, 8.5197e-07, 1.1278e-05, 4.2734e-01, 5.7262e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007227977272123098 tensor([0.0019, 0.0072, 0.0196, 0.1026, 0.8687], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0060640973970294 tensor([0.0061, 0.7527, 0.1908, 0.0012, 0.0493], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011936556547880173 tensor([2.4309e-05, 1.1937e-03, 5.6981e-02, 7.7322e-03, 9.3407e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.11718991320231e-05 tensor([9.3392e-01, 3.1731e-04, 4.5427e-09, 6.5749e-02, 1.1172e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.548795473624523e-08 tensor([4.5488e-08, 2.9027e-08, 2.2589e-05, 1.0071e-02, 9.8991e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.159195178654045e-05 tensor([1.4345e-01, 9.1592e-05, 7.3798e-08, 8.5477e-01, 1.6790e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000834579870570451 tensor([9.5884e-03, 9.7785e-01, 1.1665e-02, 5.8906e-05, 8.3458e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.1728301362600178e-05 tensor([8.9201e-01, 1.0659e-01, 3.5606e-06, 1.3661e-03, 2.1728e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006336763617582619 tensor([1.4803e-04, 6.3368e-04, 8.9652e-03, 6.7909e-02, 9.2234e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1488313376903534 tensor([0.3243, 0.1488, 0.0020, 0.3040, 0.2209], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04200897365808487 tensor([0.1510, 0.7049, 0.0189, 0.0420, 0.0833], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016186496941372752 tensor([1.6186e-03, 8.4832e-01, 1.4590e-01, 7.2961e-05, 4.0856e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7668145346760866e-06 tensor([1.7668e-06, 5.4433e-02, 9.4332e-01, 6.5445e-07, 2.2447e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.9285492903218255e-07 tensor([2.7363e-08, 1.4699e-03, 9.9311e-01, 3.9285e-07, 5.4208e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005147031042724848 tensor([9.4777e-01, 1.1947e-02, 1.4866e-06, 3.9766e-02, 5.1470e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.082623139671341e-07 tensor([8.6338e-09, 3.6703e-04, 9.7551e-01, 8.0826e-07, 2.4119e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009186097420752048 tensor([0.4687, 0.5000, 0.0013, 0.0208, 0.0092], grad_fn=<SoftmaxBackward0>)\n",
      "4 2.2990723664406687e-05 tensor([8.8727e-01, 1.1132e-01, 4.0124e-06, 1.3811e-03, 2.2991e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000722521566785872 tensor([7.2898e-01, 2.8340e-03, 1.1365e-06, 2.6747e-01, 7.2252e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.2085306972694525e-07 tensor([4.2085e-07, 8.4211e-08, 1.8536e-05, 5.1164e-02, 9.4882e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.044740388868377e-05 tensor([8.0447e-05, 5.3675e-05, 3.4392e-04, 7.7237e-02, 9.2228e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0022397271823138 tensor([2.2397e-03, 8.8739e-01, 1.0653e-01, 6.0289e-05, 3.7780e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.039744716370478e-05 tensor([7.4330e-04, 9.8143e-01, 1.7741e-02, 1.3177e-06, 8.0397e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.855117706232704e-06 tensor([7.2046e-01, 2.3189e-05, 2.2627e-10, 2.7951e-01, 5.8551e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.484407665790059e-07 tensor([1.0402e-05, 6.5088e-08, 5.4844e-07, 6.9206e-01, 3.0793e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.108777885558084e-05 tensor([8.8729e-04, 7.7328e-05, 9.1088e-05, 4.4756e-01, 5.5138e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006258917041122913 tensor([0.0063, 0.5785, 0.3023, 0.0036, 0.1094], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013748401775956154 tensor([0.0550, 0.9070, 0.0205, 0.0038, 0.0137], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00023468778817914426 tensor([1.5902e-04, 1.4439e-01, 7.6293e-01, 2.3469e-04, 9.2293e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.314136271612369e-07 tensor([2.4474e-09, 6.3141e-07, 9.2853e-03, 1.0722e-04, 9.9061e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.5301603929838166e-05 tensor([3.5302e-05, 1.0199e-01, 8.3900e-01, 3.1288e-05, 5.8947e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010969951108563691 tensor([9.2313e-05, 1.3614e-01, 8.1016e-01, 1.0970e-04, 5.3489e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005751320859417319 tensor([1.1767e-03, 9.2438e-01, 7.3848e-02, 1.6092e-05, 5.7513e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027528563514351845 tensor([1.1755e-01, 2.7529e-03, 3.6422e-05, 8.3700e-01, 4.2662e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.009053443747689e-06 tensor([2.3713e-02, 3.0091e-06, 3.0210e-09, 9.7539e-01, 8.9275e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.211572038708255e-05 tensor([7.1197e-01, 5.6320e-05, 1.0504e-09, 2.8795e-01, 3.2116e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.1094418672146276e-06 tensor([7.1094e-06, 1.0713e-01, 8.9017e-01, 1.7263e-06, 2.6875e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003389242338016629 tensor([9.2419e-03, 9.8304e-01, 7.3419e-03, 3.8164e-05, 3.3892e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04990450292825699 tensor([0.1652, 0.6118, 0.0156, 0.0499, 0.1576], grad_fn=<SoftmaxBackward0>)\n",
      "1 6.826383014413295e-06 tensor([9.5961e-03, 6.8264e-06, 8.0412e-08, 9.8826e-01, 2.1361e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.1976798052492086e-06 tensor([1.7823e-02, 2.1977e-06, 3.3982e-09, 9.8182e-01, 3.5536e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0001775539421942085 tensor([3.8627e-02, 9.5918e-01, 1.9101e-03, 1.0381e-04, 1.7755e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009736388921737671 tensor([1.3064e-02, 9.2504e-01, 5.1390e-02, 7.6521e-04, 9.7364e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009731302270665765 tensor([8.9800e-01, 1.9287e-02, 7.4863e-06, 8.1734e-02, 9.7313e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00011194800026714802 tensor([3.4024e-02, 1.1195e-04, 1.2731e-06, 9.5611e-01, 9.7571e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1106415513495449e-05 tensor([5.2963e-07, 3.8251e-03, 9.5564e-01, 1.1106e-05, 4.0519e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.155761290458031e-06 tensor([6.6822e-03, 9.9276e-01, 5.4578e-04, 1.2155e-06, 9.1558e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00019112192967440933 tensor([1.9112e-04, 7.0682e-01, 2.9160e-01, 4.5218e-06, 1.3842e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003705300623551011 tensor([7.4211e-01, 1.9366e-03, 4.8968e-07, 2.5558e-01, 3.7053e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.347270762787957e-07 tensor([3.1921e-03, 3.3473e-07, 1.4192e-09, 9.9401e-01, 2.7995e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017085089348256588 tensor([1.5249e-06, 1.6097e-03, 4.4851e-01, 1.7085e-04, 5.4971e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013851088471710682 tensor([2.0669e-01, 1.3851e-02, 2.1099e-04, 7.4316e-01, 3.6088e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004275750834494829 tensor([8.7314e-01, 1.1712e-01, 2.2231e-05, 9.2852e-03, 4.2758e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011005342239513993 tensor([1.1005e-03, 6.8859e-01, 2.8782e-01, 1.3322e-04, 2.2355e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021650365670211613 tensor([5.9040e-03, 2.1650e-04, 3.4308e-05, 6.0633e-01, 3.8752e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.955634843398002e-06 tensor([9.9309e-01, 4.9058e-03, 2.1140e-08, 2.0065e-03, 1.9556e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005722764180973172 tensor([8.7084e-06, 5.7406e-03, 6.9307e-01, 5.7228e-04, 3.0060e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004797330591827631 tensor([4.7973e-03, 8.6295e-01, 1.1725e-01, 4.8574e-04, 1.4521e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001035735331242904 tensor([9.8701e-08, 1.8723e-04, 4.1155e-01, 1.0357e-04, 5.8815e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.1116851459955797e-06 tensor([2.2070e-08, 3.1117e-06, 1.1947e-02, 3.6629e-04, 9.8768e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019152123481035233 tensor([0.0013, 0.0192, 0.0397, 0.0214, 0.9184], grad_fn=<SoftmaxBackward0>)\n",
      "3 8.503779827151448e-05 tensor([7.6596e-02, 9.2262e-01, 6.3270e-04, 8.5038e-05, 6.1549e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012156669981777668 tensor([1.2157e-04, 4.5046e-01, 5.4413e-01, 1.0312e-05, 5.2790e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.855129074916476e-06 tensor([9.8792e-01, 2.1268e-03, 2.1026e-08, 9.9488e-03, 5.8551e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.839544165704865e-05 tensor([1.4430e-01, 1.8395e-05, 5.0481e-09, 8.5539e-01, 2.9820e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.6881025405600667e-05 tensor([3.5409e-06, 2.6881e-05, 2.6394e-03, 8.6045e-03, 9.8873e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00022554045426659286 tensor([8.0687e-01, 1.0099e-03, 1.0477e-07, 1.9190e-01, 2.2554e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004426034865900874 tensor([4.4260e-04, 5.7948e-01, 4.0709e-01, 5.9368e-05, 1.2926e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04143340885639191 tensor([0.1905, 0.6720, 0.0168, 0.0414, 0.0793], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.6487542779941577e-06 tensor([2.3630e-07, 1.6488e-06, 6.4963e-04, 3.2108e-03, 9.9614e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.722844096249901e-05 tensor([2.9547e-07, 8.4384e-04, 6.3305e-01, 4.7228e-05, 3.6606e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013121187686920166 tensor([0.6298, 0.1851, 0.0009, 0.1711, 0.0131], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.049628954380750656 tensor([0.5308, 0.2923, 0.0017, 0.1256, 0.0496], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004715094401035458 tensor([1.9951e-01, 4.7151e-04, 7.0782e-07, 7.9725e-01, 2.7674e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0045851366594433784 tensor([0.0047, 0.0046, 0.0037, 0.2238, 0.7632], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.3832056942628697e-05 tensor([3.3832e-05, 3.5491e-01, 6.4218e-01, 2.1374e-06, 2.8783e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00139214051887393 tensor([1.3921e-03, 5.3514e-01, 4.0209e-01, 3.6910e-04, 6.1006e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007289968430995941 tensor([6.2413e-01, 1.4005e-02, 1.7067e-05, 3.5456e-01, 7.2900e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03035891242325306 tensor([0.0135, 0.0324, 0.0304, 0.2172, 0.7066], grad_fn=<SoftmaxBackward0>)\n",
      "1 5.137566404300742e-05 tensor([1.8178e-03, 5.1376e-05, 2.1407e-05, 7.9024e-01, 2.0787e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012707509100437164 tensor([2.3775e-01, 1.2708e-02, 1.4883e-04, 6.7075e-01, 7.8642e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04621584340929985 tensor([0.0431, 0.4537, 0.0844, 0.0462, 0.3725], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00020027281425427645 tensor([4.9443e-04, 9.5817e-01, 4.1130e-02, 1.8740e-06, 2.0027e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002315641031600535 tensor([2.3156e-04, 8.1082e-01, 1.8827e-01, 3.4614e-06, 6.8017e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007235570345073938 tensor([5.9218e-02, 7.2356e-03, 4.7381e-04, 6.2271e-01, 3.1037e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.822206756216474e-05 tensor([3.1001e-04, 2.0845e-05, 4.8222e-05, 4.5660e-01, 5.4303e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012990889372304082 tensor([5.0982e-04, 1.5220e-01, 6.1551e-01, 1.2991e-03, 2.3048e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00013240934640634805 tensor([7.1298e-01, 2.8343e-01, 5.9673e-05, 3.3951e-03, 1.3241e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014344528317451477 tensor([6.5118e-01, 2.6614e-03, 1.2339e-06, 3.4472e-01, 1.4345e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.7373149552877294e-07 tensor([4.8996e-07, 4.4954e-09, 3.7373e-07, 3.8201e-01, 6.1799e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00037399338907562196 tensor([5.5925e-05, 4.1614e-02, 7.1919e-01, 3.7399e-04, 2.3877e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006896726554259658 tensor([6.8967e-04, 8.9316e-01, 1.0536e-01, 9.2582e-06, 7.8662e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00024343097175005823 tensor([2.4343e-04, 5.3831e-01, 4.5624e-01, 1.8844e-05, 5.1851e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005793205345980823 tensor([1.8341e-01, 5.7932e-04, 1.6373e-06, 8.1294e-01, 3.0745e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7516703110231902e-06 tensor([1.7517e-06, 5.9943e-08, 4.0054e-06, 3.2669e-01, 6.7331e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3136770721521316e-08 tensor([8.9083e-11, 4.7643e-05, 9.9469e-01, 2.3137e-08, 5.2619e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.173355748411268e-05 tensor([5.1734e-05, 1.8602e-01, 7.9316e-01, 1.5481e-05, 2.0750e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008378760539926589 tensor([6.5452e-01, 2.2555e-03, 9.4986e-07, 3.4239e-01, 8.3788e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.328979230194818e-06 tensor([8.8080e-07, 9.3858e-03, 9.6996e-01, 4.3290e-06, 2.0648e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005028593586757779 tensor([3.4095e-01, 5.0286e-04, 3.2326e-07, 6.5723e-01, 1.3167e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7068315830547363e-05 tensor([2.7068e-05, 2.8530e-05, 5.8111e-04, 4.9788e-02, 9.4958e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0024793664924800396 tensor([1.4569e-02, 9.6498e-01, 1.7721e-02, 2.4640e-04, 2.4794e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009092312306165695 tensor([0.0091, 0.6441, 0.1556, 0.0040, 0.1872], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.4214924110310676e-07 tensor([7.5428e-08, 5.1935e-03, 9.9153e-01, 2.4215e-07, 3.2764e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.119086497667013e-06 tensor([4.1191e-06, 3.9804e-07, 1.6770e-05, 1.5669e-01, 8.4329e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0024339482188224792 tensor([6.4335e-05, 9.2980e-03, 1.8202e-01, 2.4339e-03, 8.0618e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11902086436748505 tensor([0.1190, 0.1433, 0.0123, 0.3281, 0.3972], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007628408493474126 tensor([8.4113e-01, 1.3040e-01, 6.6064e-05, 2.7645e-02, 7.6284e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.9120821309902567e-08 tensor([7.7900e-01, 3.1233e-07, 4.6120e-14, 2.2100e-01, 2.9121e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.4219599456264405e-06 tensor([2.3622e-01, 3.4220e-06, 1.0691e-10, 7.6376e-01, 1.5603e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4573563021258451e-05 tensor([6.0855e-07, 3.5872e-03, 9.3035e-01, 1.4574e-05, 6.6052e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5519950693487772e-06 tensor([6.7563e-07, 1.4330e-02, 9.7513e-01, 1.5520e-06, 1.0540e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1561553947103675e-05 tensor([9.8665e-01, 5.9659e-03, 1.1395e-07, 7.3688e-03, 1.1562e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00031678026425652206 tensor([6.5056e-02, 9.3219e-01, 2.1184e-03, 3.1678e-04, 3.2056e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1755556442949455e-05 tensor([7.1687e-01, 2.1682e-05, 1.8725e-10, 2.8309e-01, 1.1756e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00011403996904846281 tensor([9.4671e-04, 1.0286e-04, 1.1404e-04, 4.7864e-01, 5.2020e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09162116050720215 tensor([0.3153, 0.0997, 0.0022, 0.4911, 0.0916], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000306261470541358 tensor([3.0626e-04, 5.9776e-01, 3.9694e-01, 1.8834e-05, 4.9724e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.3366126040637027e-05 tensor([1.1590e-02, 1.3366e-05, 1.5063e-07, 9.8443e-01, 3.9713e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.417058855528012e-05 tensor([1.1021e-02, 5.4171e-05, 1.7116e-06, 9.4932e-01, 3.9604e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02073991671204567 tensor([4.4112e-01, 2.0740e-02, 9.6470e-05, 5.0046e-01, 3.7581e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05038377642631531 tensor([0.5426, 0.2781, 0.0019, 0.1270, 0.0504], grad_fn=<SoftmaxBackward0>)\n",
      "1 8.241264026764838e-08 tensor([1.6529e-09, 8.2413e-08, 1.3816e-03, 5.0072e-04, 9.9812e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.289526719323476e-06 tensor([9.9075e-01, 2.4436e-03, 2.2721e-08, 6.8065e-03, 2.2895e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0919773131608963 tensor([0.4693, 0.1581, 0.0015, 0.2791, 0.0920], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014635090483352542 tensor([0.3376, 0.6556, 0.0010, 0.0043, 0.0015], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.942146577173844e-06 tensor([8.9159e-01, 1.0805e-01, 1.1054e-06, 3.5076e-04, 3.9421e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002018240193137899 tensor([3.0412e-07, 2.0182e-04, 2.0310e-01, 2.4814e-04, 7.9645e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.503161896238453e-07 tensor([9.8605e-01, 1.7656e-04, 3.4491e-10, 1.3773e-02, 7.5032e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.534749559250486e-07 tensor([5.9513e-05, 3.4370e-07, 6.5347e-07, 7.3509e-01, 2.6485e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.4498588636088243e-07 tensor([3.1007e-08, 1.6526e-03, 9.8495e-01, 4.4499e-07, 1.3394e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0020164214074611664 tensor([6.7978e-01, 2.9664e-01, 3.2102e-04, 2.1248e-02, 2.0164e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013406024081632495 tensor([1.3406e-03, 6.6332e-01, 3.1202e-01, 2.3028e-04, 2.3084e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015644689556211233 tensor([7.9355e-06, 1.6110e-03, 1.8547e-01, 1.5645e-03, 8.1134e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.6384203263442032e-05 tensor([1.4220e-03, 1.6384e-05, 5.4705e-06, 8.2680e-01, 1.7176e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.218518592504552e-06 tensor([9.7251e-07, 8.3540e-03, 9.5826e-01, 6.2185e-06, 3.3380e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00018696142069529742 tensor([1.8696e-04, 7.2462e-01, 2.7411e-01, 3.5843e-06, 1.0815e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.029728364199399948 tensor([0.0297, 0.4286, 0.0677, 0.0253, 0.4486], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0369733900006395e-05 tensor([1.0370e-05, 1.3001e-01, 8.6630e-01, 2.3898e-06, 3.6763e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7594385326447082e-06 tensor([1.0779e-04, 8.2187e-07, 1.7594e-06, 8.5756e-01, 1.4233e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00024964174372144043 tensor([5.2911e-07, 3.3145e-04, 2.2357e-01, 2.4964e-04, 7.7585e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00046192732406780124 tensor([9.2824e-01, 4.4288e-02, 9.8966e-06, 2.6996e-02, 4.6193e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.741474261507392e-05 tensor([6.8506e-01, 3.1446e-01, 1.0535e-05, 4.5459e-04, 1.7415e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00396014703437686 tensor([4.9218e-04, 7.1132e-02, 3.9635e-01, 3.9601e-03, 5.2807e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8332789295527618e-06 tensor([6.2553e-10, 2.2562e-05, 7.9476e-01, 1.8333e-06, 2.0522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001555335766170174 tensor([2.2806e-07, 1.5553e-04, 1.5234e-01, 1.9831e-04, 8.4731e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005621555028483272 tensor([8.9214e-01, 9.8082e-03, 2.8885e-06, 9.7486e-02, 5.6216e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004099408979527652 tensor([5.0413e-04, 9.2219e-01, 7.6893e-02, 3.9824e-06, 4.0994e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0047386568039655685 tensor([4.7387e-03, 8.1950e-01, 1.5364e-01, 5.2615e-04, 2.1596e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.562308113051586e-09 tensor([3.6140e-05, 7.5623e-09, 2.0975e-09, 9.9368e-01, 6.2866e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.3390388024563435e-06 tensor([2.9685e-09, 7.3390e-06, 1.6595e-01, 3.8715e-05, 8.3401e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010091321310028434 tensor([0.2188, 0.7778, 0.0010, 0.0016, 0.0008], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000645727792289108 tensor([2.8179e-01, 7.1559e-01, 4.4419e-04, 1.5295e-03, 6.4573e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004339162260293961 tensor([7.2869e-01, 8.9560e-03, 6.5022e-06, 2.5801e-01, 4.3392e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013009649701416492 tensor([2.9974e-01, 1.3010e-02, 9.3995e-05, 6.5999e-01, 2.7167e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0023971989285200834 tensor([6.1543e-02, 2.3972e-03, 6.1456e-05, 6.7940e-01, 2.5660e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.461396434227936e-05 tensor([6.0904e-02, 9.3882e-01, 2.2433e-04, 2.4614e-05, 2.4350e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00134499731939286 tensor([1.0197e-05, 1.7125e-03, 1.0572e-01, 1.3450e-03, 8.9121e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.550312456151005e-05 tensor([9.7844e-01, 1.4296e-02, 4.7848e-07, 7.2390e-03, 2.5503e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.857219965652803e-08 tensor([3.7154e-09, 3.8572e-08, 2.2381e-04, 8.5698e-04, 9.9892e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15679624676704407 tensor([0.1662, 0.1568, 0.0063, 0.2254, 0.4452], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015821276232600212 tensor([7.7238e-01, 1.1416e-01, 1.8818e-04, 9.7446e-02, 1.5821e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010110429488122463 tensor([6.2515e-01, 3.2163e-01, 5.7447e-04, 4.2544e-02, 1.0110e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0076252613216638565 tensor([2.2736e-04, 7.6253e-03, 9.3726e-02, 1.6258e-02, 8.8216e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.036385394632816315 tensor([0.0245, 0.3953, 0.1221, 0.0364, 0.4217], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00012019596761092544 tensor([6.4040e-05, 8.8090e-02, 8.1219e-01, 1.2020e-04, 9.9536e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002135238843038678 tensor([2.1352e-03, 8.2207e-01, 1.6771e-01, 1.6752e-04, 7.9168e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006328959134407341 tensor([6.3290e-04, 8.6048e-01, 1.3803e-01, 1.0733e-05, 8.4510e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.753813638875727e-06 tensor([5.1248e-06, 4.4242e-02, 9.4694e-01, 5.7538e-06, 8.8082e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.7053343981388025e-06 tensor([5.6334e-04, 1.7053e-06, 3.8828e-07, 9.5726e-01, 4.2177e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.290590353164589e-05 tensor([2.6284e-09, 1.2906e-05, 3.6026e-01, 2.3300e-05, 6.3970e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009557448327541351 tensor([0.5498, 0.4015, 0.0015, 0.0376, 0.0096], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.941198156098835e-05 tensor([3.4222e-06, 1.0553e-02, 9.1600e-01, 3.9412e-05, 7.3408e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004150501452386379 tensor([0.0042, 0.5119, 0.2838, 0.0031, 0.1971], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.737025622285728e-07 tensor([1.7523e-04, 3.7370e-07, 1.7853e-07, 9.3396e-01, 6.5861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010437153978273273 tensor([1.8130e-02, 1.0437e-03, 9.7387e-05, 7.7143e-01, 2.0930e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011788717529270798 tensor([2.5890e-06, 4.5802e-03, 7.6919e-01, 1.1789e-04, 2.2611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006443005986511707 tensor([6.4430e-04, 8.4357e-01, 1.5428e-01, 1.5734e-05, 1.4963e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.471278614597395e-05 tensor([3.0671e-06, 3.4713e-05, 4.5151e-03, 8.0964e-03, 9.8735e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007639813702553511 tensor([1.2670e-01, 7.6398e-03, 1.5739e-04, 7.5558e-01, 1.0992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.355632760663866e-07 tensor([2.9091e-07, 7.3556e-07, 2.1098e-04, 6.9795e-03, 9.9281e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.091054456192069e-05 tensor([6.8184e-02, 9.3140e-01, 3.0676e-04, 4.4169e-05, 6.0911e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.047343516802357e-06 tensor([1.0473e-06, 3.9678e-02, 9.5886e-01, 4.2722e-07, 1.4584e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.5321810426248703e-08 tensor([3.0421e-02, 2.5322e-08, 4.5023e-13, 9.6958e-01, 1.3406e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01721830666065216 tensor([1.7794e-01, 1.7218e-02, 4.7340e-04, 7.0012e-01, 1.0425e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4056877262191847e-06 tensor([1.3348e-09, 1.4057e-06, 3.8218e-02, 4.1235e-05, 9.6174e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007971076411195099 tensor([7.9575e-01, 1.8860e-01, 9.2771e-05, 1.4760e-02, 7.9711e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0021321794483810663 tensor([4.7175e-02, 9.4418e-01, 5.8784e-03, 6.3212e-04, 2.1322e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007839055615477264 tensor([9.8184e-06, 3.5574e-03, 3.3187e-01, 7.8391e-04, 6.6378e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0046562752686440945 tensor([6.3640e-01, 4.6563e-03, 2.5373e-06, 3.5426e-01, 4.6839e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.202819036436267e-05 tensor([4.3691e-07, 4.2028e-05, 2.4078e-02, 9.1156e-04, 9.7497e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.3599826565478e-05 tensor([3.3600e-05, 3.6629e-01, 6.3250e-01, 1.7443e-06, 1.1723e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00027605335344560444 tensor([9.3925e-01, 1.6602e-02, 3.2112e-06, 4.3870e-02, 2.7605e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.018497928977012634 tensor([1.7348e-01, 1.8498e-02, 4.5311e-04, 7.3006e-01, 7.7505e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.9168857508921064e-05 tensor([1.0780e-03, 2.9169e-05, 2.1126e-05, 7.7987e-01, 2.1900e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.517304551474808e-07 tensor([2.5173e-07, 1.4139e-02, 9.8336e-01, 2.5789e-07, 2.4992e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005649899248965085 tensor([8.4480e-01, 1.3926e-01, 5.3678e-05, 1.5320e-02, 5.6499e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016167666763067245 tensor([2.5683e-02, 1.6168e-03, 1.3808e-04, 7.5200e-01, 2.2057e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.7422218661522493e-05 tensor([3.9424e-03, 1.7422e-05, 1.2407e-06, 9.7116e-01, 2.4884e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1281413584947586 tensor([0.1281, 0.1855, 0.0131, 0.2312, 0.4421], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2967776456207503e-05 tensor([9.6566e-07, 5.6480e-03, 9.2375e-01, 1.2968e-05, 7.0590e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010901985689997673 tensor([0.0018, 0.1264, 0.3894, 0.0109, 0.4715], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001256583142094314 tensor([4.4739e-03, 9.6468e-01, 2.9541e-02, 5.2033e-05, 1.2566e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.650639660510933e-07 tensor([9.8963e-01, 9.6544e-05, 8.3666e-11, 1.0276e-02, 1.6506e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.6894169852530467e-07 tensor([6.5585e-05, 2.4306e-07, 3.6894e-07, 8.8727e-01, 1.1266e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.304761665072874e-06 tensor([2.7305e-09, 7.7403e-05, 8.8617e-01, 2.3048e-06, 1.1375e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010166011052206159 tensor([5.7607e-02, 9.3596e-01, 4.8828e-03, 5.3483e-04, 1.0166e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017267240036744624 tensor([2.7906e-06, 3.8819e-03, 7.6225e-01, 1.7267e-04, 2.3370e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.7056022342294455e-05 tensor([2.7877e-06, 6.5302e-03, 8.8549e-01, 5.7056e-05, 1.0792e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008293865248560905 tensor([2.5655e-01, 8.2939e-03, 5.5895e-05, 6.7779e-01, 5.7310e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002345492597669363 tensor([1.4209e-04, 9.6705e-03, 6.3524e-02, 2.3455e-03, 9.2432e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0027516032569110394 tensor([1.8984e-04, 3.1810e-02, 3.4985e-01, 2.7516e-03, 6.1540e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018268460407853127 tensor([0.0011, 0.2571, 0.5172, 0.0018, 0.2228], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4809462527409778e-06 tensor([6.3813e-04, 1.4809e-06, 2.6723e-07, 9.7931e-01, 2.0049e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5592459021718241e-06 tensor([1.0287e-06, 1.5592e-06, 1.8571e-04, 1.0694e-02, 9.8912e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036788645666092634 tensor([0.0006, 0.0799, 0.3883, 0.0037, 0.5275], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0027568964287638664 tensor([6.3371e-03, 9.6080e-01, 2.9995e-02, 1.1189e-04, 2.7569e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00016028295794967562 tensor([1.3780e-03, 9.6818e-01, 3.0280e-02, 6.0978e-06, 1.6028e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006114680785685778 tensor([8.3657e-01, 4.8802e-02, 4.1923e-05, 1.0847e-01, 6.1147e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1144136048387736e-05 tensor([4.1144e-05, 1.1777e-05, 1.6873e-04, 1.6234e-01, 8.3743e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3217469359005918e-06 tensor([2.7703e-08, 8.2397e-04, 9.6841e-01, 1.3217e-06, 3.0762e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.1744746138429036e-06 tensor([3.1745e-06, 1.3549e-01, 8.6384e-01, 2.9917e-07, 6.7290e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016468478133901954 tensor([1.6468e-03, 7.8993e-01, 1.9813e-01, 1.2150e-04, 1.0168e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.924337579519488e-05 tensor([2.5003e-07, 3.9243e-05, 4.8114e-02, 8.3120e-04, 9.5102e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.621373126516119e-06 tensor([3.4059e-03, 8.6214e-06, 4.7919e-07, 9.6763e-01, 2.8957e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.326426162355347e-06 tensor([9.6983e-01, 3.6363e-04, 2.6317e-09, 2.9805e-02, 5.3264e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3096279872115701e-05 tensor([3.8774e-01, 6.1218e-01, 1.3096e-05, 6.1296e-05, 5.6678e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0016580343944951892 tensor([0.2887, 0.7040, 0.0014, 0.0042, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.553100971970707e-05 tensor([2.2909e-01, 7.7063e-01, 9.5531e-05, 1.4980e-04, 3.5782e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.931323812546907e-06 tensor([1.2870e-03, 5.9313e-06, 1.0986e-06, 9.3327e-01, 6.5435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006929101655259728 tensor([8.8222e-01, 1.5891e-02, 4.7088e-06, 1.0119e-01, 6.9291e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 3], [4, 2], [2, 1], [0, 1], [0, 1], [2, 4], [3, 0], [2, 3], [1, 0], [0, 1], [3, 0], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [3, 4], [3, 0], [1, 2], [0, 1], [3, 0], [0, 1], [2, 4], [1, 0], [2, 1], [3, 4], [2, 4], [0, 1], [2, 1], [2, 3], [3, 0], [3, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [2, 4], [1, 0], [1, 0], [3, 0], [3, 4], [2, 4], [1, 2], [1, 2], [3, 0], [3, 4], [0, 3], [0, 1], [3, 0], [0, 3], [3, 4], [2, 1], [2, 1], [2, 4], [3, 0], [3, 4], [2, 3], [2, 1], [2, 1], [3, 4], [3, 4], [2, 4], [2, 1], [0, 3], [3, 4], [3, 0], [2, 4], [2, 1], [0, 3], [2, 1], [2, 4], [3, 0], [2, 1], [2, 4], [0, 3], [3, 0], [0, 3], [0, 1], [0, 1], [4, 3], [3, 0], [2, 4], [2, 1], [0, 1], [2, 4], [3, 0], [2, 3], [0, 1], [0, 3], [2, 4], [2, 4], [2, 1], [2, 1], [3, 0], [3, 0], [2, 4], [0, 2], [2, 1], [2, 1], [0, 3], [3, 4], [3, 0], [2, 1], [1, 2], [0, 3], [2, 4], [2, 4], [1, 2], [0, 3], [3, 0], [3, 0], [2, 1], [1, 0], [0, 3], [3, 0], [2, 4], [0, 3], [2, 1], [1, 0], [3, 4], [3, 0], [0, 3], [1, 0], [0, 3], [2, 0], [2, 4], [2, 4], [2, 1], [0, 3], [0, 3], [2, 4], [4, 3], [2, 4], [1, 2], [2, 4], [3, 0], [2, 1], [2, 1], [2, 1], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [1, 2], [0, 3], [2, 4], [3, 0], [0, 3], [2, 1], [0, 3], [3, 0], [3, 0], [3, 0], [1, 2], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [3, 4], [3, 0], [2, 1], [0, 1], [4, 2], [2, 4], [2, 4], [2, 1], [2, 1], [4, 3], [0, 3], [2, 4], [0, 1], [2, 1], [2, 4], [2, 4], [0, 1], [0, 3], [0, 3], [3, 0], [3, 0], [0, 3], [2, 1], [0, 1], [2, 4], [0, 3], [3, 0], [2, 1], [2, 1], [0, 3], [3, 0], [0, 1], [2, 1], [0, 1], [3, 4], [3, 0], [2, 1], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [2, 1], [0, 1], [3, 0], [2, 4], [2, 1], [2, 1], [3, 0], [2, 4], [2, 1], [2, 1], [2, 0], [0, 3], [0, 3], [3, 4], [2, 4], [1, 2], [0, 3], [3, 4], [0, 3], [0, 3], [2, 1], [0, 3], [0, 3], [0, 3], [2, 1], [0, 1], [0, 3], [3, 4], [3, 4], [2, 4], [1, 0], [0, 3], [3, 0], [3, 0], [0, 1], [2, 1], [2, 4], [4, 2], [2, 4], [4, 2], [2, 1], [2, 4]]\n",
      "[[1, 2, 4], [0, 1, 3], [0, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 4], [2, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 2], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 2], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 2], [0, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 2], [0, 1, 4], [0, 3, 4], [0, 3, 4], [0, 1, 2], [0, 1, 2], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 2], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 2], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 2], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 2], [0, 1, 2], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3]]\n",
      "NL_pred of 1th iteration [[0, 3], [4, 2], [2, 1], [0, 1], [0, 1], [2, 4], [3, 0], [2, 3], [1, 0], [0, 1], [3, 0], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [3, 4], [3, 0], [1, 2], [0, 1], [3, 0], [0, 1], [2, 4], [1, 0], [2, 1], [3, 4], [2, 4], [0, 1], [2, 1], [2, 3], [3, 0], [3, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 4], [2, 4], [1, 0], [1, 0], [3, 0], [3, 4], [2, 4], [1, 2], [1, 2], [3, 0], [3, 4], [0, 3], [0, 1], [3, 0], [0, 3], [3, 4], [2, 1], [2, 1], [2, 4], [3, 0], [3, 4], [2, 3], [2, 1], [2, 1], [3, 4], [3, 4], [2, 4], [2, 1], [0, 3], [3, 4], [3, 0], [2, 4], [2, 1], [0, 3], [2, 1], [2, 4], [3, 0], [2, 1], [2, 4], [0, 3], [3, 0], [0, 3], [0, 1], [0, 1], [4, 3], [3, 0], [2, 4], [2, 1], [0, 1], [2, 4], [3, 0], [2, 3], [0, 1], [0, 3], [2, 4], [2, 4], [2, 1], [2, 1], [3, 0], [3, 0], [2, 4], [0, 2], [2, 1], [2, 1], [0, 3], [3, 4], [3, 0], [2, 1], [1, 2], [0, 3], [2, 4], [2, 4], [1, 2], [0, 3], [3, 0], [3, 0], [2, 1], [1, 0], [0, 3], [3, 0], [2, 4], [0, 3], [2, 1], [1, 0], [3, 4], [3, 0], [0, 3], [1, 0], [0, 3], [2, 0], [2, 4], [2, 4], [2, 1], [0, 3], [0, 3], [2, 4], [4, 3], [2, 4], [1, 2], [2, 4], [3, 0], [2, 1], [2, 1], [2, 1], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [0, 1], [2, 4], [1, 2], [0, 3], [2, 4], [3, 0], [0, 3], [2, 1], [0, 3], [3, 0], [3, 0], [3, 0], [1, 2], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [0, 1], [2, 4], [3, 4], [3, 0], [2, 1], [0, 1], [4, 2], [2, 4], [2, 4], [2, 1], [2, 1], [4, 3], [0, 3], [2, 4], [0, 1], [2, 1], [2, 4], [2, 4], [0, 1], [0, 3], [0, 3], [3, 0], [3, 0], [0, 3], [2, 1], [0, 1], [2, 4], [0, 3], [3, 0], [2, 1], [2, 1], [0, 3], [3, 0], [0, 1], [2, 1], [0, 1], [3, 4], [3, 0], [2, 1], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [2, 1], [0, 1], [3, 0], [2, 4], [2, 1], [2, 1], [3, 0], [2, 4], [2, 1], [2, 1], [2, 0], [0, 3], [0, 3], [3, 4], [2, 4], [1, 2], [0, 3], [3, 4], [0, 3], [0, 3], [2, 1], [0, 3], [0, 3], [0, 3], [2, 1], [0, 1], [0, 3], [3, 4], [3, 4], [2, 4], [1, 0], [0, 3], [3, 0], [3, 0], [0, 1], [2, 1], [2, 4], [4, 2], [2, 4], [4, 2], [2, 1], [2, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004484755039215088  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004484750270843506  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004484740257263184  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0044847259521484375  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004484708786010742  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004484687328338623  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004484664440155029  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.00448463773727417  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004484610080718994  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004484578609466553  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004484546184539795  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004484513282775879  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004484478950500488  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004484443664550781  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004484406471252441  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00448436975479126  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004484332084655761  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004484293937683105  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004484255790710449  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004484217166900634  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.004918299615383148 tensor([1.4161e-05, 4.9183e-03, 3.6432e-01, 1.1599e-03, 6.2959e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00015218502085190266 tensor([4.3675e-01, 5.6306e-01, 2.2297e-05, 1.5219e-04, 1.5294e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07490281760692596 tensor([0.0749, 0.0250, 0.0019, 0.6219, 0.2762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010415102588012815 tensor([4.1124e-05, 6.3646e-05, 1.0415e-03, 5.5148e-02, 9.4371e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005598531570285559 tensor([1.4601e-05, 1.8155e-05, 5.5985e-04, 4.2079e-02, 9.5733e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011527942260727286 tensor([2.8395e-01, 7.1385e-01, 5.3277e-04, 1.1528e-03, 5.1736e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04103740304708481 tensor([0.0207, 0.8765, 0.0595, 0.0022, 0.0410], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12052373588085175 tensor([0.2839, 0.4885, 0.0098, 0.0973, 0.1205], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.304497972247191e-05 tensor([2.2310e-06, 8.3597e-07, 5.3045e-05, 4.4957e-02, 9.5499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005333019653335214 tensor([7.7859e-07, 2.0465e-04, 9.2031e-02, 5.3330e-04, 9.0723e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.018569378182291985 tensor([3.4483e-05, 1.5893e-01, 8.2245e-01, 1.2594e-05, 1.8569e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0028758267872035503 tensor([8.3373e-08, 5.3131e-03, 9.9181e-01, 2.5266e-07, 2.8758e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.020467163994908333 tensor([9.2584e-01, 2.0467e-02, 4.4882e-06, 5.3124e-02, 5.6139e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007380627794191241 tensor([3.4661e-06, 8.7398e-06, 7.3806e-04, 1.5344e-02, 9.8391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00022679736139252782 tensor([3.8912e-06, 4.7946e-06, 2.2680e-04, 1.9477e-02, 9.8029e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000703943835105747 tensor([9.5895e-01, 4.0338e-02, 3.3135e-07, 7.0394e-04, 2.6302e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002281891880556941 tensor([1.6178e-02, 9.8127e-01, 2.2819e-03, 3.4294e-05, 2.3774e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15440300107002258 tensor([0.0111, 0.6591, 0.1703, 0.0052, 0.1544], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.105823477904778e-05 tensor([2.1058e-05, 8.2269e-07, 1.1621e-05, 4.1478e-01, 5.8519e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01973625458776951 tensor([0.0017, 0.0068, 0.0197, 0.0984, 0.8734], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05214930325746536 tensor([0.0058, 0.7386, 0.2023, 0.0012, 0.0521], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007348615676164627 tensor([2.1978e-05, 1.1185e-03, 5.7546e-02, 7.3486e-03, 9.3397e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003237574710510671 tensor([9.3073e-01, 3.2376e-04, 4.9412e-09, 6.8934e-02, 1.2170e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.2889906176715158e-05 tensor([4.3044e-08, 2.8197e-08, 2.2890e-05, 9.7594e-03, 9.9022e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0017710017273202538 tensor([1.3703e-01, 9.0252e-05, 7.8034e-08, 8.6111e-01, 1.7710e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009276757948100567 tensor([9.2768e-03, 9.7728e-01, 1.2500e-02, 5.9898e-05, 8.8576e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014082359848544002 tensor([8.8896e-01, 1.0960e-01, 3.8669e-06, 1.4082e-03, 2.3220e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009063461795449257 tensor([1.3433e-04, 5.9693e-04, 9.0635e-03, 6.4678e-02, 9.2553e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23531919717788696 tensor([0.3082, 0.1455, 0.0021, 0.3089, 0.2353], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08928059786558151 tensor([0.1451, 0.7026, 0.0203, 0.0427, 0.0893], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00432668998837471 tensor([1.5482e-03, 8.3847e-01, 1.5558e-01, 7.3789e-05, 4.3267e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0022515980526804924 tensor([1.6288e-06, 5.1156e-02, 9.4659e-01, 6.3493e-07, 2.2516e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001405708841048181 tensor([2.5860e-08, 1.4057e-03, 9.9313e-01, 3.8780e-07, 5.4629e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01232069730758667 tensor([9.4517e-01, 1.2321e-02, 1.6505e-06, 4.1940e-02, 5.7046e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003506199864204973 tensor([8.1788e-09, 3.5062e-04, 9.7526e-01, 8.0206e-07, 2.4392e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021533064544200897 tensor([0.4603, 0.5068, 0.0014, 0.0215, 0.0099], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014183305902406573 tensor([8.8416e-01, 1.1439e-01, 4.3440e-06, 1.4183e-03, 2.4444e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002877443563193083 tensor([7.1753e-01, 2.8774e-03, 1.2422e-06, 2.7881e-01, 7.8940e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.884641096694395e-05 tensor([3.9617e-07, 8.1654e-08, 1.8846e-05, 4.9372e-02, 9.5061e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003475239500403404 tensor([7.2801e-05, 5.0415e-05, 3.4752e-04, 7.3595e-02, 9.2593e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004013265483081341 tensor([2.1482e-03, 8.7983e-01, 1.1395e-01, 6.1140e-05, 4.0133e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007194586214609444 tensor([7.1946e-04, 9.8033e-01, 1.8868e-02, 1.3291e-06, 8.4185e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.332727672182955e-05 tensor([7.1172e-01, 2.3327e-05, 2.4052e-10, 2.8825e-01, 6.2399e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.809214134293143e-06 tensor([9.8092e-06, 6.3637e-08, 5.7284e-07, 6.8063e-01, 3.1936e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008200117154046893 tensor([8.2001e-04, 7.4103e-05, 9.3739e-05, 4.3494e-01, 5.6407e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11385320127010345 tensor([0.0059, 0.5609, 0.3158, 0.0036, 0.1139], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.022182544693350792 tensor([0.0529, 0.9063, 0.0222, 0.0039, 0.0148], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09279569238424301 tensor([1.4423e-04, 1.3534e-01, 7.7150e-01, 2.2527e-04, 9.2796e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010484626545803621 tensor([2.3378e-09, 6.1361e-07, 9.3331e-03, 1.0485e-04, 9.9056e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05916571617126465 tensor([3.2443e-05, 9.5860e-02, 8.4491e-01, 3.0320e-05, 5.9166e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.053922105580568314 tensor([8.3964e-05, 1.2757e-01, 8.1832e-01, 1.0575e-04, 5.3922e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011296509765088558 tensor([1.1297e-03, 9.1909e-01, 7.9153e-02, 1.6300e-05, 6.0894e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04514504596590996 tensor([1.1168e-01, 2.7202e-03, 3.8821e-05, 8.4042e-01, 4.5145e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009345258004032075 tensor([2.2513e-02, 2.9414e-06, 3.1653e-09, 9.7655e-01, 9.3453e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.698598033632152e-05 tensor([7.0038e-01, 5.6986e-05, 1.1416e-09, 2.9953e-01, 3.5000e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002700906479731202 tensor([6.5499e-06, 1.0098e-01, 8.9631e-01, 1.6733e-06, 2.7009e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007846076972782612 tensor([8.9205e-03, 9.8284e-01, 7.8461e-03, 3.8421e-05, 3.5575e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15755915641784668 tensor([0.1576, 0.6070, 0.0168, 0.0504, 0.1683], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0022520108614116907 tensor([9.1150e-03, 6.7233e-06, 8.5165e-08, 9.8863e-01, 2.2520e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00037353913648985326 tensor([1.6895e-02, 2.1529e-06, 3.5813e-09, 9.8273e-01, 3.7354e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002036084420979023 tensor([3.7370e-02, 9.6030e-01, 2.0361e-03, 1.0452e-04, 1.8629e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012554403394460678 tensor([1.2554e-02, 9.2091e-01, 5.5318e-02, 7.7972e-04, 1.0437e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01984737627208233 tensor([8.9315e-01, 1.9847e-02, 8.2805e-06, 8.5920e-02, 1.0723e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010289558209478855 tensor([3.2269e-02, 1.1010e-04, 1.3483e-06, 9.5733e-01, 1.0290e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003574044443666935 tensor([4.8344e-07, 3.5740e-03, 9.5610e-01, 1.0688e-05, 4.0311e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005766984540969133 tensor([6.5062e-03, 9.9291e-01, 5.7670e-04, 1.2242e-06, 9.5170e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014509198954328895 tensor([1.8063e-04, 6.9066e-01, 3.0770e-01, 4.5266e-06, 1.4509e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001973006408661604 tensor([7.3172e-01, 1.9730e-03, 5.3611e-07, 2.6590e-01, 4.0433e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002941715531051159 tensor([3.0395e-03, 3.2957e-07, 1.4981e-09, 9.9402e-01, 2.9417e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015135728754103184 tensor([1.4001e-06, 1.5136e-03, 4.5016e-01, 1.6514e-04, 5.4816e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.038368016481399536 tensor([1.9751e-01, 1.3760e-02, 2.2588e-04, 7.5014e-01, 3.8368e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009752619080245495 tensor([8.6877e-01, 1.2098e-01, 2.4720e-05, 9.7526e-03, 4.7235e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02343745343387127 tensor([1.0417e-03, 6.7236e-01, 3.0303e-01, 1.3353e-04, 2.3437e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005475316662341356 tensor([5.4753e-03, 2.0840e-04, 3.5658e-05, 5.9387e-01, 4.0041e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002061527455225587 tensor([9.9294e-01, 4.9983e-03, 2.2522e-08, 2.0615e-03, 2.0637e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0053658816032111645 tensor([7.9342e-06, 5.3659e-03, 6.9477e-01, 5.4898e-04, 2.9931e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0154794966802001 tensor([4.5777e-03, 8.5366e-01, 1.2580e-01, 4.9222e-04, 1.5479e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00017640271107666194 tensor([9.0204e-08, 1.7640e-04, 4.1460e-01, 9.9292e-05, 5.8512e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003550622204784304 tensor([2.0816e-08, 3.0075e-06, 1.2076e-02, 3.5506e-04, 9.8757e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020408520475029945 tensor([0.0012, 0.0179, 0.0400, 0.0204, 0.9206], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006722431862726808 tensor([7.4264e-02, 9.2491e-01, 6.7224e-04, 8.5465e-05, 6.4250e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00542479008436203 tensor([1.1267e-04, 4.3165e-01, 5.6280e-01, 1.0110e-05, 5.4248e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002173306420445442 tensor([9.8754e-01, 2.1733e-03, 2.2607e-08, 1.0284e-02, 6.2345e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00031512771965935826 tensor([1.3792e-01, 1.8160e-05, 5.3518e-09, 8.6175e-01, 3.1513e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0026585098821669817 tensor([3.2584e-06, 2.5422e-05, 2.6585e-03, 8.2812e-03, 9.8903e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010300595313310623 tensor([7.9723e-01, 1.0301e-03, 1.1543e-07, 2.0149e-01, 2.4856e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013440855778753757 tensor([4.1287e-04, 5.5989e-01, 4.2619e-01, 5.8784e-05, 1.3441e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08504618704319 tensor([0.1830, 0.6717, 0.0182, 0.0421, 0.0850], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006593262660317123 tensor([2.1986e-07, 1.5853e-06, 6.5933e-04, 3.0854e-03, 9.9625e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007951954612508416 tensor([2.7291e-07, 7.9520e-04, 6.3380e-01, 4.5842e-05, 3.6536e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17768795788288116 tensor([0.6184, 0.1886, 0.0010, 0.1777, 0.0143], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1299896091222763 tensor([0.5189, 0.2954, 0.0019, 0.1300, 0.0538], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0029369164258241653 tensor([1.9089e-01, 4.6640e-04, 7.5341e-07, 8.0570e-01, 2.9369e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0043040732853114605 tensor([0.0043, 0.0043, 0.0038, 0.2149, 0.7726], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0029420522041618824 tensor([3.1473e-05, 3.3871e-01, 6.5832e-01, 2.1002e-06, 2.9421e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06333696842193604 tensor([1.3020e-03, 5.1628e-01, 4.1872e-01, 3.6613e-04, 6.3337e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014204803854227066 tensor([6.0994e-01, 1.4205e-02, 1.8719e-05, 3.6787e-01, 7.9616e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030642546713352203 tensor([0.0124, 0.0306, 0.0308, 0.2102, 0.7160], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0017062198603525758 tensor([1.7062e-03, 5.0199e-05, 2.2563e-05, 7.8071e-01, 2.1751e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08318277448415756 tensor([2.2656e-01, 1.2496e-02, 1.5746e-04, 6.7760e-01, 8.3183e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08822191506624222 tensor([0.0402, 0.4389, 0.0882, 0.0456, 0.3871], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004790071689058095 tensor([4.7901e-04, 9.5571e-01, 4.3597e-02, 1.8900e-06, 2.0934e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007146478164941072 tensor([2.2078e-04, 7.9920e-01, 1.9987e-01, 3.4827e-06, 7.1465e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05521533265709877 tensor([5.5215e-02, 7.0164e-03, 4.9685e-04, 6.1410e-01, 3.2317e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00028698734240606427 tensor([2.8699e-04, 2.0000e-05, 4.9630e-05, 4.4408e-01, 5.5556e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1426541954278946 tensor([4.6332e-04, 1.4265e-01, 6.2301e-01, 1.2532e-03, 2.3262e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0035043826792389154 tensor([7.0582e-01, 2.9047e-01, 6.5149e-05, 3.5044e-03, 1.4237e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027035267557948828 tensor([6.3833e-01, 2.7035e-03, 1.3525e-06, 3.5740e-01, 1.5658e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.67015183858166e-07 tensor([4.6702e-07, 4.4088e-09, 3.8288e-07, 3.7283e-01, 6.2716e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038762167096138 tensor([5.0683e-05, 3.8762e-02, 7.2257e-01, 3.5859e-04, 2.3826e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008311084820888937 tensor([6.6219e-04, 8.8603e-01, 1.1247e-01, 9.3727e-06, 8.3111e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005360500421375036 tensor([2.2719e-04, 5.1948e-01, 4.7491e-01, 1.8588e-05, 5.3605e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003251431044191122 tensor([1.7593e-01, 5.7407e-04, 1.7386e-06, 8.2024e-01, 3.2514e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.09934000344947e-06 tensor([1.6426e-06, 5.8076e-08, 4.0993e-06, 3.1632e-01, 6.8368e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.608495146385394e-05 tensor([8.5631e-11, 4.6085e-05, 9.9464e-01, 2.2991e-08, 5.3111e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02091887593269348 tensor([4.7483e-05, 1.7545e-01, 8.0356e-01, 1.4993e-05, 2.0919e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002288525691255927 tensor([6.4164e-01, 2.2885e-03, 1.0393e-06, 3.5516e-01, 9.1389e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008788909763097763 tensor([8.0659e-07, 8.7889e-03, 9.7064e-01, 4.1746e-06, 2.0570e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014038824010640383 tensor([3.2850e-01, 4.9921e-04, 3.4478e-07, 6.6960e-01, 1.4039e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000586913141887635 tensor([2.4567e-05, 2.6867e-05, 5.8691e-04, 4.7419e-02, 9.5194e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014080020599067211 tensor([1.4080e-02, 9.6392e-01, 1.9090e-02, 2.5195e-04, 2.6607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16441293060779572 tensor([0.0085, 0.6267, 0.1644, 0.0040, 0.1964], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0033019918482750654 tensor([7.0982e-08, 4.9436e-03, 9.9175e-01, 2.3868e-07, 3.3020e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.705019167275168e-05 tensor([3.7825e-06, 3.7917e-07, 1.7050e-05, 1.4996e-01, 8.5002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008646762929856777 tensor([5.8056e-05, 8.6468e-03, 1.8336e-01, 2.3284e-03, 8.0561e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13863304257392883 tensor([0.1098, 0.1386, 0.0130, 0.3223, 0.4163], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02913948893547058 tensor([8.3527e-01, 1.3467e-01, 7.3885e-05, 2.9139e-02, 8.4712e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.1451585869035625e-07 tensor([7.7315e-01, 3.1452e-07, 4.8615e-14, 2.2685e-01, 3.0637e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.6503669030498713e-05 tensor([2.2734e-01, 3.3822e-06, 1.1290e-10, 7.7264e-01, 1.6504e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003359999042004347 tensor([5.5897e-07, 3.3600e-03, 9.3062e-01, 1.4118e-05, 6.6000e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010503784753382206 tensor([6.1934e-07, 1.3428e-02, 9.7607e-01, 1.4976e-06, 1.0504e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006092987488955259 tensor([9.8627e-01, 6.0930e-03, 1.2245e-07, 7.6233e-03, 1.2306e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0022553266026079655 tensor([6.3049e-02, 9.3404e-01, 2.2553e-03, 3.1945e-04, 3.3634e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.187520840379875e-05 tensor([7.0809e-01, 2.1875e-05, 2.0013e-10, 2.9188e-01, 1.2572e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008710941183380783 tensor([8.7109e-04, 9.8396e-05, 1.1774e-04, 4.6493e-01, 5.3398e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09900366514921188 tensor([0.3040, 0.0990, 0.0024, 0.4977, 0.0969], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005167481489479542 tensor([2.8727e-04, 5.7947e-01, 4.1506e-01, 1.8679e-05, 5.1675e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004176087211817503 tensor([1.0996e-02, 1.3128e-05, 1.5914e-07, 9.8481e-01, 4.1761e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01042880117893219 tensor([1.0429e-02, 5.3147e-05, 1.8083e-06, 9.4786e-01, 4.1660e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04033789411187172 tensor([4.2630e-01, 2.0710e-02, 1.0369e-04, 5.1254e-01, 4.0338e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13108329474925995 tensor([0.5301, 0.2821, 0.0020, 0.1311, 0.0547], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004853340215049684 tensor([1.5656e-09, 8.0019e-08, 1.3980e-03, 4.8533e-04, 9.9812e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0024939829017966986 tensor([9.9050e-01, 2.4940e-03, 2.4269e-08, 7.0005e-03, 2.4143e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15808913111686707 tensor([0.4559, 0.1581, 0.0016, 0.2860, 0.0984], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004431585315614939 tensor([0.3297, 0.6631, 0.0011, 0.0044, 0.0016], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003588598337955773 tensor([8.8896e-01, 1.1067e-01, 1.1861e-06, 3.5886e-04, 4.1529e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00023892363242339343 tensor([2.7864e-07, 1.9025e-04, 2.0470e-01, 2.3892e-04, 7.9487e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00017927552107721567 tensor([9.8563e-01, 1.7928e-04, 3.6642e-10, 1.4192e-02, 7.9286e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.5883090681163594e-05 tensor([5.5883e-05, 3.3471e-07, 6.8367e-07, 7.2436e-01, 2.7558e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015747837023809552 tensor([2.9228e-08, 1.5748e-03, 9.8490e-01, 4.4008e-07, 1.3520e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02217697724699974 tensor([6.7133e-01, 3.0393e-01, 3.5425e-04, 2.2177e-02, 2.2129e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024223968386650085 tensor([1.2619e-03, 6.4546e-01, 3.2882e-01, 2.3032e-04, 2.4224e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015189772238954902 tensor([7.2423e-06, 1.5190e-03, 1.8763e-01, 1.4967e-03, 8.0935e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001342429080978036 tensor([1.3424e-03, 1.6013e-05, 5.7345e-06, 8.1944e-01, 1.7919e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007817902602255344 tensor([8.8994e-07, 7.8179e-03, 9.5891e-01, 5.9960e-06, 3.3261e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011337186442688107 tensor([1.7704e-04, 7.0929e-01, 2.8940e-01, 3.5931e-06, 1.1337e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0706746056675911 tensor([0.0275, 0.4121, 0.0707, 0.0248, 0.4649], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003685346571728587 tensor([9.4880e-06, 1.2212e-01, 8.7418e-01, 2.3056e-06, 3.6853e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010231899796053767 tensor([1.0232e-04, 8.0977e-07, 1.8555e-06, 8.5082e-01, 1.4907e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00031263328855857253 tensor([4.8618e-07, 3.1263e-04, 2.2494e-01, 2.4094e-04, 7.7451e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0284986924380064 tensor([9.2497e-01, 4.6006e-02, 1.1127e-05, 2.8499e-02, 5.1512e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004647785099223256 tensor([6.7850e-01, 3.2101e-01, 1.1351e-05, 4.6478e-04, 1.8434e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06659837812185287 tensor([4.4636e-04, 6.6598e-02, 3.9945e-01, 3.7945e-03, 5.2971e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.1951262169750407e-05 tensor([6.0509e-10, 2.1951e-05, 7.9354e-01, 1.8180e-06, 2.0643e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019102427177131176 tensor([2.0927e-07, 1.4691e-04, 1.5379e-01, 1.9102e-04, 8.4587e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010096773505210876 tensor([8.8634e-01, 1.0097e-02, 3.2137e-06, 1.0293e-01, 6.2376e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000484767893794924 tensor([4.8477e-04, 9.1665e-01, 8.2422e-02, 4.0477e-06, 4.3534e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02284436672925949 tensor([4.5256e-03, 8.0862e-01, 1.6348e-01, 5.3162e-04, 2.2844e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.456085323705338e-05 tensor([3.4561e-05, 7.4763e-09, 2.2146e-09, 9.9337e-01, 6.5998e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.798405305133201e-05 tensor([2.8409e-09, 7.1098e-06, 1.6565e-01, 3.7984e-05, 8.3430e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001582995057106018 tensor([0.2134, 0.7832, 0.0011, 0.0016, 0.0008], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015684968093410134 tensor([2.7495e-01, 7.2231e-01, 4.7950e-04, 1.5685e-03, 6.9304e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009136282838881016 tensor([7.1658e-01, 9.1363e-03, 7.1782e-06, 2.6950e-01, 4.7773e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.029008759185671806 tensor([2.8798e-01, 1.2977e-02, 1.0099e-04, 6.6994e-01, 2.9009e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05757993087172508 tensor([5.7580e-02, 2.3259e-03, 6.4417e-05, 6.7232e-01, 2.6771e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00023775834415573627 tensor([5.9063e-02, 9.4065e-01, 2.3776e-04, 2.4689e-05, 2.5356e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016087564872577786 tensor([9.3032e-06, 1.6088e-03, 1.0686e-01, 1.2941e-03, 8.9023e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007482650689780712 tensor([9.7780e-01, 1.4686e-02, 5.1759e-07, 7.4827e-03, 2.7242e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00022630702005699277 tensor([3.5403e-09, 3.7588e-08, 2.2631e-04, 8.3396e-04, 9.9894e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15478010475635529 tensor([0.1548, 0.1500, 0.0065, 0.2244, 0.4643], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10177899897098541 tensor([7.6360e-01, 1.1705e-01, 2.0771e-04, 1.0178e-01, 1.7369e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04440541937947273 tensor([0.6154, 0.3285, 0.0006, 0.0444, 0.0111], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015467282384634018 tensor([2.0516e-04, 7.1324e-03, 9.4413e-02, 1.5467e-02, 8.8278e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1266784816980362 tensor([0.0227, 0.3789, 0.1267, 0.0357, 0.4361], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08258995413780212 tensor([5.8612e-05, 8.2590e-02, 8.1749e-01, 1.1608e-04, 9.9745e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008384675718843937 tensor([2.0278e-03, 8.1026e-01, 1.7916e-01, 1.6866e-04, 8.3847e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008883782429620624 tensor([6.0587e-04, 8.5169e-01, 1.4681e-01, 1.0811e-05, 8.8838e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008757062256336212 tensor([4.6381e-06, 4.1211e-02, 9.5002e-01, 5.4931e-06, 8.7571e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005336381145752966 tensor([5.3364e-04, 1.6760e-06, 4.1120e-07, 9.5497e-01, 4.4495e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.28630979108857e-05 tensor([2.5227e-09, 1.2546e-05, 3.6021e-01, 2.2863e-05, 6.3976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03902241215109825 tensor([0.5402, 0.4087, 0.0017, 0.0390, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009855594485998154 tensor([3.1186e-06, 9.8556e-03, 9.1708e-01, 3.7888e-05, 7.3022e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20443592965602875 tensor([0.0038, 0.4924, 0.2963, 0.0030, 0.2044], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00016645393043290824 tensor([1.6645e-04, 3.6757e-07, 1.8835e-07, 9.3069e-01, 6.9139e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016951266676187515 tensor([1.6951e-02, 1.0179e-03, 1.0293e-04, 7.6280e-01, 2.1913e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004278573207557201 tensor([2.3582e-06, 4.2786e-03, 7.7070e-01, 1.1325e-04, 2.2490e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015817886451259255 tensor([6.1453e-04, 8.3318e-01, 1.6461e-01, 1.5871e-05, 1.5818e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004563390742987394 tensor([2.8128e-06, 3.2852e-05, 4.5634e-03, 7.7626e-03, 9.8764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11589247733354568 tensor([1.1953e-01, 7.4717e-03, 1.6640e-04, 7.5694e-01, 1.1589e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002133609086740762 tensor([2.7128e-07, 7.0528e-07, 2.1336e-04, 6.7245e-03, 9.9306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00032710537197999656 tensor([6.5985e-02, 9.3358e-01, 3.2711e-04, 4.4474e-05, 6.3940e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014708898961544037 tensor([9.7777e-07, 3.7597e-02, 9.6093e-01, 4.1873e-07, 1.4709e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.391797695760033e-06 tensor([2.9144e-02, 2.4791e-08, 4.6773e-13, 9.7085e-01, 1.3918e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11002568900585175 tensor([1.6900e-01, 1.6922e-02, 5.0129e-04, 7.0355e-01, 1.1003e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.054016244481318e-05 tensor([1.2831e-09, 1.3661e-06, 3.8168e-02, 4.0540e-05, 9.6179e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015481969341635704 tensor([7.8923e-01, 1.9431e-01, 1.0308e-04, 1.5482e-02, 8.8023e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006370385177433491 tensor([4.5566e-02, 9.4511e-01, 6.3704e-03, 6.4852e-04, 2.3036e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003336729947477579 tensor([8.9187e-06, 3.3367e-03, 3.3519e-01, 7.5008e-04, 6.6072e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005102355033159256 tensor([6.2242e-01, 4.7002e-03, 2.7622e-06, 3.6778e-01, 5.1024e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008798118215054274 tensor([4.0334e-07, 3.9904e-05, 2.4328e-02, 8.7981e-04, 9.7475e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011940306285396218 tensor([3.1407e-05, 3.5148e-01, 6.4729e-01, 1.7132e-06, 1.1940e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.017158806324005127 tensor([9.3631e-01, 1.7159e-02, 3.5718e-06, 4.6226e-02, 3.0535e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08220434933900833 tensor([1.6496e-01, 1.8311e-02, 4.8429e-04, 7.3404e-01, 8.2204e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010134015465155244 tensor([1.0134e-03, 2.8474e-05, 2.2175e-05, 7.7046e-01, 2.2848e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002518435474485159 tensor([2.3682e-07, 1.3461e-02, 9.8402e-01, 2.5399e-07, 2.5184e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016090642660856247 tensor([8.3938e-01, 1.4384e-01, 5.9825e-05, 1.6091e-02, 6.2473e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.024000506848096848 tensor([2.4001e-02, 1.5711e-03, 1.4518e-04, 7.4387e-01, 2.3041e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00374408345669508 tensor([3.7441e-03, 1.7212e-05, 1.3184e-06, 9.6998e-01, 2.6259e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17849735915660858 tensor([0.1179, 0.1785, 0.0138, 0.2271, 0.4627], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005299047101289034 tensor([8.8865e-07, 5.2990e-03, 9.2418e-01, 1.2567e-05, 7.0505e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11925981938838959 tensor([0.0017, 0.1193, 0.3937, 0.0105, 0.4749], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004303961526602507 tensor([4.3040e-03, 9.6247e-01, 3.1834e-02, 5.2858e-05, 1.3387e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.811764903133735e-05 tensor([9.8935e-01, 9.8118e-05, 8.8692e-11, 1.0550e-02, 1.7335e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.233422027435154e-05 tensor([6.2334e-05, 2.3909e-07, 3.8818e-07, 8.8206e-01, 1.1788e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.456616731360555e-05 tensor([2.6099e-09, 7.4566e-05, 8.8529e-01, 2.2825e-06, 1.1464e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00522591033950448 tensor([5.5757e-02, 9.3740e-01, 5.2259e-03, 5.4184e-04, 1.0762e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0036194620188325644 tensor([2.5346e-06, 3.6195e-03, 7.6377e-01, 1.6564e-04, 2.3245e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0061159804463386536 tensor([2.5538e-06, 6.1160e-03, 8.8630e-01, 5.5069e-05, 1.0753e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06122643128037453 tensor([2.4498e-01, 8.2456e-03, 6.0112e-05, 6.8549e-01, 6.1226e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009039320982992649 tensor([1.2888e-04, 9.0393e-03, 6.4162e-02, 2.2498e-03, 9.2442e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02963707223534584 tensor([1.6999e-04, 2.9637e-02, 3.5328e-01, 2.6064e-03, 6.1430e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22625219821929932 tensor([0.0010, 0.2430, 0.5279, 0.0018, 0.2263], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000606259040068835 tensor([6.0626e-04, 1.4566e-06, 2.8191e-07, 9.7831e-01, 2.1081e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00018778110097628087 tensor([9.4879e-07, 1.4826e-06, 1.8778e-04, 1.0272e-02, 9.8954e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07497069239616394 tensor([0.0006, 0.0750, 0.3916, 0.0035, 0.5293], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006113414652645588 tensor([6.1134e-03, 9.5856e-01, 3.2268e-02, 1.1399e-04, 2.9419e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013375096023082733 tensor([1.3375e-03, 9.6648e-01, 3.2006e-02, 6.1463e-06, 1.6708e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05014573782682419 tensor([8.2897e-01, 5.0146e-02, 4.6533e-05, 1.1407e-01, 6.7620e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00017148493498098105 tensor([3.7492e-05, 1.1153e-05, 1.7148e-04, 1.5516e-01, 8.4462e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007845710497349501 tensor([2.6097e-08, 7.8457e-04, 9.6817e-01, 1.3062e-06, 3.1047e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006780714029446244 tensor([2.9789e-06, 1.2921e-01, 8.7011e-01, 2.9370e-07, 6.7807e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010744696483016014 tensor([1.5598e-03, 7.7639e-01, 2.1119e-01, 1.2210e-04, 1.0745e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008012546459212899 tensor([2.3149e-07, 3.7376e-05, 4.8603e-02, 8.0125e-04, 9.5056e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003236140590161085 tensor([3.2361e-03, 8.4963e-06, 5.0696e-07, 9.6628e-01, 3.0473e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00037098414031788707 tensor([9.6871e-01, 3.7098e-04, 2.8308e-09, 3.0916e-02, 5.7107e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.194422167027369e-05 tensor([3.8081e-01, 6.1911e-01, 1.3926e-05, 6.1944e-05, 5.9148e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0043457988649606705 tensor([0.2809, 0.7114, 0.0015, 0.0043, 0.0018], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00015122981858439744 tensor([2.2366e-01, 7.7605e-01, 1.0164e-04, 1.5123e-04, 3.7457e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00121990405023098 tensor([1.2199e-03, 5.8311e-06, 1.1614e-06, 9.2995e-01, 6.8822e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016335733234882355 tensor([8.7590e-01, 1.6336e-02, 5.2383e-06, 1.0698e-01, 7.7008e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 3, 1], [4, 2, 3], [2, 1, 0], [0, 1, 2], [0, 1, 2], [2, 4, 3], [3, 0, 4], [2, 3, 4], [1, 0, 2], [0, 1, 3], [3, 0, 4], [0, 3, 4], [2, 4, 1], [0, 1, 2], [0, 1, 2], [2, 4, 3], [3, 4, 2], [3, 0, 4], [1, 2, 0], [0, 1, 2], [3, 0, 4], [0, 1, 3], [2, 4, 1], [1, 0, 2], [2, 1, 4], [3, 4, 0], [2, 4, 3], [0, 1, 2], [2, 1], [2, 3, 4], [3, 0, 4], [3, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3], [2, 4, 1], [1, 0, 2], [1, 0, 2], [3, 0, 4], [3, 4, 0], [2, 4, 1], [1, 2, 0], [1, 2, 0], [3, 0, 4], [3, 4, 2], [0, 3, 4], [0, 1, 3], [3, 0, 4], [0, 3, 4], [3, 4, 0], [2, 1, 4], [2, 1, 4], [2, 4, 1], [3, 0, 4], [3, 4, 2], [2, 3, 0], [2, 1, 4], [2, 1, 4], [3, 4, 2], [3, 4, 0], [2, 4, 1], [2, 1, 4], [0, 3, 1], [3, 4, 2], [3, 0, 4], [2, 4, 1], [2, 1, 4], [0, 3, 1], [2, 1, 4], [2, 4, 3], [3, 0, 4], [2, 1, 0], [2, 4, 3], [0, 3, 1], [3, 0, 4], [0, 3, 1], [0, 1, 3], [0, 1, 3], [4, 3, 2], [3, 0, 4], [2, 4, 1], [2, 1, 4], [0, 1, 2], [2, 4, 1], [3, 0, 4], [2, 3, 4], [0, 1, 2], [0, 3, 1], [2, 4, 3], [2, 4, 3], [2, 1, 4], [2, 1, 0], [3, 0, 4], [3, 0, 4], [2, 4, 1], [0, 2, 1], [2, 1, 0], [2, 1, 4], [0, 3, 2], [3, 4, 0], [3, 0, 4], [2, 1, 0], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 4, 1], [1, 2, 0], [0, 3, 1], [3, 0, 4], [3, 0, 4], [2, 1, 4], [1, 0, 2], [0, 3, 1], [3, 0, 4], [2, 4, 1], [0, 3, 1], [2, 1, 4], [1, 0, 2], [3, 4, 0], [3, 0, 2], [0, 3, 4], [1, 0, 2], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 4, 1], [2, 1, 4], [0, 3, 1], [0, 3, 4], [2, 4, 1], [4, 3, 2], [2, 4, 1], [1, 2, 0], [2, 4, 1], [3, 0, 4], [2, 1, 4], [2, 1, 0], [2, 1, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [1, 2, 0], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [3, 0, 4], [3, 0, 2], [3, 0, 4], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 4, 3], [0, 3, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [3, 4, 0], [3, 0, 4], [2, 1, 0], [0, 1, 3], [4, 2, 3], [2, 4, 3], [2, 4, 1], [2, 1, 4], [2, 1, 0], [4, 3, 2], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 1, 0], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 3, 2], [0, 3, 1], [3, 0, 4], [3, 0, 4], [0, 3, 4], [2, 1, 0], [0, 1, 3], [2, 4, 3], [0, 3, 1], [3, 0], [2, 1, 0], [2, 1, 0], [0, 3, 1], [3, 0, 4], [0, 1, 2], [2, 1, 4], [0, 1, 2], [3, 4, 2], [3, 0, 4], [2, 1, 4], [2, 1, 4], [0, 1, 3], [2, 4, 3], [3, 4, 2], [0, 3, 1], [2, 1, 4], [0, 1, 3], [3, 0, 4], [2, 4, 1], [2, 1, 4], [2, 1, 0], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 0], [2, 0, 1], [0, 3, 1], [0, 3, 1], [3, 4, 0], [2, 4, 1], [1, 2, 0], [0, 3, 1], [3, 4, 2], [0, 3, 1], [0, 3, 1], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 3], [2, 1, 0], [0, 1, 2], [0, 3, 1], [3, 4, 0], [3, 4, 0], [2, 4, 1], [1, 0, 2], [0, 3, 1], [3, 0, 4], [3, 0, 4], [0, 1, 3], [2, 1, 0], [2, 4, 1], [4, 2, 3], [2, 4, 3], [4, 2, 3], [2, 1, 0], [2, 4, 1]]\n",
      "[[2, 4], [0, 1], [3, 4], [3, 4], [3, 4], [0, 1], [1, 2], [0, 1], [3, 4], [2, 4], [1, 2], [1, 2], [0, 3], [3, 4], [3, 4], [0, 1], [0, 1], [1, 2], [3, 4], [3, 4], [1, 2], [2, 4], [0, 3], [3, 4], [0, 3], [1, 2], [0, 1], [3, 4], [0, 3, 4], [0, 1], [1, 2], [1, 2], [2, 4], [0, 3], [2, 4], [0, 1], [0, 1], [0, 3], [3, 4], [3, 4], [1, 2], [1, 2], [0, 3], [3, 4], [3, 4], [1, 2], [0, 1], [1, 2], [2, 4], [1, 2], [1, 2], [1, 2], [0, 3], [0, 3], [0, 3], [1, 2], [0, 1], [1, 4], [0, 3], [0, 3], [0, 1], [1, 2], [0, 3], [0, 3], [2, 4], [0, 1], [1, 2], [0, 3], [0, 3], [2, 4], [0, 3], [0, 1], [1, 2], [3, 4], [0, 1], [2, 4], [1, 2], [2, 4], [2, 4], [2, 4], [0, 1], [1, 2], [0, 3], [0, 3], [3, 4], [0, 3], [1, 2], [0, 1], [3, 4], [2, 4], [0, 1], [0, 1], [0, 3], [3, 4], [1, 2], [1, 2], [0, 3], [3, 4], [3, 4], [0, 3], [1, 4], [1, 2], [1, 2], [3, 4], [3, 4], [2, 4], [0, 1], [0, 3], [3, 4], [2, 4], [1, 2], [1, 2], [0, 3], [3, 4], [2, 4], [1, 2], [0, 3], [2, 4], [0, 3], [3, 4], [1, 2], [1, 4], [1, 2], [3, 4], [2, 4], [3, 4], [0, 1], [0, 3], [0, 3], [2, 4], [1, 2], [0, 3], [0, 1], [0, 3], [3, 4], [0, 3], [1, 2], [0, 3], [3, 4], [0, 3], [0, 1], [2, 4], [0, 3], [0, 3], [0, 1], [0, 1], [2, 4], [0, 3], [3, 4], [2, 4], [0, 1], [1, 2], [2, 4], [3, 4], [2, 4], [1, 2], [1, 4], [1, 2], [3, 4], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [2, 4], [0, 3], [1, 2], [1, 2], [3, 4], [2, 4], [0, 1], [0, 1], [0, 3], [0, 3], [3, 4], [0, 1], [2, 4], [0, 1], [3, 4], [3, 4], [0, 1], [0, 1], [2, 4], [1, 4], [2, 4], [1, 2], [1, 2], [1, 2], [3, 4], [2, 4], [0, 1], [2, 4], [1, 2, 4], [3, 4], [3, 4], [2, 4], [1, 2], [3, 4], [0, 3], [3, 4], [0, 1], [1, 2], [0, 3], [0, 3], [2, 4], [0, 1], [0, 1], [2, 4], [0, 3], [2, 4], [1, 2], [0, 3], [0, 3], [3, 4], [1, 2], [0, 1], [3, 4], [3, 4], [3, 4], [2, 4], [2, 4], [1, 2], [0, 3], [3, 4], [2, 4], [0, 1], [2, 4], [2, 4], [0, 3], [2, 4], [2, 4], [1, 2, 4], [3, 4], [3, 4], [2, 4], [1, 2], [1, 2], [0, 3], [3, 4], [2, 4], [1, 2], [1, 2], [2, 4], [3, 4], [0, 3], [0, 1], [0, 1], [0, 1], [3, 4], [0, 3]]\n",
      "NL_pred of 2th iteration [[0, 3, 1], [4, 2, 3], [2, 1, 0], [0, 1, 2], [0, 1, 2], [2, 4, 3], [3, 0, 4], [2, 3, 4], [1, 0, 2], [0, 1, 3], [3, 0, 4], [0, 3, 4], [2, 4, 1], [0, 1, 2], [0, 1, 2], [2, 4, 3], [3, 4, 2], [3, 0, 4], [1, 2, 0], [0, 1, 2], [3, 0, 4], [0, 1, 3], [2, 4, 1], [1, 0, 2], [2, 1, 4], [3, 4, 0], [2, 4, 3], [0, 1, 2], [2, 3, 4], [3, 0, 4], [3, 0, 4], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3], [2, 4, 1], [1, 0, 2], [1, 0, 2], [3, 0, 4], [3, 4, 0], [2, 4, 1], [1, 2, 0], [1, 2, 0], [3, 0, 4], [3, 4, 2], [0, 3, 4], [0, 1, 3], [3, 0, 4], [0, 3, 4], [3, 4, 0], [2, 1, 4], [2, 1, 4], [2, 4, 1], [3, 0, 4], [3, 4, 2], [2, 3, 0], [2, 1, 4], [2, 1, 4], [3, 4, 2], [3, 4, 0], [2, 4, 1], [2, 1, 4], [0, 3, 1], [3, 4, 2], [3, 0, 4], [2, 4, 1], [2, 1, 4], [0, 3, 1], [2, 1, 4], [2, 4, 3], [3, 0, 4], [2, 1, 0], [2, 4, 3], [0, 3, 1], [3, 0, 4], [0, 3, 1], [0, 1, 3], [0, 1, 3], [4, 3, 2], [3, 0, 4], [2, 4, 1], [2, 1, 4], [0, 1, 2], [2, 4, 1], [3, 0, 4], [2, 3, 4], [0, 1, 2], [0, 3, 1], [2, 4, 3], [2, 4, 3], [2, 1, 4], [2, 1, 0], [3, 0, 4], [3, 0, 4], [2, 4, 1], [0, 2, 1], [2, 1, 0], [2, 1, 4], [0, 3, 2], [3, 4, 0], [3, 0, 4], [2, 1, 0], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 4, 1], [1, 2, 0], [0, 3, 1], [3, 0, 4], [3, 0, 4], [2, 1, 4], [1, 0, 2], [0, 3, 1], [3, 0, 4], [2, 4, 1], [0, 3, 1], [2, 1, 4], [1, 0, 2], [3, 4, 0], [3, 0, 2], [0, 3, 4], [1, 0, 2], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 4, 1], [2, 1, 4], [0, 3, 1], [0, 3, 4], [2, 4, 1], [4, 3, 2], [2, 4, 1], [1, 2, 0], [2, 4, 1], [3, 0, 4], [2, 1, 4], [2, 1, 0], [2, 1, 4], [2, 4, 3], [0, 1, 3], [2, 4, 1], [2, 4, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 4, 1], [1, 2, 0], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [3, 0, 4], [3, 0, 2], [3, 0, 4], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 4, 3], [0, 3, 1], [0, 3, 1], [0, 1, 3], [2, 4, 1], [3, 4, 0], [3, 0, 4], [2, 1, 0], [0, 1, 3], [4, 2, 3], [2, 4, 3], [2, 4, 1], [2, 1, 4], [2, 1, 0], [4, 3, 2], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 1, 0], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 3, 2], [0, 3, 1], [3, 0, 4], [3, 0, 4], [0, 3, 4], [2, 1, 0], [0, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0], [2, 1, 0], [0, 3, 1], [3, 0, 4], [0, 1, 2], [2, 1, 4], [0, 1, 2], [3, 4, 2], [3, 0, 4], [2, 1, 4], [2, 1, 4], [0, 1, 3], [2, 4, 3], [3, 4, 2], [0, 3, 1], [2, 1, 4], [0, 1, 3], [3, 0, 4], [2, 4, 1], [2, 1, 4], [2, 1, 0], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 0], [2, 0, 1], [0, 3, 1], [0, 3, 1], [3, 4, 0], [2, 4, 1], [1, 2, 0], [0, 3, 1], [3, 4, 2], [0, 3, 1], [0, 3, 1], [2, 1, 4], [0, 3, 1], [0, 3, 1], [2, 1, 0], [0, 1, 2], [0, 3, 1], [3, 4, 0], [3, 4, 0], [2, 4, 1], [1, 0, 2], [0, 3, 1], [3, 0, 4], [3, 0, 4], [0, 1, 3], [2, 1, 0], [2, 4, 1], [4, 2, 3], [2, 4, 3], [4, 2, 3], [2, 1, 0], [2, 4, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004606272527563427  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004606257083444943  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004606227643094082  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.00460618517176825  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004606131117353555  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004606068375622213  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004605996946574223  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004605918278095693  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004605835748587543  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004605746462277556  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004605653797566649  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00460555871971223  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004605461228714298  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004605359876686745  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0046052594899165964  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004605158137889044  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004605056303232788  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004604953985947829  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0046048521512915734  Accuracy on Support set:0.0\n",
      "torch.Size([247, 2048]) torch.Size([247])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0046047512818927225  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.36663809418678284 tensor([1.6317e-05, 5.4225e-03, 3.6664e-01, 1.2397e-03, 6.2668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4475630819797516 tensor([4.4756e-01, 5.5225e-01, 2.0858e-05, 1.5106e-04, 1.4352e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2565656304359436 tensor([0.0816, 0.0256, 0.0018, 0.6343, 0.2566], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06046033650636673 tensor([4.7995e-05, 6.9538e-05, 1.0432e-03, 6.0460e-02, 9.3838e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.046168964356184006 tensor([1.6936e-05, 1.9698e-05, 5.5951e-04, 4.6169e-02, 9.5323e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.295816570520401 tensor([2.9582e-01, 7.0207e-01, 4.8897e-04, 1.1455e-03, 4.7702e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05475477874279022 tensor([0.0220, 0.8841, 0.0548, 0.0022, 0.0370], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3012830913066864 tensor([0.3013, 0.4882, 0.0088, 0.0949, 0.1067], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.048412349075078964 tensor([2.5171e-06, 8.9336e-07, 5.2871e-05, 4.8412e-02, 9.5153e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09227953106164932 tensor([8.7991e-07, 2.2125e-04, 9.2280e-02, 5.6782e-04, 9.0693e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17090173065662384 tensor([3.8533e-05, 1.7090e-01, 8.1102e-01, 1.3042e-05, 1.8031e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005628305021673441 tensor([9.0726e-08, 5.6283e-03, 9.9156e-01, 2.5888e-07, 2.8148e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04973448067903519 tensor([9.3046e-01, 1.9326e-02, 3.8760e-06, 4.9734e-02, 4.7928e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01664120890200138 tensor([3.9595e-06, 9.4200e-06, 7.3645e-04, 1.6641e-02, 9.8261e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021012941375374794 tensor([4.4221e-06, 5.1646e-06, 2.2692e-04, 2.1013e-02, 9.7875e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03906110301613808 tensor([9.6025e-01, 3.9061e-02, 3.0775e-07, 6.8487e-04, 2.4474e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017017394304275513 tensor([1.7017e-02, 9.8061e-01, 2.1132e-03, 3.4078e-05, 2.2060e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15971334278583527 tensor([0.0121, 0.6820, 0.1597, 0.0051, 0.1411], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4368975758552551 tensor([2.3296e-05, 8.5703e-07, 1.1251e-05, 4.3690e-01, 5.6307e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10696668922901154 tensor([0.0020, 0.0073, 0.0196, 0.1070, 0.8641], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18978393077850342 tensor([0.0061, 0.7552, 0.1898, 0.0012, 0.0477], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.058264221996068954 tensor([2.5547e-05, 1.2346e-03, 5.8264e-02, 7.9933e-03, 9.3248e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0655970349907875 tensor([9.3408e-01, 3.1257e-04, 4.4768e-09, 6.5597e-02, 1.0829e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010460413061082363 tensor([4.7344e-08, 2.9381e-08, 2.2713e-05, 1.0460e-02, 9.8952e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14555901288986206 tensor([1.4556e-01, 9.1138e-05, 7.2457e-08, 8.5274e-01, 1.6129e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011502648703753948 tensor([9.7907e-03, 9.7784e-01, 1.1503e-02, 5.9129e-05, 8.1146e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10581700503826141 tensor([8.9280e-01, 1.0582e-01, 3.5489e-06, 1.3601e-03, 2.1224e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07073844224214554 tensor([1.5593e-04, 6.4959e-04, 9.0706e-03, 7.0738e-02, 9.1939e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21297918260097504 tensor([0.3306, 0.1465, 0.0020, 0.3080, 0.2130], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15556485950946808 tensor([0.1556, 0.7031, 0.0184, 0.0426, 0.0803], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14412392675876617 tensor([1.6549e-03, 8.5017e-01, 1.4412e-01, 7.3260e-05, 3.9730e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0551128126680851 tensor([1.8207e-06, 5.5113e-02, 9.4267e-01, 6.6355e-07, 2.2128e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005320321768522263 tensor([2.8095e-08, 1.4888e-03, 9.9319e-01, 3.9502e-07, 5.3203e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03923780471086502 tensor([9.4858e-01, 1.1688e-02, 1.4361e-06, 3.9238e-02, 4.8811e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02383475936949253 tensor([8.7854e-09, 3.6775e-04, 9.7580e-01, 8.1674e-07, 2.3835e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4739605486392975 tensor([0.4740, 0.4955, 0.0012, 0.0205, 0.0087], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1101173534989357 tensor([8.8848e-01, 1.1012e-01, 3.9734e-06, 1.3767e-03, 2.2417e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2655976712703705 tensor([7.3091e-01, 2.7998e-03, 1.1180e-06, 2.6560e-01, 6.9585e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05295652151107788 tensor([4.3851e-07, 8.5727e-08, 1.8707e-05, 5.2957e-02, 9.4702e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08011285960674286 tensor([8.4107e-05, 5.4752e-05, 3.4790e-04, 8.0113e-02, 9.1940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10583514720201492 tensor([2.2726e-03, 8.8815e-01, 1.0584e-01, 6.0274e-05, 3.6806e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017639001831412315 tensor([7.5832e-04, 9.8152e-01, 1.7639e-02, 1.3336e-06, 7.9412e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2781680226325989 tensor([7.2180e-01, 2.2968e-05, 2.2410e-10, 2.7817e-01, 5.6721e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30091238021850586 tensor([1.0601e-05, 6.5185e-08, 5.4661e-07, 6.9908e-01, 3.0091e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4578615128993988 tensor([9.1645e-04, 7.7890e-05, 9.0790e-05, 4.5786e-01, 5.4105e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29948779940605164 tensor([0.0064, 0.5856, 0.2995, 0.0036, 0.1049], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.056195229291915894 tensor([0.0562, 0.9065, 0.0202, 0.0038, 0.0133], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14685297012329102 tensor([1.6381e-04, 1.4685e-01, 7.6302e-01, 2.3525e-04, 8.9728e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009392989799380302 tensor([2.5294e-09, 6.4161e-07, 9.3930e-03, 1.0991e-04, 9.9050e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10354717820882797 tensor([3.6606e-05, 1.0355e-01, 8.3817e-01, 3.1894e-05, 5.8213e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.138218492269516 tensor([9.5477e-05, 1.3822e-01, 8.0921e-01, 1.1100e-04, 5.2362e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07313263416290283 tensor([1.1978e-03, 9.2509e-01, 7.3133e-02, 1.6124e-05, 5.6289e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11948885023593903 tensor([1.1949e-01, 2.7263e-03, 3.5490e-05, 8.3692e-01, 4.0827e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.023974914103746414 tensor([2.3975e-02, 2.9851e-06, 2.9726e-09, 9.7516e-01, 8.6100e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2861537039279938 tensor([7.1376e-01, 5.5442e-05, 1.0284e-09, 2.8615e-01, 3.0801e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1083439290523529 tensor([7.3306e-06, 1.0834e-01, 8.8900e-01, 1.7540e-06, 2.6508e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009414508007466793 tensor([9.4145e-03, 9.8293e-01, 7.2849e-03, 3.8464e-05, 3.3267e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15399101376533508 tensor([0.1665, 0.6140, 0.0157, 0.0498, 0.1540], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009660014882683754 tensor([9.6600e-03, 6.7302e-06, 7.8778e-08, 9.8827e-01, 2.0600e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01804891601204872 tensor([1.8049e-02, 2.1892e-06, 3.3590e-09, 9.8161e-01, 3.4355e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.039527639746665955 tensor([3.9528e-02, 9.5830e-01, 1.8953e-03, 1.0549e-04, 1.7491e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.050842318683862686 tensor([1.3279e-02, 9.2571e-01, 5.0842e-02, 7.6305e-04, 9.4046e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08048055320978165 tensor([8.9971e-01, 1.8878e-02, 7.2145e-06, 8.0481e-02, 9.1962e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03438793122768402 tensor([3.4388e-02, 1.1029e-04, 1.2390e-06, 9.5614e-01, 9.3578e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0400647409260273 tensor([5.4822e-07, 3.8812e-03, 9.5604e-01, 1.1317e-05, 4.0065e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006798176094889641 tensor([6.7982e-03, 9.9265e-01, 5.4440e-04, 1.2295e-06, 9.0552e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29044419527053833 tensor([1.9498e-04, 7.0799e-01, 2.9044e-01, 4.5688e-06, 1.3626e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2533864974975586 tensor([7.4435e-01, 1.9081e-03, 4.7857e-07, 2.5339e-01, 3.5507e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0032174012158066034 tensor([3.2174e-03, 3.3182e-07, 1.4026e-09, 9.9407e-01, 2.7097e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.45086023211479187 tensor([1.5910e-06, 1.6420e-03, 4.5086e-01, 1.7558e-04, 5.4732e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21017538011074066 tensor([2.1018e-01, 1.3847e-02, 2.0815e-04, 7.4105e-01, 3.4717e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.114847332239151 tensor([8.7558e-01, 1.1485e-01, 2.1485e-05, 9.1458e-03, 4.0491e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2846330404281616 tensor([1.1239e-03, 6.9255e-01, 2.8463e-01, 1.3260e-04, 2.1558e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37895363569259644 tensor([6.0488e-03, 2.1745e-04, 3.4194e-05, 6.1475e-01, 3.7895e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00484361220151186 tensor([9.9314e-01, 4.8436e-03, 2.0977e-08, 2.0098e-03, 1.9137e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2969592809677124 tensor([9.0621e-06, 5.8724e-03, 6.9658e-01, 5.8363e-04, 2.9696e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11606468260288239 tensor([4.8859e-03, 8.6449e-01, 1.1606e-01, 4.8569e-04, 1.4072e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.41603630781173706 tensor([1.0232e-07, 1.9117e-04, 4.1604e-01, 1.0563e-04, 5.8367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012100255116820335 tensor([2.2982e-08, 3.1774e-06, 1.2100e-02, 3.7781e-04, 9.8752e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.040283795446157455 tensor([0.0014, 0.0196, 0.0403, 0.0222, 0.9165], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07813241332769394 tensor([7.8132e-02, 9.2109e-01, 6.2889e-04, 8.6339e-05, 6.0819e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.45263782143592834 tensor([1.2399e-04, 4.5264e-01, 5.4207e-01, 1.0353e-05, 5.1574e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009905078448355198 tensor([9.8799e-01, 2.1003e-03, 2.0754e-08, 9.9051e-03, 5.6785e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1469343900680542 tensor([1.4693e-01, 1.8460e-05, 5.0000e-09, 8.5276e-01, 2.8758e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008952440693974495 tensor([3.7120e-06, 2.7431e-05, 2.6617e-03, 8.9524e-03, 9.8835e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18990379571914673 tensor([8.0888e-01, 9.9956e-04, 1.0314e-07, 1.8990e-01, 2.1655e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4048328697681427 tensor([4.5042e-04, 5.8211e-01, 4.0483e-01, 5.9204e-05, 1.2547e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19412872195243835 tensor([0.1941, 0.6729, 0.0165, 0.0410, 0.0755], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0033150515519082546 tensor([2.4488e-07, 1.6746e-06, 6.5784e-04, 3.3151e-03, 9.9603e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3634937107563019 tensor([3.0613e-07, 8.5688e-04, 6.3560e-01, 4.8274e-05, 3.6349e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18407610058784485 tensor([0.6352, 0.1841, 0.0008, 0.1674, 0.0124], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2923656404018402 tensor([0.5346, 0.2924, 0.0017, 0.1238, 0.0476], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20267340540885925 tensor([2.0267e-01, 4.7149e-04, 6.9894e-07, 7.9419e-01, 2.6613e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.231271892786026 tensor([0.0049, 0.0046, 0.0037, 0.2313, 0.7554], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3573629558086395 tensor([3.4640e-05, 3.5736e-01, 6.3978e-01, 2.1556e-06, 2.8223e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.39970919489860535 tensor([1.4218e-03, 5.3937e-01, 3.9971e-01, 3.6831e-04, 5.9132e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35127171874046326 tensor([6.2792e-01, 1.3831e-02, 1.6683e-05, 3.5127e-01, 6.9650e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22493548691272736 tensor([0.0140, 0.0325, 0.0301, 0.2249, 0.6984], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20131856203079224 tensor([1.8503e-03, 5.1059e-05, 2.1064e-05, 7.9676e-01, 2.0132e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24076001346111298 tensor([2.4076e-01, 1.2373e-02, 1.4196e-04, 6.7208e-01, 7.4649e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36442863941192627 tensor([0.0448, 0.4595, 0.0840, 0.0472, 0.3644], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04091349616646767 tensor([5.0354e-04, 9.5838e-01, 4.0913e-02, 1.8913e-06, 1.9736e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18699173629283905 tensor([2.3585e-04, 8.1210e-01, 1.8699e-01, 3.4774e-06, 6.6717e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30151280760765076 tensor([6.0627e-02, 7.2345e-03, 4.6840e-04, 6.3016e-01, 3.0151e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4672456979751587 tensor([3.2063e-04, 2.0998e-05, 4.7985e-05, 4.6725e-01, 5.3236e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2275281548500061 tensor([5.1779e-04, 1.5262e-01, 6.1802e-01, 1.3124e-03, 2.2753e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2805783450603485 tensor([7.1586e-01, 2.8058e-01, 5.8900e-05, 3.3790e-03, 1.2855e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3408387303352356 tensor([6.5515e-01, 2.6403e-03, 1.2124e-06, 3.4084e-01, 1.3722e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.390428751707077 tensor([5.0582e-07, 4.5467e-09, 3.7535e-07, 3.9043e-01, 6.0957e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23716481029987335 tensor([5.7890e-05, 4.2158e-02, 7.2024e-01, 3.8314e-04, 2.3716e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10387778282165527 tensor([7.0685e-04, 8.9464e-01, 1.0388e-01, 9.3245e-06, 7.6801e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.45352867245674133 tensor([2.4922e-04, 5.4113e-01, 4.5353e-01, 1.8994e-05, 5.0730e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18613620102405548 tensor([1.8614e-01, 5.7668e-04, 1.6064e-06, 8.1033e-01, 2.9520e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3359850347042084 tensor([1.8213e-06, 6.0811e-08, 4.0176e-06, 3.3599e-01, 6.6401e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005199069157242775 tensor([9.0796e-11, 4.7743e-05, 9.9475e-01, 2.3398e-08, 5.1991e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18895654380321503 tensor([5.3430e-05, 1.8896e-01, 7.9071e-01, 1.5600e-05, 2.0265e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3394012749195099 tensor([6.5758e-01, 2.2205e-03, 9.2480e-07, 3.3940e-01, 7.9955e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020275257527828217 tensor([9.0137e-07, 9.4805e-03, 9.7024e-01, 4.3553e-06, 2.0275e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3441234827041626 tensor([3.4412e-01, 4.9786e-04, 3.1655e-07, 6.5411e-01, 1.2637e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.051925864070653915 tensor([2.8450e-05, 2.9122e-05, 5.8620e-04, 5.1926e-02, 9.4743e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017508886754512787 tensor([1.4847e-02, 9.6500e-01, 1.7509e-02, 2.4691e-04, 2.4009e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18197105824947357 tensor([0.0094, 0.6502, 0.1545, 0.0040, 0.1820], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00525394594296813 tensor([7.7650e-08, 5.2539e-03, 9.9151e-01, 2.4517e-07, 3.2310e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16195456683635712 tensor([4.3058e-06, 4.0665e-07, 1.6951e-05, 1.6195e-01, 8.3802e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18399813771247864 tensor([6.6416e-05, 9.4392e-03, 1.8400e-01, 2.4969e-03, 8.0400e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3330775201320648 tensor([0.1213, 0.1433, 0.0123, 0.3331, 0.3900], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12815752625465393 tensor([8.4392e-01, 1.2816e-01, 6.3824e-05, 2.7139e-02, 7.2076e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22071336209774017 tensor([7.7929e-01, 3.1046e-07, 4.6286e-14, 2.2071e-01, 2.8560e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23790092766284943 tensor([2.3790e-01, 3.4004e-06, 1.0596e-10, 7.6208e-01, 1.5112e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06561218947172165 tensor([6.2605e-07, 3.6164e-03, 9.3076e-01, 1.4869e-05, 6.5612e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01453395001590252 tensor([6.9554e-07, 1.4534e-02, 9.7512e-01, 1.5642e-06, 1.0342e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007337398361414671 tensor([9.8675e-01, 5.8978e-03, 1.1269e-07, 7.3374e-03, 1.1242e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06656838208436966 tensor([6.6568e-02, 9.3069e-01, 2.1033e-03, 3.2267e-04, 3.1623e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2818310558795929 tensor([7.1814e-01, 2.1432e-05, 1.8490e-10, 2.8183e-01, 1.1363e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4890716075897217 tensor([9.7686e-04, 1.0353e-04, 1.1360e-04, 4.8907e-01, 5.0973e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32163700461387634 tensor([0.3216, 0.0983, 0.0022, 0.4910, 0.0869], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3945528566837311 tensor([3.1196e-04, 6.0027e-01, 3.9455e-01, 1.8850e-05, 4.8428e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011840440332889557 tensor([1.1840e-02, 1.3443e-05, 1.4935e-07, 9.8432e-01, 3.8308e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.038173411041498184 tensor([1.1166e-02, 5.3741e-05, 1.6813e-06, 9.5061e-01, 3.8173e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.44509339332580566 tensor([4.4509e-01, 2.0276e-02, 9.2742e-05, 4.9881e-01, 3.5730e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27835813164711 tensor([0.5472, 0.2784, 0.0018, 0.1246, 0.0480], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014034609775990248 tensor([1.7267e-09, 8.4371e-08, 1.4035e-03, 5.1787e-04, 9.9808e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006789798382669687 tensor([9.9079e-01, 2.4213e-03, 2.2594e-08, 6.7898e-03, 2.2417e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2800453305244446 tensor([0.4780, 0.1538, 0.0014, 0.2800, 0.0867], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.342767596244812 tensor([0.3428, 0.6505, 0.0010, 0.0043, 0.0014], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10750728845596313 tensor([8.9214e-01, 1.0751e-01, 1.1068e-06, 3.4985e-04, 3.8752e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20708061754703522 tensor([3.1411e-07, 2.0679e-04, 2.0708e-01, 2.5221e-04, 7.9246e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013751523569226265 tensor([9.8607e-01, 1.7541e-04, 3.4489e-10, 1.3752e-02, 7.3480e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25810444355010986 tensor([6.0447e-05, 3.4241e-07, 6.4834e-07, 7.4183e-01, 2.5810e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01317422091960907 tensor([3.1785e-08, 1.6680e-03, 9.8516e-01, 4.4878e-07, 1.3174e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.293000727891922 tensor([6.8384e-01, 2.9300e-01, 3.1253e-04, 2.0935e-02, 1.9159e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3100684881210327 tensor([1.3584e-03, 6.6606e-01, 3.1007e-01, 2.2809e-04, 2.2285e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18778738379478455 tensor([8.2795e-06, 1.6503e-03, 1.8779e-01, 1.6106e-03, 8.0894e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1660555899143219 tensor([1.4470e-03, 1.6276e-05, 5.3733e-06, 8.3248e-01, 1.6606e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.033025339245796204 tensor([1.0078e-06, 8.4798e-03, 9.5849e-01, 6.3395e-06, 3.3025e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.27311351895332336 tensor([1.9000e-04, 7.2563e-01, 2.7311e-01, 3.6015e-06, 1.0614e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.43471336364746094 tensor([0.0308, 0.4347, 0.0678, 0.0258, 0.4408], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13173730671405792 tensor([1.0654e-05, 1.3174e-01, 8.6465e-01, 2.4059e-06, 3.6041e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13791945576667786 tensor([1.0930e-04, 8.1626e-07, 1.7339e-06, 8.6197e-01, 1.3792e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22659872472286224 tensor([5.4681e-07, 3.3798e-04, 2.2660e-01, 2.5476e-04, 7.7281e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.043583791702985764 tensor([9.2962e-01, 4.3584e-02, 9.5648e-06, 2.6354e-02, 4.3407e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3116631507873535 tensor([6.8785e-01, 3.1166e-01, 1.0484e-05, 4.5602e-04, 1.7084e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40010765194892883 tensor([5.0408e-04, 7.1818e-02, 4.0011e-01, 4.0330e-03, 5.2354e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20269441604614258 tensor([6.3915e-10, 2.2716e-05, 7.9728e-01, 1.8537e-06, 2.0269e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1545160859823227 tensor([2.3652e-07, 1.5891e-04, 1.5452e-01, 2.0298e-04, 8.4512e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09644190222024918 tensor([8.9344e-01, 9.5829e-03, 2.7876e-06, 9.6442e-02, 5.3405e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07643735408782959 tensor([5.1127e-04, 9.2265e-01, 7.6437e-02, 3.9811e-06, 4.0078e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15200550854206085 tensor([4.8402e-03, 8.2162e-01, 1.5201e-01, 5.2783e-04, 2.1002e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006099154241383076 tensor([3.6641e-05, 7.5662e-09, 2.0882e-09, 9.9386e-01, 6.0992e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16834799945354462 tensor([3.0610e-09, 7.4689e-06, 1.6835e-01, 3.9523e-05, 8.3161e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22168074548244476 tensor([2.2168e-01, 7.7500e-01, 1.0010e-03, 1.5592e-03, 7.6019e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28670641779899597 tensor([2.8671e-01, 7.1069e-01, 4.3681e-04, 1.5372e-03, 6.2480e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25480473041534424 tensor([7.3225e-01, 8.8187e-03, 6.3162e-06, 2.5480e-01, 4.1210e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3046759366989136 tensor([3.0468e-01, 1.2868e-02, 9.1014e-05, 6.5655e-01, 2.5811e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25032657384872437 tensor([6.2348e-02, 2.3842e-03, 6.0998e-05, 6.8488e-01, 2.5033e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.062012068927288055 tensor([6.2012e-02, 9.3772e-01, 2.2385e-04, 2.4970e-05, 2.4058e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10741564631462097 tensor([1.0590e-05, 1.7545e-03, 1.0742e-01, 1.3766e-03, 8.8944e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01403805147856474 tensor([9.7872e-01, 1.4038e-02, 4.6907e-07, 7.2177e-03, 2.4700e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008847180288285017 tensor([3.8346e-09, 3.8892e-08, 2.2522e-04, 8.8472e-04, 9.9889e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2293550968170166 tensor([0.1680, 0.1557, 0.0063, 0.2294, 0.4405], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11271636188030243 tensor([7.7719e-01, 1.1272e-01, 1.8157e-04, 9.5089e-02, 1.4821e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3197241723537445 tensor([6.2824e-01, 3.1972e-01, 5.6539e-04, 4.1832e-02, 9.6412e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09560460597276688 tensor([2.3918e-04, 7.8717e-03, 9.5605e-02, 1.6830e-02, 8.7945e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3981143534183502 tensor([0.0255, 0.3981, 0.1216, 0.0377, 0.4171], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0983542948961258 tensor([6.5878e-05, 8.9004e-02, 8.1245e-01, 1.2215e-04, 9.8354e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.164953351020813 tensor([2.1829e-03, 8.2509e-01, 1.6495e-01, 1.6667e-04, 7.6076e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1369926631450653 tensor([6.4441e-04, 8.6152e-01, 1.3699e-01, 1.0779e-05, 8.2898e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04506518319249153 tensor([5.3255e-06, 4.5065e-02, 9.4624e-01, 5.8550e-06, 8.6855e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.040819328278303146 tensor([5.6987e-04, 1.6958e-06, 3.8401e-07, 9.5861e-01, 4.0819e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3640243709087372 tensor([2.7130e-09, 1.3115e-05, 3.6402e-01, 2.3769e-05, 6.3594e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3990747928619385 tensor([0.5535, 0.3991, 0.0015, 0.0369, 0.0091], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07207794487476349 tensor([3.4998e-06, 1.0666e-02, 9.1721e-01, 3.9630e-05, 7.2078e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1922684609889984 tensor([0.0043, 0.5172, 0.2831, 0.0031, 0.1923], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06353773921728134 tensor([1.7769e-04, 3.7109e-07, 1.7562e-07, 9.3628e-01, 6.3538e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20261341333389282 tensor([1.8625e-02, 1.0501e-03, 9.6512e-05, 7.7762e-01, 2.0261e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22341379523277283 tensor([2.6704e-06, 4.6484e-03, 7.7182e-01, 1.1970e-04, 2.2341e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1527254283428192 tensor([6.5931e-04, 8.4514e-01, 1.5273e-01, 1.5862e-05, 1.4634e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008393386378884315 tensor([3.2201e-06, 3.5711e-05, 4.5924e-03, 8.3934e-03, 9.8698e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1288129985332489 tensor([1.2881e-01, 7.4921e-03, 1.5128e-04, 7.5880e-01, 1.0475e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007258801721036434 tensor([3.0528e-07, 7.5236e-07, 2.1324e-04, 7.2588e-03, 9.9253e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0696631520986557 tensor([6.9663e-02, 9.2993e-01, 3.0431e-04, 4.4788e-05, 5.9803e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04010026901960373 tensor([1.0754e-06, 4.0100e-02, 9.5846e-01, 4.3198e-07, 1.4376e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03066876158118248 tensor([3.0669e-02, 2.5306e-08, 4.5062e-13, 9.6933e-01, 1.3099e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18184874951839447 tensor([1.8185e-01, 1.7020e-02, 4.5651e-04, 7.0152e-01, 9.9160e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03858504071831703 tensor([1.3732e-09, 1.4212e-06, 3.8585e-02, 4.2229e-05, 9.6137e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1855112612247467 tensor([7.9905e-01, 1.8551e-01, 9.0129e-05, 1.4590e-02, 7.6003e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.048171963542699814 tensor([4.8172e-02, 9.4331e-01, 5.8104e-03, 6.3623e-04, 2.0702e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3377455472946167 tensor([1.0200e-05, 3.6644e-03, 3.3775e-01, 7.9598e-04, 6.5778e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35073545575141907 tensor([6.4025e-01, 4.5691e-03, 2.4526e-06, 3.5074e-01, 4.4409e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024332664906978607 tensor([4.5461e-07, 4.2839e-05, 2.4333e-02, 9.4035e-04, 9.7468e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3706446588039398 tensor([3.4864e-05, 3.7064e-01, 6.2816e-01, 1.7750e-06, 1.1551e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04317129775881767 tensor([9.4018e-01, 1.6379e-02, 3.1374e-06, 4.3171e-02, 2.6311e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17737965285778046 tensor([1.7738e-01, 1.8420e-02, 4.4154e-04, 7.2980e-01, 7.3955e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2124556303024292 tensor([1.0974e-03, 2.9032e-05, 2.0831e-05, 7.8640e-01, 2.1246e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014296227134764194 tensor([2.5898e-07, 1.4296e-02, 9.8324e-01, 2.6115e-07, 2.4649e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1379501223564148 tensor([8.4644e-01, 1.3795e-01, 5.2572e-05, 1.5017e-02, 5.3752e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21476325392723083 tensor([2.6081e-02, 1.6133e-03, 1.3718e-04, 7.5741e-01, 2.1476e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02396339736878872 tensor([4.0058e-03, 1.7348e-05, 1.2210e-06, 9.7201e-01, 2.3963e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23541118204593658 tensor([0.1299, 0.1846, 0.0131, 0.2354, 0.4370], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06961394846439362 tensor([9.8711e-07, 5.6904e-03, 9.2468e-01, 1.3096e-05, 6.9614e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.391935259103775 tensor([0.0019, 0.1289, 0.3919, 0.0112, 0.4660], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02921369858086109 tensor([4.5535e-03, 9.6496e-01, 2.9214e-02, 5.2060e-05, 1.2221e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010279989801347256 tensor([9.8962e-01, 9.5904e-05, 8.3869e-11, 1.0280e-02, 1.6257e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10937188565731049 tensor([6.6334e-05, 2.4156e-07, 3.6524e-07, 8.9056e-01, 1.0937e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11224931478500366 tensor([2.7926e-09, 7.7914e-05, 8.8767e-01, 2.3319e-06, 1.1225e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05846446007490158 tensor([5.8464e-02, 9.3519e-01, 4.8300e-03, 5.3390e-04, 9.8595e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2301996350288391 tensor([2.8602e-06, 3.9334e-03, 7.6569e-01, 1.7416e-04, 2.3020e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10699546337127686 tensor([2.8795e-06, 6.6128e-03, 8.8633e-01, 5.8194e-05, 1.0700e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26002833247184753 tensor([2.6003e-01, 8.1990e-03, 5.4461e-05, 6.7687e-01, 5.4848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06437522172927856 tensor([1.4816e-04, 9.9126e-03, 6.4375e-02, 2.4100e-03, 9.2315e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35574913024902344 tensor([1.9758e-04, 3.2716e-02, 3.5575e-01, 2.8033e-03, 6.0853e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21882236003875732 tensor([0.0011, 0.2611, 0.5171, 0.0019, 0.2188], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01930004544556141 tensor([6.4752e-04, 1.4722e-06, 2.6280e-07, 9.8005e-01, 1.9300e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011105470359325409 tensor([1.0753e-06, 1.5885e-06, 1.8729e-04, 1.1105e-02, 9.8870e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3914301097393036 tensor([0.0007, 0.0812, 0.3914, 0.0038, 0.5230], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.029557954519987106 tensor([6.4559e-03, 9.6121e-01, 2.9558e-02, 1.1171e-04, 2.6660e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030061595141887665 tensor([1.4105e-03, 9.6836e-01, 3.0062e-02, 6.1989e-06, 1.5875e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10693301260471344 tensor([8.3950e-01, 4.7754e-02, 4.0343e-05, 1.0693e-01, 5.7706e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16778132319450378 tensor([4.2816e-05, 1.1955e-05, 1.6982e-04, 1.6778e-01, 8.3199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030248766764998436 tensor([2.8409e-08, 8.3199e-04, 9.6892e-01, 1.3335e-06, 3.0249e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13753579556941986 tensor([3.2835e-06, 1.3754e-01, 8.6180e-01, 3.0276e-07, 6.6244e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1963375061750412 tensor([1.6651e-03, 7.9211e-01, 1.9634e-01, 1.1977e-04, 9.7718e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.048702359199523926 tensor([2.6041e-07, 4.0079e-05, 4.8702e-02, 8.5763e-04, 9.5040e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027972493320703506 tensor([3.4485e-03, 8.5665e-06, 4.7239e-07, 9.6857e-01, 2.7972e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.029720010235905647 tensor([9.6992e-01, 3.5854e-04, 2.5965e-09, 2.9720e-02, 5.1647e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3915266692638397 tensor([3.9153e-01, 6.0839e-01, 1.3039e-05, 6.1777e-05, 5.5823e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.292073518037796 tensor([0.2921, 0.7007, 0.0014, 0.0042, 0.0016], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23258063197135925 tensor([2.3258e-01, 7.6714e-01, 9.4587e-05, 1.5091e-04, 3.5023e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06313061714172363 tensor([1.3072e-03, 5.9028e-06, 1.0817e-06, 9.3556e-01, 6.3131e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10001775622367859 tensor([8.8371e-01, 1.5603e-02, 4.5799e-06, 1.0002e-01, 6.6013e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 3, 1], [4, 2, 3], [2, 1, 0], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3], [3, 0, 4, 2], [2, 3, 4], [1, 0, 2, 3], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0], [0, 1, 2, 3], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 1, 3], [1, 0, 2, 3], [2, 1, 4, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1], [2, 3, 4, 0], [3, 0, 4, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3], [2, 4, 3, 1], [2, 4, 1], [1, 0, 2, 3], [1, 0, 2, 3], [3, 0, 4, 2], [3, 4, 0, 2], [2, 4, 1], [1, 2, 0], [1, 2, 0], [3, 0, 4], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [3, 4, 0, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 1], [3, 0, 4, 1], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 1, 4, 0], [3, 4, 2, 0], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [3, 4, 2, 0], [3, 0, 4], [2, 4, 1], [2, 1, 4, 0], [0, 3, 1], [2, 1, 4], [2, 4, 3, 1], [3, 0, 4], [2, 1, 0], [2, 4, 3, 1], [0, 3, 1], [3, 0, 4, 2], [0, 3, 1], [0, 1, 3, 2], [0, 1, 3, 2], [4, 3, 2, 0], [3, 0, 4], [2, 4, 1, 3], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4], [2, 3, 4, 0], [0, 1, 2, 3], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3], [2, 1, 4], [2, 1, 0], [3, 0, 4], [3, 0, 4], [2, 4, 1], [0, 2, 1], [2, 1, 0], [2, 1, 4], [0, 3, 2], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 4, 1], [1, 2, 0], [0, 3, 1], [3, 0, 4, 2], [3, 0, 4], [2, 1, 4, 0], [1, 0, 2], [0, 3, 1, 4], [3, 0, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4], [1, 0, 2, 3], [3, 4, 0, 2], [3, 0, 2, 4], [0, 3, 4, 1], [1, 0, 2, 3], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3, 1], [2, 4, 1], [2, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [4, 3, 2, 0], [2, 4, 1], [1, 2, 0], [2, 4, 1], [3, 0, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 1], [2, 4, 3], [2, 4, 3, 1], [0, 1, 3], [2, 4, 1, 3], [1, 2, 0], [0, 3, 1, 4], [2, 4, 3], [3, 0, 4], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 4], [3, 0, 4], [3, 0, 2], [3, 0, 4, 1], [1, 2, 0, 4], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [0, 1, 3, 2], [4, 2, 3], [2, 4, 3], [2, 4, 1], [2, 1, 4], [2, 1, 0], [4, 3, 2, 0], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0], [2, 4, 3, 1], [2, 4, 3], [0, 1, 3, 2], [0, 3, 2], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3], [2, 4, 3], [0, 3, 1, 4], [3, 0, 4], [2, 1, 0, 4], [2, 1, 0], [0, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 2, 3], [3, 4, 2, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1], [2, 1, 4], [0, 1, 3, 2], [3, 0, 4], [2, 4, 1, 3], [2, 1, 4, 0], [2, 1, 0], [3, 0, 4, 1], [2, 4, 3, 1], [2, 1, 0], [2, 1, 0, 4], [2, 0, 1], [0, 3, 1, 4], [0, 3, 1], [3, 4, 0, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1], [0, 3, 1, 4], [2, 1, 4], [0, 3, 1, 2], [0, 3, 1], [0, 3], [2, 1, 0, 4], [0, 1, 2, 3], [0, 3, 1], [3, 4, 0, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 0, 2, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [4, 2, 3], [2, 4, 3], [4, 2, 3], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "[[2, 4], [0, 1], [3, 4], [4], [4], [0, 1], [1], [0, 1], [4], [4], [2], [2], [0], [4], [4], [0], [1], [1], [3, 4], [4], [1], [4], [0], [4], [3], [1], [0], [4], [0, 3, 4], [1], [1], [2], [2], [0], [2], [0, 1], [0], [0, 3], [4], [4], [1], [1], [0, 3], [3, 4], [3, 4], [1, 2], [1], [2], [4], [2], [2], [1], [3], [3], [0, 3], [2], [1], [1], [3], [3], [1], [1], [0], [3], [2], [1], [1, 2], [0, 3], [3], [2, 4], [0, 3], [0], [1, 2], [3, 4], [0], [2, 4], [1], [2, 4], [4], [4], [1], [1, 2], [0], [3], [4], [0], [1, 2], [1], [4], [2, 4], [0], [0, 1], [0, 3], [3, 4], [1, 2], [1, 2], [0, 3], [3, 4], [3, 4], [0, 3], [1, 4], [1], [1], [3, 4], [3, 4], [2, 4], [0, 1], [0, 3], [3, 4], [2, 4], [1], [1, 2], [3], [3, 4], [2], [2], [0, 3], [2], [0, 3], [4], [1], [1], [2], [4], [4], [3, 4], [0], [0, 3], [0, 3], [2], [2], [0], [1], [0, 3], [3, 4], [0, 3], [1, 2], [3], [3], [0, 3], [0, 1], [4], [0], [0, 3], [0, 1], [0], [2, 4], [0], [3, 4], [2], [0, 1], [1, 2], [4], [3], [2], [1, 2], [1, 4], [2], [3], [2, 4], [0], [0, 1], [2, 4], [2, 4], [4], [0], [1], [1], [3], [4], [0, 1], [0, 1], [0, 3], [0, 3], [3, 4], [1], [4], [0], [4], [3, 4], [0], [0, 1], [4], [1, 4], [2], [1], [1], [2], [3], [2, 4], [0, 1], [2], [1, 2], [3], [3, 4], [2, 4], [1], [4], [3], [4], [1], [2], [3], [3], [4], [0], [1], [2, 4], [0, 3], [4], [1, 2], [0], [3], [3, 4], [2], [0], [3, 4], [3], [3, 4], [2], [2, 4], [1], [0], [3], [2], [1], [2, 4], [2], [0, 3], [4], [2, 4], [1, 2, 4], [3], [4], [2, 4], [1], [1], [0], [4], [2], [2], [1], [4], [3], [0], [0, 1], [0, 1], [0, 1], [3], [0]]\n",
      "NL_pred of 3th iteration [[0, 1, 2, 3], [0, 1, 2, 3], [3, 0, 4, 2], [1, 0, 2, 3], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [0, 1, 2, 3], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 1, 3], [1, 0, 2, 3], [2, 1, 4, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 3, 4, 0], [3, 0, 4, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 1], [1, 0, 2, 3], [1, 0, 2, 3], [3, 0, 4, 2], [3, 4, 0, 2], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [3, 4, 0, 2], [2, 1, 4, 0], [2, 1, 4, 0], [3, 0, 4, 1], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 1, 4, 0], [3, 4, 2, 0], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [3, 4, 2, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 1, 3, 2], [0, 1, 3, 2], [4, 3, 2, 0], [2, 4, 1, 3], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 1, 3], [2, 3, 4, 0], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [3, 0, 4, 2], [3, 0, 4, 2], [2, 1, 4, 0], [0, 3, 1, 4], [3, 0, 4, 1], [0, 3, 1, 4], [1, 0, 2, 3], [3, 4, 0, 2], [3, 0, 2, 4], [0, 3, 4, 1], [1, 0, 2, 3], [0, 3, 1, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [4, 3, 2, 0], [2, 1, 4, 0], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 4], [3, 0, 4, 1], [1, 2, 0, 4], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [0, 1, 3, 2], [4, 3, 2, 0], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 3, 1], [0, 1, 3, 2], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [3, 0, 4], [2, 1, 0, 4], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 2, 3], [3, 4, 2, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 1, 3, 2], [2, 4, 1, 3], [2, 1, 4, 0], [3, 0, 4, 1], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [3, 4, 0, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4], [0, 3, 1, 2], [2, 1, 0, 4], [0, 1, 2, 3], [3, 4, 0, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 0, 2, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.007746542111421242  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.007746260135601728  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0077457366845546625  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.007745014551358345  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.007744138821577415  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.007743153816614396  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.007742103093709701  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.007741019511834169  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.007739935165796525  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00773886304635268  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.0077378199650691105  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.007736815856053279  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.0077358598892505355  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.007734949772174542  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.007734083976501074  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.007733258681419568  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0077324662453089  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.007731701319034283  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.007730952440164028  Accuracy on Support set:0.0\n",
      "torch.Size([156, 2048]) torch.Size([156])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.007730213495401235  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.30285537242889404 tensor([2.2284e-05, 5.7033e-03, 3.0286e-01, 1.6703e-03, 6.8975e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4909262955188751 tensor([5.0888e-01, 4.9093e-01, 1.4925e-05, 1.6652e-04, 1.2720e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20861691236495972 tensor([0.0883, 0.0203, 0.0011, 0.6817, 0.2086], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3533974587917328 tensor([3.5340e-01, 6.4449e-01, 3.5706e-04, 1.3178e-03, 4.3436e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.36012277007102966 tensor([0.3601, 0.4302, 0.0060, 0.1100, 0.0937], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4940539002418518 tensor([2.5849e-05, 7.4335e-07, 8.0637e-06, 4.9405e-01, 5.0591e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17635828256607056 tensor([0.3676, 0.1208, 0.0013, 0.3340, 0.1764], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4268364906311035 tensor([0.5418, 0.4268, 0.0008, 0.0229, 0.0076], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2642471492290497 tensor([7.3305e-01, 2.1643e-03, 6.9170e-07, 2.6425e-01, 5.3834e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28222185373306274 tensor([7.1775e-01, 1.9230e-05, 1.6101e-10, 2.8222e-01, 4.7841e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2625758945941925 tensor([1.0986e-05, 5.6119e-08, 4.0142e-07, 7.3741e-01, 2.6258e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48254793882369995 tensor([1.0275e-03, 6.7001e-05, 6.3548e-05, 5.1629e-01, 4.8255e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.250131219625473 tensor([0.0090, 0.6250, 0.2501, 0.0048, 0.1111], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28448718786239624 tensor([7.1544e-01, 4.5621e-05, 6.9941e-10, 2.8449e-01, 2.4667e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23990312218666077 tensor([2.5442e-04, 7.5853e-01, 2.3990e-01, 5.2933e-06, 1.3098e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25186923146247864 tensor([7.4635e-01, 1.5055e-03, 3.0658e-07, 2.5187e-01, 2.7987e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3900490999221802 tensor([2.0394e-06, 1.7077e-03, 3.9005e-01, 2.2390e-04, 6.0802e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21463796496391296 tensor([2.1464e-01, 1.0629e-02, 1.2729e-04, 7.4736e-01, 2.7242e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23812943696975708 tensor([1.5168e-03, 7.3740e-01, 2.3813e-01, 1.6979e-04, 2.2783e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3242837190628052 tensor([6.4955e-03, 1.8031e-04, 2.3120e-05, 6.6902e-01, 3.2428e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35303738713264465 tensor([1.3147e-05, 6.7497e-03, 6.3937e-01, 8.3110e-04, 3.5304e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3475818634033203 tensor([1.3480e-07, 1.9718e-04, 3.4758e-01, 1.3948e-04, 6.5208e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.48069289326667786 tensor([1.7231e-04, 5.1367e-01, 4.8069e-01, 1.3109e-05, 5.4541e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33926594257354736 tensor([6.4849e-04, 6.4667e-01, 3.3927e-01, 7.9029e-05, 1.3337e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4145576059818268 tensor([3.9363e-07, 9.2439e-04, 5.8446e-01, 6.1577e-05, 4.1456e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23979738354682922 tensor([0.5900, 0.2398, 0.0011, 0.1307, 0.0384], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20316073298454285 tensor([2.0316e-01, 3.6747e-04, 4.4028e-07, 7.9438e-01, 2.0913e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.280464231967926 tensor([0.0060, 0.0042, 0.0027, 0.2805, 0.7066], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.41271382570266724 tensor([4.7731e-05, 4.1271e-01, 5.8425e-01, 2.6657e-06, 2.9866e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.34375038743019104 tensor([1.9847e-03, 5.8873e-01, 3.4375e-01, 4.9235e-04, 6.5045e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34627974033355713 tensor([6.3810e-01, 1.0388e-02, 9.7194e-06, 3.4628e-01, 5.2236e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2718493640422821 tensor([0.0171, 0.0300, 0.0223, 0.2718, 0.6588], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16555188596248627 tensor([1.9006e-03, 4.0553e-05, 1.3700e-05, 8.3249e-01, 1.6555e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24752716720104218 tensor([2.4753e-01, 9.6355e-03, 8.8651e-05, 6.8335e-01, 5.9401e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3614487051963806 tensor([0.0583, 0.4548, 0.0657, 0.0598, 0.3614], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24524562060832977 tensor([6.6551e-02, 5.8055e-03, 2.9152e-04, 6.8211e-01, 2.4525e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4717748761177063 tensor([3.5591e-04, 1.7817e-05, 3.3223e-05, 5.2782e-01, 4.7177e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26308971643447876 tensor([0.0008, 0.1776, 0.5567, 0.0019, 0.2631], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22502782940864563 tensor([7.7131e-01, 2.2503e-01, 3.6728e-05, 3.5213e-03, 1.0267e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.339003324508667 tensor([6.5789e-01, 2.0437e-03, 7.4975e-07, 3.3900e-01, 1.0607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4311417043209076 tensor([5.4103e-07, 4.0828e-09, 2.9338e-07, 4.3114e-01, 5.6886e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2800496220588684 tensor([8.3347e-05, 4.8869e-02, 6.7046e-01, 5.3423e-04, 2.8005e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3923666477203369 tensor([3.3935e-04, 6.0205e-01, 3.9237e-01, 2.3440e-05, 5.2158e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38258859515190125 tensor([2.0204e-06, 5.4915e-08, 3.0808e-06, 3.8259e-01, 6.1741e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.33933696150779724 tensor([6.5832e-01, 1.7183e-03, 5.7669e-07, 3.3934e-01, 6.2476e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.34771353006362915 tensor([3.4771e-01, 3.9794e-04, 2.0480e-07, 6.5089e-01, 1.0012e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34281229972839355 tensor([0.1419, 0.1250, 0.0084, 0.3818, 0.3428], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2251625657081604 tensor([7.7484e-01, 2.6541e-07, 3.4492e-14, 2.2516e-01, 2.4921e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23738835752010345 tensor([2.3739e-01, 2.8552e-06, 7.5359e-11, 7.6260e-01, 1.2514e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28646042943000793 tensor([7.1351e-01, 1.8052e-05, 1.3463e-10, 2.8646e-01, 9.6785e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4432070553302765 tensor([1.1097e-03, 8.7325e-05, 7.5810e-05, 5.5552e-01, 4.4321e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3367280662059784 tensor([0.3367, 0.0776, 0.0014, 0.5135, 0.0708], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3388928771018982 tensor([4.1535e-04, 6.5573e-01, 3.3889e-01, 2.2949e-05, 4.9403e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.45286744832992554 tensor([4.5287e-01, 1.5975e-02, 5.9497e-05, 5.0236e-01, 2.8741e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22471794486045837 tensor([0.6020, 0.2247, 0.0011, 0.1333, 0.0388], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29505419731140137 tensor([0.5118, 0.1223, 0.0009, 0.2951, 0.0700], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.40616703033447266 tensor([0.4062, 0.5869, 0.0007, 0.0049, 0.0013], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1671476662158966 tensor([3.9216e-07, 2.0546e-04, 1.6715e-01, 3.0920e-04, 8.3234e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.222502201795578 tensor([6.2780e-05, 2.9436e-07, 4.7056e-07, 7.7743e-01, 2.2250e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2360510230064392 tensor([7.3999e-01, 2.3605e-01, 1.9829e-04, 2.2194e-02, 1.5678e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2576429843902588 tensor([1.8612e-03, 7.1696e-01, 2.5764e-01, 2.9118e-04, 2.3249e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22530634701251984 tensor([2.4734e-04, 7.7342e-01, 2.2531e-01, 4.1777e-06, 1.0203e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.43354952335357666 tensor([0.0405, 0.4335, 0.0530, 0.0329, 0.4400], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18733437359333038 tensor([6.6115e-07, 3.3437e-04, 1.8733e-01, 3.0503e-04, 8.1203e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2548501491546631 tensor([7.4465e-01, 2.5485e-01, 6.7878e-06, 4.8294e-04, 1.4113e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.34368425607681274 tensor([0.0007, 0.0779, 0.3437, 0.0055, 0.5723], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22850120067596436 tensor([7.2917e-10, 2.3277e-05, 7.7147e-01, 2.1925e-06, 2.2850e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2751745581626892 tensor([2.7517e-01, 7.2154e-01, 7.2560e-04, 1.8577e-03, 7.0112e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3505517840385437 tensor([3.5055e-01, 6.4676e-01, 3.0932e-04, 1.8139e-03, 5.6914e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25391897559165955 tensor([7.3620e-01, 6.7227e-03, 3.8278e-06, 2.5392e-01, 3.1535e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3120322525501251 tensor([3.1203e-01, 9.9311e-03, 5.5620e-05, 6.5785e-01, 2.0132e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20844943821430206 tensor([6.5517e-02, 1.9415e-03, 4.0400e-05, 7.2405e-01, 2.0845e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26038700342178345 tensor([0.1994, 0.1421, 0.0045, 0.2604, 0.3936], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2583140432834625 tensor([6.8983e-01, 2.5831e-01, 3.4903e-04, 4.3839e-02, 7.6671e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.39920753240585327 tensor([0.0333, 0.3992, 0.0973, 0.0482, 0.4220], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3191906809806824 tensor([2.9851e-09, 1.2371e-05, 3.1919e-01, 2.7589e-05, 6.8077e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3352695405483246 tensor([0.6166, 0.3353, 0.0010, 0.0396, 0.0076], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2332611083984375 tensor([0.0059, 0.5565, 0.2333, 0.0040, 0.2003], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16342787444591522 tensor([1.9537e-02, 8.2129e-04, 5.9916e-05, 8.1615e-01, 1.6343e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2719622254371643 tensor([3.8796e-06, 5.3837e-03, 7.2248e-01, 1.7097e-04, 2.7196e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2819982171058655 tensor([1.3120e-05, 3.7666e-03, 2.8200e-01, 1.0099e-03, 7.1321e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.34896570444107056 tensor([6.4406e-01, 3.5381e-03, 1.5187e-06, 3.4897e-01, 3.4313e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.42307528853416443 tensor([4.6345e-05, 4.2308e-01, 5.7570e-01, 2.1045e-06, 1.1790e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17353588342666626 tensor([1.1417e-03, 2.3169e-05, 1.3449e-05, 8.2529e-01, 1.7354e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17356239259243011 tensor([2.7572e-02, 1.2732e-03, 8.5827e-05, 7.9751e-01, 1.7356e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27278828620910645 tensor([0.1536, 0.1668, 0.0094, 0.2728, 0.3975], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3329962491989136 tensor([0.0027, 0.1373, 0.3330, 0.0157, 0.5114], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.277306467294693 tensor([4.2258e-06, 4.6397e-03, 7.1780e-01, 2.4784e-04, 2.7731e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26370903849601746 tensor([2.6371e-01, 6.3446e-03, 3.4081e-05, 6.8640e-01, 4.3515e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29643359780311584 tensor([2.8186e-04, 3.5304e-02, 2.9643e-01, 3.8561e-03, 6.6412e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2489733248949051 tensor([0.0016, 0.2932, 0.4536, 0.0026, 0.2490], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33563148975372314 tensor([0.0009, 0.0869, 0.3356, 0.0049, 0.5716], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4516502916812897 tensor([4.5165e-01, 5.4827e-01, 9.3480e-06, 6.8364e-05, 4.9394e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3599652647972107 tensor([0.3600, 0.6328, 0.0009, 0.0049, 0.0014], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28214821219444275 tensor([2.8215e-01, 7.1758e-01, 6.9636e-05, 1.7460e-04, 3.1824e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 3, 1], [4, 2, 3], [2, 1, 0], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3], [3, 0, 4, 2], [2, 3, 4], [1, 0, 2, 3], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0], [0, 1, 2, 3], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 1, 3], [1, 0, 2, 3], [2, 1, 4, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 4], [2, 3, 4, 0], [3, 0, 4, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3], [2, 4, 3, 1], [2, 4, 1], [1, 0, 2, 3], [1, 0, 2, 3], [3, 0, 4, 2], [3, 4, 0, 2], [2, 4, 1], [1, 2, 0], [1, 2, 0], [3, 0, 4], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [3, 4, 0, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 1], [3, 0, 4, 1], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 1, 4, 0], [3, 4, 2, 0], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [3, 4, 2, 0], [3, 0, 4], [2, 4, 1], [2, 1, 4, 0], [0, 3, 1], [2, 1, 4], [2, 4, 3, 1], [3, 0, 4], [2, 1, 0], [2, 4, 3, 1], [0, 3, 1], [3, 0, 4, 2], [0, 3, 1], [0, 1, 3, 2], [0, 1, 3, 2], [4, 3, 2, 0], [3, 0, 4], [2, 4, 1, 3], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4], [2, 3, 4, 0], [0, 1, 2, 3], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3], [2, 1, 4], [2, 1, 0], [3, 0, 4], [3, 0, 4], [2, 4, 1], [0, 2, 1], [2, 1, 0, 4], [2, 1, 4], [0, 3, 2], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 4, 1], [1, 2, 0], [0, 3, 1], [3, 0, 4, 2], [3, 0, 4], [2, 1, 4, 0], [1, 0, 2], [0, 3, 1, 4], [3, 0, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4], [1, 0, 2, 3], [3, 4, 0, 2], [3, 0, 2, 4], [0, 3, 4, 1], [1, 0, 2, 3], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3, 1], [2, 4, 1], [2, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [4, 3, 2, 0], [2, 4, 1], [1, 2, 0], [2, 4, 1], [3, 0, 4], [2, 1, 4, 0], [2, 1, 0, 4], [2, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 1], [2, 4, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [1, 2, 0], [0, 3, 1, 4], [2, 4, 3], [3, 0, 4], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 4], [3, 0, 4], [3, 0, 2], [3, 0, 4, 1], [1, 2, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [0, 1, 3, 2], [4, 2, 3], [2, 4, 3], [2, 4, 1], [2, 1, 4], [2, 1, 0], [4, 3, 2, 0], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0], [2, 4, 3, 1], [2, 4, 3], [0, 1, 3, 2], [0, 3, 2], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3], [2, 4, 3], [0, 3, 1, 4], [3, 0, 4], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 2, 3], [3, 4, 2, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1], [2, 1, 4], [0, 1, 3, 2], [3, 0, 4], [2, 4, 1, 3], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 0, 1], [0, 3, 1, 4], [0, 3, 1], [3, 4, 0, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1], [0, 3, 1, 4], [2, 1, 4], [0, 3, 1, 2], [0, 3, 1], [0, 3], [2, 1, 0, 4], [0, 1, 2, 3], [0, 3, 1], [3, 4, 0, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 0, 2, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [4, 2, 3], [2, 4, 3], [4, 2, 3], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "[[2, 4], [0, 1], [3, 4], [4], [4], [0, 1], [1], [0, 1], [4], [4], [2], [2], [0], [4], [4], [0], [1], [1], [3, 4], [4], [1], [4], [0], [4], [3], [1], [0], [4], [0, 3], [1], [1], [2], [2], [0], [2], [0, 1], [0], [0, 3], [4], [4], [1], [1], [0, 3], [3, 4], [3, 4], [1, 2], [1], [2], [4], [2], [2], [1], [3], [3], [0, 3], [2], [1], [1], [3], [3], [1], [1], [0], [3], [2], [1], [1, 2], [0, 3], [3], [2, 4], [0, 3], [0], [1, 2], [3, 4], [0], [2, 4], [1], [2, 4], [4], [4], [1], [1, 2], [0], [3], [4], [0], [1, 2], [1], [4], [2, 4], [0], [0, 1], [0, 3], [3, 4], [1, 2], [1, 2], [0, 3], [3, 4], [3], [0, 3], [1, 4], [1], [1], [3, 4], [3, 4], [2, 4], [0, 1], [0, 3], [3, 4], [2, 4], [1], [1, 2], [3], [3, 4], [2], [2], [0, 3], [2], [0, 3], [4], [1], [1], [2], [4], [4], [3, 4], [0], [0, 3], [0, 3], [2], [2], [0], [1], [0, 3], [3, 4], [0, 3], [1, 2], [3], [3], [0, 3], [0, 1], [4], [0], [0, 3], [0, 1], [0], [4], [0], [3, 4], [2], [0, 1], [1, 2], [4], [3], [2], [1, 2], [1, 4], [2], [3], [4], [0], [0, 1], [2, 4], [2, 4], [4], [0], [1], [1], [3], [4], [0, 1], [0, 1], [0, 3], [0, 3], [3, 4], [1], [4], [0], [4], [3, 4], [0], [0, 1], [4], [1, 4], [2], [1], [1], [2], [3], [2, 4], [0, 1], [2], [1, 2], [3], [3], [2, 4], [1], [4], [3], [4], [1], [2], [3], [3], [4], [0], [1], [2, 4], [0, 3], [4], [1, 2], [0], [3], [3], [2], [0], [3], [3], [3, 4], [2], [2, 4], [1], [0], [3], [2], [1], [2, 4], [2], [0, 3], [4], [2, 4], [1, 2, 4], [3], [4], [2, 4], [1], [1], [0], [4], [2], [2], [1], [4], [3], [0], [0, 1], [0, 1], [0, 1], [3], [0]]\n",
      "NL_pred of 4th iteration [[2, 1, 4], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1, 2], [2, 1, 0, 4], [2, 1, 0, 4], [2, 1, 0, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.18565520218440465  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.1842184237071446  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.1817497525896345  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.17875782081059047  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.17569409097943986  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.17288565635681152  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.17050632408687047  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.1685781819479806  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.16706112452915736  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.16589270319257463  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.1649974925177438  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.16431437219892228  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.16378957884652273  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.163382921900068  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.16306565489087785  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.1628169502530779  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.16261798994881765  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.16245719364711217  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.16232616560799734  Accuracy on Support set:0.0\n",
      "torch.Size([7, 2048]) torch.Size([7])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.1622176170349121  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.02888624183833599 tensor([0.0022, 0.0130, 0.0289, 0.0996, 0.8564], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.049931012094020844 tensor([9.4973e-01, 4.9931e-02, 2.1628e-07, 3.3635e-04, 1.6635e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005209584720432758 tensor([1.7862e-01, 9.9496e-04, 2.7911e-06, 8.1518e-01, 5.2096e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05819922313094139 tensor([9.3886e-01, 5.8199e-02, 2.8147e-06, 2.9002e-03, 3.6369e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024896303191781044 tensor([8.1199e-01, 2.4896e-02, 1.9668e-05, 1.5985e-01, 3.2441e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027594057843089104 tensor([9.1500e-05, 1.4410e-07, 9.9132e-08, 9.7231e-01, 2.7594e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32707488536834717 tensor([6.6538e-01, 4.5889e-03, 2.0307e-06, 3.2707e-01, 2.9568e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02161998860538006 tensor([9.5349e-01, 2.1620e-02, 2.6158e-06, 2.4664e-02, 2.2376e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19948871433734894 tensor([8.0036e-01, 1.3200e-04, 3.5345e-09, 1.9949e-01, 1.5972e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2764468193054199 tensor([7.2355e-01, 3.2296e-06, 5.3463e-12, 2.7645e-01, 4.5039e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012666831724345684 tensor([2.6568e-05, 1.0344e-08, 5.5735e-09, 9.8731e-01, 1.2667e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02067839540541172 tensor([3.0406e-03, 6.9870e-06, 3.7435e-07, 9.7627e-01, 2.0678e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012034555897116661 tensor([0.2852, 0.5599, 0.0120, 0.0897, 0.0533], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26899999380111694 tensor([7.3099e-01, 5.9008e-06, 1.4501e-11, 2.6900e-01, 1.5560e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015126390382647514 tensor([5.5673e-03, 9.7872e-01, 1.5126e-02, 3.5705e-05, 5.5336e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19496077299118042 tensor([8.0493e-01, 9.6962e-05, 1.7858e-09, 1.9496e-01, 9.2892e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.039603717625141144 tensor([8.8795e-05, 3.2542e-03, 3.9604e-02, 5.3618e-03, 9.5169e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32174035906791687 tensor([3.2174e-01, 6.2788e-04, 5.1780e-07, 6.7683e-01, 8.0518e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013812231831252575 tensor([0.0558, 0.9121, 0.0138, 0.0029, 0.0153], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011288045905530453 tensor([1.5169e-02, 1.6307e-05, 1.2369e-07, 9.7353e-01, 1.1288e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1290922313928604 tensor([0.0017, 0.0268, 0.1291, 0.0658, 0.7766], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030322523787617683 tensor([6.1680e-06, 3.4019e-04, 3.0323e-02, 3.9860e-03, 9.6535e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03927498683333397 tensor([6.9210e-03, 9.4969e-01, 3.9275e-02, 1.8320e-04, 3.9262e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021465906873345375 tensor([0.0292, 0.9374, 0.0215, 0.0016, 0.0103], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0861375704407692 tensor([1.9369e-05, 2.2506e-03, 8.6138e-02, 1.6764e-03, 9.0992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010782865807414055 tensor([8.6751e-01, 1.0783e-02, 3.0814e-06, 1.2071e-01, 9.9773e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2860788404941559 tensor([2.8608e-01, 2.8364e-05, 2.6250e-09, 7.1382e-01, 6.8023e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.044405560940504074 tensor([3.2843e-02, 5.4390e-04, 1.6076e-05, 9.2219e-01, 4.4406e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06946305185556412 tensor([1.6160e-03, 9.2681e-01, 6.9463e-02, 2.7194e-05, 2.0824e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02188142202794552 tensor([0.0892, 0.8305, 0.0219, 0.0102, 0.0482], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.260850191116333 tensor([7.3852e-01, 5.1224e-04, 3.4269e-08, 2.6085e-01, 1.2221e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05078459531068802 tensor([8.6885e-02, 4.2992e-03, 1.7392e-04, 8.5786e-01, 5.0785e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3628040552139282 tensor([3.6280e-01, 5.3089e-04, 3.1388e-07, 6.3518e-01, 1.4826e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04635585471987724 tensor([0.5184, 0.1070, 0.0008, 0.3274, 0.0464], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006627810187637806 tensor([1.3253e-01, 3.4879e-04, 9.5343e-07, 8.6049e-01, 6.6278e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020422786474227905 tensor([1.0521e-03, 2.0153e-06, 2.1589e-07, 9.7852e-01, 2.0423e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0675647184252739 tensor([0.0658, 0.4385, 0.0676, 0.0858, 0.3424], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010600421577692032 tensor([9.8479e-01, 1.0600e-02, 1.7467e-07, 4.6051e-03, 5.7856e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25995752215385437 tensor([7.3990e-01, 1.1487e-04, 3.2620e-09, 2.5996e-01, 2.8073e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04744970425963402 tensor([1.8275e-06, 1.0455e-09, 6.5082e-09, 9.5255e-01, 4.7450e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1296430230140686 tensor([0.0100, 0.1958, 0.1296, 0.0325, 0.6320], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030711907893419266 tensor([1.1212e-02, 9.5421e-01, 3.0712e-02, 2.9661e-04, 3.5713e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.043091170489788055 tensor([9.1514e-06, 1.4371e-08, 5.0988e-08, 9.5690e-01, 4.3091e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2543533444404602 tensor([7.4553e-01, 9.9040e-05, 2.5303e-09, 2.5435e-01, 1.6417e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.45169541239738464 tensor([4.5170e-01, 2.9832e-05, 1.2385e-09, 5.4824e-01, 3.1648e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011730448342859745 tensor([3.6052e-01, 7.3471e-03, 2.5357e-05, 6.2038e-01, 1.1730e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24708342552185059 tensor([7.5292e-01, 5.9451e-08, 2.1594e-15, 2.4708e-01, 4.1110e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.276430606842041 tensor([2.7643e-01, 5.1253e-07, 2.2018e-12, 7.2357e-01, 1.0051e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27986887097358704 tensor([7.2013e-01, 2.6106e-06, 3.4451e-12, 2.7987e-01, 7.3147e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015340633690357208 tensor([3.1527e-03, 7.2411e-06, 3.1904e-07, 9.8150e-01, 1.5341e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4756213128566742 tensor([5.1976e-01, 3.0673e-03, 2.9214e-06, 4.7562e-01, 1.5478e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02372823655605316 tensor([1.2146e-02, 9.6105e-01, 2.3728e-02, 2.3962e-04, 2.8348e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42924559116363525 tensor([5.6924e-01, 7.9931e-04, 2.1151e-07, 4.2925e-01, 7.1554e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008538400754332542 tensor([8.6809e-01, 8.5384e-03, 2.5201e-06, 1.2251e-01, 8.6014e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25913769006729126 tensor([7.3542e-01, 4.1725e-03, 1.5696e-06, 2.5914e-01, 1.2637e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03858278691768646 tensor([9.5323e-01, 3.8583e-02, 3.1855e-06, 8.1202e-03, 6.1437e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009234479628503323 tensor([1.5016e-04, 4.8915e-08, 5.3909e-09, 9.9062e-01, 9.2345e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008204166777431965 tensor([9.7273e-01, 8.2042e-03, 4.1015e-07, 1.9030e-02, 3.4762e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015459934249520302 tensor([0.0736, 0.8897, 0.0155, 0.0057, 0.0156], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014677907340228558 tensor([5.2524e-03, 9.7958e-01, 1.4678e-02, 2.8803e-05, 4.5622e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07470279932022095 tensor([0.5333, 0.1452, 0.0009, 0.2460, 0.0747], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014641541987657547 tensor([9.8466e-01, 1.4642e-02, 4.7554e-08, 6.9676e-04, 1.0613e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0371730662882328 tensor([0.0423, 0.1478, 0.0372, 0.1976, 0.5751], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35792282223701477 tensor([9.4553e-09, 3.7300e-05, 3.5792e-01, 3.1175e-05, 6.4201e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07012984156608582 tensor([9.2512e-01, 7.0130e-02, 5.0908e-06, 4.6899e-03, 5.5623e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04915180429816246 tensor([9.4706e-01, 4.9152e-02, 1.6905e-06, 3.7499e-03, 3.5892e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1902831345796585 tensor([8.0934e-01, 3.0948e-04, 1.2265e-08, 1.9028e-01, 6.8344e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4238527715206146 tensor([4.2385e-01, 4.6215e-04, 1.6361e-07, 5.7522e-01, 4.6492e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005815115291625261 tensor([1.1890e-01, 1.2216e-04, 1.5236e-07, 8.7517e-01, 5.8151e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015054197981953621 tensor([5.3673e-01, 1.0112e-02, 1.7572e-05, 4.3808e-01, 1.5054e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010021982714533806 tensor([9.5185e-01, 1.0022e-02, 8.2111e-07, 3.7949e-02, 1.7514e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06645254045724869 tensor([0.4387, 0.1260, 0.0014, 0.3674, 0.0665], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05141053348779678 tensor([2.9851e-08, 1.0370e-05, 5.1411e-02, 2.8186e-04, 9.4830e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014181580394506454 tensor([9.4729e-01, 1.4182e-02, 2.5443e-06, 3.8331e-02, 1.9287e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01153859868645668 tensor([0.2285, 0.5694, 0.0115, 0.0856, 0.1049], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16205909848213196 tensor([4.1976e-04, 2.4265e-02, 1.6206e-01, 9.6314e-03, 8.0362e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02761346660554409 tensor([0.0009, 0.0080, 0.0276, 0.0414, 0.9220], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.26500463485717773 tensor([7.3473e-01, 1.8260e-04, 5.6287e-09, 2.6500e-01, 8.0328e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0837014690041542 tensor([1.1610e-03, 9.1435e-01, 8.3701e-02, 1.5980e-05, 7.7637e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015536174178123474 tensor([4.6368e-01, 1.1995e-02, 3.4214e-05, 5.0876e-01, 1.5536e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.019381986930966377 tensor([0.1138, 0.1563, 0.0194, 0.4040, 0.3065], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16803282499313354 tensor([5.0471e-04, 2.1264e-02, 1.6803e-01, 1.6580e-02, 7.9362e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3669258654117584 tensor([3.6693e-01, 3.1259e-04, 1.0451e-07, 6.3178e-01, 9.7675e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.028524968773126602 tensor([0.0233, 0.0739, 0.0285, 0.1784, 0.6958], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.039933837950229645 tensor([0.1061, 0.5117, 0.0399, 0.0986, 0.2437], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03030698001384735 tensor([0.0628, 0.1626, 0.0303, 0.1928, 0.5515], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06226860359311104 tensor([9.3758e-01, 6.2269e-02, 1.4678e-07, 1.5135e-04, 6.8714e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.049029164016246796 tensor([9.4148e-01, 4.9029e-02, 5.2328e-06, 9.3992e-03, 8.4949e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10254965722560883 tensor([8.9691e-01, 1.0255e-01, 1.1621e-06, 5.3216e-04, 5.0877e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[0, 3, 1, 2], [4, 2, 3, 1], [2, 1, 0, 4], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 3, 4, 1], [1, 0, 2, 3], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0, 4], [0, 1, 2, 3], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 1, 3], [1, 0, 2, 3], [2, 1, 4, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 4], [2, 3, 4, 0], [3, 0, 4, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 1], [2, 4, 3, 1], [2, 4, 1, 3], [1, 0, 2, 3], [1, 0, 2, 3], [3, 0, 4, 2], [3, 4, 0, 2], [2, 4, 1], [1, 2, 0, 4], [1, 2, 0, 4], [3, 0, 4, 2], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [3, 4, 0, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 1], [3, 0, 4, 1], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 1, 4, 0], [3, 4, 2, 0], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [3, 4, 2, 0], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 2], [2, 1, 4], [2, 4, 3, 1], [3, 0, 4, 2], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 1, 2], [0, 1, 3, 2], [0, 1, 3, 2], [4, 3, 2, 0], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 3, 4, 0], [0, 1, 2, 3], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4], [2, 1, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 1], [0, 2, 1, 4], [2, 1, 0, 4], [2, 1, 4], [0, 3, 2, 4], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [1, 2, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 1], [1, 2, 0, 4], [0, 3, 1, 2], [3, 0, 4, 2], [3, 0, 4, 2], [2, 1, 4, 0], [1, 0, 2, 4], [0, 3, 1, 4], [3, 0, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4], [1, 0, 2, 3], [3, 4, 0, 2], [3, 0, 2, 4], [0, 3, 4, 1], [1, 0, 2, 3], [0, 3, 1, 2], [2, 0, 1, 4], [2, 4, 3, 1], [2, 4, 1], [2, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [4, 3, 2, 0], [2, 4, 1], [1, 2, 0, 4], [2, 4, 1], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 0, 4], [2, 1, 4], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 2, 4], [3, 0, 4, 1], [1, 2, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [0, 1, 3, 2], [4, 2, 3, 1], [2, 4, 3, 1], [2, 4, 1, 3], [2, 1, 4], [2, 1, 0, 4], [4, 3, 2, 0], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [0, 3, 2, 4], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [3, 0, 4, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1, 2], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 2, 3], [3, 4, 2, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 2], [2, 1, 4], [0, 1, 3, 2], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 0, 1, 4], [0, 3, 1, 4], [0, 3, 1, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2], [2, 1, 0, 4], [0, 1, 2, 3], [0, 3, 1, 2], [3, 4, 0, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 0, 2, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [4, 2, 3, 1], [2, 4, 3, 1], [4, 2, 3, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "[[4], [0], [3], [4], [4], [0], [1], [0], [4], [4], [2], [2], [0], [4], [4], [0], [1], [1], [3], [4], [1], [4], [0], [4], [3], [1], [0], [4], [0, 3], [1], [1], [2], [2], [0], [2], [0], [0], [0], [4], [4], [1], [1], [0, 3], [3], [3], [1], [1], [2], [4], [2], [2], [1], [3], [3], [0, 3], [2], [1], [1], [3], [3], [1], [1], [0], [3], [2], [1], [1], [0], [3], [4], [0, 3], [0], [1], [3], [0], [4], [1], [4], [4], [4], [1], [1], [0], [3], [4], [0], [1], [1], [4], [4], [0], [0], [0, 3], [3], [1], [1], [0, 3], [3], [3], [0, 3], [1], [1], [1], [3], [3], [4], [0], [0, 3], [3], [4], [1], [1], [3], [3], [2], [2], [0, 3], [2], [0, 3], [4], [1], [1], [2], [4], [4], [3], [0], [0, 3], [0, 3], [2], [2], [0], [1], [0, 3], [3], [0, 3], [1], [3], [3], [0, 3], [0], [4], [0], [0, 3], [0], [0], [4], [0], [3], [2], [0], [1], [4], [3], [2], [1], [1], [2], [3], [4], [0], [0], [4], [2, 4], [4], [0], [1], [1], [3], [4], [0], [0], [0], [0, 3], [3], [1], [4], [0], [4], [3], [0], [0], [4], [1], [2], [1], [1], [2], [3], [4], [0], [2], [1], [3], [3], [4], [1], [4], [3], [4], [1], [2], [3], [3], [4], [0], [1], [4], [0, 3], [4], [1], [0], [3], [3], [2], [0], [3], [3], [3], [2], [4], [1], [0], [3], [2], [1], [4], [2], [0, 3], [4], [4], [1, 4], [3], [4], [4], [1], [1], [0], [4], [2], [2], [1], [4], [3], [0], [0], [0], [0], [3], [0]]\n",
      "NL_pred of 5th iteration [[0, 3, 1, 2], [4, 2, 3, 1], [2, 1, 0, 4], [2, 4, 3, 1], [2, 3, 4, 1], [1, 2, 0, 4], [2, 4, 3, 1], [2, 4, 1, 3], [1, 2, 0, 4], [1, 2, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 1, 3], [0, 3, 1, 2], [3, 0, 4, 2], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 1, 2], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 1, 2], [2, 4, 3, 1], [2, 1, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [0, 2, 1, 4], [0, 3, 2, 4], [2, 1, 0, 4], [1, 2, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [1, 2, 0, 4], [0, 3, 1, 2], [3, 0, 4, 2], [1, 0, 2, 4], [2, 0, 1, 4], [1, 2, 0, 4], [3, 0, 4, 2], [2, 4, 3, 1], [2, 4, 3, 1], [1, 2, 0, 4], [2, 4, 3, 1], [3, 0, 4, 2], [3, 0, 4, 2], [3, 0, 2, 4], [2, 4, 3, 1], [0, 3, 1, 2], [4, 2, 3, 1], [2, 4, 3, 1], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 2, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 2], [0, 3, 1, 2], [3, 0, 4, 2], [2, 0, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2], [0, 3, 1, 2], [4, 2, 3, 1], [2, 4, 3, 1], [4, 2, 3, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.016924674960150234  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.016916427059450012  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.016901119895603344  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.01688009586887083  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.016854714656221695  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.016826275466144947  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.016795913378397625  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.016764720280965168  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.016733442527660423  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.016702798829562424  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.01667326947917109  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.01664524665777234  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.01661880119987156  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.016594033310378807  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.016571046649545864  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.01654974619547526  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0165300991224206  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.016511977582738018  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.016495359116706295  Accuracy on Support set:0.0\n",
      "torch.Size([69, 2048]) torch.Size([69])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.016480224719945934  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[0, 3, 1, 2], [4, 2, 3, 1], [2, 1, 0, 4], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 0, 4, 2], [2, 3, 4, 1], [1, 0, 2, 3], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0, 4], [0, 1, 2, 3], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 1, 3], [1, 0, 2, 3], [2, 1, 4, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 4], [2, 3, 4, 0], [3, 0, 4, 2], [3, 0, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 1], [2, 4, 3, 1], [2, 4, 1, 3], [1, 0, 2, 3], [1, 0, 2, 3], [3, 0, 4, 2], [3, 4, 0, 2], [2, 4, 1], [1, 2, 0, 4], [1, 2, 0, 4], [3, 0, 4, 2], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [3, 0, 4, 1], [0, 3, 4, 1], [3, 4, 0, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 1], [3, 0, 4, 1], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 1, 4, 0], [3, 4, 2, 0], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 4], [3, 4, 2, 0], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 3, 1, 2], [2, 1, 4], [2, 4, 3, 1], [3, 0, 4, 2], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 1, 2], [0, 1, 3, 2], [0, 1, 3, 2], [4, 3, 2, 0], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 3, 4, 0], [0, 1, 2, 3], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4], [2, 1, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 1], [0, 2, 1, 4], [2, 1, 0, 4], [2, 1, 4], [0, 3, 2, 4], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [1, 2, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 1], [1, 2, 0, 4], [0, 3, 1, 2], [3, 0, 4, 2], [3, 0, 4, 2], [2, 1, 4, 0], [1, 0, 2, 4], [0, 3, 1, 4], [3, 0, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4], [1, 0, 2, 3], [3, 4, 0, 2], [3, 0, 2, 4], [0, 3, 4, 1], [1, 0, 2, 3], [0, 3, 1, 2], [2, 0, 1, 4], [2, 4, 3, 1], [2, 4, 1], [2, 1, 4], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [4, 3, 2, 0], [2, 4, 1], [1, 2, 0, 4], [2, 4, 1], [3, 0, 4, 2], [2, 1, 4, 0], [2, 1, 0, 4], [2, 1, 4], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [2, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 2, 4], [3, 0, 4, 1], [1, 2, 0, 4], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 0, 2], [3, 0, 4, 2], [2, 1, 0, 4], [0, 1, 3, 2], [4, 2, 3, 1], [2, 4, 3, 1], [2, 4, 1, 3], [2, 1, 4], [2, 1, 0, 4], [4, 3, 2, 0], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [0, 3, 2, 4], [0, 3, 1, 4], [3, 0, 4, 2], [3, 0, 4, 2], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [3, 0, 4, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1, 2], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 0], [0, 1, 2, 3], [3, 4, 2, 0], [3, 0, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 2], [2, 1, 4], [0, 1, 3, 2], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [2, 0, 1, 4], [0, 3, 1, 4], [0, 3, 1, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 2, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 2], [0, 3, 1, 4], [2, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 2], [2, 1, 0, 4], [0, 1, 2, 3], [0, 3, 1, 2], [3, 4, 0, 2], [3, 4, 0, 2], [2, 4, 1, 3], [1, 0, 2, 3], [0, 3, 1, 4], [3, 0, 4, 1], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [4, 2, 3, 1], [2, 4, 3, 1], [4, 2, 3, 1], [2, 1, 0, 4], [2, 4, 1, 3]]\n",
      "POSITION :  [[4], [0], [3], [4], [4], [0], [1], [0], [4], [4], [2], [2], [0], [4], [4], [0], [1], [1], [3], [4], [1], [4], [0], [4], [3], [1], [0], [4], [0, 3], [1], [1], [2], [2], [0], [2], [0], [0], [0], [4], [4], [1], [1], [0, 3], [3], [3], [1], [1], [2], [4], [2], [2], [1], [3], [3], [0, 3], [2], [1], [1], [3], [3], [1], [1], [0], [3], [2], [1], [1], [0], [3], [4], [0, 3], [0], [1], [3], [0], [4], [1], [4], [4], [4], [1], [1], [0], [3], [4], [0], [1], [1], [4], [4], [0], [0], [0, 3], [3], [1], [1], [0, 3], [3], [3], [0, 3], [1], [1], [1], [3], [3], [4], [0], [0, 3], [3], [4], [1], [1], [3], [3], [2], [2], [0, 3], [2], [0, 3], [4], [1], [1], [2], [4], [4], [3], [0], [0, 3], [0, 3], [2], [2], [0], [1], [0, 3], [3], [0, 3], [1], [3], [3], [0, 3], [0], [4], [0], [0, 3], [0], [0], [4], [0], [3], [2], [0], [1], [4], [3], [2], [1], [1], [2], [3], [4], [0], [0], [4], [2, 4], [4], [0], [1], [1], [3], [4], [0], [0], [0], [0, 3], [3], [1], [4], [0], [4], [3], [0], [0], [4], [1], [2], [1], [1], [2], [3], [4], [0], [2], [1], [3], [3], [4], [1], [4], [3], [4], [1], [2], [3], [3], [4], [0], [1], [4], [0, 3], [4], [1], [0], [3], [3], [2], [0], [3], [3], [3], [2], [4], [1], [0], [3], [2], [1], [4], [2], [0, 3], [4], [4], [1, 4], [3], [4], [4], [1], [1], [0], [4], [2], [2], [1], [4], [3], [0], [0], [0], [0], [3], [0]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.424\n",
      "tensor([4, 0, 3, 4, 4, 0, 1, 0, 4, 4, 2, 2, 0, 4, 4, 0, 1, 1, 3, 4, 1, 4, 0, 4,\n",
      "        3, 1, 0, 4, 1, 1, 2, 2, 0, 2, 0, 0, 0, 4, 4, 1, 1, 3, 3, 1, 1, 2, 4, 2,\n",
      "        2, 1, 3, 3, 2, 1, 1, 3, 3, 1, 1, 0, 3, 2, 1, 1, 0, 3, 4, 0, 1, 3, 0, 4,\n",
      "        1, 4, 4, 4, 1, 1, 0, 3, 4, 0, 1, 1, 4, 4, 0, 0, 3, 1, 1, 3, 3, 1, 1, 1,\n",
      "        3, 3, 4, 0, 3, 4, 1, 1, 3, 3, 2, 2, 2, 4, 1, 1, 2, 4, 4, 3, 0, 2, 2, 0,\n",
      "        1, 3, 1, 3, 3, 0, 4, 0, 0, 0, 4, 0, 3, 2, 0, 1, 4, 3, 2, 1, 1, 2, 3, 4,\n",
      "        0, 0, 4, 4, 0, 1, 1, 3, 4, 0, 0, 0, 3, 1, 4, 0, 4, 3, 0, 0, 4, 1, 2, 1,\n",
      "        1, 2, 3, 4, 0, 2, 1, 3, 3, 4, 1, 4, 3, 4, 1, 2, 3, 3, 4, 0, 1, 4, 4, 1,\n",
      "        0, 3, 3, 2, 0, 3, 3, 3, 2, 4, 1, 0, 3, 2, 1, 4, 2, 4, 4, 3, 4, 4, 1, 1,\n",
      "        0, 4, 2, 2, 1, 4, 3, 0, 0, 0, 0, 3, 0])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 96.94323144104804\n",
      "Epoch: 1  Loss: 98.2532751091703\n",
      "Epoch: 2  Loss: 98.68995633187772\n",
      "Epoch: 3  Loss: 98.2532751091703\n",
      "Epoch: 4  Loss: 98.2532751091703\n",
      "Epoch: 5  Loss: 98.68995633187772\n",
      "Epoch: 6  Loss: 100.0\n",
      "Epoch: 7  Loss: 100.0\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 11/15 [09:04<03:36, 54.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  36.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 5.419987876942032  Accuracy on Support set:28.000000000000004\n",
      "Train_Epoch: 1  Train_Loss: 3.3051634220406414  Accuracy on Support set:56.00000000000001\n",
      "Train_Epoch: 2  Train_Loss: 1.7892166577465831  Accuracy on Support set:60.0\n",
      "Train_Epoch: 3  Train_Loss: 0.8498774169199169  Accuracy on Support set:76.0\n",
      "Train_Epoch: 4  Train_Loss: 0.5224408539105206  Accuracy on Support set:92.0\n",
      "Train_Epoch: 5  Train_Loss: 0.3379861285816878  Accuracy on Support set:96.0\n",
      "Train_Epoch: 6  Train_Loss: 0.2195071528945118  Accuracy on Support set:100.0\n",
      "Train_Epoch: 7  Train_Loss: 0.15819657895714045  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.1190030967630446  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.09818708661943674  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.08598574664443731  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.07662010902538896  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.06938297811895609  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.06347543799318373  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.05866530261468142  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.05456214112229645  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.05109642909839749  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.04804399818181992  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.04538631346076727  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.04300488474778831  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.040877045220695435  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.0389756244700402  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.03723738886415959  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.03568913017399609  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.03426548277959227  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.03294618588406593  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.031739258309826254  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.030627902271226047  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.029585265591740607  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.028601273000240325  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.027712213867343963  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.02685011243680492  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.026051248107105494  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.025300493470858783  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.024601719360798597  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.02393542336532846  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.023300179056823255  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.02271540145156905  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.02214306966168806  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.02161108418367803  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.021106373032089322  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.020613013801630588  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.020152294472791255  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.019715673620812595  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.019282215933781117  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.0188828275189735  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.018496545150410385  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.01811706749955192  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.017767542893998324  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.017423576526343822  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  51.33333333333333\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 2.0008061873966199e-10 tensor([9.5406e-01, 4.1596e-05, 2.0008e-10, 4.5895e-02, 1.0455e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.17092098309513e-07 tensor([6.1709e-07, 4.4143e-02, 9.5088e-01, 3.8472e-06, 4.9775e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8548667430072783e-08 tensor([2.8549e-08, 4.4723e-03, 9.8908e-01, 2.5333e-06, 6.4410e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3865017939101651e-14 tensor([9.8684e-01, 3.6932e-07, 1.3865e-14, 1.3163e-02, 1.5939e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006872372468933463 tensor([0.0007, 0.0009, 0.0010, 0.5227, 0.4747], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.565292096638586e-07 tensor([5.5785e-02, 9.4420e-01, 1.6302e-05, 2.2155e-06, 6.5653e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6873362255864777e-05 tensor([1.6873e-05, 2.5892e-01, 7.3633e-01, 2.4849e-05, 4.7015e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007897705072537065 tensor([0.0008, 0.3541, 0.4467, 0.0049, 0.1936], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.920626219653059e-06 tensor([8.4701e-01, 1.3879e-02, 6.9206e-06, 1.3865e-01, 4.6033e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00010662900604074821 tensor([8.5985e-03, 9.0370e-04, 1.0663e-04, 9.2020e-01, 7.0186e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.068332196271456e-10 tensor([1.9389e-01, 7.3484e-06, 9.0683e-10, 8.0609e-01, 1.7835e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.256774334658985e-07 tensor([2.3635e-06, 4.6093e-01, 5.3870e-01, 3.2568e-07, 3.7155e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.478199475803194e-09 tensor([8.4782e-09, 1.6644e-03, 9.8999e-01, 2.6021e-06, 8.3402e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1045365511108685e-07 tensor([1.1045e-07, 3.5366e-04, 4.7632e-01, 1.3337e-03, 5.2200e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016033388674259186 tensor([0.0160, 0.6381, 0.0703, 0.0475, 0.2281], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.227808794864956e-11 tensor([8.4080e-01, 1.4186e-05, 9.2278e-11, 1.5918e-01, 1.7654e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2823525139538106e-05 tensor([1.3967e-03, 9.8462e-01, 1.3801e-02, 1.2824e-05, 1.6955e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.245648778043119e-09 tensor([2.2456e-09, 6.9167e-05, 4.8831e-01, 8.7921e-05, 5.1154e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010244679870083928 tensor([0.5519, 0.3967, 0.0010, 0.0491, 0.0012], grad_fn=<SoftmaxBackward0>)\n",
      "1 5.181515916774515e-06 tensor([4.9165e-05, 5.1815e-06, 1.3862e-05, 8.3864e-01, 1.6129e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3034269844003354e-12 tensor([1.4332e-01, 1.3661e-07, 1.3034e-12, 8.5668e-01, 8.8965e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.1575309549225494e-06 tensor([1.5341e-02, 9.8455e-01, 1.0489e-04, 3.1575e-06, 3.5652e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.117421435192227e-06 tensor([6.1174e-06, 1.5165e-02, 7.2468e-01, 1.1029e-03, 2.5904e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00011167907359777018 tensor([5.9332e-01, 5.3079e-02, 1.1168e-04, 3.4945e-01, 4.0325e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.4749906830409145e-09 tensor([5.2273e-08, 5.4750e-09, 1.2615e-06, 4.4370e-01, 5.5630e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.743748261011206e-06 tensor([3.5479e-03, 6.6041e-05, 5.7437e-06, 9.8540e-01, 1.0984e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.8477040220641356e-07 tensor([3.3305e-03, 9.9653e-01, 1.4073e-04, 3.8477e-07, 1.0726e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.353714083052182e-06 tensor([4.1936e-06, 3.4131e-01, 6.5798e-01, 1.3537e-06, 7.0071e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.985089122375939e-05 tensor([2.5573e-03, 1.2931e-04, 2.9851e-05, 9.6540e-01, 3.1886e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3958433520144808e-08 tensor([1.1135e-04, 9.3387e-08, 1.3958e-08, 9.9740e-01, 2.4915e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00031344054150395095 tensor([3.1344e-04, 4.4332e-04, 6.8212e-04, 3.1197e-01, 6.8659e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.003770663985051e-05 tensor([4.0038e-05, 1.5558e-01, 7.9770e-01, 2.7839e-04, 4.6396e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009295152267441154 tensor([0.0009, 0.5779, 0.3806, 0.0017, 0.0389], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012853869702666998 tensor([0.5080, 0.2604, 0.0013, 0.2082, 0.0220], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.919776306953281e-06 tensor([3.6027e-02, 3.1073e-04, 2.9198e-06, 9.5586e-01, 7.7940e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.183191013699798e-09 tensor([9.4089e-01, 4.3538e-04, 8.1832e-09, 5.8654e-02, 1.8473e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.039863605001301e-06 tensor([2.7545e-03, 9.9670e-01, 5.4108e-04, 1.0399e-06, 3.6241e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.81744777403037e-08 tensor([4.8174e-08, 2.8670e-04, 3.6867e-01, 4.0747e-04, 6.3064e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03399874269962311 tensor([0.0340, 0.8130, 0.0626, 0.0387, 0.0518], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008976851240731776 tensor([0.0009, 0.0199, 0.0536, 0.2085, 0.7172], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.308972220856913e-13 tensor([4.8881e-01, 3.5598e-07, 7.3090e-13, 5.1119e-01, 2.7535e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.246320066973567e-05 tensor([2.4691e-03, 9.7661e-01, 1.9817e-02, 8.2463e-05, 1.0179e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.111556426548816e-10 tensor([8.1116e-10, 1.4857e-03, 9.9670e-01, 1.2894e-07, 1.8095e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.311135057832871e-07 tensor([4.3111e-07, 9.0672e-02, 9.0856e-01, 6.7965e-07, 7.7180e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.144285803420189e-09 tensor([4.1443e-09, 5.3222e-07, 9.4903e-04, 2.9080e-03, 9.9614e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.196836003866338e-07 tensor([9.7076e-01, 2.5483e-02, 9.1968e-07, 3.7408e-03, 1.5209e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4068279597267974e-05 tensor([1.1386e-03, 9.8512e-01, 1.3495e-02, 1.4068e-05, 2.3434e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.055192613985128e-08 tensor([6.0552e-08, 1.6079e-03, 8.9555e-01, 7.0591e-05, 1.0277e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.60034663976694e-07 tensor([9.4430e-01, 5.5066e-02, 5.6003e-07, 6.2957e-04, 1.9386e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.005722579833673e-08 tensor([8.2384e-03, 9.0682e-06, 7.0057e-08, 9.8935e-01, 2.4071e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.433629203186683e-08 tensor([9.8275e-01, 1.7050e-02, 3.4336e-08, 2.0138e-04, 6.9984e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.714754014567006e-06 tensor([5.7352e-05, 7.0848e-01, 2.8986e-01, 9.7148e-06, 1.5948e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.279469117638655e-05 tensor([7.2173e-01, 2.7659e-01, 2.2795e-05, 1.6201e-03, 3.3796e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3835371071024838e-08 tensor([1.3835e-08, 7.0142e-05, 3.2705e-01, 7.9524e-04, 6.7208e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.58446783366162e-07 tensor([3.3138e-04, 1.9541e-06, 3.5845e-07, 9.8219e-01, 1.7476e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5456242188904845e-11 tensor([9.9693e-01, 4.0435e-05, 1.5456e-11, 3.0285e-03, 2.6162e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010568284429609776 tensor([0.0155, 0.8595, 0.0856, 0.0106, 0.0288], grad_fn=<SoftmaxBackward0>)\n",
      "0 7.945616387061705e-12 tensor([7.9456e-12, 4.0380e-05, 9.9344e-01, 1.1634e-07, 6.5161e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.686307141175348e-07 tensor([3.4301e-02, 4.6900e-05, 2.6863e-07, 9.6473e-01, 9.2401e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014750200789421797 tensor([0.0164, 0.0196, 0.0015, 0.5682, 0.3944], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.779435762245157e-09 tensor([9.9297e-01, 1.2310e-03, 7.7794e-09, 5.8003e-03, 3.4060e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0288260909874225e-06 tensor([1.8591e-05, 8.0944e-01, 1.9020e-01, 1.0288e-06, 3.3771e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7865257884164265e-11 tensor([1.7865e-11, 6.0184e-05, 9.9329e-01, 1.7260e-07, 6.6482e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.446577102295123e-05 tensor([3.4466e-05, 4.0170e-02, 8.1094e-01, 3.2489e-03, 1.4560e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.773725206381641e-06 tensor([5.7737e-06, 3.5778e-05, 9.7752e-04, 1.3298e-01, 8.6600e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00030363924452103674 tensor([4.0808e-02, 7.4563e-03, 3.0364e-04, 9.0507e-01, 4.6360e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.903004704217892e-05 tensor([9.8730e-04, 9.7237e-01, 2.6306e-02, 1.9030e-05, 3.1894e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.432950788668677e-07 tensor([4.4330e-07, 1.5372e-05, 3.0595e-03, 1.8475e-02, 9.7845e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.2257903512800112e-06 tensor([3.2258e-06, 1.9188e-05, 2.2266e-03, 1.6827e-01, 8.2948e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.305471166953794e-07 tensor([9.3055e-07, 2.8669e-02, 9.2712e-01, 3.5909e-05, 4.4177e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.9768221543899926e-08 tensor([1.3604e-03, 1.3000e-06, 4.9768e-08, 9.9720e-01, 1.4381e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.6422456357977353e-05 tensor([2.0037e-02, 9.7933e-01, 5.7482e-04, 2.6422e-05, 3.0982e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.215704855894728e-06 tensor([1.9049e-03, 1.5031e-05, 1.2157e-06, 9.7924e-01, 1.8836e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0023869583383202553 tensor([0.4467, 0.4143, 0.0024, 0.1287, 0.0079], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.849743966886308e-06 tensor([9.3475e-01, 6.1783e-02, 1.8497e-06, 3.4255e-03, 3.6992e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.02615353850655e-12 tensor([9.8493e-01, 9.4803e-06, 5.0262e-12, 1.5055e-02, 7.3916e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.811538459151052e-05 tensor([3.0391e-05, 4.0208e-01, 5.9415e-01, 1.8115e-05, 3.7198e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.059546274807872e-08 tensor([1.0568e-07, 1.0977e-01, 8.9000e-01, 8.0595e-08, 2.2933e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004402634687721729 tensor([0.0044, 0.0356, 0.0402, 0.3648, 0.5549], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004969796631485224 tensor([0.1993, 0.2640, 0.0050, 0.4194, 0.1123], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4541871706796883e-08 tensor([9.7153e-01, 7.2814e-04, 1.4542e-08, 2.7733e-02, 6.2521e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.324253728147596e-06 tensor([8.9138e-01, 1.0746e-01, 2.3243e-06, 1.1486e-03, 1.3191e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3848473479072254e-08 tensor([3.2686e-01, 7.1960e-05, 1.3848e-08, 6.7294e-01, 1.3070e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.1639359576729476e-08 tensor([3.1639e-08, 7.5951e-04, 8.8292e-01, 8.1811e-05, 1.1624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004367885645478964 tensor([0.2593, 0.6131, 0.0044, 0.0859, 0.0373], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.444092003401238e-08 tensor([3.6607e-01, 2.8981e-04, 8.4441e-08, 6.3335e-01, 2.9002e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.653707949524687e-07 tensor([3.6537e-07, 3.3960e-02, 9.6307e-01, 2.3934e-06, 2.9679e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016429465031251311 tensor([0.0016, 0.3009, 0.3772, 0.0178, 0.3025], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004153898626100272 tensor([4.1539e-04, 1.0665e-01, 5.0282e-01, 2.0819e-02, 3.6929e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.061386569745082e-07 tensor([6.3393e-07, 1.0614e-07, 5.1630e-06, 3.7893e-01, 6.2107e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.528094835595144e-14 tensor([9.9566e-01, 1.1392e-06, 3.5281e-14, 4.3419e-03, 1.9983e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.5996572478325106e-07 tensor([2.2923e-03, 9.9750e-01, 2.0894e-04, 3.5997e-07, 1.2327e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0558983335194903e-11 tensor([2.0559e-11, 1.1115e-04, 9.9573e-01, 7.4311e-08, 4.1615e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7639409932712624e-09 tensor([9.7598e-01, 2.0100e-04, 1.7639e-09, 2.3815e-02, 1.3110e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.873089404748953e-09 tensor([3.8731e-09, 8.1021e-08, 1.3004e-04, 7.6143e-03, 9.9226e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.54674490902152e-12 tensor([8.8124e-01, 4.5187e-06, 8.5467e-12, 1.1876e-01, 2.4619e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0105744877364486e-05 tensor([3.5253e-03, 9.9378e-01, 2.6253e-03, 1.0106e-05, 6.0174e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3972210883016487e-08 tensor([2.3972e-08, 6.0984e-06, 1.0949e-02, 3.7073e-03, 9.8534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.622839019750245e-05 tensor([8.3986e-01, 3.2320e-02, 2.6228e-05, 1.2673e-01, 1.0686e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0018524262122809887 tensor([0.0078, 0.0084, 0.0019, 0.6932, 0.2888], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0058793360367417336 tensor([0.1329, 0.8378, 0.0067, 0.0167, 0.0059], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.6347147493434022e-06 tensor([7.8816e-03, 9.9192e-01, 1.9586e-04, 1.6347e-06, 2.8788e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.718395808187779e-05 tensor([6.9829e-05, 5.9412e-01, 4.0304e-01, 2.7184e-05, 2.7437e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014743671636097133 tensor([7.7117e-01, 1.2886e-01, 1.4744e-04, 9.7992e-02, 1.8335e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2635549018777965e-07 tensor([4.6761e-04, 2.7219e-06, 3.2636e-07, 9.8275e-01, 1.6782e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5241768958063062e-10 tensor([5.8677e-03, 1.7142e-07, 1.5242e-10, 9.9407e-01, 5.9290e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.812242526801128e-07 tensor([6.2998e-04, 9.9627e-01, 3.0836e-03, 8.8122e-07, 1.5261e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009435623651370406 tensor([0.0009, 0.2410, 0.3629, 0.0125, 0.3826], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.713041223818436e-05 tensor([3.7130e-05, 3.2387e-04, 8.9549e-03, 2.2493e-01, 7.6576e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00043127869139425457 tensor([4.3128e-04, 9.6850e-04, 1.8210e-03, 3.7008e-01, 6.2670e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.977183496317593e-06 tensor([1.6544e-01, 8.3449e-01, 3.1824e-05, 3.4994e-05, 3.9772e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.310866309722769e-06 tensor([1.3109e-06, 1.7425e-02, 9.3186e-01, 7.3086e-05, 5.0642e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006995198782533407 tensor([1.4275e-03, 7.9858e-04, 6.9952e-04, 7.5175e-01, 2.4532e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.48111456105471e-07 tensor([4.0546e-07, 3.4811e-07, 6.0869e-05, 2.4907e-01, 7.5086e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002115030656568706 tensor([2.1150e-04, 4.2393e-03, 3.0372e-02, 1.3944e-01, 8.2574e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.7994812157604585e-11 tensor([9.8054e-01, 2.5382e-05, 3.7995e-11, 1.9439e-02, 2.1024e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.05258395706187e-05 tensor([6.8176e-05, 7.3975e-01, 2.5889e-01, 1.0526e-05, 1.2842e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.187465542562549e-10 tensor([1.1875e-10, 1.5590e-04, 9.9269e-01, 4.3589e-07, 7.1516e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.5497855656576576e-06 tensor([1.5498e-06, 1.6206e-05, 2.3570e-03, 1.0489e-01, 8.9273e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.45243131252937e-05 tensor([1.6351e-04, 4.4524e-05, 9.2396e-05, 6.8055e-01, 3.1915e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4069729559196276e-06 tensor([7.7298e-01, 2.2690e-01, 1.4070e-06, 1.1617e-04, 2.0302e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007093701278790832 tensor([0.0007, 0.5107, 0.4074, 0.0020, 0.0791], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.002502827064717e-11 tensor([8.0025e-11, 1.7694e-04, 9.9597e-01, 1.9693e-07, 3.8522e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010859638132387772 tensor([3.5407e-03, 9.8050e-01, 1.5174e-02, 1.0860e-04, 6.7269e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.196612112787989e-07 tensor([5.9845e-01, 1.1329e-03, 3.1966e-07, 3.9993e-01, 4.8799e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.760081760148751e-06 tensor([2.4767e-02, 3.2643e-04, 6.7601e-06, 9.7033e-01, 4.5689e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9744774704122392e-07 tensor([9.8403e-01, 1.1780e-02, 1.9745e-07, 4.1753e-03, 1.3612e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8429943199625995e-07 tensor([2.8430e-07, 2.5058e-05, 8.5407e-03, 8.8991e-03, 9.8253e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.540412928439764e-08 tensor([5.5404e-08, 7.5418e-06, 8.2569e-03, 8.1787e-03, 9.8356e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.038798261442935e-08 tensor([3.7320e-06, 6.0388e-08, 3.8161e-07, 9.0125e-01, 9.8747e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00951069500297308 tensor([0.1204, 0.3622, 0.0095, 0.2272, 0.2807], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.5964960716228234e-06 tensor([4.8408e-06, 2.6938e-01, 7.2866e-01, 3.5965e-06, 1.9563e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.823849172273185e-07 tensor([2.8238e-07, 3.9371e-03, 8.9783e-01, 1.1330e-04, 9.8122e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5131212194319232e-06 tensor([9.6315e-01, 2.1494e-02, 1.5131e-06, 1.5324e-02, 3.5132e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2641411331060226e-06 tensor([1.2641e-06, 3.6440e-05, 1.7025e-03, 1.9926e-02, 9.7833e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7360703152835555e-10 tensor([9.5280e-01, 3.9434e-05, 1.7361e-10, 4.7156e-02, 7.5877e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0031598960049450397 tensor([0.0032, 0.5921, 0.2665, 0.0086, 0.1297], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2336714689809014e-06 tensor([9.6779e-01, 1.8368e-02, 1.2337e-06, 1.3811e-02, 3.1012e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015708575025200844 tensor([0.1057, 0.3636, 0.0157, 0.2263, 0.2887], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0572439041188773e-07 tensor([2.0572e-07, 1.8525e-06, 2.6985e-04, 3.7564e-02, 9.6216e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6166465499622973e-08 tensor([9.7603e-01, 8.4086e-04, 1.6166e-08, 2.3119e-02, 6.4332e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.5546150936861522e-05 tensor([1.7747e-04, 7.6647e-01, 2.3081e-01, 2.5546e-05, 2.5232e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001987291732802987 tensor([0.0059, 0.0061, 0.0020, 0.6236, 0.3624], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3908467028377345e-06 tensor([2.3908e-06, 1.1329e-02, 9.1437e-01, 5.4870e-04, 7.3753e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.133788191509666e-05 tensor([1.1338e-05, 1.5371e-03, 4.8210e-02, 1.9026e-02, 9.3122e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.524138421402313e-05 tensor([8.3716e-01, 1.4274e-01, 4.5241e-05, 1.9707e-02, 3.5341e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.4021263584094186e-09 tensor([3.4021e-09, 3.7154e-03, 9.9308e-01, 2.2900e-07, 3.2069e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.513722385330766e-07 tensor([5.5137e-07, 2.9720e-05, 6.2944e-03, 1.5048e-02, 9.7863e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011808847775682807 tensor([0.0100, 0.9532, 0.0324, 0.0012, 0.0031], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01159485150128603 tensor([0.1206, 0.7176, 0.0116, 0.0860, 0.0643], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1498250351849038e-08 tensor([9.9524e-01, 2.6832e-03, 1.1498e-08, 2.0734e-03, 6.1239e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001484841777710244 tensor([2.6512e-01, 2.4849e-02, 1.4848e-04, 6.8884e-01, 2.1037e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003636397887021303 tensor([0.0039, 0.7933, 0.1637, 0.0036, 0.0354], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2092023382592743e-07 tensor([9.4728e-01, 5.2612e-02, 1.2092e-07, 1.0793e-04, 2.7102e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.959235532922321e-06 tensor([2.7806e-03, 2.5770e-05, 1.9592e-06, 9.9093e-01, 6.2659e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0801463662678543e-08 tensor([3.6949e-01, 9.8655e-05, 2.0801e-08, 6.3030e-01, 1.0760e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0275646999957644e-08 tensor([1.0276e-08, 1.1710e-03, 9.8058e-01, 6.5140e-06, 1.8240e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.880170152901428e-08 tensor([1.8802e-08, 1.3883e-03, 9.6963e-01, 1.2021e-05, 2.8966e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.263497885403922e-06 tensor([4.2635e-06, 1.8908e-05, 1.5445e-03, 2.0310e-01, 7.9533e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.477380934986286e-05 tensor([3.1191e-04, 4.4774e-05, 4.5413e-05, 8.6568e-01, 1.3392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.901014977714112e-08 tensor([9.9240e-01, 3.7858e-03, 3.9010e-08, 3.8140e-03, 3.7779e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.264176815624523e-07 tensor([2.7995e-04, 9.9211e-01, 7.5845e-03, 7.2642e-07, 2.3182e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.022172899771249e-08 tensor([5.0222e-08, 4.6015e-02, 9.5351e-01, 1.2132e-07, 4.7883e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.524679530324647e-06 tensor([2.5247e-06, 1.2909e-02, 8.0191e-01, 5.1068e-04, 1.8467e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.8799144452816847e-10 tensor([6.1531e-05, 7.1433e-09, 3.8799e-10, 9.9946e-01, 4.7780e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.1343034585925786e-14 tensor([4.6198e-01, 5.3719e-08, 3.1343e-14, 5.3802e-01, 4.0911e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.618130439368542e-06 tensor([1.6167e-05, 4.2947e-01, 5.6826e-01, 7.6181e-06, 2.2392e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.122538601019187e-06 tensor([3.1225e-06, 3.4148e-04, 2.1866e-02, 1.0891e-02, 9.6690e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.16654516444126e-10 tensor([4.1665e-10, 2.7091e-04, 9.8805e-01, 9.7129e-07, 1.1681e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010375697165727615 tensor([0.5870, 0.3530, 0.0010, 0.0564, 0.0026], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.696813576427928e-10 tensor([6.4410e-01, 2.2839e-05, 5.6968e-10, 3.5586e-01, 9.8023e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.497828965417284e-07 tensor([5.1829e-05, 9.5240e-01, 4.7508e-02, 5.4978e-07, 4.0038e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.762337635355607e-09 tensor([4.7623e-09, 1.0365e-03, 9.8614e-01, 2.7051e-06, 1.2820e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000284008274320513 tensor([4.7323e-02, 6.7997e-03, 2.8401e-04, 9.0167e-01, 4.3925e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.64116523694247e-05 tensor([9.7183e-03, 9.8823e-01, 1.7916e-03, 6.6412e-05, 1.9615e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.105579876068077e-09 tensor([2.7246e-01, 2.5617e-05, 4.1056e-09, 7.2745e-01, 5.6943e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00020462798420339823 tensor([1.1404e-02, 9.8213e-01, 5.7003e-03, 2.0463e-04, 5.5885e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.671747847050938e-08 tensor([9.6717e-08, 1.1315e-02, 9.8081e-01, 2.9523e-06, 7.8732e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3960421913216123e-07 tensor([2.3960e-07, 8.4471e-02, 9.1451e-01, 4.4335e-07, 1.0144e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.1525891109349686e-08 tensor([9.5128e-05, 1.7329e-07, 5.1526e-08, 9.9470e-01, 5.2082e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.718325036234816e-11 tensor([9.9946e-01, 3.6756e-04, 4.7183e-11, 1.7095e-04, 1.0993e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.90344654180808e-06 tensor([4.4818e-03, 9.9380e-01, 1.6746e-03, 8.9034e-06, 3.7032e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.390074314462254e-06 tensor([1.1651e-05, 5.0450e-01, 4.9487e-01, 2.3901e-06, 6.1478e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007138009532354772 tensor([0.5999, 0.3147, 0.0007, 0.0797, 0.0049], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.0578818192880135e-06 tensor([1.5214e-02, 2.3839e-04, 6.0579e-06, 9.7543e-01, 9.1089e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.062216704848632e-12 tensor([9.9536e-01, 1.7069e-05, 5.0622e-12, 4.6226e-03, 4.1995e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.104201045469381e-05 tensor([1.5562e-03, 9.7478e-01, 2.2942e-02, 5.1042e-05, 6.6958e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.8027617433342726e-11 tensor([5.8028e-11, 6.2697e-05, 9.8345e-01, 8.0935e-07, 1.6487e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03631320223212242 tensor([0.0619, 0.3174, 0.0363, 0.3295, 0.2549], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004675126983784139 tensor([4.6751e-04, 6.4368e-04, 6.0964e-04, 2.7775e-01, 7.2053e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.4886119831535325e-08 tensor([7.6991e-03, 4.3546e-06, 2.4886e-08, 9.9150e-01, 7.9995e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7866303991809218e-09 tensor([9.9715e-01, 2.6139e-03, 1.7866e-09, 2.3297e-04, 8.4210e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9422675652313046e-05 tensor([9.0471e-02, 2.6986e-03, 2.9423e-05, 8.9077e-01, 1.6033e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9287180041137617e-06 tensor([2.9287e-06, 1.1846e-01, 8.7635e-01, 7.7293e-06, 5.1773e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.382551221875474e-05 tensor([3.3826e-05, 2.9466e-04, 3.7792e-03, 1.2694e-01, 8.6895e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2969862683842592e-10 tensor([4.6054e-01, 6.2950e-06, 1.2970e-10, 5.3945e-01, 5.4833e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.501332801330136e-06 tensor([1.2701e-04, 9.3331e-01, 6.6348e-02, 2.5013e-06, 2.1214e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3303631021699402e-05 tensor([1.3304e-05, 2.3937e-02, 4.9304e-01, 1.4146e-03, 4.8160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00017269376257900149 tensor([2.2231e-01, 7.7644e-01, 2.4502e-04, 8.3753e-04, 1.7269e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7849732358854453e-08 tensor([1.7850e-08, 5.4559e-07, 4.3359e-04, 1.0708e-02, 9.8886e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.0355479374865055e-12 tensor([9.9964e-01, 6.0220e-05, 4.0355e-12, 2.9745e-04, 3.6660e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3800633496430237e-05 tensor([6.0516e-02, 9.3925e-01, 1.8513e-04, 3.5425e-05, 1.3801e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.1924248527760426e-10 tensor([2.1924e-10, 1.0321e-04, 9.7388e-01, 2.1886e-06, 2.6012e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010965008288621902 tensor([0.0992, 0.8339, 0.0176, 0.0382, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0273819245630875e-07 tensor([3.3907e-04, 1.2133e-06, 2.0274e-07, 9.9136e-01, 8.2949e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1430890678454375e-09 tensor([6.1721e-01, 3.0480e-05, 1.1431e-09, 3.8275e-01, 1.4849e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.269105268875137e-06 tensor([6.9520e-04, 9.9558e-01, 3.7076e-03, 1.2691e-06, 2.0053e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00032456140615977347 tensor([7.4982e-04, 3.2456e-04, 4.4963e-04, 8.0746e-01, 1.9101e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00040552549762651324 tensor([5.7007e-02, 9.7219e-03, 4.0553e-04, 8.7132e-01, 6.1545e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9515521216817433e-06 tensor([9.1926e-01, 1.2361e-02, 1.9516e-06, 6.8159e-02, 2.1315e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2368077915991815e-11 tensor([9.9851e-01, 5.2816e-05, 1.2368e-11, 1.4341e-03, 2.9393e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00036751877632923424 tensor([9.7280e-04, 7.9648e-01, 1.9119e-01, 3.6752e-04, 1.0993e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7714854038786143e-05 tensor([1.7715e-05, 1.2066e-03, 3.0488e-02, 2.7801e-02, 9.4049e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.1234038589173e-06 tensor([8.1234e-06, 7.9797e-02, 9.0998e-01, 8.6376e-05, 1.0128e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006167687242850661 tensor([0.0008, 0.0006, 0.0008, 0.6150, 0.3828], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.266144829576714e-13 tensor([9.7972e-01, 2.3598e-06, 6.2661e-13, 2.0282e-02, 1.7522e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.6797462737704336e-07 tensor([2.9753e-03, 9.9691e-01, 1.1428e-04, 2.6797e-07, 6.9987e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.507485465576337e-09 tensor([9.5075e-09, 1.8690e-03, 9.9161e-01, 2.4399e-06, 6.5224e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.74804440070875e-05 tensor([3.7708e-02, 1.4537e-03, 3.7480e-05, 9.3449e-01, 2.6315e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1272317098919302e-05 tensor([1.6028e-03, 1.1474e-04, 2.1272e-05, 9.0508e-01, 9.3182e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1375319530770867e-07 tensor([1.0060e-04, 9.9531e-01, 4.5875e-03, 1.1375e-07, 5.0019e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.122089987708023e-06 tensor([1.0165e-04, 8.7744e-01, 1.2203e-01, 5.1221e-06, 4.2549e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.449665086918685e-07 tensor([3.4497e-07, 1.7926e-02, 9.7577e-01, 8.1776e-06, 6.2980e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.100956007529021e-07 tensor([3.1010e-07, 1.3594e-02, 9.7005e-01, 1.5907e-05, 1.6341e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0422964652766495e-08 tensor([2.0486e-04, 2.0427e-07, 2.0423e-08, 9.9526e-01, 4.5328e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00554337864741683 tensor([0.1282, 0.1469, 0.0055, 0.5584, 0.1609], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003129634424112737 tensor([1.0022e-02, 9.7937e-01, 9.1623e-03, 3.1296e-04, 1.1330e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.625763949661632e-08 tensor([4.6258e-08, 1.4180e-02, 9.8296e-01, 7.3635e-07, 2.8607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019865352660417557 tensor([0.0199, 0.0936, 0.0273, 0.5858, 0.2734], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.0120620067464188e-05 tensor([1.4956e-02, 9.8398e-01, 9.9288e-04, 3.0121e-05, 4.5842e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.751369477977278e-07 tensor([3.3805e-01, 9.2448e-04, 8.7514e-07, 6.5959e-01, 1.4295e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.898863950752457e-09 tensor([9.9346e-01, 1.3682e-03, 9.8989e-09, 5.1693e-03, 9.7931e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003244000254198909 tensor([3.2440e-04, 5.2901e-02, 2.2994e-01, 3.0395e-02, 6.8644e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.119474728118803e-07 tensor([8.1195e-07, 2.2671e-03, 5.1241e-01, 9.7873e-04, 4.8435e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0022500848426716e-06 tensor([9.7604e-04, 1.2366e-05, 2.0023e-06, 9.7634e-01, 2.2672e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.966058880498167e-05 tensor([6.0909e-03, 2.6979e-04, 1.9661e-05, 9.6084e-01, 3.2783e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.042239874659572e-06 tensor([2.3877e-03, 9.9397e-01, 3.5826e-03, 6.0422e-06, 5.3471e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3452931477142549e-11 tensor([1.3453e-11, 2.4263e-05, 9.6325e-01, 6.4538e-07, 3.6722e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.0304077348073406e-08 tensor([5.0304e-08, 1.7928e-03, 9.6864e-01, 3.3679e-05, 2.9530e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.13563123349536e-08 tensor([9.6773e-01, 8.3251e-04, 1.1356e-08, 3.1426e-02, 9.1598e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1757467888173778e-08 tensor([7.7778e-01, 2.6327e-04, 1.1757e-08, 2.2189e-01, 7.0848e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.1829872309808707e-07 tensor([4.0023e-05, 9.7218e-01, 2.7752e-02, 2.1830e-07, 2.4685e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.041335171540595e-08 tensor([5.0413e-08, 1.2858e-04, 1.6025e-01, 9.7520e-04, 8.3865e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012707257643342018 tensor([0.0127, 0.1836, 0.1365, 0.2998, 0.3674], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7901032808586024e-06 tensor([2.5166e-04, 6.1563e-06, 2.7901e-06, 9.3570e-01, 6.4035e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.55574414198945e-11 tensor([9.9675e-01, 9.1500e-05, 6.5557e-11, 3.1543e-03, 7.6523e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.648648973670788e-05 tensor([8.3225e-01, 6.5849e-02, 4.6486e-05, 1.0034e-01, 1.5135e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9117534722900018e-05 tensor([2.9118e-05, 4.7009e-03, 7.5517e-02, 1.2371e-02, 9.0738e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006305606570094824 tensor([0.2837, 0.4483, 0.0063, 0.2320, 0.0297], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.760668161907233e-06 tensor([2.0151e-03, 5.2180e-05, 4.7607e-06, 9.5810e-01, 3.9823e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [0], [0], [2], [0], [4], [0], [0], [2], [2], [2], [3], [0], [0], [0], [2], [3], [0], [2], [1], [2], [3], [0], [2], [1], [2], [3], [3], [2], [2], [0], [0], [0], [2], [2], [2], [3], [0], [0], [0], [2], [3], [0], [0], [0], [2], [3], [0], [2], [2], [2], [3], [2], [0], [2], [2], [3], [0], [2], [2], [2], [3], [0], [0], [0], [2], [3], [0], [0], [0], [2], [3], [2], [2], [2], [2], [3], [3], [0], [2], [2], [2], [2], [0], [2], [2], [0], [0], [0], [1], [2], [3], [0], [2], [0], [2], [3], [0], [2], [2], [4], [3], [3], [2], [2], [2], [3], [0], [0], [0], [4], [0], [2], [1], [0], [2], [3], [0], [0], [1], [2], [0], [0], [3], [2], [2], [2], [0], [0], [1], [2], [3], [0], [2], [0], [2], [0], [2], [2], [0], [2], [3], [2], [0], [0], [2], [0], [0], [3], [2], [2], [2], [3], [2], [2], [2], [0], [0], [0], [1], [2], [3], [0], [0], [2], [2], [3], [0], [0], [2], [2], [3], [0], [2], [3], [2], [3], [0], [0], [2], [2], [3], [3], [2], [2], [2], [3], [0], [2], [0], [2], [2], [2], [0], [0], [2], [3], [0], [4], [0], [2], [4], [0], [4], [2], [2], [3], [1], [2], [2], [2], [3], [0], [0], [1], [2], [3], [0], [2], [2], [3], [3], [0], [0], [2], [2], [3], [0], [0], [3], [2], [2], [0], [0], [2], [2], [3], [0], [0], [2], [2], [3], [0], [0], [2], [2], [2], [0], [2], [2]]\n",
      "[[0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [0], [0], [2], [0], [4], [0], [0], [2], [2], [2], [3], [0], [0], [0], [2], [3], [0], [2], [1], [2], [3], [0], [2], [1], [2], [3], [3], [2], [2], [0], [0], [0], [2], [2], [2], [3], [0], [0], [0], [2], [3], [0], [0], [0], [2], [3], [0], [2], [2], [2], [3], [2], [0], [2], [2], [3], [0], [2], [2], [2], [3], [0], [0], [0], [2], [3], [0], [0], [0], [2], [3], [2], [2], [2], [2], [3], [3], [0], [2], [2], [2], [2], [0], [2], [2], [0], [0], [0], [1], [2], [3], [0], [2], [0], [2], [3], [0], [2], [2], [4], [3], [3], [2], [2], [2], [3], [0], [0], [0], [4], [0], [2], [1], [0], [2], [3], [0], [0], [1], [2], [0], [0], [3], [2], [2], [2], [0], [0], [1], [2], [3], [0], [2], [0], [2], [0], [2], [2], [0], [2], [3], [2], [0], [0], [2], [0], [0], [3], [2], [2], [2], [3], [2], [2], [2], [0], [0], [0], [1], [2], [3], [0], [0], [2], [2], [3], [0], [0], [2], [2], [3], [0], [2], [3], [2], [3], [0], [0], [2], [2], [3], [3], [2], [2], [2], [3], [0], [2], [0], [2], [2], [2], [0], [0], [2], [3], [0], [4], [0], [2], [4], [0], [4], [2], [2], [3], [1], [2], [2], [2], [3], [0], [0], [1], [2], [3], [0], [2], [2], [3], [3], [0], [0], [2], [2], [3], [0], [0], [3], [2], [2], [0], [0], [2], [2], [3], [0], [0], [2], [2], [3], [0], [0], [2], [2], [2], [0], [2], [2]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004479447841644287  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0044794464111328125  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004479444503784179  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.00447944164276123  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004479436874389648  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004479431629180909  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0044794268608093265  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004479421615600586  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004479414463043213  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004479406833648681  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004479399681091309  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0044793920516967775  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004479383945465088  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.0044793753623962405  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004479366779327393  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0044793572425842285  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0044793481826782226  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004479339122772217  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004479329586029052  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004479320049285889  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 1.0818442888194113e-06 tensor([9.5296e-01, 4.1813e-05, 2.0548e-10, 4.6993e-02, 1.0818e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.8728003346477635e-06 tensor([6.0543e-07, 4.3335e-02, 9.5162e-01, 3.8728e-06, 5.0383e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.5610074771975633e-06 tensor([2.8289e-08, 4.4192e-03, 9.8905e-01, 2.5610e-06, 6.5253e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.6360550691985054e-09 tensor([9.8662e-01, 3.7232e-07, 1.4204e-14, 1.3379e-02, 1.6361e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000908960064407438 tensor([0.0007, 0.0009, 0.0010, 0.5191, 0.4783], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.242157734144712e-06 tensor([5.5590e-02, 9.4439e-01, 1.6532e-05, 2.2422e-06, 6.6824e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.5059458494069986e-05 tensor([1.6582e-05, 2.5513e-01, 7.4005e-01, 2.5059e-05, 4.7754e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004920203238725662 tensor([0.0008, 0.3479, 0.4490, 0.0049, 0.1974], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004795273707713932 tensor([8.4303e-01, 1.3947e-02, 7.1457e-06, 1.4253e-01, 4.7953e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008862626855261624 tensor([8.3521e-03, 8.8626e-04, 1.0710e-04, 9.1954e-01, 7.1118e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.282269962161081e-06 tensor([1.8966e-01, 7.2823e-06, 9.2154e-10, 8.1031e-01, 1.8226e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3365932975139003e-06 tensor([2.3366e-06, 4.5644e-01, 5.4318e-01, 3.2944e-07, 3.7815e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.6315651666664053e-06 tensor([8.4065e-09, 1.6454e-03, 9.8990e-01, 2.6316e-06, 8.4515e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00034777127439156175 tensor([1.0916e-07, 3.4777e-04, 4.7227e-01, 1.3419e-03, 5.2604e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.048304520547389984 tensor([0.0157, 0.6299, 0.0713, 0.0483, 0.2347], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.8358052784606116e-06 tensor([8.3701e-01, 1.4278e-05, 9.5285e-11, 1.6297e-01, 1.8358e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00017386376566719264 tensor([1.3881e-03, 9.8435e-01, 1.4078e-02, 1.3032e-05, 1.7386e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.834900705143809e-05 tensor([2.2316e-09, 6.8349e-05, 4.8450e-01, 8.8566e-05, 5.1534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001270689070224762 tensor([0.5501, 0.3971, 0.0010, 0.0504, 0.0013], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.39496378324111e-05 tensor([4.7779e-05, 5.0941e-06, 1.3950e-05, 8.3643e-01, 1.6350e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.3557665567986987e-07 tensor([1.4047e-01, 1.3558e-07, 1.3229e-12, 8.5953e-01, 9.0723e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.6517119497148087e-06 tensor([1.5237e-02, 9.8465e-01, 1.0697e-04, 3.2036e-06, 3.6517e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011088663013651967 tensor([5.9870e-06, 1.4830e-02, 7.2228e-01, 1.1089e-03, 2.6177e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004177276045084 tensor([5.8608e-01, 5.3018e-02, 1.1470e-04, 3.5661e-01, 4.1773e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.144431014514339e-08 tensor([5.1444e-08, 5.4372e-09, 1.2648e-06, 4.4115e-01, 5.5885e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.494289118563756e-05 tensor([3.4505e-03, 6.4943e-05, 5.7863e-06, 9.8533e-01, 1.1152e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.093945570573851e-06 tensor([3.3173e-03, 9.9654e-01, 1.4294e-04, 3.8996e-07, 1.0939e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.138034455536399e-06 tensor([4.1380e-06, 3.3738e-01, 6.6190e-01, 1.3668e-06, 7.1209e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00012678041821345687 tensor([2.4840e-03, 1.2678e-04, 2.9974e-05, 9.6504e-01, 3.2315e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.25489374026256e-08 tensor([1.0880e-04, 9.2549e-08, 1.4148e-08, 9.9736e-01, 2.5352e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004296882834751159 tensor([3.0114e-04, 4.2969e-04, 6.7829e-04, 3.0903e-01, 6.8956e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002820683585014194 tensor([3.9392e-05, 1.5288e-01, 7.9947e-01, 2.8207e-04, 4.7324e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001741875195875764 tensor([0.0009, 0.5719, 0.3855, 0.0017, 0.0399], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022803544998168945 tensor([0.5035, 0.2594, 0.0013, 0.2131, 0.0228], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003053584659937769 tensor([3.4997e-02, 3.0536e-04, 2.9453e-06, 9.5677e-01, 7.9247e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.9427245206316002e-05 tensor([9.3890e-01, 4.3959e-04, 8.5206e-09, 6.0645e-02, 1.9427e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.695793338920339e-06 tensor([2.7453e-03, 9.9670e-01, 5.4932e-04, 1.0543e-06, 3.6958e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002812459715642035 tensor([4.7399e-08, 2.8125e-04, 3.6546e-01, 4.0865e-04, 6.3385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03947020694613457 tensor([0.0336, 0.8095, 0.0640, 0.0395, 0.0535], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019245726987719536 tensor([0.0009, 0.0192, 0.0531, 0.2069, 0.7199], grad_fn=<SoftmaxBackward0>)\n",
      "4 2.8239054472578573e-07 tensor([4.8310e-01, 3.5541e-07, 7.4526e-13, 5.1690e-01, 2.8239e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001054746680893004 tensor([2.4503e-03, 9.7607e-01, 2.0336e-02, 8.4455e-05, 1.0547e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3032490642217454e-07 tensor([8.0403e-10, 1.4687e-03, 9.9670e-01, 1.3032e-07, 1.8331e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.847475901849975e-07 tensor([4.2592e-07, 8.9527e-02, 9.0969e-01, 6.8475e-07, 7.8082e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.241637950348377e-07 tensor([4.0772e-09, 5.2416e-07, 9.4200e-04, 2.8968e-03, 9.9616e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.564555350341834e-05 tensor([9.7057e-01, 2.5592e-02, 9.4021e-07, 3.8221e-03, 1.5646e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002416588831692934 tensor([1.1312e-03, 9.8481e-01, 1.3806e-02, 1.4358e-05, 2.4166e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.163274858612567e-05 tensor([6.0037e-08, 1.5854e-03, 8.9390e-01, 7.1633e-05, 1.0444e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.994104422919918e-06 tensor([9.4390e-01, 5.5460e-02, 5.7466e-07, 6.4195e-04, 1.9941e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.915381840779446e-06 tensor([7.9887e-03, 8.9154e-06, 7.0843e-08, 9.8955e-01, 2.4513e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.163139912336192e-07 tensor([9.8268e-01, 1.7115e-02, 3.4984e-08, 2.0466e-04, 7.1631e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.66432427149266e-05 tensor([5.6643e-05, 7.0358e-01, 2.9472e-01, 9.8655e-06, 1.6366e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.4867818612838164e-05 tensor([7.1985e-01, 2.7844e-01, 2.3441e-05, 1.6533e-03, 3.4868e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.911599484737962e-05 tensor([1.3706e-08, 6.9116e-05, 3.2368e-01, 7.9885e-04, 6.7546e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9278181753179524e-06 tensor([3.2210e-04, 1.9278e-06, 3.6308e-07, 9.8187e-01, 1.7807e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.6835598632146684e-08 tensor([9.9688e-01, 4.0646e-05, 1.5789e-11, 3.0826e-03, 2.6836e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015342327766120434 tensor([0.0153, 0.8566, 0.0876, 0.0108, 0.0298], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1779491870811398e-07 tensor([7.9377e-12, 4.0160e-05, 9.9336e-01, 1.1779e-07, 6.6016e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.604318746714853e-05 tensor([3.3354e-02, 4.6043e-05, 2.7029e-07, 9.6566e-01, 9.3796e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015790792182087898 tensor([0.0158, 0.0190, 0.0015, 0.5657, 0.3980], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.5070861486019567e-06 tensor([9.9284e-01, 1.2378e-03, 7.9624e-09, 5.9224e-03, 3.5071e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8380635083303787e-05 tensor([1.8381e-05, 8.0564e-01, 1.9399e-01, 1.0464e-06, 3.4743e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.7471862179263553e-07 tensor([1.7846e-11, 5.9858e-05, 9.9321e-01, 1.7472e-07, 6.7345e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00329417223110795 tensor([3.4101e-05, 3.9548e-02, 8.0913e-01, 3.2942e-03, 1.4800e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.4763641451718286e-05 tensor([5.5641e-06, 3.4764e-05, 9.7003e-04, 1.3142e-01, 8.6757e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007322757970541716 tensor([3.9642e-02, 7.3228e-03, 3.0590e-04, 9.0563e-01, 4.7104e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003291702305432409 tensor([9.8038e-04, 9.7174e-01, 2.6935e-02, 1.9433e-05, 3.2917e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.503693601989653e-05 tensor([4.3340e-07, 1.5037e-05, 3.0285e-03, 1.8394e-02, 9.7856e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.8818243916030042e-05 tensor([3.1606e-06, 1.8818e-05, 2.2042e-03, 1.6745e-01, 8.3033e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.626432589953765e-05 tensor([9.1308e-07, 2.8098e-02, 9.2703e-01, 3.6264e-05, 4.4837e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2845233641201048e-06 tensor([1.3259e-03, 1.2845e-06, 5.0378e-08, 9.9721e-01, 1.4634e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.175642268615775e-05 tensor([1.9909e-02, 9.7945e-01, 5.8607e-04, 2.6834e-05, 3.1756e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4773275324841961e-05 tensor([1.8511e-03, 1.4773e-05, 1.2250e-06, 9.7901e-01, 1.9125e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008187397383153439 tensor([0.4438, 0.4137, 0.0024, 0.1319, 0.0082], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.85121202270966e-05 tensor([9.3401e-01, 6.2429e-02, 1.9161e-06, 3.5181e-03, 3.8512e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.59793934435038e-08 tensor([9.8466e-01, 9.5435e-06, 5.1477e-12, 1.5333e-02, 7.5979e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9892917154938914e-05 tensor([2.9893e-05, 3.9701e-01, 5.9915e-01, 1.8292e-05, 3.7885e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0444473730331083e-07 tensor([1.0444e-07, 1.0843e-01, 8.9134e-01, 8.1276e-08, 2.3207e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03458790481090546 tensor([0.0043, 0.0346, 0.0399, 0.3635, 0.5578], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11527144908905029 tensor([0.1950, 0.2604, 0.0050, 0.4243, 0.1153], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.525093340314925e-06 tensor([9.7065e-01, 7.3359e-04, 1.5045e-08, 2.8609e-02, 6.5251e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3644978025695309e-05 tensor([8.9052e-01, 1.0829e-01, 2.3920e-06, 1.1752e-03, 1.3645e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.14693233021535e-05 tensor([3.2018e-01, 7.1469e-05, 1.4148e-08, 6.7962e-01, 1.3442e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.316417370224372e-05 tensor([3.1491e-08, 7.5067e-04, 8.8098e-01, 8.3164e-05, 1.1819e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.038685910403728485 tensor([0.2572, 0.6115, 0.0045, 0.0882, 0.0387], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002875163045246154 tensor([3.5881e-01, 2.8752e-04, 8.6154e-08, 6.4061e-01, 2.9835e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.413637503195787e-06 tensor([3.6039e-07, 3.3466e-02, 9.6353e-01, 2.4136e-06, 3.0048e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01790693774819374 tensor([0.0016, 0.2950, 0.3779, 0.0179, 0.3076], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020926326513290405 tensor([4.0608e-04, 1.0425e-01, 5.0075e-01, 2.0926e-02, 3.7367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.188549832586432e-07 tensor([6.1885e-07, 1.0473e-07, 5.1698e-06, 3.7577e-01, 6.2422e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.0526569333156885e-09 tensor([9.9558e-01, 1.1488e-06, 3.6172e-14, 4.4155e-03, 2.0527e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.2590195410666638e-06 tensor([2.2801e-03, 9.9751e-01, 2.1263e-04, 3.6490e-07, 1.2590e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.529807533046551e-08 tensor([2.0498e-11, 1.1031e-04, 9.9567e-01, 7.5298e-08, 4.2190e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3474697198034846e-06 tensor([9.7552e-01, 2.0207e-04, 1.8054e-09, 2.4281e-02, 1.3475e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.993946127271556e-08 tensor([3.8105e-09, 7.9939e-08, 1.2927e-04, 7.5780e-03, 9.9229e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.545787651797582e-07 tensor([8.7873e-01, 4.5455e-06, 8.7869e-12, 1.2127e-01, 2.5458e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.168651452753693e-05 tensor([3.5069e-03, 9.9375e-01, 2.6755e-03, 1.0275e-05, 6.1687e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.007552656228654e-06 tensor([2.3588e-08, 6.0076e-06, 1.0864e-02, 3.6923e-03, 9.8544e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011093291686847806 tensor([8.3629e-01, 3.2396e-02, 2.6946e-05, 1.3018e-01, 1.1093e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007538344711065292 tensor([0.0075, 0.0082, 0.0019, 0.6906, 0.2918], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006899750325828791 tensor([0.1321, 0.8378, 0.0069, 0.0171, 0.0061], grad_fn=<SoftmaxBackward0>)\n",
      "4 2.935550583060831e-06 tensor([7.8486e-03, 9.9195e-01, 1.9896e-04, 1.6564e-06, 2.9356e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.893683894304559e-05 tensor([6.8937e-05, 5.8867e-01, 4.0842e-01, 2.7614e-05, 2.8104e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0019130597356706858 tensor([7.6750e-01, 1.2966e-01, 1.5257e-04, 1.0078e-01, 1.9131e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.6849086225411156e-06 tensor([4.5559e-04, 2.6849e-06, 3.2952e-07, 9.8249e-01, 1.7055e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.6928089507928235e-07 tensor([5.7087e-03, 1.6928e-07, 1.5454e-10, 9.9423e-01, 6.0395e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.5623636500095017e-05 tensor([6.2655e-04, 9.9622e-01, 3.1414e-03, 8.9498e-07, 1.5624e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01255933940410614 tensor([0.0009, 0.2356, 0.3629, 0.0126, 0.3880], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003164740919601172 tensor([3.6291e-05, 3.1647e-04, 8.8489e-03, 2.2399e-01, 7.6681e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009410717175342143 tensor([4.1535e-04, 9.4107e-04, 1.8133e-03, 3.6688e-01, 6.2995e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.232963354093954e-05 tensor([1.6499e-01, 8.3494e-01, 3.2330e-05, 3.5534e-05, 4.0616e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.373584230663255e-05 tensor([1.2841e-06, 1.7056e-02, 9.3149e-01, 7.3736e-05, 5.1380e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007792179239913821 tensor([1.3811e-03, 7.7922e-04, 6.9965e-04, 7.4925e-01, 2.4789e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.9868416479293955e-07 tensor([3.9868e-07, 3.4323e-07, 6.0448e-05, 2.4792e-01, 7.5202e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0041023013181984425 tensor([2.0407e-04, 4.1023e-03, 3.0012e-02, 1.3843e-01, 8.2725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.15891446941896e-07 tensor([9.8018e-01, 2.5538e-05, 3.8884e-11, 1.9790e-02, 2.1589e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.732760812155902e-05 tensor([6.7328e-05, 7.3504e-01, 2.6356e-01, 1.0694e-05, 1.3192e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.4145534161543765e-07 tensor([1.1831e-10, 1.5466e-04, 9.9260e-01, 4.4146e-07, 7.2491e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5845256712054834e-05 tensor([1.5115e-06, 1.5845e-05, 2.3329e-03, 1.0417e-01, 8.9348e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.251781011698768e-05 tensor([1.5833e-04, 4.3541e-05, 9.2518e-05, 6.7730e-01, 3.2240e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.082596893160371e-06 tensor([7.7215e-01, 2.2773e-01, 1.4364e-06, 1.1830e-04, 2.0826e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0020835690665990114 tensor([0.0007, 0.5039, 0.4121, 0.0021, 0.0812], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9933204953304084e-07 tensor([7.9573e-11, 1.7528e-04, 9.9592e-01, 1.9933e-07, 3.9042e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006958706653676927 tensor([3.5219e-03, 9.8013e-01, 1.5538e-02, 1.1136e-04, 6.9587e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005046717706136405 tensor([5.9097e-01, 1.1305e-03, 3.2757e-07, 4.0739e-01, 5.0467e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003208898415323347 tensor([2.4065e-02, 3.2089e-04, 6.8191e-06, 9.7096e-01, 4.6440e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.4159405509417411e-05 tensor([9.8380e-01, 1.1900e-02, 2.0430e-07, 4.2876e-03, 1.4159e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.455060530337505e-05 tensor([2.7819e-07, 2.4551e-05, 8.4620e-03, 8.8584e-03, 9.8265e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.404757525364403e-06 tensor([5.4582e-08, 7.4048e-06, 8.1555e-03, 8.1692e-03, 9.8367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.872192166909372e-07 tensor([3.6448e-06, 5.9921e-08, 3.8722e-07, 8.9954e-01, 1.0046e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11782792955636978 tensor([0.1178, 0.3564, 0.0096, 0.2295, 0.2867], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.7422427087440155e-06 tensor([4.7422e-06, 2.6510e-01, 7.3291e-01, 3.6184e-06, 1.9870e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011481767432997003 tensor([2.7957e-07, 3.8792e-03, 8.9638e-01, 1.1482e-04, 9.9629e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.6606506910175085e-05 tensor([9.6249e-01, 2.1700e-02, 1.5662e-06, 1.5770e-02, 3.6607e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.54625990439672e-05 tensor([1.2237e-06, 3.5463e-05, 1.6888e-03, 1.9735e-02, 9.7854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.852339649616624e-07 tensor([9.5172e-01, 3.9739e-05, 1.7885e-10, 4.8244e-02, 7.8523e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008742978796362877 tensor([0.0031, 0.5845, 0.2703, 0.0087, 0.1334], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.2259027648251504e-05 tensor([9.6724e-01, 1.8513e-02, 1.2734e-06, 1.4210e-02, 3.2259e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10364578664302826 tensor([0.1036, 0.3575, 0.0158, 0.2286, 0.2944], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.8166526842833264e-06 tensor([2.0080e-07, 1.8167e-06, 2.6819e-04, 3.7285e-02, 9.6245e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.710190518788295e-06 tensor([9.7532e-01, 8.4783e-04, 1.6730e-08, 2.3825e-02, 6.7102e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017523477436043322 tensor([1.7523e-04, 7.6193e-01, 2.3528e-01, 2.5964e-05, 2.5950e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005679328925907612 tensor([0.0057, 0.0060, 0.0020, 0.6205, 0.3659], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005553571390919387 tensor([2.3656e-06, 1.1168e-02, 9.1338e-01, 5.5536e-04, 7.4893e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014848210848867893 tensor([1.0895e-05, 1.4848e-03, 4.7679e-02, 1.8835e-02, 9.3199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003692778409458697 tensor([8.3558e-01, 1.4368e-01, 4.6792e-05, 2.0332e-02, 3.6928e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3133880233672244e-07 tensor([3.3656e-09, 3.6684e-03, 9.9308e-01, 2.3134e-07, 3.2499e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.9055692721158266e-05 tensor([5.3865e-07, 2.9056e-05, 6.2304e-03, 1.4977e-02, 9.7876e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032244352623820305 tensor([0.0100, 0.9524, 0.0332, 0.0012, 0.0032], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06652935594320297 tensor([0.1195, 0.7142, 0.0118, 0.0880, 0.0665], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.282604658736091e-07 tensor([9.9519e-01, 2.6962e-03, 1.1745e-08, 2.1128e-03, 6.2826e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.021566692739725113 tensor([2.5907e-01, 2.4575e-02, 1.5095e-04, 6.9464e-01, 2.1567e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0038780856411904097 tensor([0.0039, 0.7887, 0.1671, 0.0037, 0.0366], grad_fn=<SoftmaxBackward0>)\n",
      "4 2.768156548427214e-07 tensor([9.4716e-01, 5.2728e-02, 1.2287e-07, 1.0966e-04, 2.7682e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.5356473997817375e-05 tensor([2.7051e-03, 2.5356e-05, 1.9749e-06, 9.9090e-01, 6.3629e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.809578477870673e-05 tensor([3.6278e-01, 9.8096e-05, 2.1236e-08, 6.3701e-01, 1.1060e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.599952484975802e-06 tensor([1.0196e-08, 1.1573e-03, 9.8032e-01, 6.6000e-06, 1.8512e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2228039850015193e-05 tensor([1.8716e-08, 1.3733e-03, 9.6913e-01, 1.2228e-05, 2.9480e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.852526474976912e-05 tensor([4.1748e-06, 1.8525e-05, 1.5284e-03, 2.0222e-01, 7.9623e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.578921289066784e-05 tensor([3.0280e-04, 4.4040e-05, 4.5789e-05, 8.6365e-01, 1.3595e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.8916095945751294e-06 tensor([9.9229e-01, 3.8066e-03, 3.9958e-08, 3.8976e-03, 3.8916e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.37064541579457e-05 tensor([2.7861e-04, 9.9198e-01, 7.7176e-03, 7.3728e-07, 2.3706e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2233677182393876e-07 tensor([4.9576e-08, 4.5390e-02, 9.5413e-01, 1.2234e-07, 4.8446e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005159311112947762 tensor([2.4790e-06, 1.2635e-02, 7.9952e-01, 5.1593e-04, 1.8733e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.0968124710191205e-09 tensor([6.0184e-05, 7.0968e-09, 3.9417e-10, 9.9945e-01, 4.8687e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.192531122271248e-08 tensor([4.5652e-01, 5.3650e-08, 3.1961e-14, 5.4348e-01, 4.1925e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.588745362823829e-05 tensor([1.5887e-05, 4.2405e-01, 5.7364e-01, 7.6907e-06, 2.2822e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00033253116998821497 tensor([3.0422e-06, 3.3253e-04, 2.1605e-02, 1.0846e-02, 9.6721e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.835246146394638e-07 tensor([4.1355e-10, 2.6789e-04, 9.8788e-01, 9.8352e-07, 1.1847e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002668227069079876 tensor([0.5846, 0.3538, 0.0011, 0.0579, 0.0027], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.0175609531870577e-05 tensor([6.3682e-01, 2.2888e-05, 5.8748e-10, 3.6315e-01, 1.0176e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.092108065378852e-05 tensor([5.1542e-05, 9.5159e-01, 4.8313e-02, 5.5766e-07, 4.0921e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.735482894422603e-06 tensor([4.7213e-09, 1.0246e-03, 9.8598e-01, 2.7355e-06, 1.2990e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006646779831498861 tensor([4.5951e-02, 6.6468e-03, 2.8437e-04, 9.0261e-01, 4.4503e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00020313549612183124 tensor([9.6605e-03, 9.8823e-01, 1.8363e-03, 6.8062e-05, 2.0314e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.543495429563336e-05 tensor([2.6697e-01, 2.5435e-05, 4.1826e-09, 7.3294e-01, 5.8347e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000579505693167448 tensor([1.1337e-02, 9.8203e-01, 5.8473e-03, 2.1002e-04, 5.7951e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.9859008918720065e-06 tensor([9.5708e-08, 1.1167e-02, 9.8085e-01, 2.9859e-06, 7.9821e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.466951395443175e-07 tensor([2.3607e-07, 8.3228e-02, 9.1574e-01, 4.4670e-07, 1.0268e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.7190831158586661e-07 tensor([9.2967e-05, 1.7191e-07, 5.2279e-08, 9.9460e-01, 5.3025e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1270161692777947e-08 tensor([9.9946e-01, 3.6928e-04, 4.8149e-11, 1.7396e-04, 1.1270e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.803779691224918e-05 tensor([4.4516e-03, 9.9379e-01, 1.7111e-03, 9.0577e-06, 3.8038e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.150919524661731e-05 tensor([1.1509e-05, 4.9981e-01, 4.9955e-01, 2.4162e-06, 6.2616e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005048085935413837 tensor([0.5969, 0.3154, 0.0007, 0.0819, 0.0050], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002341901999898255 tensor([1.4766e-02, 2.3419e-04, 6.1115e-06, 9.7573e-01, 9.2634e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.313382362397533e-08 tensor([9.9528e-01, 1.7186e-05, 5.1834e-12, 4.7063e-03, 4.3134e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006929300143383443 tensor([1.5465e-03, 9.7420e-01, 2.3512e-02, 5.2292e-05, 6.9293e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.185085675904702e-07 tensor([5.7977e-11, 6.2400e-05, 9.8324e-01, 8.1851e-07, 1.6694e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.060614410787820816 tensor([0.0606, 0.3112, 0.0364, 0.3325, 0.2593], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006053815013729036 tensor([4.4950e-04, 6.2349e-04, 6.0538e-04, 2.7529e-01, 7.2303e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.290021934139077e-06 tensor([7.4822e-03, 4.2900e-06, 2.5176e-08, 9.9170e-01, 8.1405e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.622647840184072e-08 tensor([9.9714e-01, 2.6235e-03, 1.8211e-09, 2.3695e-04, 8.6226e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0026542036794126034 tensor([8.8074e-02, 2.6542e-03, 2.9672e-05, 8.9294e-01, 1.6307e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.77324748924002e-06 tensor([2.8699e-06, 1.1628e-01, 8.7847e-01, 7.7732e-06, 5.2428e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002862920518964529 tensor([3.2670e-05, 2.8629e-04, 3.7452e-03, 1.2571e-01, 8.7023e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.649930244544521e-06 tensor([4.5332e-01, 6.2796e-06, 1.3289e-10, 5.4667e-01, 5.6499e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012598471948876977 tensor([1.2598e-04, 9.3190e-01, 6.7755e-02, 2.5452e-06, 2.1805e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014166595647111535 tensor([1.2954e-05, 2.3317e-02, 4.9021e-01, 1.4167e-03, 4.8504e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002502882853150368 tensor([2.2138e-01, 7.7733e-01, 2.5029e-04, 8.5469e-04, 1.7773e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.374911324906861e-07 tensor([1.7533e-08, 5.3749e-07, 4.3089e-04, 1.0648e-02, 9.8892e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.759390576618671e-09 tensor([9.9964e-01, 6.0476e-05, 4.1170e-12, 3.0299e-04, 3.7594e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.593842848204076e-05 tensor([6.0291e-02, 9.3947e-01, 1.8810e-04, 3.5938e-05, 1.4091e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.2177268874656875e-06 tensor([2.1904e-10, 1.0260e-04, 9.7350e-01, 2.2177e-06, 2.6392e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017995040863752365 tensor([0.0989, 0.8325, 0.0180, 0.0393, 0.0113], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1996014563919744e-06 tensor([3.3022e-04, 1.1996e-06, 2.0554e-07, 9.9122e-01, 8.4523e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.535931005491875e-05 tensor([6.1023e-01, 3.0512e-05, 1.1749e-09, 3.8972e-01, 1.5359e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.0537869204417802e-05 tensor([6.9138e-04, 9.9551e-01, 3.7784e-03, 1.2893e-06, 2.0538e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004513841704465449 tensor([7.2762e-04, 3.1826e-04, 4.5138e-04, 8.0506e-01, 1.9344e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009518875740468502 tensor([5.5408e-02, 9.5189e-03, 4.0657e-04, 8.7229e-01, 6.2377e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00022360842558555305 tensor([9.1693e-01, 1.2483e-02, 2.0303e-06, 7.0364e-02, 2.2361e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.0164336806137726e-08 tensor([9.9849e-01, 5.3051e-05, 1.2627e-11, 1.4612e-03, 3.0164e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009629997657611966 tensor([9.6300e-04, 7.9204e-01, 1.9524e-01, 3.7643e-04, 1.1381e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011687858495861292 tensor([1.7111e-05, 1.1688e-03, 3.0116e-02, 2.7577e-02, 9.4112e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.724240615265444e-05 tensor([8.0011e-06, 7.8560e-02, 9.1107e-01, 8.7242e-05, 1.0278e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007446336094290018 tensor([7.4463e-04, 6.0085e-04, 7.9525e-04, 6.1185e-01, 3.8601e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.7968702081816446e-08 tensor([9.7939e-01, 2.3776e-06, 6.4145e-13, 2.0605e-02, 1.7969e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.133588155738835e-07 tensor([2.9615e-03, 9.9692e-01, 1.1611e-04, 2.7137e-07, 7.1336e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.4685068638063967e-06 tensor([9.4294e-09, 1.8476e-03, 9.9154e-01, 2.4685e-06, 6.6106e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014212438836693764 tensor([3.6618e-02, 1.4212e-03, 3.7527e-05, 9.3527e-01, 2.6649e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00011256679863436148 tensor([1.5528e-03, 1.1257e-04, 2.1437e-05, 9.0371e-01, 9.4600e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.1104238991683815e-06 tensor([1.0009e-04, 9.9523e-01, 4.6661e-03, 1.1535e-07, 5.1104e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010076801845571026 tensor([1.0077e-04, 8.7486e-01, 1.2460e-01, 5.2203e-06, 4.3812e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.261355105787516e-06 tensor([3.4003e-07, 1.7638e-02, 9.7597e-01, 8.2614e-06, 6.3826e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.6147869246196933e-05 tensor([3.0716e-07, 1.3402e-02, 9.6996e-01, 1.6148e-05, 1.6622e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.0230331188031414e-07 tensor([2.0000e-04, 2.0230e-07, 2.0699e-08, 9.9519e-01, 4.6125e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1250016838312149 tensor([0.1250, 0.1444, 0.0056, 0.5612, 0.1638], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011738520115613937 tensor([9.9714e-03, 9.7915e-01, 9.3871e-03, 3.2122e-04, 1.1739e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.433857263094978e-07 tensor([4.5792e-08, 1.4007e-02, 9.8309e-01, 7.4339e-07, 2.8968e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.027394382283091545 tensor([0.0192, 0.0915, 0.0274, 0.5850, 0.2768], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.704882303485647e-05 tensor([1.4845e-02, 9.8406e-01, 1.0140e-03, 3.0599e-05, 4.7049e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009159725741483271 tensor([3.3155e-01, 9.1597e-04, 8.8918e-07, 6.6607e-01, 1.4647e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.0059692385766539e-06 tensor([9.9335e-01, 1.3750e-03, 1.0122e-08, 5.2720e-03, 1.0060e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030224060639739037 tensor([3.1343e-04, 5.1355e-02, 2.2827e-01, 3.0224e-02, 6.8984e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009830871131271124 tensor([7.9617e-07, 2.2176e-03, 5.0878e-01, 9.8309e-04, 4.8802e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2179511031717993e-05 tensor([9.4949e-04, 1.2180e-05, 2.0206e-06, 9.7599e-01, 2.3042e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00026503775734454393 tensor([5.9124e-03, 2.6504e-04, 1.9820e-05, 9.6050e-01, 3.3303e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.479486571857706e-05 tensor([2.3742e-03, 9.9391e-01, 3.6522e-03, 6.1395e-06, 5.4795e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.545215569531138e-07 tensor([1.3467e-11, 2.4151e-05, 9.6272e-01, 6.5452e-07, 3.7255e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.4263906854903325e-05 tensor([5.0104e-08, 1.7739e-03, 9.6813e-01, 3.4264e-05, 3.0058e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.61041405389551e-06 tensor([9.6668e-01, 8.4077e-04, 1.1807e-08, 3.2470e-02, 9.6104e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.402039045700803e-05 tensor([7.7181e-01, 2.6445e-04, 1.2170e-08, 2.2785e-01, 7.4020e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.525333547964692e-05 tensor([3.9801e-05, 9.7169e-01, 2.8248e-02, 2.2155e-07, 2.5253e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00012586730008479208 tensor([4.9331e-08, 1.2587e-04, 1.5880e-01, 9.7240e-04, 8.4011e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13590635359287262 tensor([0.0124, 0.1792, 0.1359, 0.3010, 0.3715], grad_fn=<SoftmaxBackward0>)\n",
      "1 6.065082743589301e-06 tensor([2.4454e-04, 6.0651e-06, 2.8193e-06, 9.3465e-01, 6.5095e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.862173845296638e-08 tensor([9.9669e-01, 9.1892e-05, 6.6977e-11, 3.2173e-03, 7.8622e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015823043650016189 tensor([8.2873e-01, 6.6274e-02, 4.8141e-05, 1.0336e-01, 1.5823e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004554076585918665 tensor([2.8097e-05, 4.5541e-03, 7.4729e-02, 1.2269e-02, 9.0842e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03061625361442566 tensor([0.2807, 0.4453, 0.0064, 0.2369, 0.0306], grad_fn=<SoftmaxBackward0>)\n",
      "1 5.128276825416833e-05 tensor([1.9567e-03, 5.1283e-05, 4.7998e-06, 9.5754e-01, 4.0444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [0, 3], [0, 3], [2, 4], [0, 1], [4, 3], [0, 3], [0, 3], [2, 4], [2, 1], [2, 1], [3, 0], [0, 3], [0, 1], [0, 3], [2, 4], [3, 4], [0, 1], [2, 4], [1, 2], [2, 1], [3, 4], [0, 3], [2, 4], [1, 0], [2, 1], [3, 4], [3, 0], [2, 1], [2, 1], [0, 1], [0, 3], [0, 3], [2, 4], [2, 1], [2, 4], [3, 4], [0, 1], [0, 3], [0, 1], [2, 4], [3, 4], [0, 3], [0, 3], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [2, 1], [2, 4], [3, 0], [2, 4], [0, 1], [2, 1], [2, 4], [3, 0], [0, 3], [2, 1], [2, 0], [2, 4], [3, 0], [0, 3], [0, 3], [0, 1], [2, 1], [3, 4], [0, 1], [0, 1], [0, 3], [2, 1], [3, 4], [2, 1], [2, 4], [2, 4], [2, 4], [3, 0], [3, 0], [0, 1], [2, 4], [2, 4], [2, 4], [2, 1], [0, 3], [2, 4], [2, 1], [0, 3], [0, 3], [0, 3], [1, 0], [2, 4], [3, 4], [0, 3], [2, 4], [0, 1], [2, 4], [3, 4], [0, 1], [2, 4], [2, 0], [4, 2], [3, 4], [3, 0], [2, 4], [2, 1], [2, 1], [3, 4], [0, 3], [0, 1], [0, 1], [4, 2], [0, 3], [2, 1], [1, 0], [0, 1], [2, 4], [3, 0], [0, 3], [0, 1], [1, 2], [2, 4], [0, 3], [0, 3], [3, 4], [2, 4], [2, 1], [2, 4], [0, 1], [0, 1], [1, 2], [2, 0], [3, 0], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [2, 4], [2, 0], [0, 1], [2, 4], [3, 0], [2, 0], [0, 3], [0, 1], [2, 4], [0, 3], [0, 1], [3, 4], [2, 4], [2, 4], [2, 4], [3, 0], [2, 4], [2, 1], [2, 1], [0, 3], [0, 3], [0, 1], [1, 2], [2, 4], [3, 4], [0, 3], [0, 3], [2, 1], [2, 4], [3, 0], [0, 1], [0, 3], [2, 4], [2, 4], [3, 4], [0, 3], [2, 1], [3, 4], [2, 1], [3, 4], [0, 3], [0, 3], [2, 1], [2, 4], [3, 4], [3, 0], [2, 4], [2, 1], [2, 4], [3, 4], [0, 3], [2, 0], [0, 2], [2, 1], [2, 4], [2, 1], [0, 3], [0, 1], [2, 4], [3, 0], [0, 3], [4, 2], [0, 1], [2, 4], [4, 3], [0, 3], [4, 2], [2, 1], [2, 4], [3, 4], [1, 2], [2, 1], [2, 4], [2, 4], [3, 0], [0, 1], [0, 3], [1, 0], [2, 4], [3, 4], [0, 3], [2, 1], [2, 1], [3, 4], [3, 0], [0, 3], [0, 3], [2, 1], [2, 0], [3, 4], [0, 3], [0, 2], [3, 4], [2, 1], [2, 4], [0, 3], [0, 3], [2, 1], [2, 1], [3, 4], [0, 3], [0, 3], [2, 4], [2, 4], [3, 4], [0, 1], [0, 2], [2, 1], [2, 4], [2, 4], [0, 1], [2, 4], [2, 1]]\n",
      "[[0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 2], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 2], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 2], [2, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 2], [2, 3, 4], [2, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 2], [2, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 2], [0, 1, 3], [0, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [0, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 2], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 2], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [1, 3, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 2], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 2], [1, 2, 4], [1, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 2], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 2], [2, 3, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [0, 3], [0, 3], [2, 4], [0, 1], [4, 3], [0, 3], [0, 3], [2, 4], [2, 1], [2, 1], [3, 0], [0, 3], [0, 1], [0, 3], [2, 4], [3, 4], [0, 1], [2, 4], [1, 2], [2, 1], [3, 4], [0, 3], [2, 4], [1, 0], [2, 1], [3, 4], [3, 0], [2, 1], [2, 1], [0, 1], [0, 3], [0, 3], [2, 4], [2, 1], [2, 4], [3, 4], [0, 1], [0, 3], [0, 1], [2, 4], [3, 4], [0, 3], [0, 3], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [2, 1], [2, 4], [3, 0], [2, 4], [0, 1], [2, 1], [2, 4], [3, 0], [0, 3], [2, 1], [2, 0], [2, 4], [3, 0], [0, 3], [0, 3], [0, 1], [2, 1], [3, 4], [0, 1], [0, 1], [0, 3], [2, 1], [3, 4], [2, 1], [2, 4], [2, 4], [2, 4], [3, 0], [3, 0], [0, 1], [2, 4], [2, 4], [2, 4], [2, 1], [0, 3], [2, 4], [2, 1], [0, 3], [0, 3], [0, 3], [1, 0], [2, 4], [3, 4], [0, 3], [2, 4], [0, 1], [2, 4], [3, 4], [0, 1], [2, 4], [2, 0], [4, 2], [3, 4], [3, 0], [2, 4], [2, 1], [2, 1], [3, 4], [0, 3], [0, 1], [0, 1], [4, 2], [0, 3], [2, 1], [1, 0], [0, 1], [2, 4], [3, 0], [0, 3], [0, 1], [1, 2], [2, 4], [0, 3], [0, 3], [3, 4], [2, 4], [2, 1], [2, 4], [0, 1], [0, 1], [1, 2], [2, 0], [3, 0], [0, 3], [2, 4], [0, 1], [2, 4], [0, 3], [2, 4], [2, 0], [0, 1], [2, 4], [3, 0], [2, 0], [0, 3], [0, 1], [2, 4], [0, 3], [0, 1], [3, 4], [2, 4], [2, 4], [2, 4], [3, 0], [2, 4], [2, 1], [2, 1], [0, 3], [0, 3], [0, 1], [1, 2], [2, 4], [3, 4], [0, 3], [0, 3], [2, 1], [2, 4], [3, 0], [0, 1], [0, 3], [2, 4], [2, 4], [3, 4], [0, 3], [2, 1], [3, 4], [2, 1], [3, 4], [0, 3], [0, 3], [2, 1], [2, 4], [3, 4], [3, 0], [2, 4], [2, 1], [2, 4], [3, 4], [0, 3], [2, 0], [0, 2], [2, 1], [2, 4], [2, 1], [0, 3], [0, 1], [2, 4], [3, 0], [0, 3], [4, 2], [0, 1], [2, 4], [4, 3], [0, 3], [4, 2], [2, 1], [2, 4], [3, 4], [1, 2], [2, 1], [2, 4], [2, 4], [3, 0], [0, 1], [0, 3], [1, 0], [2, 4], [3, 4], [0, 3], [2, 1], [2, 1], [3, 4], [3, 0], [0, 3], [0, 3], [2, 1], [2, 0], [3, 4], [0, 3], [0, 2], [3, 4], [2, 1], [2, 4], [0, 3], [0, 3], [2, 1], [2, 1], [3, 4], [0, 3], [0, 3], [2, 4], [2, 4], [3, 4], [0, 1], [0, 2], [2, 1], [2, 4], [2, 4], [0, 1], [2, 4], [2, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004497646331787109  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004497642517089844  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004497633934020996  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004497622966766358  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004497608661651611  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004497592926025391  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004497572898864746  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004497552394866943  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004497529983520508  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004497504234313965  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004497479438781738  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004497451782226563  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004497424125671386  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004497395038604736  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004497365951538086  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004497335910797119  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004497305393218994  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004497273921966553  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004497242450714111  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004497211933135986  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 4.273322701919824e-05 tensor([9.5126e-01, 4.2733e-05, 2.1826e-10, 4.8699e-02, 1.1481e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005062862765043974 tensor([5.7133e-07, 4.1756e-02, 9.5318e-01, 3.8215e-06, 5.0629e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004310224670916796 tensor([2.7169e-08, 4.3102e-03, 9.8912e-01, 2.5456e-06, 6.5694e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.80282102696583e-07 tensor([9.8629e-01, 3.8028e-07, 1.4887e-14, 1.3705e-02, 1.7040e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010460648918524384 tensor([0.0006, 0.0009, 0.0010, 0.5083, 0.4892], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.708992749627214e-05 tensor([5.3996e-02, 9.4598e-01, 1.7090e-05, 2.2358e-06, 6.8144e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004820254631340504 tensor([1.5622e-05, 2.4719e-01, 7.4795e-01, 2.4660e-05, 4.8203e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20022346079349518 tensor([0.0007, 0.3358, 0.4584, 0.0048, 0.2002], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014478854835033417 tensor([8.3653e-01, 1.4479e-02, 7.8213e-06, 1.4846e-01, 5.1907e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007960408926010132 tensor([7.9604e-03, 8.8361e-04, 1.1227e-04, 9.1721e-01, 7.3829e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.9034479919355363e-05 tensor([1.8241e-01, 7.2527e-06, 9.6404e-10, 8.1756e-01, 1.9034e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00038479690556414425 tensor([2.2495e-06, 4.4812e-01, 5.5149e-01, 3.2954e-07, 3.8480e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001605078810825944 tensor([8.0757e-09, 1.6051e-03, 9.8988e-01, 2.6169e-06, 8.5124e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013236484955996275 tensor([1.0429e-07, 3.3785e-04, 4.7105e-01, 1.3236e-03, 5.2729e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07417231798171997 tensor([0.0148, 0.6213, 0.0742, 0.0477, 0.2419], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4572564396075904e-05 tensor([8.3107e-01, 1.4573e-05, 1.0155e-10, 1.6891e-01, 1.9582e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013383810874074697 tensor([1.3384e-03, 9.8371e-01, 1.4757e-02, 1.3110e-05, 1.8024e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.742963836994022e-05 tensor([2.1559e-09, 6.7232e-05, 4.8432e-01, 8.7430e-05, 5.1552e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05176536366343498 tensor([0.5421, 0.4037, 0.0011, 0.0518, 0.0013], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.544680268736556e-05 tensor([4.5447e-05, 5.0869e-06, 1.4649e-05, 8.3030e-01, 1.6963e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.388224952999735e-07 tensor([1.3575e-01, 1.3490e-07, 1.3703e-12, 8.6425e-01, 9.3882e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001114232509280555 tensor([1.4716e-02, 9.8517e-01, 1.1142e-04, 3.2061e-06, 3.7557e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014185570180416107 tensor([5.5488e-06, 1.4186e-02, 7.2443e-01, 1.0746e-03, 2.6031e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05440667271614075 tensor([5.7471e-01, 5.4407e-02, 1.2385e-04, 3.6630e-01, 4.4619e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2970161833436578e-06 tensor([4.9509e-08, 5.4216e-09, 1.2970e-06, 4.3360e-01, 5.6640e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0032770291436463594 tensor([3.2770e-03, 6.4823e-05, 6.1120e-06, 9.8499e-01, 1.1661e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014792314323130995 tensor([3.2207e-03, 9.9663e-01, 1.4792e-04, 3.9040e-07, 1.1196e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007227641763165593 tensor([3.9446e-06, 3.2929e-01, 6.6998e-01, 1.3578e-06, 7.2276e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0023597057443112135 tensor([2.3597e-03, 1.2619e-04, 3.1495e-05, 9.6381e-01, 3.3672e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010386021313024685 tensor([1.0386e-04, 9.1960e-08, 1.4779e-08, 9.9726e-01, 2.6381e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006954286945983768 tensor([2.7666e-04, 4.1524e-04, 6.9543e-04, 2.9916e-01, 6.9945e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04765353351831436 tensor([3.6627e-05, 1.4667e-01, 8.0536e-01, 2.7558e-04, 4.7654e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.040828585624694824 tensor([0.0009, 0.5583, 0.3983, 0.0017, 0.0408], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21876324713230133 tensor([0.4912, 0.2642, 0.0014, 0.2188, 0.0244], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008261431939899921 tensor([3.3480e-02, 3.0579e-04, 3.1023e-06, 9.5795e-01, 8.2614e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004572280158754438 tensor([9.3575e-01, 4.5723e-04, 9.3813e-09, 6.3776e-02, 2.1299e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005688873352482915 tensor([2.6656e-03, 9.9676e-01, 5.6889e-04, 1.0562e-06, 3.7862e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004001572960987687 tensor([4.4877e-08, 2.7331e-04, 3.6632e-01, 4.0016e-04, 6.3301e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05551084876060486 tensor([0.0321, 0.8061, 0.0668, 0.0395, 0.0555], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05372627452015877 tensor([0.0008, 0.0185, 0.0537, 0.2011, 0.7259], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.574149616270006e-07 tensor([4.7355e-01, 3.5741e-07, 7.7854e-13, 5.2645e-01, 2.9508e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0023488327860832214 tensor([2.3488e-03, 9.7499e-01, 2.1469e-02, 8.5169e-05, 1.1020e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014334142906591296 tensor([7.7299e-10, 1.4334e-03, 9.9672e-01, 1.2971e-07, 1.8474e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007866263622418046 tensor([4.0761e-07, 8.7166e-02, 9.1205e-01, 6.7920e-07, 7.8663e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009580880869179964 tensor([3.8825e-09, 5.1711e-07, 9.5809e-04, 2.8212e-03, 9.9622e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003944274038076401 tensor([9.6946e-01, 2.6581e-02, 1.0162e-06, 3.9443e-03, 1.6631e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010906332172453403 tensor([1.0906e-03, 9.8414e-01, 1.4500e-02, 1.4499e-05, 2.5170e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015371558256447315 tensor([5.7060e-08, 1.5372e-03, 8.9370e-01, 7.0597e-05, 1.0469e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006578174652531743 tensor([9.4192e-01, 5.7420e-02, 6.1583e-07, 6.5782e-04, 2.0982e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0025571538135409355 tensor([7.6261e-03, 8.9067e-06, 7.4560e-08, 9.8981e-01, 2.5572e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00020899959781672806 tensor([9.8213e-01, 1.7657e-02, 3.7224e-08, 2.0900e-04, 7.5075e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001690182718448341 tensor([5.4017e-05, 6.9319e-01, 3.0505e-01, 9.8823e-06, 1.6902e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0016875993460416794 tensor([7.1072e-01, 2.8753e-01, 2.5271e-05, 1.6876e-03, 3.6851e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007909301784820855 tensor([1.3234e-08, 6.7648e-05, 3.2214e-01, 7.9093e-04, 6.7700e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003083622723352164 tensor([3.0836e-04, 1.9335e-06, 3.8265e-07, 9.8111e-01, 1.8581e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.163368430454284e-05 tensor([9.9680e-01, 4.1634e-05, 1.6627e-11, 3.1555e-03, 2.7992e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03098505735397339 tensor([0.0146, 0.8517, 0.0920, 0.0107, 0.0310], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.961869515478611e-05 tensor([7.7390e-12, 3.9619e-05, 9.9331e-01, 1.1754e-07, 6.6497e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009727186989039183 tensor([3.1771e-02, 4.5472e-05, 2.8080e-07, 9.6721e-01, 9.7272e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0187021866440773 tensor([0.0148, 0.0187, 0.0015, 0.5561, 0.4089], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012846271274611354 tensor([9.9258e-01, 1.2846e-03, 8.6093e-09, 6.1275e-03, 3.7481e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003603819932322949 tensor([1.7636e-05, 7.9804e-01, 2.0158e-01, 1.0542e-06, 3.6038e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.90597846894525e-05 tensor([1.7397e-11, 5.9060e-05, 9.9316e-01, 1.7421e-07, 6.7798e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038124825805425644 tensor([3.2065e-05, 3.8125e-02, 8.1008e-01, 3.2269e-03, 1.4853e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009851157665252686 tensor([5.1629e-06, 3.3724e-05, 9.8512e-04, 1.2682e-01, 8.7216e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03766992688179016 tensor([3.7670e-02, 7.3067e-03, 3.2278e-04, 9.0548e-01, 4.9217e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000945508771110326 tensor([9.4551e-04, 9.7040e-01, 2.8296e-02, 1.9670e-05, 3.4361e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0030684354715049267 tensor([4.0744e-07, 1.4674e-05, 3.0684e-03, 1.7836e-02, 9.7908e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00221844669431448 tensor([2.9849e-06, 1.8334e-05, 2.2184e-03, 1.6327e-01, 8.3449e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026887521147727966 tensor([8.4904e-07, 2.6888e-02, 9.2842e-01, 3.5269e-05, 4.4655e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012633572332561016 tensor([1.2634e-03, 1.2722e-06, 5.2503e-08, 9.9721e-01, 1.5219e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006131099653430283 tensor([1.9150e-02, 9.8018e-01, 6.1311e-04, 2.6828e-05, 3.2783e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0017653732793405652 tensor([1.7654e-03, 1.4762e-05, 1.2904e-06, 9.7826e-01, 1.9959e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13509728014469147 tensor([0.4338, 0.4198, 0.0026, 0.1351, 0.0087], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036509279161691666 tensor([9.3078e-01, 6.5529e-02, 2.1157e-06, 3.6509e-03, 4.1682e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.752529877005145e-06 tensor([9.8423e-01, 9.7525e-06, 5.4255e-12, 1.5764e-02, 7.9716e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0038517764769494534 tensor([2.8008e-05, 3.8494e-01, 6.1116e-01, 1.8020e-05, 3.8518e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002338239282835275 tensor([9.9990e-08, 1.0565e-01, 8.9411e-01, 8.0736e-08, 2.3382e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.040319085121154785 tensor([0.0040, 0.0333, 0.0403, 0.3567, 0.5657], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18810142576694489 tensor([0.1881, 0.2620, 0.0053, 0.4250, 0.1196], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007627986487932503 tensor([9.6929e-01, 7.6280e-04, 1.6465e-08, 2.9938e-02, 7.0759e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012145874788984656 tensor([8.8606e-01, 1.1270e-01, 2.6062e-06, 1.2146e-03, 1.4615e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000142302131280303 tensor([3.0945e-01, 7.2109e-05, 1.5075e-08, 6.9033e-01, 1.4230e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007308165077120066 tensor([3.0153e-08, 7.3082e-04, 8.8036e-01, 8.2414e-05, 1.1883e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08906727284193039 tensor([0.2491, 0.6166, 0.0047, 0.0891, 0.0405], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00031630913144908845 tensor([3.4710e-01, 2.9026e-04, 9.1858e-08, 6.5229e-01, 3.1631e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0030258705373853445 tensor([3.4410e-07, 3.2503e-02, 9.6447e-01, 2.3945e-06, 3.0259e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28516238927841187 tensor([0.0015, 0.2852, 0.3851, 0.0175, 0.3108], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10010834783315659 tensor([3.7685e-04, 1.0011e-01, 5.0386e-01, 2.0341e-02, 3.7531e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.317401701177005e-06 tensor([5.8416e-07, 1.0349e-07, 5.3174e-06, 3.6597e-01, 6.3403e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1762994063246879e-06 tensor([9.9547e-01, 1.1763e-06, 3.8148e-14, 4.5327e-03, 2.1488e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00022043034550733864 tensor([2.2108e-03, 9.9757e-01, 2.2043e-04, 3.6537e-07, 1.2902e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010838257730938494 tensor([1.9892e-11, 1.0838e-04, 9.9564e-01, 7.5154e-08, 4.2522e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00020703770860563964 tensor([9.7483e-01, 2.0704e-04, 1.9101e-09, 2.4964e-02, 1.4131e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00013142224634066224 tensor([3.6185e-09, 7.8915e-08, 1.3142e-04, 7.3567e-03, 9.9251e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.629138402378885e-06 tensor([8.7492e-01, 4.6291e-06, 9.2700e-12, 1.2507e-01, 2.6840e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0028000124730169773 tensor([3.3800e-03, 9.9375e-01, 2.8000e-03, 1.0312e-05, 6.3741e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036018704995512962 tensor([2.2489e-08, 5.9202e-06, 1.1020e-02, 3.6019e-03, 9.8537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03358903154730797 tensor([8.2881e-01, 3.3589e-02, 2.9603e-05, 1.3636e-01, 1.2089e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008068173192441463 tensor([0.0071, 0.0081, 0.0019, 0.6826, 0.3003], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017250899225473404 tensor([0.1267, 0.8423, 0.0073, 0.0173, 0.0064], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002058956742985174 tensor([7.6216e-03, 9.9217e-01, 2.0590e-04, 1.6585e-06, 3.0049e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002881400054320693 tensor([6.5560e-05, 5.7734e-01, 4.1969e-01, 2.7535e-05, 2.8814e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10479172319173813 tensor([7.5862e-01, 1.3434e-01, 1.6706e-04, 1.0479e-01, 2.0717e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00043436247506178916 tensor([4.3436e-04, 2.6882e-06, 3.4809e-07, 9.8171e-01, 1.7848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.286989810178056e-05 tensor([5.4197e-03, 1.6721e-07, 1.6122e-10, 9.9452e-01, 6.2870e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006065022898837924 tensor([6.0650e-04, 9.9610e-01, 3.2741e-03, 9.0087e-07, 1.6143e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22641856968402863 tensor([0.0008, 0.2264, 0.3689, 0.0122, 0.3916], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008859320543706417 tensor([3.4428e-05, 3.0733e-04, 8.8593e-03, 2.1994e-01, 7.7086e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0018620557384565473 tensor([3.8733e-04, 9.1890e-04, 1.8621e-03, 3.5736e-01, 6.3947e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.55911033693701e-05 tensor([1.6079e-01, 8.3913e-01, 3.3544e-05, 3.5591e-05, 4.1633e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016303591430187225 tensor([1.1954e-06, 1.6304e-02, 9.3231e-01, 7.1929e-05, 5.1310e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013035008450970054 tensor([1.3035e-03, 7.7047e-04, 7.2952e-04, 7.4123e-01, 2.5596e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.138541357358918e-05 tensor([3.8043e-07, 3.3915e-07, 6.1385e-05, 2.4235e-01, 7.5759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03031684271991253 tensor([1.9012e-04, 3.9653e-03, 3.0317e-02, 1.3444e-01, 8.3109e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.6119658286916092e-05 tensor([9.7962e-01, 2.6120e-05, 4.1062e-11, 2.0349e-02, 2.2654e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0013645251747220755 tensor([6.4177e-05, 7.2505e-01, 2.7351e-01, 1.0715e-05, 1.3645e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00015190856356639415 tensor([1.1479e-10, 1.5191e-04, 9.9254e-01, 4.4047e-07, 7.3061e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002355523407459259 tensor([1.4171e-06, 1.5422e-05, 2.3555e-03, 1.0093e-01, 8.9669e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001504567771917209 tensor([1.5046e-04, 4.3286e-05, 9.6253e-05, 6.6852e-01, 3.3119e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00012058855645591393 tensor([7.6574e-01, 2.3413e-01, 1.5297e-06, 1.2059e-04, 2.1808e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08319742232561111 tensor([0.0007, 0.4911, 0.4229, 0.0021, 0.0832], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00017188108176924288 tensor([7.7063e-11, 1.7188e-04, 9.9589e-01, 1.9885e-07, 3.9353e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0033895731903612614 tensor([3.3896e-03, 9.7942e-01, 1.6347e-02, 1.1266e-04, 7.2660e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011575024109333754 tensor([5.7998e-01, 1.1575e-03, 3.5265e-07, 4.1832e-01, 5.3883e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004842972382903099 tensor([2.2947e-02, 3.2041e-04, 7.1792e-06, 9.7188e-01, 4.8430e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004457924980670214 tensor([9.8307e-01, 1.2461e-02, 2.2437e-07, 4.4579e-03, 1.5298e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008558325469493866 tensor([2.6253e-07, 2.3959e-05, 8.5583e-03, 8.6291e-03, 9.8279e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008018797263503075 tensor([5.2189e-08, 7.2288e-06, 8.1561e-03, 8.0188e-03, 9.8382e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.4919410154543584e-06 tensor([3.4919e-06, 6.0100e-08, 4.0686e-07, 8.9551e-01, 1.0448e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22780050337314606 tensor([0.1100, 0.3521, 0.0101, 0.2278, 0.3000], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002008529845625162 tensor([4.4534e-06, 2.5648e-01, 7.4150e-01, 3.5586e-06, 2.0085e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0037431276869028807 tensor([2.6341e-07, 3.7431e-03, 8.9675e-01, 1.1229e-04, 9.9396e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016494926065206528 tensor([9.6074e-01, 2.2720e-02, 1.7276e-06, 1.6495e-02, 3.9792e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017153441440314054 tensor([1.1349e-06, 3.4346e-05, 1.7153e-03, 1.9010e-02, 9.7924e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.086595436092466e-05 tensor([9.5003e-01, 4.0866e-05, 1.9134e-10, 4.9926e-02, 8.3356e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1371718943119049 tensor([0.0029, 0.5713, 0.2800, 0.0087, 0.1372], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01480251457542181 tensor([9.6587e-01, 1.9295e-02, 1.3902e-06, 1.4803e-02, 3.4768e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2270612120628357 tensor([0.0969, 0.3521, 0.0166, 0.2271, 0.3073], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00027233167202211916 tensor([1.8852e-07, 1.7744e-06, 2.7233e-04, 3.6119e-02, 9.6361e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008828246500343084 tensor([9.7416e-01, 8.8282e-04, 1.8356e-08, 2.4952e-02, 7.2906e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0026915916241705418 tensor([1.6669e-04, 7.5192e-01, 2.4520e-01, 2.6025e-05, 2.6916e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0058709983713924885 tensor([0.0053, 0.0059, 0.0021, 0.6103, 0.3765], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010795327834784985 tensor([2.2413e-06, 1.0795e-02, 9.1351e-01, 5.4627e-04, 7.5146e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018158121034502983 tensor([1.0002e-05, 1.4206e-03, 4.8146e-02, 1.8158e-02, 9.3227e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021248798817396164 tensor([8.2854e-01, 1.4976e-01, 5.1689e-05, 2.1249e-02, 4.0298e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032735541462898254 tensor([3.2221e-09, 3.5710e-03, 9.9316e-01, 2.2983e-07, 3.2736e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0062867822125554085 tensor([5.0828e-07, 2.8327e-05, 6.2868e-03, 1.4571e-02, 9.7911e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00952571164816618 tensor([0.0095, 0.9509, 0.0350, 0.0012, 0.0034], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08796709030866623 tensor([0.1151, 0.7158, 0.0123, 0.0880, 0.0688], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0021647477988153696 tensor([9.9505e-01, 2.7856e-03, 1.2526e-08, 2.1647e-03, 6.5915e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02483258955180645 tensor([2.4944e-01, 2.4833e-02, 1.6112e-04, 7.0278e-01, 2.2784e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03802771866321564 tensor([0.0037, 0.7797, 0.1748, 0.0037, 0.0380], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011168864148203284 tensor([9.4587e-01, 5.4019e-02, 1.2910e-07, 1.1169e-04, 2.8726e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0025807013735175133 tensor([2.5807e-03, 2.5322e-05, 2.0766e-06, 9.9076e-01, 6.6274e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00011696617002598941 tensor([3.5145e-01, 9.8875e-05, 2.2553e-08, 6.4834e-01, 1.1697e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011261564213782549 tensor([9.7621e-09, 1.1262e-03, 9.8022e-01, 6.5575e-06, 1.8652e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0013389615342020988 tensor([1.7957e-08, 1.3390e-03, 9.6900e-01, 1.2134e-05, 2.9651e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001534248236566782 tensor([3.9609e-06, 1.8045e-05, 1.5342e-03, 1.9814e-01, 8.0030e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00028812672826461494 tensor([2.8813e-04, 4.4013e-05, 4.8127e-05, 8.5842e-01, 1.4120e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003946410957723856 tensor([9.9204e-01, 3.9464e-03, 4.2962e-08, 4.0117e-03, 4.1210e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00027011099155060947 tensor([2.7011e-04, 9.9168e-01, 8.0269e-03, 7.4195e-07, 2.4454e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00048745362437330186 tensor([4.7383e-08, 4.4161e-02, 9.5535e-01, 1.2140e-07, 4.8745e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012108429335057735 tensor([2.3116e-06, 1.2108e-02, 8.0055e-01, 5.0271e-04, 1.8684e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.762850196333602e-05 tensor([5.7629e-05, 7.0680e-09, 4.1167e-10, 9.9944e-01, 5.0636e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.39899716045511e-08 tensor([4.4802e-01, 5.3990e-08, 3.3302e-14, 5.5198e-01, 4.3603e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0023251173552125692 tensor([1.4957e-05, 4.1228e-01, 5.8537e-01, 7.6079e-06, 2.3251e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010601702146232128 tensor([2.8689e-06, 3.2289e-04, 2.1793e-02, 1.0602e-02, 9.6728e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00026111476472578943 tensor([3.9675e-10, 2.6111e-04, 9.8780e-01, 9.7821e-07, 1.1941e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05942020192742348 tensor([0.5741, 0.3625, 0.0011, 0.0594, 0.0028], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.3258427972905338e-05 tensor([6.2591e-01, 2.3258e-05, 6.2607e-10, 3.7405e-01, 1.0825e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.008034349884838e-05 tensor([5.0080e-05, 9.4993e-01, 4.9981e-02, 5.6098e-07, 4.2080e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000999252311885357 tensor([4.5306e-09, 9.9925e-04, 9.8592e-01, 2.7171e-06, 1.3074e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.043738480657339096 tensor([4.3738e-02, 6.5659e-03, 2.9511e-04, 9.0328e-01, 4.6116e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019371359376236796 tensor([9.2775e-03, 9.8850e-01, 1.9371e-03, 6.8755e-05, 2.1232e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.135583680588752e-05 tensor([2.5751e-01, 2.5464e-05, 4.4081e-09, 7.4241e-01, 6.1356e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006181719712913036 tensor([1.0858e-02, 9.8214e-01, 6.1817e-03, 2.1180e-04, 6.0577e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008043044246733189 tensor([9.1566e-08, 1.0857e-02, 9.8110e-01, 2.9683e-06, 8.0430e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010331958765164018 tensor([2.2527e-07, 8.0903e-02, 9.1806e-01, 4.4265e-07, 1.0332e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.943984721554443e-05 tensor([8.9440e-05, 1.7181e-07, 5.4527e-08, 9.9441e-01, 5.4994e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017787926481105387 tensor([9.9944e-01, 3.7931e-04, 5.0890e-11, 1.7788e-04, 1.1777e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0017919869860634208 tensor([4.2921e-03, 9.9387e-01, 1.7920e-03, 9.1020e-06, 3.9352e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006383287254720926 tensor([1.0986e-05, 4.8978e-01, 5.0957e-01, 2.4052e-06, 6.3833e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08409401029348373 tensor([0.5858, 0.3239, 0.0008, 0.0841, 0.0054], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009639612399041653 tensor([1.4142e-02, 2.3460e-04, 6.4263e-06, 9.7598e-01, 9.6396e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.7579846826265566e-05 tensor([9.9515e-01, 1.7580e-05, 5.4667e-12, 4.8355e-03, 4.5218e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014914021594449878 tensor([1.4914e-03, 9.7301e-01, 2.4726e-02, 5.3005e-05, 7.2427e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.150046101538464e-05 tensor([5.6335e-11, 6.1500e-05, 9.8314e-01, 8.1392e-07, 1.6797e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26913365721702576 tensor([0.0568, 0.3066, 0.0381, 0.3293, 0.2691], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000603390100877732 tensor([4.1548e-04, 6.0339e-04, 6.1829e-04, 2.6700e-01, 7.3136e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008487006998620927 tensor([7.1063e-03, 4.2471e-06, 2.6340e-08, 9.9204e-01, 8.4870e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00024174660211429 tensor([9.9707e-01, 2.6921e-03, 1.9202e-09, 2.4175e-04, 8.9839e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017095815390348434 tensor([8.4136e-02, 2.6611e-03, 3.1422e-05, 8.9608e-01, 1.7096e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005257671698927879 tensor([2.6738e-06, 1.1140e-01, 8.8333e-01, 7.6001e-06, 5.2577e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003803073661401868 tensor([3.0207e-05, 2.7695e-04, 3.8031e-03, 1.2119e-01, 8.7470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.321498858596897e-06 tensor([4.4159e-01, 6.3215e-06, 1.4030e-10, 5.5840e-01, 5.9558e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00022656709188595414 tensor([1.2151e-04, 9.2888e-01, 7.0774e-02, 2.5708e-06, 2.2657e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02232716605067253 tensor([1.1966e-05, 2.2327e-02, 4.9386e-01, 1.3698e-03, 4.8243e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008594003738835454 tensor([2.1443e-01, 7.8426e-01, 2.6372e-04, 8.5940e-04, 1.8473e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00043718895176425576 tensor([1.6713e-08, 5.3052e-07, 4.3719e-04, 1.0361e-02, 9.8920e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.194122397573665e-05 tensor([9.9963e-01, 6.1941e-05, 4.3317e-12, 3.1004e-04, 3.9183e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00019536068430170417 tensor([5.8448e-02, 9.4131e-01, 1.9536e-04, 3.5917e-05, 1.4447e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010094299796037376 tensor([2.1249e-10, 1.0094e-04, 9.7331e-01, 2.2067e-06, 2.6584e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.039820849895477295 tensor([0.0958, 0.8337, 0.0188, 0.0398, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003173411823809147 tensor([3.1734e-04, 1.2008e-06, 2.1508e-07, 9.9090e-01, 8.7803e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.094613930443302e-05 tensor([5.9910e-01, 3.0946e-05, 1.2488e-09, 4.0085e-01, 1.6306e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006684783729724586 tensor([6.6848e-04, 9.9537e-01, 3.9390e-03, 1.2957e-06, 2.1196e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006896491395309567 tensor([6.8965e-04, 3.1694e-04, 4.7371e-04, 7.9784e-01, 2.0068e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05254849046468735 tensor([5.2548e-02, 9.4799e-03, 4.2896e-04, 8.7230e-01, 6.5242e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013071523979306221 tensor([9.1281e-01, 1.3072e-02, 2.2537e-06, 7.3871e-02, 2.4537e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.4429816373158246e-05 tensor([9.9845e-01, 5.4430e-05, 1.3366e-11, 1.5006e-03, 3.1650e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011826006695628166 tensor([9.1631e-04, 7.8285e-01, 2.0403e-01, 3.7798e-04, 1.1826e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026511000469326973 tensor([1.5731e-05, 1.1215e-03, 3.0376e-02, 2.6511e-02, 9.4198e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01031836960464716 tensor([7.5155e-06, 7.5726e-02, 9.1386e-01, 8.5490e-05, 1.0318e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000823432463221252 tensor([7.0128e-04, 5.9177e-04, 8.2343e-04, 6.0224e-01, 3.9564e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.4298744847328635e-06 tensor([9.7886e-01, 2.4299e-06, 6.7499e-13, 2.1139e-02, 1.8779e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012010239152004942 tensor([2.8751e-03, 9.9700e-01, 1.2010e-04, 2.7145e-07, 7.2908e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001804569037631154 tensor([9.0798e-09, 1.8046e-03, 9.9153e-01, 2.4571e-06, 6.6590e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027749106287956238 tensor([3.4772e-02, 1.4085e-03, 3.9259e-05, 9.3603e-01, 2.7749e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014733158750459552 tensor([1.4733e-03, 1.1215e-04, 2.2544e-05, 8.9996e-01, 9.8429e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.705276897875592e-05 tensor([9.7053e-05, 9.9505e-01, 4.8476e-03, 1.1599e-07, 5.2649e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004557391512207687 tensor([9.7006e-05, 8.6944e-01, 1.3000e-01, 5.2743e-06, 4.5574e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00641222158446908 tensor([3.2327e-07, 1.7071e-02, 9.7651e-01, 8.1692e-06, 6.4122e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012949831783771515 tensor([2.9078e-07, 1.2950e-02, 9.7039e-01, 1.5878e-05, 1.6641e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00019213490304537117 tensor([1.9213e-04, 2.0203e-07, 2.1613e-08, 9.9502e-01, 4.7903e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14302033185958862 tensor([0.1173, 0.1430, 0.0059, 0.5613, 0.1725], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009580159559845924 tensor([9.5802e-03, 9.7897e-01, 9.8951e-03, 3.2455e-04, 1.2257e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0029169165063649416 tensor([4.3971e-08, 1.3659e-02, 9.8342e-01, 7.3883e-07, 2.9169e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09002077579498291 tensor([0.0182, 0.0900, 0.0283, 0.5790, 0.2845], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001056991401128471 tensor([1.4314e-02, 9.8455e-01, 1.0570e-03, 3.0584e-05, 4.8444e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015554232522845268 tensor([3.2067e-01, 9.2840e-04, 9.5301e-07, 6.7685e-01, 1.5554e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014164779568091035 tensor([9.9318e-01, 1.4165e-03, 1.0762e-08, 5.4066e-03, 1.0549e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.049248214811086655 tensor([2.8694e-04, 4.9248e-02, 2.3116e-01, 2.9011e-02, 6.9029e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0021348504815250635 tensor([7.4332e-07, 2.1349e-03, 5.1094e-01, 9.5592e-04, 4.8597e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009077434078790247 tensor([9.0774e-04, 1.2213e-05, 2.1301e-06, 9.7505e-01, 2.4029e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005604837089776993 tensor([5.6048e-03, 2.6464e-04, 2.0979e-05, 9.5926e-01, 3.4850e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002293969038873911 tensor([2.2940e-03, 9.9383e-01, 3.8144e-03, 6.1728e-06, 5.6631e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.3825059543014504e-05 tensor([1.3118e-11, 2.3825e-05, 9.6248e-01, 6.5218e-07, 3.7499e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017317937454208732 tensor([4.8495e-08, 1.7318e-03, 9.6779e-01, 3.4352e-05, 3.0445e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008788079721853137 tensor([9.6488e-01, 8.7881e-04, 1.3095e-08, 3.4230e-02, 1.0582e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002727294049691409 tensor([7.6244e-01, 2.7273e-04, 1.3277e-08, 2.3721e-01, 8.0346e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.8589059840887785e-05 tensor([3.8589e-05, 9.7059e-01, 2.9342e-02, 2.2303e-07, 2.6050e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009459293214604259 tensor([4.6404e-08, 1.2250e-04, 1.6050e-01, 9.4593e-04, 8.3843e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1733577996492386 tensor([0.0118, 0.1734, 0.1368, 0.2999, 0.3782], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00023357151076197624 tensor([2.3357e-04, 6.0883e-06, 2.9757e-06, 9.3193e-01, 6.7829e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.400861017638817e-05 tensor([9.9661e-01, 9.4009e-05, 7.0481e-11, 3.2989e-03, 8.2124e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06905407458543777 tensor([8.2141e-01, 6.9054e-02, 5.3067e-05, 1.0776e-01, 1.7228e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011840643361210823 tensor([2.5799e-05, 4.3534e-03, 7.5399e-02, 1.1841e-02, 9.0838e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24030473828315735 tensor([0.2713, 0.4493, 0.0068, 0.2403, 0.0322], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001870904816314578 tensor([1.8709e-03, 5.1339e-05, 5.0447e-06, 9.5604e-01, 4.2037e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [0, 3, 4], [0, 3, 1], [2, 4, 1], [0, 1, 2], [4, 3, 2], [0, 3, 4], [0, 3], [2, 4, 1], [2, 1, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1], [0, 1, 3], [0, 3, 2], [2, 4, 1], [3, 4, 0], [0, 1, 3], [2, 4, 3], [1, 2, 0], [2, 1, 4], [3, 4, 2], [0, 3, 1], [2, 4, 1], [1, 0, 2], [2, 1, 0], [3, 4, 2], [3, 0, 4], [2, 1, 0], [2, 1, 0], [0, 1, 2], [0, 3, 4], [0, 3, 4], [2, 4], [2, 1, 4], [2, 4, 1], [3, 4, 2], [0, 1, 3], [0, 3, 4], [0, 1, 2], [2, 4, 1], [3, 4, 0], [0, 3, 1], [0, 3, 4], [0, 1, 2], [2, 4, 3], [3, 4, 0], [0, 3, 1], [2, 4, 3], [2, 1, 4], [2, 4, 3], [3, 0, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0], [2, 4, 1], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 3, 1], [0, 1, 2], [2, 1, 0], [3, 4, 0], [0, 1, 2], [0, 1, 2], [0, 3, 1], [2, 1, 0], [3, 4, 2], [2, 1, 0], [2, 4, 3], [2, 4, 3], [2, 4, 1], [3, 0, 4], [3, 0, 4], [0, 1, 2], [2, 4, 0], [2, 4, 1], [2, 4, 3], [2, 1, 4], [0, 3, 1], [2, 4, 3], [2, 1, 4], [0, 3, 4], [0, 3], [0, 3, 1], [1, 0, 2], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [3, 4, 2], [0, 1, 3], [2, 4, 1], [2, 0, 1], [4, 2, 3], [3, 4, 2], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 4], [3, 4, 0], [0, 3], [0, 1, 2], [0, 1, 2], [4, 2, 3], [0, 3, 1], [2, 1, 0], [1, 0, 2], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 1, 2], [1, 2, 0], [2, 4, 3], [0, 3, 4], [0, 3, 1], [3, 4, 0], [2, 4, 1], [2, 1, 4], [2, 4, 3], [0, 1, 2], [0, 1, 3], [1, 2, 0], [2, 0], [3, 0, 4], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 4, 1], [0, 3, 4], [2, 4, 3], [2, 0], [0, 1, 2], [2, 4, 1], [3, 0, 4], [2, 0, 1], [0, 3, 1], [0, 1, 3], [2, 4, 3], [0, 3, 4], [0, 1, 2], [3, 4, 0], [2, 4, 3], [2, 4, 3], [2, 4, 1], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 1, 2], [1, 2, 0], [2, 4, 1], [3, 4, 0], [0, 3, 4], [0, 3, 1], [2, 1, 0], [2, 4, 1], [3, 0, 4], [0, 1, 3], [0, 3, 1], [2, 4, 3], [2, 4, 1], [3, 4, 0], [0, 3, 1], [2, 1, 0], [3, 4, 2], [2, 1, 4], [3, 4, 2], [0, 3, 4], [0, 3, 4], [2, 1, 0], [2, 4, 3], [3, 4, 2], [3, 0, 4], [2, 4, 3], [2, 1, 4], [2, 4, 1], [3, 4, 0], [0, 3, 1], [2, 0], [0, 2, 1], [2, 1, 4], [2, 4, 3], [2, 1, 4], [0, 3, 4], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [4, 2, 3], [0, 1, 2], [2, 4, 1], [4, 3, 2], [0, 3, 1], [4, 2, 3], [2, 1, 0], [2, 4, 1], [3, 4, 0], [1, 2, 0], [2, 1, 0], [2, 4, 1], [2, 4, 1], [3, 0, 4], [0, 1, 3], [0, 3, 4], [1, 0, 2], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 1, 4], [2, 1, 0], [3, 4, 0], [3, 0, 4], [0, 3, 4], [0, 3, 1], [2, 1, 0], [2, 0, 1], [3, 4, 0], [0, 3, 4], [0, 2, 1], [3, 4, 2], [2, 1, 4], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 1, 0], [2, 1, 0], [3, 4, 0], [0, 3, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [3, 4, 0], [0, 1, 3], [0, 2, 1], [2, 1, 0], [2, 4, 1], [2, 4, 1], [0, 1, 3], [2, 4], [2, 1, 0]]\n",
      "[[0, 3], [1, 2], [2, 4], [0, 3], [3, 4], [0, 1], [1, 2], [1, 2, 4], [0, 3], [3, 4], [0, 3], [1, 2], [2, 4], [2, 4], [1, 4], [0, 3], [1, 2], [2, 4], [0, 1], [3, 4], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [3, 4], [0, 1], [1, 2], [3, 4], [3, 4], [3, 4], [1, 2], [1, 2], [0, 1, 3], [0, 3], [0, 3], [0, 1], [2, 4], [1, 2], [3, 4], [0, 3], [1, 2], [2, 4], [1, 2], [3, 4], [0, 1], [1, 2], [2, 4], [0, 1], [0, 3], [0, 1], [1, 2], [0, 1], [2, 4], [3, 4], [0, 3], [1, 2], [2, 4], [0, 3], [3, 4], [0, 3], [1, 2], [2, 4], [2, 4], [3, 4], [3, 4], [1, 2], [3, 4], [3, 4], [2, 4], [3, 4], [0, 1], [3, 4], [0, 1], [0, 1], [0, 3], [1, 2], [1, 2], [3, 4], [1, 3], [0, 3], [0, 1], [0, 3], [2, 4], [0, 1], [0, 3], [1, 2], [1, 2, 4], [2, 4], [3, 4], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [0, 1], [0, 1], [1, 2], [0, 1], [3, 4], [0, 3], [1, 2], [1, 2, 4], [3, 4], [3, 4], [0, 1], [2, 4], [3, 4], [3, 4], [3, 4], [0, 3], [1, 2], [2, 4], [3, 4], [3, 4], [0, 1], [1, 2], [2, 4], [1, 2], [0, 3], [0, 3], [0, 1], [3, 4], [2, 4], [3, 4], [1, 3, 4], [1, 2], [2, 4], [0, 1], [3, 4], [0, 3], [1, 2], [0, 1], [1, 3, 4], [3, 4], [0, 3], [1, 2], [3, 4], [2, 4], [2, 4], [0, 1], [1, 2], [3, 4], [1, 2], [0, 1], [0, 1], [0, 3], [1, 2], [0, 1], [3, 4], [0, 3], [2, 4], [2, 4], [3, 4], [3, 4], [0, 3], [1, 2], [1, 2], [2, 4], [3, 4], [0, 3], [1, 2], [2, 4], [2, 4], [0, 1], [0, 3], [1, 2], [2, 4], [3, 4], [0, 1], [0, 3], [0, 1], [1, 2], [1, 2], [3, 4], [0, 1], [0, 1], [1, 2], [0, 1], [0, 3], [0, 3], [1, 2], [2, 4], [1, 3, 4], [3, 4], [0, 3], [0, 1], [0, 3], [1, 2], [3, 4], [0, 3], [1, 2], [2, 4], [0, 1], [3, 4], [0, 3], [0, 1], [2, 4], [0, 1], [3, 4], [0, 3], [1, 2], [3, 4], [3, 4], [0, 3], [0, 3], [1, 2], [2, 4], [1, 2], [3, 4], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [1, 2], [1, 2], [1, 2], [2, 4], [3, 4], [3, 4], [1, 2], [1, 2], [3, 4], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [3, 4], [3, 4], [1, 2], [2, 4], [2, 4], [0, 3], [0, 3], [1, 2], [2, 4], [3, 4], [3, 4], [0, 3], [0, 3], [2, 4], [0, 1, 3], [3, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 1], [0, 3, 4], [0, 3, 1], [2, 4, 1], [0, 1, 2], [4, 3, 2], [0, 3, 4], [2, 4, 1], [2, 1, 0], [2, 1, 4], [3, 0, 4], [0, 3, 1], [0, 1, 3], [0, 3, 2], [2, 4, 1], [3, 4, 0], [0, 1, 3], [2, 4, 3], [1, 2, 0], [2, 1, 4], [3, 4, 2], [0, 3, 1], [2, 4, 1], [1, 0, 2], [2, 1, 0], [3, 4, 2], [3, 0, 4], [2, 1, 0], [2, 1, 0], [0, 1, 2], [0, 3, 4], [0, 3, 4], [2, 1, 4], [2, 4, 1], [3, 4, 2], [0, 1, 3], [0, 3, 4], [0, 1, 2], [2, 4, 1], [3, 4, 0], [0, 3, 1], [0, 3, 4], [0, 1, 2], [2, 4, 3], [3, 4, 0], [0, 3, 1], [2, 4, 3], [2, 1, 4], [2, 4, 3], [3, 0, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0], [2, 4, 1], [3, 0, 4], [0, 3, 1], [2, 1, 4], [2, 0, 1], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 3, 1], [0, 1, 2], [2, 1, 0], [3, 4, 0], [0, 1, 2], [0, 1, 2], [0, 3, 1], [2, 1, 0], [3, 4, 2], [2, 1, 0], [2, 4, 3], [2, 4, 3], [2, 4, 1], [3, 0, 4], [3, 0, 4], [0, 1, 2], [2, 4, 0], [2, 4, 1], [2, 4, 3], [2, 1, 4], [0, 3, 1], [2, 4, 3], [2, 1, 4], [0, 3, 4], [0, 3, 1], [1, 0, 2], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [3, 4, 2], [0, 1, 3], [2, 4, 1], [2, 0, 1], [4, 2, 3], [3, 4, 2], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 4], [3, 4, 0], [0, 1, 2], [0, 1, 2], [4, 2, 3], [0, 3, 1], [2, 1, 0], [1, 0, 2], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 1, 2], [1, 2, 0], [2, 4, 3], [0, 3, 4], [0, 3, 1], [3, 4, 0], [2, 4, 1], [2, 1, 4], [2, 4, 3], [0, 1, 2], [0, 1, 3], [1, 2, 0], [3, 0, 4], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 4, 1], [0, 3, 4], [2, 4, 3], [0, 1, 2], [2, 4, 1], [3, 0, 4], [2, 0, 1], [0, 3, 1], [0, 1, 3], [2, 4, 3], [0, 3, 4], [0, 1, 2], [3, 4, 0], [2, 4, 3], [2, 4, 3], [2, 4, 1], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 3, 1], [0, 1, 2], [1, 2, 0], [2, 4, 1], [3, 4, 0], [0, 3, 4], [0, 3, 1], [2, 1, 0], [2, 4, 1], [3, 0, 4], [0, 1, 3], [0, 3, 1], [2, 4, 3], [2, 4, 1], [3, 4, 0], [0, 3, 1], [2, 1, 0], [3, 4, 2], [2, 1, 4], [3, 4, 2], [0, 3, 4], [0, 3, 4], [2, 1, 0], [2, 4, 3], [3, 4, 2], [3, 0, 4], [2, 4, 3], [2, 1, 4], [2, 4, 1], [3, 4, 0], [0, 3, 1], [0, 2, 1], [2, 1, 4], [2, 4, 3], [2, 1, 4], [0, 3, 4], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [4, 2, 3], [0, 1, 2], [2, 4, 1], [4, 3, 2], [0, 3, 1], [4, 2, 3], [2, 1, 0], [2, 4, 1], [3, 4, 0], [1, 2, 0], [2, 1, 0], [2, 4, 1], [2, 4, 1], [3, 0, 4], [0, 1, 3], [0, 3, 4], [1, 0, 2], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 1, 4], [2, 1, 0], [3, 4, 0], [3, 0, 4], [0, 3, 4], [0, 3, 1], [2, 1, 0], [2, 0, 1], [3, 4, 0], [0, 3, 4], [0, 2, 1], [3, 4, 2], [2, 1, 4], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 1, 0], [2, 1, 0], [3, 4, 0], [0, 3, 1], [0, 3, 1], [2, 4, 1], [2, 4, 1], [3, 4, 0], [0, 1, 3], [0, 2, 1], [2, 1, 0], [2, 4, 1], [2, 4, 1], [0, 1, 3], [2, 1, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004686088108819379  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004686082690215308  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004686073330808277  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0046860595379979155  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004686042789585334  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004686021607769422  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.00468599796295166  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004685970869931308  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004685941806509475  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004685910280085792  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004685877768461369  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004685843286435466  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004685807819208823  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.00468576988898033  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0046857309735510964  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004685691072921123  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0046856496944900385  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004685608808659325  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004685565952427131  Accuracy on Support set:0.0\n",
      "torch.Size([242, 2048]) torch.Size([242])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004685523588795307  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.049135416746139526 tensor([9.5082e-01, 4.2932e-05, 2.2301e-10, 4.9135e-02, 1.1739e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04108940064907074 tensor([5.5700e-07, 4.1089e-02, 9.5386e-01, 3.7657e-06, 5.0418e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0065633575432002544 tensor([2.6512e-08, 4.2407e-03, 9.8919e-01, 2.5181e-06, 6.5634e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013783063739538193 tensor([9.8622e-01, 3.8125e-07, 1.5138e-14, 1.3783e-02, 1.7319e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.49451255798339844 tensor([0.0006, 0.0009, 0.0011, 0.5030, 0.4945], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.053290415555238724 tensor([5.3290e-02, 9.4669e-01, 1.7372e-05, 2.2135e-06, 6.8489e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24320413172245026 tensor([1.5134e-05, 2.4320e-01, 7.5196e-01, 2.4141e-05, 4.8004e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20152701437473297 tensor([0.0007, 0.3291, 0.4639, 0.0047, 0.2015], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14936563372612 tensor([8.3541e-01, 1.4681e-02, 8.0792e-06, 1.4937e-01, 5.3224e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07551757246255875 tensor([7.7633e-03, 8.7582e-04, 1.1470e-04, 9.1573e-01, 7.5518e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17979620397090912 tensor([1.7980e-01, 7.2004e-06, 9.7912e-10, 8.2018e-01, 1.9412e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4437468349933624 tensor([2.2094e-06, 4.4375e-01, 5.5586e-01, 3.2749e-07, 3.8636e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008494351990520954 tensor([7.9143e-09, 1.5850e-03, 9.8992e-01, 2.5874e-06, 8.4944e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4702240228652954 tensor([1.0123e-07, 3.2991e-04, 4.7022e-01, 1.3088e-03, 5.2814e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2465088963508606 tensor([0.0145, 0.6158, 0.0755, 0.0476, 0.2465], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17127808928489685 tensor([8.2871e-01, 1.4615e-05, 1.0415e-10, 1.7128e-01, 2.0169e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015082644298672676 tensor([1.3157e-03, 9.8341e-01, 1.5083e-02, 1.3016e-05, 1.8202e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4842432737350464 tensor([2.1128e-09, 6.6310e-05, 4.8424e-01, 8.6565e-05, 5.1560e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4086320102214813 tensor([0.5375, 0.4086, 0.0011, 0.0514, 0.0014], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17318712174892426 tensor([4.4263e-05, 5.0333e-06, 1.4935e-05, 8.2675e-01, 1.7319e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13458804786205292 tensor([1.3459e-01, 1.3444e-07, 1.3893e-12, 8.6541e-01, 9.5361e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014465536922216415 tensor([1.4466e-02, 9.8541e-01, 1.1379e-04, 3.1756e-06, 3.7856e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2597348093986511 tensor([5.3301e-06, 1.3799e-02, 7.2541e-01, 1.0542e-03, 2.5973e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3686535358428955 tensor([5.7188e-01, 5.4773e-02, 1.2713e-04, 3.6865e-01, 4.5671e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42887693643569946 tensor([4.8463e-08, 5.3702e-09, 1.3100e-06, 4.2888e-01, 5.7112e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011907326057553291 tensor([3.2169e-03, 6.4468e-05, 6.2341e-06, 9.8481e-01, 1.1907e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0031710404437035322 tensor([3.1710e-03, 9.9668e-01, 1.5040e-04, 3.8590e-07, 1.1238e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3255620300769806 tensor([3.8612e-06, 3.2556e-01, 6.7371e-01, 1.3434e-06, 7.2367e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.034376777708530426 tensor([2.3178e-03, 1.2568e-04, 3.2158e-05, 9.6315e-01, 3.4377e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0026841661892831326 tensor([1.0163e-04, 9.0669e-08, 1.4939e-08, 9.9721e-01, 2.6842e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29496631026268005 tensor([2.6753e-04, 4.0656e-04, 6.9896e-04, 2.9497e-01, 7.0366e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14415062963962555 tensor([3.5316e-05, 1.4415e-01, 8.0842e-01, 2.6743e-04, 4.7125e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40390312671661377 tensor([0.0008, 0.5527, 0.4039, 0.0017, 0.0409], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2213689386844635 tensor([0.4868, 0.2652, 0.0015, 0.2214, 0.0252], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03288837522268295 tensor([3.2888e-02, 3.0489e-04, 3.1752e-06, 9.5835e-01, 8.4537e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06495466083288193 tensor([9.3456e-01, 4.6253e-04, 9.7501e-09, 6.4955e-02, 2.2179e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0026286812499165535 tensor([2.6287e-03, 9.9679e-01, 5.7875e-04, 1.0474e-06, 3.8093e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36656126379966736 tensor([4.3645e-08, 2.6827e-04, 3.6656e-01, 3.9506e-04, 6.3278e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06801039725542068 tensor([0.0315, 0.8054, 0.0680, 0.0390, 0.0560], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19827419519424438 tensor([0.0008, 0.0181, 0.0539, 0.1983, 0.7289], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.469699501991272 tensor([4.6970e-01, 3.5571e-07, 7.9088e-13, 5.3030e-01, 3.0142e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021963421255350113 tensor([2.3084e-03, 9.7453e-01, 2.1963e-02, 8.4629e-05, 1.1152e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0018457613186910748 tensor([7.5750e-10, 1.4135e-03, 9.9674e-01, 1.2850e-07, 1.8458e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08597822487354279 tensor([3.9883e-07, 8.5978e-02, 9.1324e-01, 6.7127e-07, 7.8525e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002779813716188073 tensor([3.7739e-09, 5.0692e-07, 9.5879e-04, 2.7798e-03, 9.9626e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027009544894099236 tensor([9.6901e-01, 2.7010e-02, 1.0513e-06, 3.9608e-03, 1.7026e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014824152924120426 tensor([1.0699e-03, 9.8384e-01, 1.4824e-02, 1.4352e-05, 2.5377e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10481137782335281 tensor([5.5442e-08, 1.5042e-03, 8.9361e-01, 6.9818e-05, 1.0481e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.058157000690698624 tensor([9.4118e-01, 5.8157e-02, 6.3316e-07, 6.5856e-04, 2.1347e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007485335227102041 tensor([7.4853e-03, 8.8961e-06, 7.6614e-08, 9.8988e-01, 2.6239e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01783142425119877 tensor([9.8196e-01, 1.7831e-02, 3.8150e-08, 2.0955e-04, 7.6411e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3096059560775757 tensor([5.2814e-05, 6.8863e-01, 3.0961e-01, 9.7635e-06, 1.6980e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2919754683971405 tensor([7.0628e-01, 2.9198e-01, 2.6160e-05, 1.6820e-03, 3.7584e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3207523226737976 tensor([1.2962e-08, 6.6407e-05, 3.2075e-01, 7.8554e-04, 6.7840e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019024154171347618 tensor([3.0321e-04, 1.9316e-06, 3.9219e-07, 9.8067e-01, 1.9024e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031666080467402935 tensor([9.9679e-01, 4.1858e-05, 1.6945e-11, 3.1666e-03, 2.8416e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09374069422483444 tensor([0.0143, 0.8501, 0.0937, 0.0106, 0.0313], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006647296715527773 tensor([7.6121e-12, 3.9175e-05, 9.9331e-01, 1.1653e-07, 6.6473e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.031045328825712204 tensor([3.1045e-02, 4.4720e-05, 2.8347e-07, 9.6792e-01, 9.8964e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4146988093852997 tensor([0.0143, 0.0184, 0.0015, 0.5511, 0.4147], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006168867461383343 tensor([9.9253e-01, 1.2973e-03, 8.8368e-09, 6.1689e-03, 3.8343e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20553937554359436 tensor([1.7293e-05, 7.9408e-01, 2.0554e-01, 1.0495e-06, 3.6470e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006780080962926149 tensor([1.7104e-11, 5.8372e-05, 9.9316e-01, 1.7277e-07, 6.7801e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14814393222332 tensor([3.0714e-05, 3.7116e-02, 8.1156e-01, 3.1497e-03, 1.4814e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1241736114025116 tensor([4.9484e-06, 3.2813e-05, 9.8673e-04, 1.2417e-01, 8.7480e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.050273116677999496 tensor([3.6874e-02, 7.2489e-03, 3.2919e-04, 9.0527e-01, 5.0273e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.028985049575567245 tensor([9.2942e-04, 9.6972e-01, 2.8985e-02, 1.9598e-05, 3.4847e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017512496560811996 tensor([3.9434e-07, 1.4384e-05, 3.0774e-03, 1.7512e-02, 9.7940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.160967618227005 tensor([2.8769e-06, 1.7767e-05, 2.2026e-03, 1.6097e-01, 8.3681e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04438638314604759 tensor([8.1643e-07, 2.6203e-02, 9.2938e-01, 3.4497e-05, 4.4386e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015501888701692224 tensor([1.2360e-03, 1.2558e-06, 5.3183e-08, 9.9721e-01, 1.5502e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018797144293785095 tensor([1.8797e-02, 9.8052e-01, 6.2736e-04, 2.6591e-05, 3.3143e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020450253039598465 tensor([1.7280e-03, 1.4688e-05, 1.3221e-06, 9.7781e-01, 2.0450e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4257059693336487 tensor([0.4295, 0.4257, 0.0027, 0.1334, 0.0088], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06656444817781448 tensor([9.2971e-01, 6.6564e-02, 2.1935e-06, 3.6769e-03, 4.2851e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015877703204751015 tensor([9.8411e-01, 9.7825e-06, 5.5236e-12, 1.5878e-02, 8.1218e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.38016045093536377 tensor([2.7239e-05, 3.8016e-01, 6.1595e-01, 1.7694e-05, 3.8439e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1044798269867897 tensor([9.8243e-08, 1.0448e-01, 8.9529e-01, 7.9933e-08, 2.3335e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3532526195049286 tensor([0.0038, 0.0325, 0.0404, 0.3533, 0.5700], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2623871862888336 tensor([0.1833, 0.2624, 0.0055, 0.4251, 0.1237], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030241351574659348 tensor([9.6898e-01, 7.7270e-04, 1.7027e-08, 3.0241e-02, 7.2846e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1140739843249321 tensor([8.8468e-01, 1.1407e-01, 2.6948e-06, 1.2264e-03, 1.5025e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.305910587310791 tensor([3.0591e-01, 7.2070e-05, 1.5432e-08, 6.9387e-01, 1.4587e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11901839077472687 tensor([2.9356e-08, 7.1658e-04, 8.8018e-01, 8.1615e-05, 1.1902e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24491576850414276 tensor([0.2449, 0.6209, 0.0049, 0.0882, 0.0412], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.34334054589271545 tensor([3.4334e-01, 2.9005e-04, 9.3955e-08, 6.5605e-01, 3.2417e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03215457871556282 tensor([3.3777e-07, 3.2155e-02, 9.6483e-01, 2.3650e-06, 3.0148e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2799952030181885 tensor([0.0014, 0.2800, 0.3893, 0.0172, 0.3121], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3762994110584259 tensor([3.6022e-04, 9.7269e-02, 5.0614e-01, 1.9930e-02, 3.7630e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3614737391471863 tensor([5.6930e-07, 1.0197e-07, 5.3494e-06, 3.6147e-01, 6.3852e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00455833226442337 tensor([9.9544e-01, 1.1791e-06, 3.8771e-14, 4.5583e-03, 2.1831e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002178202150389552 tensor([2.1782e-03, 9.9760e-01, 2.2435e-04, 3.6189e-07, 1.2974e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004250975325703621 tensor([1.9536e-11, 1.0702e-04, 9.9564e-01, 7.4521e-08, 4.2510e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02516551874577999 tensor([9.7462e-01, 2.0805e-04, 1.9506e-09, 2.5166e-02, 1.4409e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007241751533001661 tensor([3.5256e-09, 7.7728e-08, 1.3180e-04, 7.2418e-03, 9.9263e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12653610110282898 tensor([8.7346e-01, 4.6344e-06, 9.4565e-12, 1.2654e-01, 2.7483e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003321431577205658 tensor([3.3214e-03, 9.9375e-01, 2.8588e-03, 1.0215e-05, 6.4223e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01106593944132328 tensor([2.1938e-08, 5.8374e-06, 1.1066e-02, 3.5498e-03, 9.8538e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13907954096794128 tensor([8.2588e-01, 3.3758e-02, 3.0552e-05, 1.3908e-01, 1.2536e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3059957027435303 tensor([0.0069, 0.0079, 0.0020, 0.6772, 0.3060], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12483255565166473 tensor([0.1248, 0.8440, 0.0075, 0.0171, 0.0065], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.007516545243561268 tensor([7.5165e-03, 9.9227e-01, 2.0946e-04, 1.6450e-06, 3.0238e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.42623862624168396 tensor([6.3718e-05, 5.7077e-01, 4.2624e-01, 2.7193e-05, 2.9013e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13630151748657227 tensor([7.5673e-01, 1.3630e-01, 1.7209e-04, 1.0469e-01, 2.1108e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.018222393468022346 tensor([4.2623e-04, 2.6703e-06, 3.5464e-07, 9.8135e-01, 1.8222e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005318862851709127 tensor([5.3189e-03, 1.6557e-07, 1.6365e-10, 9.9462e-01, 6.4054e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00334099680185318 tensor([5.9742e-04, 9.9604e-01, 3.3410e-03, 8.9582e-07, 1.6304e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22275850176811218 tensor([0.0008, 0.2228, 0.3726, 0.0120, 0.3918], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21698345243930817 tensor([3.3232e-05, 2.9880e-04, 8.8296e-03, 2.1698e-01, 7.7385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3518487811088562 tensor([3.7353e-04, 9.0068e-04, 1.8790e-03, 3.5185e-01, 6.4500e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15874117612838745 tensor([1.5874e-01, 8.4119e-01, 3.4175e-05, 3.5239e-05, 4.1916e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05076824873685837 tensor([1.1461e-06, 1.5900e-02, 9.3326e-01, 6.9937e-05, 5.0768e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26066628098487854 tensor([0.0013, 0.0008, 0.0007, 0.7366, 0.2607], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2395813763141632 tensor([3.7095e-07, 3.3299e-07, 6.1349e-05, 2.3958e-01, 7.6036e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1318485289812088 tensor([1.8360e-04, 3.8932e-03, 3.0528e-02, 1.3185e-01, 8.3355e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020519310608506203 tensor([9.7945e-01, 2.6172e-05, 4.1762e-11, 2.0519e-02, 2.3056e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.27837836742401123 tensor([6.2706e-05, 7.2017e-01, 2.7838e-01, 1.0614e-05, 1.3761e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007301108445972204 tensor([1.1269e-10, 1.5004e-04, 9.9255e-01, 4.3647e-07, 7.3011e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09922695904970169 tensor([1.3681e-06, 1.5030e-05, 2.3499e-03, 9.9227e-02, 8.9841e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.33759456872940063 tensor([1.4653e-04, 4.3114e-05, 9.8721e-05, 6.6212e-01, 3.3759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2371675670146942 tensor([7.6271e-01, 2.3717e-01, 1.5796e-06, 1.2069e-04, 2.2249e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4277040958404541 tensor([0.0006, 0.4868, 0.4277, 0.0020, 0.0829], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003938855137676001 tensor([7.5602e-11, 1.6943e-04, 9.9589e-01, 1.9743e-07, 3.9389e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016771607100963593 tensor([3.3257e-03, 9.7905e-01, 1.6772e-02, 1.1212e-04, 7.3748e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4224145710468292 tensor([5.7586e-01, 1.1664e-03, 3.6465e-07, 4.2241e-01, 5.5673e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.022717179730534554 tensor([2.2717e-02, 3.2094e-04, 7.3272e-06, 9.7202e-01, 4.9312e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012676532380282879 tensor([9.8282e-01, 1.2677e-02, 2.3280e-07, 4.4885e-03, 1.5734e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008500018157064915 tensor([2.5445e-07, 2.3457e-05, 8.5663e-03, 8.5000e-03, 9.8291e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008101974613964558 tensor([5.0616e-08, 7.0376e-06, 8.1020e-03, 7.9148e-03, 9.8398e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10693816840648651 tensor([3.4079e-06, 5.9606e-08, 4.1581e-07, 8.9306e-01, 1.0694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22723908722400665 tensor([0.1075, 0.3492, 0.0103, 0.2272, 0.3057], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2527002692222595 tensor([4.3306e-06, 2.5270e-01, 7.4529e-01, 3.4982e-06, 2.0035e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09928392618894577 tensor([2.5475e-07, 3.6530e-03, 8.9695e-01, 1.1062e-04, 9.9284e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02301051653921604 tensor([9.6021e-01, 2.3011e-02, 1.7910e-06, 1.6736e-02, 4.1125e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018666420131921768 tensor([1.0912e-06, 3.3352e-05, 1.7117e-03, 1.8666e-02, 9.7959e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05047367140650749 tensor([9.4948e-01, 4.1012e-05, 1.9545e-10, 5.0474e-02, 8.5238e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2836900055408478 tensor([0.0028, 0.5685, 0.2837, 0.0085, 0.1365], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01964339055120945 tensor([9.6542e-01, 1.9643e-02, 1.4439e-06, 1.4895e-02, 3.5752e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22696420550346375 tensor([0.0946, 0.3479, 0.0169, 0.2270, 0.3136], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.035593993961811066 tensor([1.8309e-07, 1.7366e-06, 2.7211e-04, 3.5594e-02, 9.6413e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02527759037911892 tensor([9.7382e-01, 8.9413e-04, 1.9018e-08, 2.5278e-02, 7.5304e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2493211179971695 tensor([1.6313e-04, 7.4778e-01, 2.4932e-01, 2.5757e-05, 2.7094e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38175392150878906 tensor([0.0051, 0.0058, 0.0021, 0.6052, 0.3818], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07473915815353394 tensor([2.1577e-06, 1.0535e-02, 9.1419e-01, 5.3408e-04, 7.4739e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0481802299618721 tensor([9.5588e-06, 1.3770e-03, 4.8180e-02, 1.7784e-02, 9.3265e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15206225216388702 tensor([8.2608e-01, 1.5206e-01, 5.3634e-05, 2.1387e-02, 4.1464e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003522864542901516 tensor([3.1570e-09, 3.5229e-03, 9.9321e-01, 2.2748e-07, 3.2672e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014299037866294384 tensor([4.9193e-07, 2.7786e-05, 6.3075e-03, 1.4299e-02, 9.7937e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03583214059472084 tensor([0.0093, 0.9503, 0.0358, 0.0012, 0.0034], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11216761916875839 tensor([0.1122, 0.7184, 0.0128, 0.0867, 0.0700], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0028193271718919277 tensor([9.9501e-01, 2.8193e-03, 1.2859e-08, 2.1667e-03, 6.7011e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24644069373607635 tensor([2.4644e-01, 2.4886e-02, 1.6538e-04, 7.0516e-01, 2.3348e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17836184799671173 tensor([0.0036, 0.7759, 0.1784, 0.0037, 0.0384], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.054787103086709976 tensor([9.4510e-01, 5.4787e-02, 1.3271e-07, 1.1137e-04, 2.9149e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006772019900381565 tensor([2.5351e-03, 2.5233e-05, 2.1230e-06, 9.9067e-01, 6.7720e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.34713906049728394 tensor([3.4714e-01, 9.8426e-05, 2.2990e-08, 6.5264e-01, 1.1981e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.018629522994160652 tensor([9.5598e-09, 1.1108e-03, 9.8025e-01, 6.4902e-06, 1.8630e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02959783561527729 tensor([1.7514e-08, 1.3170e-03, 9.6907e-01, 1.1982e-05, 2.9598e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19534194469451904 tensor([3.8318e-06, 1.7600e-05, 1.5312e-03, 1.9534e-01, 8.0311e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14399662613868713 tensor([2.8082e-04, 4.3497e-05, 4.8934e-05, 8.5563e-01, 1.4400e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004040787927806377 tensor([9.9196e-01, 3.9928e-03, 4.4254e-08, 4.0408e-03, 4.2216e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008183802478015423 tensor([2.6630e-04, 9.9152e-01, 8.1838e-03, 7.3825e-07, 2.4688e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.043507616966962814 tensor([4.6348e-08, 4.3508e-02, 9.5601e-01, 1.2021e-07, 4.8681e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1865396648645401 tensor([2.2197e-06, 1.1762e-02, 8.0120e-01, 4.9383e-04, 1.8654e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005169067298993468 tensor([5.6673e-05, 7.0282e-09, 4.1940e-10, 9.9943e-01, 5.1691e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4457193911075592 tensor([4.4572e-01, 5.3968e-08, 3.3881e-14, 5.5428e-01, 4.4429e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4075383245944977 tensor([1.4582e-05, 4.0754e-01, 5.9012e-01, 7.4938e-06, 2.3243e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021986136212944984 tensor([2.7826e-06, 3.1792e-04, 2.1986e-02, 1.0421e-02, 9.6727e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01194017380475998 tensor([3.8880e-10, 2.5744e-04, 9.8780e-01, 9.6964e-07, 1.1940e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3675439655780792 tensor([0.5694, 0.3675, 0.0012, 0.0590, 0.0029], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3770683705806732 tensor([6.2290e-01, 2.3312e-05, 6.3983e-10, 3.7707e-01, 1.1081e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.050835635513067245 tensor([4.9431e-05, 9.4907e-01, 5.0836e-02, 5.5820e-07, 4.2426e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013079707510769367 tensor([4.4323e-09, 9.8407e-04, 9.8593e-01, 2.6930e-06, 1.3080e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.047125909477472305 tensor([4.2272e-02, 6.4040e-03, 2.9818e-04, 9.0390e-01, 4.7126e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009138120338320732 tensor([9.1381e-03, 9.8859e-01, 1.9880e-03, 6.8823e-05, 2.1646e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2539327144622803 tensor([2.5393e-01, 2.5342e-05, 4.4959e-09, 7.4598e-01, 6.2800e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010639575310051441 tensor([1.0640e-02, 9.8220e-01, 6.3415e-03, 2.1013e-04, 6.1377e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010696100071072578 tensor([8.9577e-08, 1.0696e-02, 9.8126e-01, 2.9406e-06, 8.0424e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07942796498537064 tensor([2.1941e-07, 7.9428e-02, 9.1954e-01, 4.3804e-07, 1.0334e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00561521528288722 tensor([8.7975e-05, 1.7108e-07, 5.5606e-08, 9.9430e-01, 5.6152e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00038222369039431214 tensor([9.9944e-01, 3.8222e-04, 5.1921e-11, 1.7794e-04, 1.1939e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004215999040752649 tensor([4.2160e-03, 9.9390e-01, 1.8310e-03, 9.0173e-06, 3.9670e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.48408299684524536 tensor([1.0697e-05, 4.8408e-01, 5.1526e-01, 2.3732e-06, 6.4027e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3288886845111847 tensor([0.5809, 0.3289, 0.0008, 0.0839, 0.0055], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013889612630009651 tensor([1.3890e-02, 2.3404e-04, 6.5826e-06, 9.7601e-01, 9.8614e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004870330449193716 tensor([9.9511e-01, 1.7636e-05, 5.5665e-12, 4.8703e-03, 4.6041e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02533232420682907 tensor([1.4625e-03, 9.7242e-01, 2.5332e-02, 5.2552e-05, 7.3224e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01680140569806099 tensor([5.5361e-11, 6.0793e-05, 9.8314e-01, 8.0671e-07, 1.6801e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2733476459980011 tensor([0.0557, 0.3048, 0.0388, 0.3273, 0.2733], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2627772092819214 tensor([4.0168e-04, 5.9176e-04, 6.2245e-04, 2.6278e-01, 7.3561e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0069703455083072186 tensor([6.9703e-03, 4.2142e-06, 2.6830e-08, 9.9216e-01, 8.6644e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027281255461275578 tensor([9.9703e-01, 2.7281e-03, 1.9700e-09, 2.4097e-04, 9.1102e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08305506408214569 tensor([8.3055e-02, 2.6590e-03, 3.2077e-05, 8.9681e-01, 1.7444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10944461077451706 tensor([2.5965e-06, 1.0944e-01, 8.8531e-01, 7.4594e-06, 5.2314e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11898556351661682 tensor([2.9040e-05, 2.6936e-04, 3.7993e-03, 1.1899e-01, 8.7692e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4384634494781494 tensor([4.3846e-01, 6.3160e-06, 1.4286e-10, 5.6152e-01, 6.0779e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07224714756011963 tensor([1.1962e-04, 9.2740e-01, 7.2247e-02, 2.5638e-06, 2.2941e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.481113076210022 tensor([1.1545e-05, 2.1832e-02, 4.9570e-01, 1.3439e-03, 4.8111e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21245288848876953 tensor([2.1245e-01, 7.8623e-01, 2.6974e-04, 8.6099e-04, 1.8806e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010181959718465805 tensor([1.6252e-08, 5.2197e-07, 4.3863e-04, 1.0182e-02, 9.8938e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00031048868549987674 tensor([9.9963e-01, 6.2335e-05, 4.4156e-12, 3.1049e-04, 3.9748e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05758346617221832 tensor([5.7583e-02, 9.4217e-01, 1.9886e-04, 3.5524e-05, 1.4527e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026635929942131042 tensor([2.0858e-10, 9.9628e-05, 9.7326e-01, 2.1903e-06, 2.6636e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0940864086151123 tensor([0.0941, 0.8356, 0.0192, 0.0392, 0.0119], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008988095447421074 tensor([3.1188e-04, 1.1983e-06, 2.2025e-07, 9.9070e-01, 8.9881e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.40490424633026123 tensor([5.9505e-01, 3.0984e-05, 1.2789e-09, 4.0490e-01, 1.6753e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004016275051981211 tensor([6.5741e-04, 9.9530e-01, 4.0163e-03, 1.2839e-06, 2.1347e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20476041734218597 tensor([6.7445e-04, 3.1551e-04, 4.8460e-04, 7.9377e-01, 2.0476e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06688281148672104 tensor([5.1340e-02, 9.4243e-03, 4.3974e-04, 8.7191e-01, 6.6883e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07494328916072845 tensor([9.1157e-01, 1.3227e-02, 2.3349e-06, 7.4943e-02, 2.5375e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015068509383127093 tensor([9.9844e-01, 5.4710e-05, 1.3621e-11, 1.5069e-03, 3.2174e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20802178978919983 tensor([8.9271e-04, 7.7884e-01, 2.0802e-01, 3.7188e-04, 1.1877e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030500011518597603 tensor([1.5044e-05, 1.0932e-03, 3.0500e-02, 2.5844e-02, 9.4255e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07414951175451279 tensor([7.2227e-06, 7.4150e-02, 9.1555e-01, 8.2917e-05, 1.0209e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4020273983478546 tensor([6.7852e-04, 5.8341e-04, 8.3784e-04, 5.9587e-01, 4.0203e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021246109157800674 tensor([9.7875e-01, 2.4354e-06, 6.8531e-13, 2.1246e-02, 1.9052e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002833317732438445 tensor([2.8333e-03, 9.9704e-01, 1.2218e-04, 2.6875e-07, 7.3260e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006660467013716698 tensor([8.8724e-09, 1.7756e-03, 9.9156e-01, 2.4345e-06, 6.6605e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03394471853971481 tensor([3.3945e-02, 1.3871e-03, 3.9736e-05, 9.3636e-01, 2.8272e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10034258663654327 tensor([1.4390e-03, 1.1096e-04, 2.2926e-05, 8.9808e-01, 1.0034e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0049398732371628284 tensor([9.5842e-05, 9.9496e-01, 4.9399e-03, 1.1555e-07, 5.3180e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13287869095802307 tensor([9.5139e-05, 8.6656e-01, 1.3288e-01, 5.2489e-06, 4.6170e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016739102080464363 tensor([3.1344e-07, 1.6739e-02, 9.7687e-01, 8.0235e-06, 6.3812e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016474967822432518 tensor([2.8046e-07, 1.2671e-02, 9.7084e-01, 1.5478e-05, 1.6475e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004906155169010162 tensor([1.8811e-04, 2.0065e-07, 2.2089e-08, 9.9491e-01, 4.9062e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17600731551647186 tensor([0.1141, 0.1394, 0.0060, 0.5646, 0.1760], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01013258658349514 tensor([9.3979e-03, 9.7891e-01, 1.0133e-02, 3.2197e-04, 1.2399e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013469770550727844 tensor([4.3085e-08, 1.3470e-02, 9.8361e-01, 7.3211e-07, 2.9153e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28885915875434875 tensor([0.0176, 0.0879, 0.0285, 0.5771, 0.2889], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014039181172847748 tensor([1.4039e-02, 9.8481e-01, 1.0769e-03, 3.0124e-05, 4.8642e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31744834780693054 tensor([3.1745e-01, 9.3337e-04, 9.8186e-07, 6.8002e-01, 1.5987e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005422924645245075 tensor([9.9314e-01, 1.4311e-03, 1.1037e-08, 5.4229e-03, 1.0737e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23237048089504242 tensor([2.7503e-04, 4.8040e-02, 2.3237e-01, 2.8368e-02, 6.9095e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48378875851631165 tensor([7.1908e-07, 2.0957e-03, 5.1318e-01, 9.3681e-04, 4.8379e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024553567171096802 tensor([8.9449e-04, 1.2218e-05, 2.1808e-06, 9.7454e-01, 2.4554e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03567139804363251 tensor([5.4950e-03, 2.6381e-04, 2.1500e-05, 9.5855e-01, 3.5671e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003891386790201068 tensor([2.2553e-03, 9.9379e-01, 3.8914e-03, 6.1191e-06, 5.7067e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03751901537179947 tensor([1.2920e-11, 2.3568e-05, 9.6246e-01, 6.4743e-07, 3.7519e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03063053824007511 tensor([4.7297e-08, 1.6946e-03, 9.6764e-01, 3.4214e-05, 3.0631e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.035063859075307846 tensor([9.6404e-01, 8.8868e-04, 1.3654e-08, 3.5064e-02, 1.1086e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24074578285217285 tensor([7.5890e-01, 2.7431e-04, 1.3701e-08, 2.4075e-01, 8.3180e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.029913129284977913 tensor([3.8059e-05, 9.7002e-01, 2.9913e-02, 2.2213e-07, 2.6317e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16127437353134155 tensor([4.5052e-08, 1.2041e-04, 1.6127e-01, 9.3110e-04, 8.3767e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29875385761260986 tensor([0.0113, 0.1682, 0.1375, 0.2988, 0.3842], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06925652176141739 tensor([2.3039e-04, 6.1029e-06, 3.0501e-06, 9.3050e-01, 6.9257e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003313047345727682 tensor([9.9659e-01, 9.4499e-05, 7.1816e-11, 3.3130e-03, 8.3453e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10853743553161621 tensor([8.1991e-01, 6.9730e-02, 5.4552e-05, 1.0854e-01, 1.7645e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07579442858695984 tensor([2.4895e-05, 4.2640e-03, 7.5794e-02, 1.1616e-02, 9.0830e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24040727317333221 tensor([0.2679, 0.4517, 0.0070, 0.2404, 0.0330], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04306565225124359 tensor([1.8280e-03, 5.1026e-05, 5.1676e-06, 9.5505e-01, 4.3066e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2], [4, 3, 2, 0], [0, 3, 4], [0, 3], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4], [0, 3, 1, 4], [0, 1, 3], [0, 3, 2], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3], [2, 4, 3], [1, 2, 0, 4], [2, 1, 4, 0], [3, 4, 2, 0], [0, 3, 1], [2, 4, 1], [1, 0, 2], [2, 1, 0, 4], [3, 4, 2, 0], [3, 0, 4], [2, 1, 0, 4], [2, 1, 0, 4], [0, 1, 2], [0, 3, 4, 1], [0, 3, 4], [2, 4], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3], [0, 3, 4, 2], [0, 1, 2, 3], [2, 4, 1], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [3, 0, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1], [2, 4, 1, 3], [3, 0, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 0, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 3], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4], [3, 0, 4, 1], [0, 1, 2], [2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4], [0, 3, 1, 4], [2, 4, 3], [2, 1, 4], [0, 3, 4, 1], [0, 3], [0, 3, 1], [1, 0, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 4, 1, 3], [2, 0, 1], [4, 2, 3, 0], [3, 4, 2, 0], [3, 0, 4], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3], [0, 1, 2], [0, 1, 2], [4, 2, 3, 0], [0, 3, 1, 4], [2, 1, 0], [1, 0, 2], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0], [2, 4, 3], [0, 3, 4], [0, 3, 1, 4], [3, 4, 0, 2], [2, 4, 1], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [0, 1, 3, 2], [1, 2, 0, 4], [2, 0], [3, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [0, 3, 4], [2, 4, 3, 1], [2, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4], [2, 0, 1], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [2, 4, 3, 0], [2, 4, 3, 1], [2, 4, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1], [3, 0, 4], [0, 1, 3, 2], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 4], [3, 4, 2, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 0], [0, 2, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1], [3, 0, 4, 2], [0, 3, 1], [4, 2, 3], [0, 1, 2, 3], [2, 4, 1, 3], [4, 3, 2, 0], [0, 3, 1, 4], [4, 2, 3, 0], [2, 1, 0, 4], [2, 4, 1], [3, 4, 0, 2], [1, 2, 0], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4], [0, 1, 3, 2], [0, 3, 4, 1], [1, 0, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [3, 4, 0, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 0, 1, 4], [3, 4, 0, 2], [0, 3, 4, 1], [0, 2, 1], [3, 4, 2, 0], [2, 1, 4], [2, 4, 1, 3], [0, 3, 1], [0, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1], [3, 4, 0, 2], [0, 1, 3, 2], [0, 2, 1], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 3, 2], [2, 4], [2, 1, 0, 4]]\n",
      "[[0], [2], [2], [0], [3, 4], [1], [1, 2], [1, 2, 4], [0], [3], [3], [1, 2], [2], [2, 4], [1, 4], [0], [1], [2, 4], [0, 1], [3], [3], [1], [2, 4], [0, 3], [3, 4], [3], [1], [1, 2], [3], [3], [3, 4], [2], [1, 2], [0, 1, 3], [3], [0], [1], [2, 4], [1], [4], [0, 3], [1], [2], [2], [4], [0], [1], [2], [0], [3], [0], [1, 2], [0, 1], [2, 4], [3], [0], [1], [2], [3], [3, 4], [0], [1, 2], [2], [2], [4], [3], [1], [4], [4], [2], [3], [1], [3], [0, 1], [0], [0], [1, 2], [2], [3, 4], [1, 3], [0], [0], [0, 3], [2], [0, 1], [0, 3], [2], [1, 2, 4], [2, 4], [3, 4], [0], [1], [2], [0], [4], [0], [1], [4], [0], [3, 4], [1], [1], [1, 2], [0], [3], [3], [1], [1, 2, 4], [3, 4], [3, 4], [1], [2], [3, 4], [3, 4], [4], [0], [1, 2], [2], [4], [3, 4], [0, 1], [1, 2], [2], [1], [0, 3], [3], [0], [4], [4], [3], [1, 3, 4], [1, 2], [2], [0], [4], [0], [1, 2], [0], [1, 3, 4], [4], [0], [1, 2], [3, 4], [2], [4], [0], [2], [4], [1], [1], [0], [0, 3], [1], [0], [3], [0, 3], [2], [2], [4], [3], [0], [1], [2], [2], [3], [0, 3], [1, 2], [4], [2], [0, 1], [0, 3], [1], [2], [3], [1], [0, 3], [1], [2], [2], [3], [0], [1], [1, 2], [0, 1], [3], [0], [1], [2], [1, 3, 4], [3, 4], [3], [0], [3], [2], [4], [0, 3], [1], [2, 4], [0, 1], [4], [0], [1], [2], [1], [3], [0, 3], [1], [3, 4], [3], [0], [0], [1, 2], [4], [2], [3, 4], [0], [1], [2], [3], [3], [1], [1], [2], [2], [3], [3], [1], [2], [3, 4], [1], [0, 3], [0], [2, 4], [2, 4], [3], [3], [1], [2], [2], [0], [0, 3], [1], [4], [3, 4], [3], [0], [0], [4], [0, 1, 3], [3]]\n",
      "NL_pred of 3th iteration [[2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [4, 3, 2, 0], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 0, 2], [1, 2, 0, 4], [2, 1, 4, 0], [3, 4, 2, 0], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 4, 1], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 4, 2], [0, 1, 2, 3], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 0, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 4, 1], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 4, 1, 3], [4, 2, 3, 0], [3, 4, 2, 0], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 0], [3, 4, 0, 2], [4, 2, 3, 0], [0, 3, 1, 4], [0, 1, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [3, 4, 0, 2], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [0, 1, 3, 2], [1, 2, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [2, 4, 3, 0], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1, 4], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [3, 4, 2, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 2, 0], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [3, 0, 4, 2], [0, 1, 2, 3], [2, 4, 1, 3], [4, 3, 2, 0], [0, 3, 1, 4], [4, 2, 3, 0], [2, 1, 0, 4], [3, 4, 0, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 4, 1], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [3, 4, 0, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 0, 1, 4], [3, 4, 0, 2], [0, 3, 4, 1], [3, 4, 2, 0], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 3, 2], [2, 1, 0, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.006755991663251604  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0067558765411376955  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.006755659239632743  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.006755352701459612  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.006754967825753348  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.0067545168740408765  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0067540114266531805  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.006753460339137486  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.006752873829432896  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.006752258709498814  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.006751621791294643  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.006750967843191964  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.006750305039542062  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.006749636105128697  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.006748968533107213  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.006748300279889788  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.006747636795043945  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.006746979440961565  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.006746329580034529  Accuracy on Support set:0.0\n",
      "torch.Size([175, 2048]) torch.Size([175])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.006745688574654715  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.47929152846336365 tensor([0.0008, 0.0011, 0.0011, 0.5177, 0.4793], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26645001769065857 tensor([1.6170e-05, 2.6645e-01, 7.2930e-01, 2.1565e-05, 4.2095e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16904935240745544 tensor([0.0008, 0.3739, 0.4522, 0.0041, 0.1690], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4712356626987457 tensor([2.3161e-06, 4.7124e-01, 5.2842e-01, 2.9672e-07, 3.4047e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4808420240879059 tensor([1.0538e-07, 3.8068e-04, 5.1761e-01, 1.1645e-03, 4.8084e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1981898993253708 tensor([0.0153, 0.6763, 0.0709, 0.0393, 0.1982], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4747224748134613 tensor([2.1723e-09, 7.4264e-05, 5.2512e-01, 7.8509e-05, 4.7472e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.42761656641960144 tensor([0.5292, 0.4276, 0.0011, 0.0410, 0.0011], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23363536596298218 tensor([6.0240e-06, 1.6275e-02, 7.4912e-01, 9.6514e-04, 2.3364e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3153381049633026 tensor([6.2211e-01, 5.8731e-02, 1.1397e-04, 3.1534e-01, 3.7061e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4423937499523163 tensor([5.5437e-08, 5.9379e-09, 1.3410e-06, 4.4239e-01, 5.5760e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3520718514919281 tensor([4.1185e-06, 3.5207e-01, 6.4728e-01, 1.2237e-06, 6.3984e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3098871409893036 tensor([3.5891e-04, 5.3337e-04, 7.6057e-04, 3.0989e-01, 6.8846e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3729601800441742 tensor([0.0009, 0.5915, 0.3730, 0.0014, 0.0332], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18238067626953125 tensor([0.5080, 0.2879, 0.0014, 0.1824, 0.0204], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4040084779262543 tensor([4.8488e-08, 3.1938e-04, 4.0401e-01, 3.7049e-04, 5.9530e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.48203638195991516 tensor([5.1796e-01, 3.7724e-07, 7.1507e-13, 4.8204e-01, 2.5488e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.28282734751701355 tensor([5.4291e-05, 7.1569e-01, 2.8283e-01, 8.4491e-06, 1.4181e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2926304340362549 tensor([7.0589e-01, 2.9263e-01, 2.3091e-05, 1.4253e-03, 3.1231e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36083823442459106 tensor([1.3630e-08, 7.7089e-05, 3.6084e-01, 7.2134e-04, 6.3836e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39804065227508545 tensor([0.0185, 0.0237, 0.0017, 0.5581, 0.3980], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1836394965648651 tensor([1.7898e-05, 8.1604e-01, 1.8364e-01, 9.1301e-07, 3.0163e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.43719160556793213 tensor([0.4372, 0.4459, 0.0024, 0.1075, 0.0070], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.41111060976982117 tensor([2.9002e-05, 4.1111e-01, 5.8552e-01, 1.5760e-05, 3.3217e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3460310697555542 tensor([0.0047, 0.0427, 0.0467, 0.3460, 0.5598], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30134397745132446 tensor([0.2096, 0.3013, 0.0053, 0.3788, 0.1050], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3613552153110504 tensor([3.6136e-01, 8.0873e-05, 1.4098e-08, 6.3844e-01, 1.2240e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25150033831596375 tensor([0.2515, 0.6418, 0.0042, 0.0709, 0.0315], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4029925465583801 tensor([4.0299e-01, 3.2992e-04, 8.7588e-08, 5.9640e-01, 2.7344e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.269427627325058 tensor([0.0016, 0.3236, 0.3901, 0.0153, 0.2694], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.33274713158607483 tensor([3.9416e-04, 1.1496e-01, 5.3438e-01, 1.7517e-02, 3.3275e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3791249990463257 tensor([6.9109e-07, 1.1908e-07, 5.5616e-06, 3.7912e-01, 6.2087e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29331016540527344 tensor([0.0089, 0.0102, 0.0021, 0.6856, 0.2933], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3945804536342621 tensor([6.5506e-05, 6.0292e-01, 3.9458e-01, 2.3021e-05, 2.4101e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2626686990261078 tensor([0.0009, 0.2627, 0.3812, 0.0109, 0.3443], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21204283833503723 tensor([3.8211e-05, 3.7650e-04, 1.0479e-02, 2.1204e-01, 7.7706e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.36582881212234497 tensor([4.9019e-04, 1.1674e-03, 2.0464e-03, 3.6583e-01, 6.3047e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2545430660247803 tensor([0.0016, 0.0010, 0.0008, 0.7420, 0.2545], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2391139268875122 tensor([4.1568e-07, 3.9145e-07, 6.9170e-05, 2.3911e-01, 7.6082e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25234803557395935 tensor([6.4477e-05, 7.4644e-01, 2.5235e-01, 9.1458e-06, 1.1408e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3247048258781433 tensor([1.8525e-04, 5.4083e-05, 1.0590e-04, 6.7495e-01, 3.2470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24171172082424164 tensor([7.5818e-01, 2.4171e-01, 1.4505e-06, 1.0413e-04, 1.9081e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4035165011882782 tensor([0.0007, 0.5278, 0.4035, 0.0017, 0.0664], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3674268424510956 tensor([6.3087e-01, 1.2454e-03, 3.2614e-07, 3.6743e-01, 4.5358e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20385967195034027 tensor([0.1238, 0.4025, 0.0100, 0.2039, 0.2600], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27851057052612305 tensor([4.7412e-06, 2.7851e-01, 7.1971e-01, 3.2261e-06, 1.7739e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26387712359428406 tensor([0.0030, 0.6170, 0.2639, 0.0070, 0.1091], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2019752860069275 tensor([0.1065, 0.4037, 0.0168, 0.2020, 0.2711], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22409914433956146 tensor([1.6816e-04, 7.7348e-01, 2.2410e-01, 2.2130e-05, 2.2260e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.36928537487983704 tensor([0.0066, 0.0074, 0.0023, 0.6145, 0.3693], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2933901250362396 tensor([2.9339e-01, 2.9441e-02, 1.6360e-04, 6.5643e-01, 2.0578e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4037283658981323 tensor([4.0373e-01, 1.0982e-04, 2.1117e-08, 5.9606e-01, 1.0077e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4907591640949249 tensor([4.9076e-01, 5.7044e-08, 3.0763e-14, 5.0924e-01, 3.7999e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4407767653465271 tensor([1.5716e-05, 4.4078e-01, 5.5719e-01, 6.7637e-06, 2.0130e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37383517622947693 tensor([0.5751, 0.3738, 0.0010, 0.0478, 0.0023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32334256172180176 tensor([6.7662e-01, 2.4012e-05, 5.4329e-10, 3.2334e-01, 8.7697e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29997536540031433 tensor([2.9998e-01, 2.8276e-05, 4.1462e-09, 6.9994e-01, 5.4015e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4877268970012665 tensor([1.1116e-05, 5.1170e-01, 4.8773e-01, 2.1277e-06, 5.5949e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3398962616920471 tensor([0.5872, 0.3399, 0.0007, 0.0678, 0.0044], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23879174888134003 tensor([0.0647, 0.3600, 0.0387, 0.2978, 0.2388], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2737709581851959 tensor([5.3005e-04, 7.7494e-04, 6.8475e-04, 2.7377e-01, 7.2424e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.49650853872299194 tensor([4.9651e-01, 6.7247e-06, 1.2492e-10, 5.0348e-01, 4.9831e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44552189111709595 tensor([1.3650e-05, 2.6792e-02, 5.2640e-01, 1.2723e-03, 4.4552e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21176061034202576 tensor([2.1176e-01, 7.8713e-01, 2.3715e-04, 7.2079e-04, 1.5353e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35174959897994995 tensor([6.4820e-01, 3.2094e-05, 1.1017e-09, 3.5175e-01, 1.3489e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1966775804758072 tensor([8.4357e-04, 3.9571e-04, 5.2375e-04, 8.0156e-01, 1.9668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.18606489896774292 tensor([8.8607e-04, 8.0358e-01, 1.8606e-01, 2.9426e-04, 9.1758e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39534810185432434 tensor([0.0009, 0.0008, 0.0009, 0.6021, 0.3953], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27272143959999084 tensor([0.0217, 0.1100, 0.0305, 0.5651, 0.2727], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.37062641978263855 tensor([3.7063e-01, 1.0575e-03, 9.2533e-07, 6.2694e-01, 1.3740e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2544296979904175 tensor([3.4119e-04, 6.1216e-02, 2.5443e-01, 2.7927e-02, 6.5609e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4443318843841553 tensor([8.0935e-07, 2.5127e-03, 5.5229e-01, 8.6209e-04, 4.4433e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19609591364860535 tensor([8.0356e-01, 2.7836e-04, 1.1259e-08, 1.9610e-01, 6.2151e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27104657888412476 tensor([0.0129, 0.2091, 0.1517, 0.2710, 0.3552], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19884198904037476 tensor([0.2794, 0.4882, 0.0065, 0.1988, 0.0270], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2], [4, 3, 2, 0], [0, 3, 4], [0, 3, 4], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4], [0, 3, 1, 4], [0, 1, 3], [0, 3, 2, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3], [2, 4, 3], [1, 2, 0, 4], [2, 1, 4, 0], [3, 4, 2, 0], [0, 3, 1], [2, 4, 1], [1, 0, 2], [2, 1, 0, 4], [3, 4, 2, 0], [3, 0, 4], [2, 1, 0, 4], [2, 1, 0, 4], [0, 1, 2], [0, 3, 4, 1], [0, 3, 4], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3], [0, 3, 4, 2], [0, 1, 2, 3], [2, 4, 1], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [3, 0, 4], [2, 4, 3], [0, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 0, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 3], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4], [3, 0, 4, 1], [0, 1, 2], [2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4], [0, 3, 1, 4], [2, 4, 3], [2, 1, 4], [0, 3, 4, 1], [0, 3], [0, 3, 1], [1, 0, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 4, 1, 3], [2, 0, 1], [4, 2, 3, 0], [3, 4, 2, 0], [3, 0, 4], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3], [0, 1, 2], [0, 1, 2], [4, 2, 3, 0], [0, 3, 1, 4], [2, 1, 0], [1, 0, 2], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0], [2, 4, 3], [0, 3, 4], [0, 3, 1, 4], [3, 4, 0, 2], [2, 4, 1], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [0, 1, 3, 2], [1, 2, 0, 4], [2, 0], [3, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [0, 3, 4], [2, 4, 3, 1], [2, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4], [2, 0, 1], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [2, 4, 3, 0], [2, 4, 3, 1], [2, 4, 1], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1], [3, 0, 4], [0, 1, 3, 2], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 4], [3, 4, 2, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 0], [0, 2, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1], [3, 0, 4, 2], [0, 3, 1], [4, 2, 3], [0, 1, 2, 3], [2, 4, 1, 3], [4, 3, 2, 0], [0, 3, 1, 4], [4, 2, 3, 0], [2, 1, 0, 4], [2, 4, 1], [3, 4, 0, 2], [1, 2, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 3, 4, 1], [1, 0, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [3, 4, 0, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 0, 1, 4], [3, 4, 0, 2], [0, 3, 4, 1], [0, 2, 1], [3, 4, 2, 0], [2, 1, 4], [2, 4, 1, 3], [0, 3, 1], [0, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3, 2], [0, 2, 1], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 3, 2], [2, 4, 3], [2, 1, 0, 4]]\n",
      "[[0], [2], [2], [0], [3, 4], [1], [1, 2], [1, 2], [0], [3], [3], [1, 2], [2], [2, 4], [1], [0], [1], [2, 4], [0, 1], [3], [3], [1], [2, 4], [0, 3], [3, 4], [3], [1], [1, 2], [3], [3], [3, 4], [2], [1, 2], [0, 1], [3], [0], [1], [2, 4], [1], [4], [0, 3], [1], [2], [2], [4], [0], [1], [2], [0], [3], [0], [1, 2], [0, 1], [2, 4], [3], [0], [1], [2], [3], [3, 4], [0], [1], [2], [2], [4], [3], [1], [4], [4], [2], [3], [1], [3], [0, 1], [0], [0], [1, 2], [2], [3, 4], [1, 3], [0], [0], [0, 3], [2], [0, 1], [0, 3], [2], [1, 2, 4], [2, 4], [3, 4], [0], [1], [2], [0], [4], [0], [1], [4], [0], [3, 4], [1], [1], [1, 2], [0], [3], [3], [1], [1, 2, 4], [3, 4], [3, 4], [1], [2], [3, 4], [3, 4], [4], [0], [1, 2], [2], [4], [3, 4], [0, 1], [1, 2], [2], [1], [0, 3], [3], [0], [4], [4], [3], [1, 3, 4], [1, 2], [2], [0], [4], [0], [1, 2], [0], [1, 3, 4], [4], [0], [1, 2], [3, 4], [2], [4], [0], [2], [4], [1], [1], [0], [0, 3], [1], [0], [3], [0, 3], [2], [2], [4], [3], [0], [1], [2], [2], [3], [0, 3], [1, 2], [4], [2], [0, 1], [0, 3], [1], [2], [3], [1], [0, 3], [1], [2], [2], [3], [0], [1], [1, 2], [0, 1], [3], [0], [1], [2], [1, 3, 4], [3, 4], [3], [0], [3], [2], [4], [0, 3], [1], [2, 4], [0, 1], [4], [0], [1], [2], [1], [3], [0, 3], [1], [3], [3], [0], [0], [1], [4], [2], [3, 4], [0], [1], [2], [3], [3], [1], [1], [2], [2], [3], [3], [1], [2], [3, 4], [1], [0, 3], [0], [2, 4], [2, 4], [3], [3], [1], [2], [2], [0], [0], [1], [4], [3, 4], [3], [0], [0], [4], [0, 1], [3]]\n",
      "NL_pred of 4th iteration [[0, 3, 4], [0, 3, 2, 4], [2, 4, 3], [3, 0, 4, 2], [1, 2, 0, 4], [3, 0, 4, 2], [2, 4, 1, 3], [2, 4, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.16367819905281067  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.1624194085597992  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.16023530066013336  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.15755268931388855  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.1547694355249405  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.15217965841293335  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.1499512940645218  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.14815516769886017  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.14674682915210724  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.14565888047218323  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.144817054271698  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.1441655457019806  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.1436590999364853  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.14325442910194397  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.14292950928211212  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.14266496896743774  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.14244592189788818  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.14226220548152924  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.1421075016260147  Accuracy on Support set:0.0\n",
      "torch.Size([8, 2048]) torch.Size([8])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.14197559654712677  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.1740793138742447 tensor([0.0543, 0.0415, 0.0017, 0.7285, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10601340234279633 tensor([1.4022e-04, 8.9345e-01, 1.0601e-01, 7.5184e-06, 3.9043e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03924674913287163 tensor([3.3017e-03, 9.5362e-01, 3.9247e-02, 3.2450e-04, 3.5033e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0890500620007515 tensor([9.5922e-06, 9.1090e-01, 8.9050e-02, 1.1003e-07, 4.2921e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20252926647663116 tensor([2.1464e-06, 6.6593e-03, 7.8967e-01, 1.1391e-03, 2.0253e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17587712407112122 tensor([1.7761e-08, 6.6506e-04, 8.2341e-01, 5.0864e-05, 1.7588e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.34841105341911316 tensor([6.4805e-01, 3.4841e-01, 8.1239e-05, 3.4102e-03, 4.6190e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04810720309615135 tensor([1.9259e-04, 3.2970e-01, 6.2130e-01, 7.0065e-04, 4.8107e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014037851244211197 tensor([9.3629e-01, 4.9604e-02, 5.1548e-06, 1.4038e-02, 5.9081e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25283417105674744 tensor([7.7929e-07, 3.6975e-08, 1.2997e-06, 7.4716e-01, 2.5283e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08879736810922623 tensor([2.8267e-05, 9.1111e-01, 8.8797e-02, 4.7459e-07, 6.6913e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3096978962421417 tensor([0.0379, 0.0255, 0.0013, 0.6256, 0.3097], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.022579383105039597 tensor([2.8719e-03, 9.7369e-01, 2.2579e-02, 1.1038e-04, 7.4657e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2595452666282654 tensor([7.3522e-01, 2.5955e-01, 4.9060e-05, 5.0095e-03, 1.8119e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2789568305015564 tensor([1.5315e-06, 8.4889e-03, 7.1216e-01, 3.8813e-04, 2.7896e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05787248536944389 tensor([9.4213e-01, 7.1039e-07, 1.6409e-13, 5.7872e-02, 1.8274e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01756422221660614 tensor([2.1717e-04, 9.8215e-01, 1.7564e-02, 1.6480e-06, 6.5989e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24647700786590576 tensor([7.5331e-01, 2.4648e-01, 3.0123e-06, 2.0543e-04, 2.7481e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.312391996383667 tensor([1.9577e-07, 1.0241e-03, 6.8582e-01, 7.6504e-04, 3.1239e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04105473682284355 tensor([0.4283, 0.3148, 0.0008, 0.2151, 0.0411], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36443382501602173 tensor([6.2984e-01, 3.6443e-01, 1.1547e-04, 5.4823e-03, 1.3177e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05633024126291275 tensor([1.8642e-04, 9.4327e-01, 5.6330e-02, 4.2577e-06, 2.1280e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08168628066778183 tensor([0.1165, 0.5885, 0.0267, 0.1866, 0.0817], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01925012841820717 tensor([5.4928e-01, 4.2973e-01, 2.9132e-04, 1.9250e-02, 1.4529e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04268520325422287 tensor([9.5718e-01, 1.3080e-04, 1.1274e-09, 4.2685e-02, 2.5652e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4159667491912842 tensor([4.1597e-01, 5.8036e-01, 1.8229e-04, 3.0855e-03, 4.0538e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0322628989815712 tensor([9.6722e-01, 5.1086e-04, 6.3289e-09, 3.2263e-02, 4.8855e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007890438660979271 tensor([0.0086, 0.9400, 0.0418, 0.0017, 0.0079], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026212487369775772 tensor([0.0050, 0.8116, 0.1522, 0.0050, 0.0262], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2539099454879761 tensor([3.0316e-05, 2.1652e-06, 6.4010e-06, 7.4605e-01, 2.5391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05343945324420929 tensor([0.2950, 0.1922, 0.0016, 0.4578, 0.0534], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02576170675456524 tensor([2.9519e-04, 9.7383e-01, 2.5762e-02, 4.1435e-06, 1.0660e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011477495543658733 tensor([0.0061, 0.9324, 0.0486, 0.0014, 0.0115], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4504972994327545 tensor([0.0021, 0.0136, 0.0266, 0.4505, 0.5072], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27702948451042175 tensor([0.0420, 0.0550, 0.0037, 0.6222, 0.2770], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06947554647922516 tensor([0.0979, 0.0324, 0.0010, 0.7993, 0.0695], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4753308892250061 tensor([1.2073e-05, 6.4865e-06, 1.3322e-04, 5.2452e-01, 4.7533e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014910193160176277 tensor([2.4982e-04, 9.8479e-01, 1.4910e-02, 1.7435e-06, 5.2374e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11174648255109787 tensor([1.0870e-02, 1.7412e-03, 1.4756e-04, 8.7549e-01, 1.1175e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26063278317451477 tensor([7.3935e-01, 2.6063e-01, 3.5638e-07, 2.0271e-05, 2.8136e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02728269249200821 tensor([2.5357e-03, 9.6847e-01, 2.7283e-02, 1.5220e-04, 1.5584e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012974982149899006 tensor([9.8578e-01, 1.2363e-03, 1.5035e-08, 1.2975e-02, 5.1902e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0037797337863594294 tensor([3.5201e-01, 6.3204e-01, 6.1339e-04, 1.1564e-02, 3.7797e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08525887876749039 tensor([4.7597e-05, 9.1455e-01, 8.5259e-02, 1.2058e-06, 1.4198e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013858068734407425 tensor([9.7224e-03, 9.7421e-01, 1.3858e-02, 4.5512e-04, 1.7564e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004050979390740395 tensor([0.3107, 0.6725, 0.0011, 0.0116, 0.0041], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013668128289282322 tensor([6.2314e-04, 9.8560e-01, 1.3668e-02, 4.2666e-06, 1.0250e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07639608532190323 tensor([0.2685, 0.1618, 0.0019, 0.4914, 0.0764], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0407002754509449 tensor([9.1336e-01, 4.5605e-02, 9.6466e-06, 4.0700e-02, 3.2212e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.036026086658239365 tensor([9.6378e-01, 1.8896e-04, 1.9378e-09, 3.6026e-02, 2.3483e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07909548282623291 tensor([9.2090e-01, 1.0467e-07, 7.8702e-15, 7.9095e-02, 3.4944e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04743815213441849 tensor([1.0177e-04, 9.5234e-01, 4.7438e-02, 1.7887e-06, 1.1943e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2957049608230591 tensor([7.0039e-01, 2.9570e-01, 7.4721e-05, 3.7449e-03, 8.3008e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025188002735376358 tensor([9.7478e-01, 2.9615e-05, 6.5583e-11, 2.5188e-02, 3.4296e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06051827594637871 tensor([9.3942e-01, 5.7036e-05, 4.5817e-10, 6.0518e-02, 1.7099e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.053183503448963165 tensor([5.3643e-05, 9.4671e-01, 5.3184e-02, 6.7763e-07, 5.0260e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2575887441635132 tensor([7.3965e-01, 2.5759e-01, 3.0466e-05, 2.6677e-03, 6.4403e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005271597765386105 tensor([0.2376, 0.7310, 0.0032, 0.0229, 0.0053], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3633590340614319 tensor([0.0565, 0.0449, 0.0015, 0.5337, 0.3634], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04564286768436432 tensor([9.5435e-01, 1.1180e-05, 1.8670e-11, 4.5643e-02, 2.2479e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08431615680456161 tensor([4.5114e-04, 5.3179e-01, 3.8257e-01, 8.7489e-04, 8.4316e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2924925982952118 tensor([2.9249e-01, 7.0737e-01, 2.5007e-05, 9.8794e-05, 9.4885e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.027735717594623566 tensor([9.7222e-01, 4.2459e-05, 1.4358e-10, 2.7736e-02, 5.6295e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12366458773612976 tensor([0.0593, 0.0259, 0.0012, 0.7900, 0.1237], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01832536794245243 tensor([0.2283, 0.6125, 0.0072, 0.1336, 0.0183], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03581830859184265 tensor([9.6254e-01, 1.6166e-03, 6.2450e-08, 3.5818e-02, 2.2860e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07415412366390228 tensor([0.0077, 0.7848, 0.1210, 0.0125, 0.0742], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1640777736902237 tensor([3.1916e-05, 7.3804e-02, 7.6121e-01, 8.7321e-04, 1.6408e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01754377782344818 tensor([0.0855, 0.8274, 0.0269, 0.0426, 0.0175], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4702141284942627 tensor([4.7021e-01, 5.2277e-01, 2.8296e-04, 6.4225e-03, 3.1367e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 4], [4, 3, 2, 0], [0, 3, 4, 2], [0, 3, 4, 2], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 1, 4], [0, 1, 3], [0, 3, 2, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3, 4], [2, 4, 3], [1, 2, 0, 4], [2, 1, 4, 0], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2], [2, 1, 0, 4], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 1, 2], [0, 3, 4, 1], [0, 3, 4, 2], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3], [0, 3, 4, 2], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3], [0, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 0, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 3], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4, 2], [3, 0, 4, 1], [0, 1, 2, 4], [2, 4, 0, 3], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 3], [0, 3, 1, 4], [2, 4, 3], [2, 1, 4, 3], [0, 3, 4, 1], [0, 3, 4], [0, 3, 1, 4], [1, 0, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 4, 1, 3], [2, 0, 1, 4], [4, 2, 3, 0], [3, 4, 2, 0], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 4], [0, 1, 2], [0, 1, 2], [4, 2, 3, 0], [0, 3, 1, 4], [2, 1, 0, 4], [1, 0, 2], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 3], [0, 3, 4, 2], [0, 3, 1, 4], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [0, 1, 3, 2], [1, 2, 0, 4], [2, 0, 4], [3, 0, 4, 2], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [0, 3, 4, 2], [2, 4, 3, 1], [2, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 0, 1, 4], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [2, 4, 3, 0], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 4, 3], [3, 4, 2, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 0, 4], [0, 2, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [4, 2, 3], [0, 1, 2, 3], [2, 4, 1, 3], [4, 3, 2, 0], [0, 3, 1, 4], [4, 2, 3, 0], [2, 1, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [1, 2, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 3, 4, 1], [1, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [3, 4, 0, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 0, 1, 4], [3, 4, 0, 2], [0, 3, 4, 1], [0, 2, 1, 4], [3, 4, 2, 0], [2, 1, 4, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3, 2], [0, 2, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 3, 2], [2, 4, 3], [2, 1, 0, 4]]\n",
      "[[0], [2], [2], [0], [3], [1], [1], [1], [0], [3], [3], [1], [2], [2, 4], [1], [0], [1], [2], [0, 1], [3], [3], [1], [2], [0], [3, 4], [3], [1], [1], [3], [3], [3, 4], [2], [1], [0, 1], [3], [0], [1], [2, 4], [1], [4], [0], [1], [2], [2], [4], [0], [1], [2], [0], [3], [0], [1], [0, 1], [2, 4], [3], [0], [1], [2], [3], [3], [0], [1], [2], [2], [4], [3], [1], [4], [4], [2], [3], [1], [3], [0, 1], [0], [0], [1], [2], [3], [1], [0], [0], [0], [2], [0, 1], [0], [2], [1, 2], [2], [3, 4], [0], [1], [2], [0], [4], [0], [1], [4], [0], [3], [1], [1], [1], [0], [3], [3], [1], [1, 2], [3, 4], [3, 4], [1], [2], [3], [3, 4], [4], [0], [1], [2], [4], [3], [0, 1], [1], [2], [1], [0], [3], [0], [4], [4], [3], [1, 3], [1], [2], [0], [4], [0], [1], [0], [1, 3], [4], [0], [1], [3], [2], [4], [0], [2], [4], [1], [1], [0], [0], [1], [0], [3], [0], [2], [2], [4], [3], [0], [1], [2], [2], [3], [0], [1], [4], [2], [0, 1], [0], [1], [2], [3], [1], [0], [1], [2], [2], [3], [0], [1], [1], [0, 1], [3], [0], [1], [2], [1, 3], [3, 4], [3], [0], [3], [2], [4], [0], [1], [2], [0, 1], [4], [0], [1], [2], [1], [3], [0], [1], [3], [3], [0], [0], [1], [4], [2], [3], [0], [1], [2], [3], [3], [1], [1], [2], [2], [3], [3], [1], [2], [3], [1], [0], [0], [2], [2], [3], [3], [1], [2], [2], [0], [0], [1], [4], [3], [3], [0], [0], [4], [0, 1], [3]]\n",
      "NL_pred of 5th iteration [[0, 1, 2, 4], [0, 3, 4, 2], [0, 3, 4, 2], [3, 0, 4, 2], [0, 1, 3, 4], [0, 3, 1, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 4, 2], [2, 4, 1, 3], [3, 0, 4, 2], [2, 0, 1, 4], [3, 0, 4, 2], [0, 1, 2, 4], [2, 4, 0, 3], [2, 1, 4, 3], [2, 1, 4, 3], [0, 3, 4], [0, 3, 1, 4], [2, 0, 1, 4], [3, 0, 4, 2], [0, 3, 4], [2, 1, 0, 4], [3, 0, 4, 2], [1, 2, 0, 4], [0, 3, 4, 2], [2, 4, 1, 3], [2, 0, 4], [3, 0, 4, 2], [0, 3, 4, 2], [2, 0, 4], [3, 0, 4, 2], [2, 0, 1, 4], [2, 4, 1, 3], [2, 1, 4, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 4, 1, 3], [2, 1, 4, 3], [3, 0, 4, 2], [2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2, 4], [0, 2, 1, 4], [2, 1, 4, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 2, 1, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.023501567840576172  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.02348897695541382  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0234656023979187  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.023433547019958496  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.02339491844177246  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.02335190534591675  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.023306210041046143  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.023259234428405762  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.02321232795715332  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.023166573047637938  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.02312251806259155  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0230806040763855  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.023041257858276366  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.02300461530685425  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.022970662117004395  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.02293933629989624  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.02291051149368286  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.022884137630462646  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.02286003828048706  Accuracy on Support set:0.0\n",
      "torch.Size([50, 2048]) torch.Size([50])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.022838070392608642  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 4], [4, 3, 2, 0], [0, 3, 4, 2], [0, 3, 4, 2], [2, 4, 1, 3], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 1, 4], [0, 1, 3], [0, 3, 2, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3, 4], [2, 4, 3], [1, 2, 0, 4], [2, 1, 4, 0], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [1, 0, 2], [2, 1, 0, 4], [3, 4, 2, 0], [3, 0, 4, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 1, 2], [0, 3, 4, 1], [0, 3, 4, 2], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3], [0, 3, 4, 2], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [3, 0, 4, 2], [2, 4, 3], [0, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 0, 1, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 0, 2], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 3], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4, 2], [3, 0, 4, 1], [0, 1, 2, 4], [2, 4, 0, 3], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 3], [0, 3, 1, 4], [2, 4, 3], [2, 1, 4, 3], [0, 3, 4, 1], [0, 3, 4], [0, 3, 1, 4], [1, 0, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 4, 1, 3], [2, 0, 1, 4], [4, 2, 3, 0], [3, 4, 2, 0], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 0], [3, 4, 0, 2], [0, 3, 4], [0, 1, 2], [0, 1, 2], [4, 2, 3, 0], [0, 3, 1, 4], [2, 1, 0, 4], [1, 0, 2], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 3], [0, 3, 4, 2], [0, 3, 1, 4], [3, 4, 0, 2], [2, 4, 1, 3], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [0, 1, 3, 2], [1, 2, 0, 4], [2, 0, 4], [3, 0, 4, 2], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [0, 3, 4, 2], [2, 4, 3, 1], [2, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 0, 1, 4], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [2, 4, 3, 0], [2, 4, 3, 1], [2, 4, 1, 3], [3, 0, 4, 2], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 4, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 1, 2, 3], [1, 2, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 4, 3], [3, 4, 2, 0], [0, 3, 4, 1], [0, 3, 4, 1], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 2, 0], [3, 0, 4, 2], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1, 3], [3, 4, 0, 2], [0, 3, 1, 4], [2, 0, 4], [0, 2, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [4, 2, 3], [0, 1, 2, 3], [2, 4, 1, 3], [4, 3, 2, 0], [0, 3, 1, 4], [4, 2, 3, 0], [2, 1, 0, 4], [2, 4, 1, 3], [3, 4, 0, 2], [1, 2, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 3, 4, 1], [1, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 4, 0], [2, 1, 0, 4], [3, 4, 0, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 0, 1, 4], [3, 4, 0, 2], [0, 3, 4, 1], [0, 2, 1, 4], [3, 4, 2, 0], [2, 1, 4, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 0, 4], [3, 4, 0, 2], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 4, 0, 2], [0, 1, 3, 2], [0, 2, 1, 4], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 3, 2], [2, 4, 3], [2, 1, 0, 4]]\n",
      "POSITION :  [[0], [2], [2], [0], [3], [1], [1], [1], [0], [3], [3], [1], [2], [2, 4], [1], [0], [1], [2], [0, 1], [3], [3], [1], [2], [0], [3, 4], [3], [1], [1], [3], [3], [3, 4], [2], [1], [0, 1], [3], [0], [1], [2, 4], [1], [4], [0], [1], [2], [2], [4], [0], [1], [2], [0], [3], [0], [1], [0, 1], [2, 4], [3], [0], [1], [2], [3], [3], [0], [1], [2], [2], [4], [3], [1], [4], [4], [2], [3], [1], [3], [0, 1], [0], [0], [1], [2], [3], [1], [0], [0], [0], [2], [0, 1], [0], [2], [1, 2], [2], [3, 4], [0], [1], [2], [0], [4], [0], [1], [4], [0], [3], [1], [1], [1], [0], [3], [3], [1], [1, 2], [3, 4], [3, 4], [1], [2], [3], [3, 4], [4], [0], [1], [2], [4], [3], [0, 1], [1], [2], [1], [0], [3], [0], [4], [4], [3], [1, 3], [1], [2], [0], [4], [0], [1], [0], [1, 3], [4], [0], [1], [3], [2], [4], [0], [2], [4], [1], [1], [0], [0], [1], [0], [3], [0], [2], [2], [4], [3], [0], [1], [2], [2], [3], [0], [1], [4], [2], [0, 1], [0], [1], [2], [3], [1], [0], [1], [2], [2], [3], [0], [1], [1], [0, 1], [3], [0], [1], [2], [1, 3], [3, 4], [3], [0], [3], [2], [4], [0], [1], [2], [0, 1], [4], [0], [1], [2], [1], [3], [0], [1], [3], [3], [0], [0], [1], [4], [2], [3], [0], [1], [2], [3], [3], [1], [1], [2], [2], [3], [3], [1], [2], [3], [1], [0], [0], [2], [2], [3], [3], [1], [2], [2], [0], [0], [1], [4], [3], [3], [0], [0], [4], [0, 1], [3]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.504\n",
      "tensor([0, 2, 2, 0, 3, 1, 1, 1, 0, 3, 3, 1, 2, 1, 0, 1, 2, 3, 3, 1, 2, 0, 3, 1,\n",
      "        1, 3, 3, 2, 1, 3, 0, 1, 1, 4, 0, 1, 2, 2, 4, 0, 1, 2, 0, 3, 0, 1, 3, 0,\n",
      "        1, 2, 3, 3, 0, 1, 2, 2, 4, 3, 1, 4, 4, 2, 3, 1, 3, 0, 0, 1, 2, 3, 1, 0,\n",
      "        0, 0, 2, 0, 2, 2, 0, 1, 2, 0, 4, 0, 1, 4, 0, 3, 1, 1, 1, 0, 3, 3, 1, 1,\n",
      "        2, 3, 4, 0, 1, 2, 4, 3, 1, 2, 1, 0, 3, 0, 4, 4, 3, 1, 2, 0, 4, 0, 1, 0,\n",
      "        4, 0, 1, 3, 2, 4, 0, 2, 4, 1, 1, 0, 0, 1, 0, 3, 0, 2, 2, 4, 3, 0, 1, 2,\n",
      "        2, 3, 0, 1, 4, 2, 0, 1, 2, 3, 1, 0, 1, 2, 2, 3, 0, 1, 1, 3, 0, 1, 2, 3,\n",
      "        0, 3, 2, 4, 0, 1, 2, 4, 0, 1, 2, 1, 3, 0, 1, 3, 3, 0, 0, 1, 4, 2, 3, 0,\n",
      "        1, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 2, 3, 1, 0, 0, 2, 2, 3, 3, 1, 2, 2, 0,\n",
      "        0, 1, 4, 3, 3, 0, 0, 4, 3])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 98.22222222222223\n",
      "Epoch: 1  Loss: 99.55555555555556\n",
      "Epoch: 2  Loss: 100.0\n",
      "Epoch: 3  Loss: 100.0\n",
      "Epoch: 4  Loss: 100.0\n",
      "Epoch: 5  Loss: 100.0\n",
      "Epoch: 6  Loss: 100.0\n",
      "Epoch: 7  Loss: 100.0\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 12/15 [09:58<02:42, 54.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 5.914834885150194  Accuracy on Support set:40.0\n",
      "Train_Epoch: 1  Train_Loss: 2.9515417516231537  Accuracy on Support set:44.0\n",
      "Train_Epoch: 2  Train_Loss: 1.7586140176653862  Accuracy on Support set:60.0\n",
      "Train_Epoch: 3  Train_Loss: 1.118004954457283  Accuracy on Support set:64.0\n",
      "Train_Epoch: 4  Train_Loss: 0.7553709584474564  Accuracy on Support set:88.0\n",
      "Train_Epoch: 5  Train_Loss: 0.5115627375245094  Accuracy on Support set:88.0\n",
      "Train_Epoch: 6  Train_Loss: 0.371105078458786  Accuracy on Support set:100.0\n",
      "Train_Epoch: 7  Train_Loss: 0.2896630027890205  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.23230720460414886  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.1964590749144554  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.1702280543744564  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.14993507638573647  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.13390997111797331  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.1209428307414055  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.1101369209587574  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.10110720843076706  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.0934836208820343  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.08709533050656319  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.08141319878399372  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.07645820878446102  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.0721089044958353  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.06821564726531505  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.06473705045878887  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.06157331667840481  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.05868556283414364  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.05605773314833641  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.053651651293039324  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.05146727681159973  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.04938071206212044  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.04750647246837616  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.045739280730485915  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.04409701466560364  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.04254723750054836  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.041118197441101074  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.03975358158349991  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.0384822978451848  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.03728868544101715  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.03612597487866878  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.03507923439145088  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.03405437972396612  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.0331021785363555  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.03219241488724947  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.03134096272289753  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.030516722686588765  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.029741304852068426  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.029024574123322964  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.02833054605871439  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.02767287481576204  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.027035772167146206  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.026429364271461964  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  32.666666666666664\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.00010515422036405653 tensor([1.0453e-01, 7.3804e-03, 1.0515e-04, 8.3722e-01, 5.0758e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00045128806959837675 tensor([8.2619e-02, 9.1524e-01, 6.9601e-04, 4.5129e-04, 9.9615e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.318074040085776e-07 tensor([7.3181e-07, 3.4745e-04, 2.3313e-01, 1.0770e-03, 7.6545e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007593187037855387 tensor([0.0175, 0.0303, 0.0076, 0.2995, 0.6451], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.005501352821739e-08 tensor([4.0055e-08, 2.2095e-04, 7.6368e-01, 6.6449e-05, 2.3603e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007659258553758264 tensor([0.0008, 0.0009, 0.0024, 0.2949, 0.7011], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.375027510330256e-07 tensor([9.3168e-01, 6.6727e-02, 6.3750e-07, 1.5452e-03, 4.5717e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6668227544869296e-05 tensor([7.6318e-01, 2.3300e-01, 1.6668e-05, 3.3673e-03, 4.3098e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.417491451467413e-08 tensor([1.6381e-01, 8.8565e-05, 2.4175e-08, 8.3543e-01, 6.7475e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.527078347862698e-05 tensor([6.0918e-01, 1.6208e-02, 1.5271e-05, 3.6452e-01, 1.0076e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.098705878481269e-05 tensor([5.0987e-05, 8.6363e-04, 1.8638e-02, 2.9766e-02, 9.5068e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003517184522934258 tensor([1.0773e-02, 9.6802e-01, 1.3971e-02, 3.5172e-04, 6.8833e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.781292904634029e-05 tensor([6.7813e-05, 1.2827e-02, 3.4664e-01, 5.2416e-03, 6.3522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.663147600083903e-06 tensor([4.8867e-04, 6.6541e-06, 1.6631e-06, 9.2670e-01, 7.2799e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000488476303871721 tensor([4.8848e-04, 3.6898e-02, 1.5292e-01, 1.0943e-02, 7.9875e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005484306602738798 tensor([5.4843e-04, 1.9046e-01, 5.5646e-01, 2.3848e-03, 2.5015e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00972696766257286 tensor([0.1132, 0.7845, 0.0097, 0.0207, 0.0719], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002497514651622623 tensor([2.4975e-04, 1.2743e-02, 1.0290e-01, 1.4272e-02, 8.6983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9590939448098652e-05 tensor([2.9591e-05, 9.7061e-04, 2.0986e-02, 1.3940e-02, 9.6407e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.256631458090851e-05 tensor([1.2566e-05, 3.5037e-02, 8.5458e-01, 1.4509e-04, 1.1022e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.034790992736816406 tensor([0.0688, 0.5752, 0.0348, 0.0569, 0.2643], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005883866106159985 tensor([9.3752e-02, 9.0377e-01, 6.0586e-04, 5.8839e-04, 1.2805e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01203895453363657 tensor([0.1531, 0.2604, 0.0120, 0.2384, 0.3360], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00033388796146027744 tensor([1.9122e-02, 4.2566e-03, 3.3389e-04, 6.9865e-01, 2.7764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.533264018391492e-06 tensor([3.0141e-05, 4.1936e-01, 5.7546e-01, 4.5333e-06, 5.1485e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004467270046006888 tensor([2.5159e-01, 3.8075e-02, 4.4673e-04, 6.3084e-01, 7.9050e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3556424164562486e-05 tensor([6.3804e-04, 9.4392e-01, 5.3486e-02, 1.3556e-05, 1.9425e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6377161955460906e-05 tensor([4.8017e-01, 9.9923e-03, 1.6377e-05, 4.9752e-01, 1.2296e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.914870947279269e-06 tensor([6.1385e-05, 2.9149e-06, 1.0140e-05, 7.5248e-01, 2.4744e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011865248961839825 tensor([1.4446e-02, 9.8058e-01, 3.3898e-03, 1.1865e-04, 1.4685e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002544644521549344 tensor([1.0834e-01, 1.1283e-02, 2.5446e-04, 7.7366e-01, 1.0647e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012736052740365267 tensor([0.0174, 0.9444, 0.0188, 0.0013, 0.0181], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.8945963726509945e-08 tensor([3.3157e-03, 3.3900e-06, 4.8946e-08, 9.9212e-01, 4.5603e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.683728810865432e-05 tensor([1.6021e-03, 1.7317e-04, 8.6837e-05, 7.8084e-01, 2.1729e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011722889030352235 tensor([0.0012, 0.4904, 0.3471, 0.0012, 0.1601], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.4959235159039963e-05 tensor([1.4959e-05, 1.1134e-03, 8.3092e-02, 9.1027e-03, 9.0668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00018064366304315627 tensor([9.6400e-03, 9.7712e-01, 9.7528e-03, 1.8064e-04, 3.3092e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.050418642824297e-08 tensor([8.0504e-08, 1.4525e-03, 9.6872e-01, 5.7381e-06, 2.9825e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0725062793426332e-06 tensor([1.0725e-06, 6.3618e-06, 8.9430e-04, 2.5644e-02, 9.7345e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.550641122274101e-05 tensor([1.0850e-04, 4.2561e-01, 5.5725e-01, 3.5506e-05, 1.6995e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.918024842481827e-06 tensor([4.9180e-06, 6.0292e-03, 6.4043e-01, 4.5695e-04, 3.5308e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.935908070066944e-05 tensor([2.2210e-02, 9.7596e-01, 1.2609e-03, 7.9359e-05, 4.9137e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.6702375421905344e-08 tensor([2.6702e-08, 4.5785e-03, 9.9323e-01, 1.6317e-07, 2.1909e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7423905368806345e-09 tensor([9.9541e-01, 3.3555e-03, 2.7424e-09, 1.2345e-03, 2.0758e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.51772677176632e-05 tensor([3.0134e-01, 6.9767e-01, 7.5177e-05, 5.7789e-04, 3.3774e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0023053905460983515 tensor([0.0023, 0.0666, 0.1293, 0.0378, 0.7640], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.581611508707283e-06 tensor([7.8823e-01, 2.1066e-01, 3.5816e-06, 1.0090e-03, 1.0229e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008497087983414531 tensor([0.0008, 0.0021, 0.0053, 0.1441, 0.8477], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.404930547570075e-08 tensor([1.6942e-04, 3.0347e-07, 5.4049e-08, 9.9256e-01, 7.2728e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011321358382701874 tensor([0.0113, 0.2275, 0.0668, 0.0426, 0.6517], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00023980357218533754 tensor([2.3980e-04, 4.3005e-03, 3.5671e-02, 3.8068e-02, 9.2172e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.8621048335917294e-05 tensor([2.1281e-02, 9.7767e-01, 6.9559e-04, 4.8621e-05, 3.0489e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.0927980176898018e-09 tensor([3.0928e-09, 2.2492e-04, 9.8630e-01, 9.0800e-07, 1.3478e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4880083060452307e-07 tensor([2.4880e-07, 1.4076e-05, 1.0310e-02, 5.4583e-03, 9.8422e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005951204220764339 tensor([4.3928e-02, 9.4934e-01, 2.5675e-03, 5.9512e-04, 3.5669e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0385003499686718 tensor([0.0574, 0.4280, 0.0385, 0.0750, 0.4011], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00018804986029863358 tensor([2.0488e-01, 7.9340e-01, 1.8805e-04, 6.6763e-04, 8.6090e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00043547217501327395 tensor([4.3547e-04, 4.9987e-03, 2.8277e-02, 4.5588e-02, 9.2070e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.837993790511973e-05 tensor([6.2529e-01, 3.6946e-01, 3.8380e-05, 4.1382e-03, 1.0666e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.395765467459569e-06 tensor([6.3958e-06, 4.7314e-02, 9.2940e-01, 1.9070e-05, 2.3259e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.6379355404060334e-05 tensor([2.6379e-05, 5.8307e-02, 8.6733e-01, 1.5947e-04, 7.4181e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3116744135288627e-08 tensor([9.4963e-01, 5.0322e-02, 1.3117e-08, 5.1577e-05, 8.1583e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1716714425347163e-06 tensor([1.1717e-06, 1.5979e-02, 9.6572e-01, 7.1407e-06, 1.8294e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.981547453373423e-08 tensor([4.1280e-04, 4.7640e-07, 2.9815e-08, 9.9411e-01, 5.4733e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007986146956682205 tensor([0.0008, 0.0567, 0.1632, 0.0117, 0.7675], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.724470146466047e-06 tensor([6.2248e-02, 9.1229e-04, 9.7245e-06, 9.1391e-01, 2.2922e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.168203702192841e-07 tensor([9.2934e-01, 7.3622e-03, 4.1682e-07, 6.2919e-02, 3.7453e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.355759751304092e-11 tensor([7.3558e-11, 7.2448e-05, 9.9817e-01, 2.1602e-08, 1.7548e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.979058859244105e-07 tensor([9.9791e-07, 3.1528e-05, 8.5460e-03, 8.5621e-03, 9.8286e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.019238196313381195 tensor([0.0493, 0.2046, 0.0192, 0.1299, 0.5970], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002368232235312462 tensor([0.0965, 0.8925, 0.0024, 0.0027, 0.0059], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.563256152323447e-05 tensor([4.6172e-02, 9.5346e-01, 1.8323e-04, 4.5633e-05, 1.3689e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.495002500945702e-05 tensor([7.4950e-05, 1.0226e-02, 2.2883e-01, 5.8370e-03, 7.5503e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00011335509043419734 tensor([1.8641e-02, 2.1302e-03, 1.1336e-04, 8.2077e-01, 1.5834e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.4658552320033778e-06 tensor([2.6707e-05, 5.1755e-01, 4.7880e-01, 2.4659e-06, 3.6189e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012186181265860796 tensor([0.3034, 0.0939, 0.0012, 0.4991, 0.1024], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7627487247651175e-09 tensor([9.4746e-01, 3.6152e-04, 1.7627e-09, 5.2161e-02, 1.3820e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.6965226172469556e-05 tensor([8.5645e-01, 7.8167e-02, 3.6965e-05, 6.1588e-02, 3.7566e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.674121368784199e-08 tensor([5.4603e-03, 4.7740e-06, 2.6741e-08, 9.9187e-01, 2.6600e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005827202461659908 tensor([0.0138, 0.7627, 0.0754, 0.0058, 0.1422], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014200734905898571 tensor([0.0211, 0.6716, 0.0784, 0.0142, 0.2148], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0022437640000134706 tensor([0.0022, 0.0188, 0.0267, 0.0950, 0.8572], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7544865588424727e-07 tensor([2.7545e-07, 4.3234e-03, 9.7226e-01, 6.9511e-06, 2.3413e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.034954377450049e-06 tensor([2.0350e-06, 1.0617e-05, 1.2312e-03, 3.7305e-02, 9.6145e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013760379515588284 tensor([0.0014, 0.1815, 0.2992, 0.0073, 0.5106], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004190037027001381 tensor([0.0042, 0.0826, 0.0810, 0.0461, 0.7861], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.0471768190618604e-05 tensor([4.9140e-01, 2.4874e-02, 5.0472e-05, 4.5794e-01, 2.5732e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.107901860261336e-06 tensor([7.2583e-01, 2.7289e-01, 9.1079e-06, 1.1318e-03, 1.3594e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0016662050038576126 tensor([0.0110, 0.0073, 0.0017, 0.4363, 0.5437], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00044814898865297437 tensor([2.8374e-03, 8.5756e-01, 1.1141e-01, 4.4815e-04, 2.7744e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.8331530716770885e-09 tensor([5.8332e-09, 1.8924e-03, 9.9659e-01, 7.2045e-08, 1.5129e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00015224616799969226 tensor([6.3520e-01, 3.4091e-01, 1.5225e-04, 1.7738e-02, 5.9948e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00028004602063447237 tensor([2.9469e-01, 3.1999e-02, 2.8005e-04, 6.0341e-01, 6.9628e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.196648195735179e-06 tensor([8.1966e-06, 2.7717e-05, 9.6337e-04, 5.5281e-02, 9.4372e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.782043394399807e-05 tensor([7.7820e-05, 1.8360e-01, 7.4970e-01, 1.2571e-04, 6.6488e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004750786814838648 tensor([0.0133, 0.8305, 0.0661, 0.0048, 0.0853], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.2136784536996856e-05 tensor([6.9116e-03, 9.9032e-01, 2.3294e-03, 2.2137e-05, 4.1657e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.622595036110397e-08 tensor([1.6226e-08, 7.0857e-04, 9.8705e-01, 1.4848e-06, 1.2239e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.270212907111272e-05 tensor([6.2702e-05, 4.1327e-04, 4.8943e-03, 6.0056e-02, 9.3457e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7348376957215805e-07 tensor([1.7348e-07, 1.7135e-02, 9.7928e-01, 3.8134e-07, 3.5805e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005469444440677762 tensor([3.2910e-02, 6.4170e-03, 5.4694e-04, 7.1413e-01, 2.4600e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010693308431655169 tensor([0.0011, 0.4342, 0.4315, 0.0012, 0.1320], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005106839817017317 tensor([2.0940e-01, 3.6605e-02, 5.1068e-04, 6.2628e-01, 1.2720e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.354489714306851e-09 tensor([3.0178e-03, 6.8398e-07, 2.3545e-09, 9.9622e-01, 7.5699e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3440910151985008e-05 tensor([1.3441e-05, 9.8617e-02, 8.7408e-01, 2.1986e-05, 2.7269e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006562952767126262 tensor([6.5630e-04, 2.2025e-03, 1.0478e-02, 1.7356e-01, 8.1311e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0033028472680598497 tensor([0.1227, 0.0919, 0.0033, 0.5011, 0.2811], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005251671187579632 tensor([0.0053, 0.0205, 0.0120, 0.1212, 0.8411], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.650906809431035e-06 tensor([2.7576e-02, 5.4112e-04, 7.6509e-06, 9.4320e-01, 2.8673e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005477650556713343 tensor([5.4777e-04, 8.0523e-03, 3.0157e-02, 4.2152e-02, 9.1909e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024253154173493385 tensor([0.0942, 0.2977, 0.0243, 0.1838, 0.4000], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.0948744905763306e-06 tensor([8.8885e-01, 3.6016e-02, 7.0949e-06, 7.3389e-02, 1.7341e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003539550525601953 tensor([3.5396e-04, 1.1905e-01, 5.3966e-01, 1.7937e-03, 3.3915e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.682736178234336e-06 tensor([6.4493e-04, 1.3529e-05, 3.6827e-06, 9.0913e-01, 9.0203e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004367527551949024 tensor([0.0044, 0.4266, 0.2143, 0.0069, 0.3478], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7945336569246138e-06 tensor([9.3374e-01, 5.8075e-02, 2.7945e-06, 7.9989e-03, 1.8502e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.649356459500268e-06 tensor([8.4511e-02, 9.1546e-01, 1.3566e-05, 8.6494e-06, 1.0920e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.24646577812382e-06 tensor([4.2465e-06, 4.4549e-02, 9.3220e-01, 1.4790e-05, 2.3227e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012797265662811697 tensor([3.4210e-02, 3.5952e-03, 1.2797e-04, 8.3643e-01, 1.2564e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3228323950897902e-05 tensor([1.3228e-05, 3.9443e-03, 2.1135e-01, 2.6912e-03, 7.8200e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013036848977208138 tensor([0.0329, 0.0632, 0.0130, 0.3245, 0.5664], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9676298254344147e-06 tensor([2.6855e-01, 7.3143e-01, 2.9676e-06, 1.6661e-05, 7.4242e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007635802612639964 tensor([0.5468, 0.3778, 0.0008, 0.0507, 0.0239], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2006210248216576e-08 tensor([1.2006e-08, 6.8253e-07, 2.7109e-03, 2.5457e-03, 9.9474e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.1701860532630235e-05 tensor([2.5800e-05, 1.5816e-01, 8.2141e-01, 2.1702e-05, 2.0377e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1524173714860808e-05 tensor([8.2963e-01, 2.7217e-02, 1.1524e-05, 1.4008e-01, 3.0656e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.064678371662467e-09 tensor([9.8695e-01, 1.2874e-02, 4.0647e-09, 1.7828e-04, 7.5199e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.292328069117502e-07 tensor([5.2923e-07, 4.6131e-05, 3.0544e-02, 3.8390e-03, 9.6557e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1498173989821225e-05 tensor([1.7017e-01, 3.8528e-03, 1.1498e-05, 7.9954e-01, 2.6431e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00015026911569293588 tensor([4.9495e-01, 4.9733e-01, 1.5027e-04, 5.0414e-03, 2.5234e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.899053551023826e-05 tensor([2.5537e-03, 1.8645e-04, 6.8991e-05, 8.5195e-01, 1.4524e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.777893238700926e-05 tensor([5.4244e-01, 4.5450e-01, 5.7779e-05, 2.3061e-03, 7.0266e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.26155233476311e-05 tensor([6.2616e-05, 2.1327e-02, 4.8266e-01, 2.1579e-03, 4.9379e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017161667346954346 tensor([0.0491, 0.8049, 0.0221, 0.0172, 0.1068], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004056492180097848 tensor([1.9502e-03, 7.7580e-01, 1.8907e-01, 4.0565e-04, 3.2774e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.199238775181584e-06 tensor([8.5665e-01, 2.5384e-02, 9.1992e-06, 1.1550e-01, 2.4574e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.315506885177456e-05 tensor([1.6729e-01, 8.3250e-01, 4.3155e-05, 9.0172e-05, 8.3951e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01821189559996128 tensor([0.0799, 0.7294, 0.0182, 0.0261, 0.1463], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.999239430006128e-06 tensor([1.9843e-04, 3.1427e-06, 1.9992e-06, 9.0802e-01, 9.1777e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0088724593515508e-05 tensor([1.0089e-05, 1.9053e-03, 1.5385e-01, 4.4089e-03, 8.3983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00488513195887208 tensor([0.0049, 0.3527, 0.2426, 0.0120, 0.3878], grad_fn=<SoftmaxBackward0>)\n",
      "2 5.1837361070283805e-08 tensor([8.9721e-01, 1.0273e-01, 5.1837e-08, 5.7761e-05, 1.7060e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.459979087798274e-07 tensor([7.4600e-07, 7.5439e-03, 9.7407e-01, 1.0177e-05, 1.8372e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0873922917653545e-07 tensor([6.7655e-04, 1.7556e-06, 1.0874e-07, 9.8850e-01, 1.0825e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002860761305782944 tensor([3.7411e-04, 3.9819e-01, 5.1622e-01, 2.8608e-04, 8.4926e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000527297321241349 tensor([6.9444e-04, 5.2730e-04, 1.5962e-03, 3.9838e-01, 5.9880e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003470040683168918 tensor([1.3279e-01, 8.6528e-01, 3.4700e-04, 6.1461e-04, 9.7481e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.520847829146078e-06 tensor([2.5208e-06, 2.2251e-02, 9.5198e-01, 1.3785e-05, 2.5749e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5579806245114014e-07 tensor([1.2440e-03, 3.0527e-06, 1.5580e-07, 9.9087e-01, 7.8839e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.841406386901781e-09 tensor([4.8414e-09, 6.7208e-04, 9.9400e-01, 2.9615e-07, 5.3269e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006306387949734926 tensor([0.0063, 0.0218, 0.0142, 0.1721, 0.7856], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.89018611915526e-06 tensor([1.2930e-02, 9.8683e-01, 1.9340e-04, 4.8902e-06, 3.7672e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000444916047854349 tensor([2.7569e-03, 8.3646e-01, 1.2559e-01, 4.4492e-04, 3.4752e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001278401236049831 tensor([0.0014, 0.0013, 0.0013, 0.3491, 0.6469], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.39961391343968e-05 tensor([2.3996e-05, 6.8007e-03, 3.3077e-01, 3.2998e-03, 6.5911e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5273493772838265e-05 tensor([7.2231e-01, 2.1298e-02, 1.5273e-05, 2.5044e-01, 5.9334e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00018155042198486626 tensor([2.3219e-01, 7.6598e-01, 1.8155e-04, 8.3499e-04, 8.1093e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.9344729632139206e-05 tensor([4.9345e-05, 1.5634e-02, 4.5918e-01, 2.6283e-03, 5.2251e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006618985207751393 tensor([6.6190e-04, 9.1590e-04, 1.3392e-03, 2.4737e-01, 7.4972e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002677190350368619 tensor([6.1819e-04, 5.3262e-01, 4.0957e-01, 2.6772e-04, 5.6928e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013748127035796642 tensor([0.0137, 0.2321, 0.1067, 0.0556, 0.5920], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005545247346162796 tensor([0.2182, 0.2825, 0.0055, 0.2715, 0.2222], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.662834155373275e-06 tensor([5.6628e-06, 1.6168e-02, 8.6848e-01, 1.4412e-04, 1.1520e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.426196324544776e-10 tensor([9.9627e-01, 1.0113e-03, 7.4262e-10, 2.7159e-03, 1.6802e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.844151847762987e-05 tensor([1.8442e-05, 1.2978e-01, 8.4755e-01, 2.0136e-05, 2.2629e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011046403087675571 tensor([0.0011, 0.2584, 0.4491, 0.0034, 0.2880], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007023774087429047 tensor([0.3999, 0.1167, 0.0007, 0.4000, 0.0828], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005217695725150406 tensor([0.0005, 0.1361, 0.4562, 0.0030, 0.4041], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.110829084107536e-06 tensor([1.6269e-04, 3.1157e-06, 3.1108e-06, 8.6769e-01, 1.3214e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001922528463182971 tensor([1.9225e-04, 1.0725e-01, 6.4286e-01, 1.3249e-03, 2.4838e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0059249522164464 tensor([0.0059, 0.0498, 0.0534, 0.1194, 0.7715], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.502657707736944e-06 tensor([7.9320e-01, 2.0599e-01, 2.5027e-06, 7.4232e-04, 6.1751e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.448520146775991e-05 tensor([3.4485e-05, 2.3026e-04, 5.8080e-03, 3.7700e-02, 9.5623e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.713641727808863e-05 tensor([3.2557e-04, 8.7136e-05, 2.2940e-04, 5.6012e-01, 4.3924e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.499396830695332e-06 tensor([1.4994e-06, 3.4230e-02, 9.5501e-01, 3.4617e-06, 1.0759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012453288945835084 tensor([1.2453e-04, 2.4205e-02, 3.4076e-01, 6.1259e-03, 6.2879e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0049359844997525215 tensor([0.0542, 0.8993, 0.0113, 0.0049, 0.0303], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00014217395801097155 tensor([1.4217e-04, 1.2758e-02, 1.1569e-01, 7.3622e-03, 8.6405e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.056213118834421e-07 tensor([8.0562e-07, 6.2460e-05, 1.6693e-02, 4.6832e-03, 9.7856e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.697636530996533e-07 tensor([8.6976e-07, 8.7350e-04, 4.8829e-01, 6.3901e-04, 5.1019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0022935187444090843 tensor([0.3640, 0.1958, 0.0023, 0.3237, 0.1142], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004950180649757385 tensor([0.0211, 0.8367, 0.0469, 0.0050, 0.0904], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00021440102136693895 tensor([4.6950e-01, 4.5072e-02, 2.1440e-04, 4.4209e-01, 4.3129e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7628499335842207e-06 tensor([7.1029e-04, 7.6526e-06, 1.7628e-06, 9.6329e-01, 3.5989e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00029516936047002673 tensor([2.9517e-04, 9.5204e-02, 4.2020e-01, 2.7062e-03, 4.8160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.919894156633745e-09 tensor([9.6246e-01, 8.7717e-04, 8.9199e-09, 3.6640e-02, 2.4729e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00011622079910011962 tensor([7.5351e-01, 1.5594e-01, 1.1622e-04, 8.0024e-02, 1.0413e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0026947450824081898 tensor([0.0123, 0.8380, 0.0828, 0.0027, 0.0642], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.879927185655106e-05 tensor([1.8799e-05, 9.7716e-05, 2.5690e-03, 5.8540e-02, 9.3877e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3587230569100939e-05 tensor([1.1820e-04, 6.2052e-01, 3.6952e-01, 1.3587e-05, 9.8281e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006006321404129267 tensor([0.1113, 0.8536, 0.0060, 0.0065, 0.0226], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.42211129059433e-05 tensor([5.6646e-01, 1.4865e-02, 1.4221e-05, 4.0910e-01, 9.5576e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00401369109749794 tensor([0.0818, 0.8864, 0.0063, 0.0040, 0.0215], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.504175142803433e-07 tensor([1.0729e-04, 5.0496e-07, 2.5042e-07, 9.7702e-01, 2.2869e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.327945646309672e-07 tensor([4.8276e-07, 4.1131e-02, 9.5617e-01, 4.3279e-07, 2.7025e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.807965865438746e-07 tensor([9.7456e-01, 1.4475e-02, 4.8080e-07, 1.0858e-02, 1.0902e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.495694050390739e-05 tensor([8.1018e-01, 1.8288e-01, 1.4957e-05, 6.2669e-03, 6.5546e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8212854275011523e-08 tensor([2.8213e-08, 2.2171e-03, 9.9207e-01, 5.4680e-07, 5.7086e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.252098382399708e-07 tensor([1.3646e-05, 8.2521e-07, 7.7261e-06, 5.8875e-01, 4.1123e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.022729210555553436 tensor([0.0822, 0.3680, 0.0227, 0.1289, 0.3982], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013341018930077553 tensor([0.0798, 0.1526, 0.0133, 0.2988, 0.4555], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1472195637907134e-08 tensor([9.9122e-01, 7.6939e-03, 1.1472e-08, 1.0799e-03, 4.1654e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.359045264161864e-09 tensor([3.3590e-09, 2.0910e-04, 9.8344e-01, 1.1346e-06, 1.6353e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.9138738682377152e-05 tensor([1.7874e-04, 2.9139e-05, 8.1020e-05, 6.1742e-01, 3.8229e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.884286787884776e-06 tensor([7.8843e-06, 1.7308e-02, 8.4673e-01, 1.8533e-04, 1.3577e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.2506685127154924e-05 tensor([8.2422e-05, 2.2507e-05, 1.9801e-04, 4.5345e-01, 5.4624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4211060261004604e-05 tensor([3.1521e-03, 9.9150e-01, 4.8809e-03, 1.4211e-05, 4.5328e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00018962222384288907 tensor([1.8962e-04, 3.9782e-03, 3.6476e-02, 3.1930e-02, 9.2743e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.320577666774625e-06 tensor([2.1481e-01, 1.1041e-03, 1.3206e-06, 7.7759e-01, 6.4985e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.592708026189939e-06 tensor([3.5927e-06, 1.3988e-02, 9.0438e-01, 5.4645e-05, 8.1575e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009583690552972257 tensor([0.3658, 0.1487, 0.0010, 0.3791, 0.1054], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.0496518118306994e-05 tensor([4.3098e-03, 9.9096e-01, 4.1767e-03, 2.0497e-05, 5.2873e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.3649331675842404e-05 tensor([5.3649e-05, 5.1141e-03, 1.4775e-01, 7.2167e-03, 8.3986e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.5182490541483276e-06 tensor([6.3327e-06, 5.5182e-06, 1.9978e-04, 1.4639e-01, 8.5340e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7895310122639785e-08 tensor([1.7895e-08, 5.1487e-04, 9.7155e-01, 3.6041e-06, 2.7928e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.384622687008232e-06 tensor([3.3846e-06, 3.7965e-04, 8.7723e-02, 6.6387e-03, 9.0526e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0033564140931048e-08 tensor([9.9059e-01, 6.0434e-03, 2.0034e-08, 3.3516e-03, 1.7070e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.75372076430358e-05 tensor([2.7537e-05, 3.2628e-02, 7.7416e-01, 4.1616e-04, 1.9277e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010856724111363292 tensor([1.0857e-04, 9.9194e-03, 1.1836e-01, 9.6463e-03, 8.6196e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4585393475717865e-05 tensor([2.4585e-05, 1.0227e-01, 8.5114e-01, 5.5477e-05, 4.6508e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006427813787013292 tensor([0.2539, 0.3407, 0.0064, 0.2127, 0.1863], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1484272139105087e-09 tensor([9.8740e-01, 1.2503e-02, 2.1484e-09, 9.7662e-05, 4.6178e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3061439858574886e-05 tensor([3.3472e-02, 6.1786e-04, 1.3061e-05, 9.3466e-01, 3.1240e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.6527861084323376e-05 tensor([1.5123e-02, 7.1378e-04, 3.6528e-05, 8.9619e-01, 8.7939e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.8695266488939524e-05 tensor([9.0092e-04, 8.8832e-01, 1.0270e-01, 5.8695e-05, 8.0183e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004060075734741986 tensor([3.6296e-03, 8.4908e-04, 4.0601e-04, 6.5473e-01, 3.4039e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002868156007025391 tensor([3.6519e-01, 5.9874e-02, 2.8682e-04, 5.0944e-01, 6.5204e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1393359850872e-09 tensor([4.1393e-09, 2.5389e-04, 9.8655e-01, 1.0394e-06, 1.3192e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.579665306257084e-06 tensor([2.5797e-06, 7.6250e-04, 1.3417e-01, 2.3421e-03, 8.6272e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2656091712415218e-06 tensor([1.2656e-06, 2.1647e-03, 6.5606e-01, 2.6899e-04, 3.4151e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2396008060022723e-05 tensor([3.5770e-01, 6.8008e-03, 1.2396e-05, 6.2555e-01, 9.9418e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.225312473707163e-08 tensor([9.4945e-01, 5.0501e-02, 1.2253e-08, 4.9462e-05, 6.4226e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001994695485336706 tensor([6.3661e-01, 3.3236e-01, 1.9947e-04, 2.3589e-02, 7.2425e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.971339750336483e-06 tensor([1.3626e-01, 2.5970e-03, 9.9713e-06, 8.3936e-01, 2.1776e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.157857802056242e-06 tensor([4.1579e-06, 1.8697e-03, 2.6457e-01, 1.8882e-03, 7.3166e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.636161899426952e-05 tensor([7.8381e-01, 1.0020e-01, 7.6362e-05, 1.0838e-01, 7.5398e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.753959607725847e-08 tensor([9.8169e-01, 1.6618e-02, 7.7540e-08, 1.6828e-03, 1.3410e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0403941310942173 tensor([0.0583, 0.3307, 0.0404, 0.1310, 0.4396], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.247174001648091e-05 tensor([7.4892e-03, 5.0159e-04, 4.2472e-05, 8.2921e-01, 1.6276e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0028784608002752066 tensor([0.1213, 0.0726, 0.0029, 0.4916, 0.3116], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2146570860238626e-08 tensor([1.2147e-08, 1.7460e-05, 2.3985e-01, 3.4731e-04, 7.5979e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.217225053551374e-05 tensor([4.1418e-03, 9.9308e-01, 2.4762e-03, 1.2172e-05, 2.9029e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002414209535345435 tensor([0.0127, 0.0077, 0.0024, 0.5500, 0.4272], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.0072100596735254e-05 tensor([4.8579e-01, 5.1348e-01, 2.0072e-05, 5.3880e-04, 1.7292e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004502366646192968 tensor([4.2247e-02, 9.5304e-01, 2.1605e-03, 4.5024e-04, 2.1053e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1252821423113346e-05 tensor([1.1253e-05, 2.9377e-02, 9.0718e-01, 1.1758e-04, 6.3311e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.526179951535596e-07 tensor([3.7437e-02, 9.6255e-01, 5.5575e-06, 9.5262e-07, 1.8587e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.0986108951202027e-10 tensor([4.0986e-10, 1.9107e-04, 9.9751e-01, 5.4925e-08, 2.2995e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.6120640541194007e-05 tensor([1.6099e-04, 2.6121e-05, 7.2641e-05, 5.5794e-01, 4.4180e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002807067474350333 tensor([0.1576, 0.0920, 0.0028, 0.4674, 0.2802], grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [3], [0], [2], [0], [0], [2], [2], [2], [2], [0], [3], [0], [2], [0], [0], [2], [0], [0], [0], [2], [3], [2], [2], [3], [2], [3], [2], [1], [3], [2], [3], [2], [2], [0], [0], [3], [0], [0], [3], [0], [3], [0], [2], [2], [0], [2], [0], [2], [0], [0], [3], [0], [0], [3], [2], [2], [0], [2], [0], [0], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [3], [0], [2], [3], [2], [2], [2], [2], [3], [3], [0], [0], [0], [0], [0], [2], [2], [2], [3], [0], [2], [2], [0], [0], [3], [3], [0], [0], [0], [2], [0], [2], [2], [0], [0], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [2], [2], [2], [0], [3], [2], [2], [0], [2], [2], [2], [2], [0], [3], [3], [2], [2], [2], [2], [0], [0], [2], [0], [2], [3], [1], [2], [0], [2], [0], [0], [3], [3], [1], [0], [2], [2], [0], [0], [3], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [1], [0], [0], [3], [0], [0], [0], [2], [3], [2], [2], [0], [2], [2], [3], [0], [3], [2], [2], [3], [2], [3], [2], [2], [0], [1], [2], [2], [2], [0], [1], [0], [1], [3], [0], [2], [0], [2], [3], [0], [1], [0], [0], [2], [0], [0], [0], [2], [2], [2], [2], [3], [2], [2], [0], [0], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [2], [0], [3], [2], [2], [3], [0], [3], [0], [1], [2]]\n",
      "[[0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [3], [0], [2], [0], [0], [2], [2], [2], [2], [0], [3], [0], [2], [0], [0], [2], [0], [0], [0], [2], [3], [2], [2], [3], [2], [3], [2], [1], [3], [2], [3], [2], [2], [0], [0], [3], [0], [0], [3], [0], [3], [0], [2], [2], [0], [2], [0], [2], [0], [0], [3], [0], [0], [3], [2], [2], [0], [2], [0], [0], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [3], [0], [2], [3], [2], [2], [2], [2], [3], [3], [0], [0], [0], [0], [0], [2], [2], [2], [3], [0], [2], [2], [0], [0], [3], [3], [0], [0], [0], [2], [0], [2], [2], [0], [0], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [2], [2], [2], [0], [3], [2], [2], [0], [2], [2], [2], [2], [0], [3], [3], [2], [2], [2], [2], [0], [0], [2], [0], [2], [3], [1], [2], [0], [2], [0], [0], [3], [3], [1], [0], [2], [2], [0], [0], [3], [0], [2], [0], [2], [0], [0], [2], [0], [2], [0], [0], [2], [0], [1], [0], [0], [3], [0], [0], [0], [2], [3], [2], [2], [0], [2], [2], [3], [0], [3], [2], [2], [3], [2], [3], [2], [2], [0], [1], [2], [2], [2], [0], [1], [0], [1], [3], [0], [2], [0], [2], [3], [0], [1], [0], [0], [2], [0], [0], [0], [2], [2], [2], [2], [3], [2], [2], [0], [0], [0], [2], [2], [2], [2], [0], [2], [2], [2], [2], [2], [0], [3], [2], [2], [3], [0], [3], [0], [1], [2]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004456912517547608  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004456910133361816  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004456904411315918  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004456896781921387  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.0044568867683410646  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.00445687484741211  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0044568610191345216  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004456845283508301  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.00445682954788208  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004456811904907226  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004456793785095215  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00445677375793457  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004456753253936767  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004456733226776123  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004456710815429687  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0044566884040832516  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004456666469573975  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.0044566431045532226  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0044566202163696285  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00445659589767456  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.007666664198040962 tensor([1.0915e-01, 7.6667e-03, 1.0446e-04, 8.3308e-01, 4.9999e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006777189555577934 tensor([8.2884e-02, 9.1503e-01, 6.7772e-04, 4.4142e-04, 9.6974e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003606846439652145 tensor([7.7213e-07, 3.6068e-04, 2.3267e-01, 1.0981e-03, 7.6587e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01863512583076954 tensor([0.0186, 0.0318, 0.0076, 0.3026, 0.6393], grad_fn=<SoftmaxBackward0>)\n",
      "3 6.836543616373092e-05 tensor([4.2603e-08, 2.3007e-04, 7.6204e-01, 6.8365e-05, 2.3766e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008946307352744043 tensor([0.0008, 0.0009, 0.0024, 0.2994, 0.6965], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.455490125110373e-05 tensor([9.3156e-01, 6.6892e-02, 6.2441e-07, 1.5031e-03, 4.4555e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004165806749369949 tensor([7.6512e-01, 2.3116e-01, 1.6033e-05, 3.2883e-03, 4.1658e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.250964649254456e-05 tensor([1.7040e-01, 9.2510e-05, 2.4350e-08, 8.2884e-01, 6.6926e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0097286906093359 tensor([6.1944e-01, 1.6444e-02, 1.4896e-05, 3.5437e-01, 9.7287e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009088034275919199 tensor([5.4843e-05, 9.0880e-04, 1.8608e-02, 3.0595e-02, 9.4983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006647860631346703 tensor([1.0979e-02, 9.6865e-01, 1.3377e-02, 3.4633e-04, 6.6479e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005345831625163555 tensor([7.2521e-05, 1.3491e-02, 3.4560e-01, 5.3458e-03, 6.3549e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.923437467776239e-06 tensor([5.1144e-04, 6.9234e-06, 1.6612e-06, 9.2756e-01, 7.1915e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011216836981475353 tensor([5.2622e-04, 3.8866e-02, 1.5192e-01, 1.1217e-02, 7.9747e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002433622255921364 tensor([0.0006, 0.1990, 0.5490, 0.0024, 0.2490], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020067347213625908 tensor([0.1142, 0.7874, 0.0094, 0.0201, 0.0690], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013464485295116901 tensor([2.6824e-04, 1.3464e-02, 1.0295e-01, 1.4570e-02, 8.6874e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001020650495775044 tensor([3.1513e-05, 1.0207e-03, 2.1020e-02, 1.4181e-02, 9.6375e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00014946017472539097 tensor([1.3524e-05, 3.6895e-02, 8.5190e-01, 1.4946e-04, 1.1104e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05653893202543259 tensor([0.0717, 0.5832, 0.0333, 0.0565, 0.2553], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005899908137507737 tensor([9.4168e-02, 9.0342e-01, 5.8999e-04, 5.7682e-04, 1.2480e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15862928330898285 tensor([0.1586, 0.2668, 0.0118, 0.2359, 0.3269], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004462513141334057 tensor([2.0163e-02, 4.4625e-03, 3.3414e-04, 7.0017e-01, 2.7487e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.194246528437361e-05 tensor([3.1942e-05, 4.3241e-01, 5.6245e-01, 4.6204e-06, 5.1060e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03909215331077576 tensor([2.6036e-01, 3.9092e-02, 4.3817e-04, 6.2298e-01, 7.7132e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006530158570967615 tensor([6.5302e-04, 9.4597e-01, 5.1468e-02, 1.3502e-05, 1.8961e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010200874879956245 tensor([4.9220e-01, 1.0201e-02, 1.6000e-05, 4.8569e-01, 1.1898e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.013112978398567e-05 tensor([6.4254e-05, 3.0304e-06, 1.0131e-05, 7.5498e-01, 2.4494e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014295746805146337 tensor([1.4818e-02, 9.8039e-01, 3.2394e-03, 1.1824e-04, 1.4296e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011711646802723408 tensor([1.1337e-01, 1.1712e-02, 2.5173e-04, 7.7020e-01, 1.0447e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017414206638932228 tensor([0.0176, 0.9457, 0.0180, 0.0012, 0.0174], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.530512003635522e-06 tensor([3.4533e-03, 3.5305e-06, 4.9270e-08, 9.9201e-01, 4.5343e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001803225459298119 tensor([1.6864e-03, 1.8032e-04, 8.6268e-05, 7.8374e-01, 2.1431e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011810146970674396 tensor([0.0012, 0.5028, 0.3374, 0.0012, 0.1574], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011660491582006216 tensor([1.5957e-05, 1.1660e-03, 8.2752e-02, 9.3022e-03, 9.0676e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032198121771216393 tensor([9.8118e-03, 9.7740e-01, 9.3874e-03, 1.7885e-04, 3.2198e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.931959094596095e-06 tensor([8.7028e-08, 1.5343e-03, 9.6832e-01, 5.9320e-06, 3.0136e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.617822691623587e-06 tensor([1.1307e-06, 6.6178e-06, 8.9499e-04, 2.6099e-02, 9.7300e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00011497028754092753 tensor([1.1497e-04, 4.3891e-01, 5.4410e-01, 3.6164e-05, 1.6838e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004679953563027084 tensor([5.2707e-06, 6.3435e-03, 6.3889e-01, 4.6800e-04, 3.5430e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004808955127373338 tensor([2.2571e-02, 9.7565e-01, 1.2200e-03, 7.8874e-05, 4.8090e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.672069060987269e-07 tensor([2.8315e-08, 4.7658e-03, 9.9303e-01, 1.6721e-07, 2.2071e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.0521690657915315e-06 tensor([9.9540e-01, 3.3912e-03, 2.7338e-09, 1.2084e-03, 2.0522e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00032760697649791837 tensor([3.0560e-01, 6.9343e-01, 7.2015e-05, 5.7122e-04, 3.2761e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03859256953001022 tensor([0.0025, 0.0699, 0.1284, 0.0386, 0.7607], grad_fn=<SoftmaxBackward0>)\n",
      "4 9.979364403989166e-05 tensor([7.8761e-01, 2.1131e-01, 3.5152e-06, 9.8260e-04, 9.9794e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022399909794330597 tensor([0.0009, 0.0022, 0.0053, 0.1468, 0.8447], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.150947236463253e-07 tensor([1.7619e-04, 3.1509e-07, 5.4290e-08, 9.9260e-01, 7.2198e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0430891178548336 tensor([0.0120, 0.2361, 0.0657, 0.0431, 0.6431], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0045395358465611935 tensor([2.5898e-04, 4.5395e-03, 3.5556e-02, 3.9127e-02, 9.2052e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00029817610629834235 tensor([2.1457e-02, 9.7752e-01, 6.7675e-04, 4.7935e-05, 2.9818e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.243386784874019e-07 tensor([3.2748e-09, 2.3436e-04, 9.8628e-01, 9.2434e-07, 1.3488e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4634150829806458e-05 tensor([2.6261e-07, 1.4634e-05, 1.0294e-02, 5.5621e-03, 9.8413e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0024572641123086214 tensor([4.4917e-02, 9.4857e-01, 2.4573e-03, 5.9077e-04, 3.4616e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06024413928389549 tensor([0.0602, 0.4372, 0.0371, 0.0751, 0.3903], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00065567996352911 tensor([2.0582e-01, 7.9250e-01, 1.8322e-04, 6.5568e-04, 8.3998e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005278681870549917 tensor([4.6956e-04, 5.2787e-03, 2.8219e-02, 4.6755e-02, 9.1928e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010326545452699065 tensor([6.2592e-01, 3.6899e-01, 3.7250e-05, 4.0196e-03, 1.0327e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9799506844719872e-05 tensor([6.9077e-06, 4.9816e-02, 9.2657e-01, 1.9800e-05, 2.3587e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016497631440870464 tensor([2.8454e-05, 6.1376e-02, 8.6340e-01, 1.6498e-04, 7.5029e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.104117341645178e-07 tensor([9.4928e-01, 5.0673e-02, 1.3079e-08, 5.0900e-05, 8.1041e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.387578079942614e-06 tensor([1.2624e-06, 1.6832e-02, 9.6464e-01, 7.3876e-06, 1.8516e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.960389219377248e-07 tensor([4.2973e-04, 4.9604e-07, 3.0033e-08, 9.9413e-01, 5.4425e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012010583654046059 tensor([0.0009, 0.0595, 0.1620, 0.0120, 0.7656], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009487870847806334 tensor([6.5099e-02, 9.4879e-04, 9.6754e-06, 9.1135e-01, 2.2589e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003623386437539011 tensor([9.3152e-01, 7.4222e-03, 4.0827e-07, 6.0700e-02, 3.6234e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.205512927844211e-08 tensor([7.7426e-11, 7.4989e-05, 9.9816e-01, 2.2055e-08, 1.7621e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.276320421718992e-05 tensor([1.0513e-06, 3.2763e-05, 8.5440e-03, 8.7138e-03, 9.8271e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05228859931230545 tensor([0.0523, 0.2115, 0.0188, 0.1313, 0.5861], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002648814581334591 tensor([0.0988, 0.8907, 0.0022, 0.0026, 0.0057], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00013409103848971426 tensor([4.6837e-02, 9.5281e-01, 1.7766e-04, 4.5305e-05, 1.3409e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005960541777312756 tensor([8.0463e-05, 1.0782e-02, 2.2832e-01, 5.9605e-03, 7.5486e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022356694098562002 tensor([1.9589e-02, 2.2357e-03, 1.1401e-04, 8.2089e-01, 1.5717e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8090142222936265e-05 tensor([2.8090e-05, 5.3052e-01, 4.6588e-01, 2.4960e-06, 3.5692e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09575702250003815 tensor([0.3131, 0.0958, 0.0012, 0.4908, 0.0991], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3337355994735844e-05 tensor([9.4947e-01, 3.6476e-04, 1.7249e-09, 5.0157e-02, 1.3337e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0035936860367655754 tensor([8.5884e-01, 7.8375e-02, 3.5713e-05, 5.9156e-02, 3.5937e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.987481588614173e-06 tensor([5.6907e-03, 4.9875e-06, 2.7047e-08, 9.9165e-01, 2.6525e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014182772487401962 tensor([0.0142, 0.7697, 0.0725, 0.0058, 0.1379], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021859727799892426 tensor([0.0219, 0.6803, 0.0754, 0.0141, 0.2084], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019756462424993515 tensor([0.0024, 0.0198, 0.0267, 0.0966, 0.8545], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.18635055818595e-06 tensor([2.9770e-07, 4.5664e-03, 9.7176e-01, 7.1864e-06, 2.3668e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1034700946765952e-05 tensor([2.1413e-06, 1.1035e-05, 1.2332e-03, 3.7927e-02, 9.6083e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0074096908792853355 tensor([0.0015, 0.1896, 0.2947, 0.0074, 0.5068], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.047227632254362106 tensor([0.0045, 0.0868, 0.0802, 0.0472, 0.7813], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025164447724819183 tensor([5.0114e-01, 2.5471e-02, 5.0058e-05, 4.4817e-01, 2.5164e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00013152383326087147 tensor([7.2794e-01, 2.7082e-01, 8.7631e-06, 1.1068e-03, 1.3152e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007679684553295374 tensor([0.0117, 0.0077, 0.0017, 0.4408, 0.5381], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002933945506811142 tensor([2.9339e-03, 8.6351e-01, 1.0625e-01, 4.4534e-04, 2.6853e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.373379418140757e-08 tensor([6.1676e-09, 1.9654e-03, 9.9651e-01, 7.3734e-08, 1.5233e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005759308580309153 tensor([6.3516e-01, 3.4188e-01, 1.4789e-04, 1.7055e-02, 5.7593e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03277815505862236 tensor([3.0442e-01, 3.2778e-02, 2.7394e-04, 5.9482e-01, 6.7704e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.9056614948785864e-05 tensor([8.7176e-06, 2.9057e-05, 9.6468e-04, 5.6323e-02, 9.4267e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001293503155466169 tensor([8.3422e-05, 1.9200e-01, 7.4102e-01, 1.2935e-04, 6.6770e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013689970597624779 tensor([0.0137, 0.8361, 0.0632, 0.0047, 0.0823], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004082129162270576 tensor([7.0098e-03, 9.9030e-01, 2.2585e-03, 2.1983e-05, 4.0821e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5142674101298326e-06 tensor([1.7205e-08, 7.3916e-04, 9.8699e-01, 1.5143e-06, 1.2271e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004354986303951591 tensor([6.7175e-05, 4.3550e-04, 4.9063e-03, 6.1399e-02, 9.3319e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.9504871551798715e-07 tensor([1.8648e-07, 1.7997e-02, 9.7837e-01, 3.9505e-07, 3.6302e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006697435397654772 tensor([3.4688e-02, 6.6974e-03, 5.4324e-04, 7.1558e-01, 2.4249e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001220628502778709 tensor([0.0011, 0.4458, 0.4221, 0.0012, 0.1298], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03794199228286743 tensor([2.1787e-01, 3.7942e-02, 5.0562e-04, 6.1922e-01, 1.2446e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.139265107980464e-07 tensor([3.1417e-03, 7.1393e-07, 2.3815e-09, 9.9610e-01, 7.5525e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.2706022718921304e-05 tensor([1.4432e-05, 1.0340e-01, 8.6902e-01, 2.2706e-05, 2.7547e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002307349815964699 tensor([7.0345e-04, 2.3073e-03, 1.0398e-02, 1.7768e-01, 8.0891e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09542400389909744 tensor([0.1274, 0.0954, 0.0033, 0.4968, 0.2771], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011949249543249607 tensor([0.0056, 0.0215, 0.0119, 0.1236, 0.8373], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005666354554705322 tensor([2.8955e-02, 5.6664e-04, 7.6702e-06, 9.4208e-01, 2.8393e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008476371876895428 tensor([5.9102e-04, 8.4764e-03, 2.9959e-02, 4.3338e-02, 9.1764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09832371026277542 tensor([0.0983, 0.3047, 0.0235, 0.1834, 0.3900], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0016747716581448913 tensor([8.9122e-01, 3.6190e-02, 6.9157e-06, 7.0907e-02, 1.6748e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018343389965593815 tensor([3.7930e-04, 1.2478e-01, 5.3468e-01, 1.8343e-03, 3.3832e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4006843230163213e-05 tensor([6.7516e-04, 1.4007e-05, 3.6481e-06, 9.1058e-01, 8.8730e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0069916811771690845 tensor([0.0046, 0.4381, 0.2083, 0.0070, 0.3420], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00017994835798162967 tensor([9.3392e-01, 5.8122e-02, 2.7260e-06, 7.7795e-03, 1.7995e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.0793383808049839e-05 tensor([8.5328e-02, 9.1464e-01, 1.3277e-05, 8.6195e-06, 1.0793e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5298548532882705e-05 tensor([4.5781e-06, 4.6918e-02, 9.2956e-01, 1.5299e-05, 2.3506e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0037708228919655085 tensor([3.5924e-02, 3.7708e-03, 1.2859e-04, 8.3561e-01, 1.2456e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002754479181021452 tensor([1.4212e-05, 4.1526e-03, 2.1006e-01, 2.7545e-03, 7.8301e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03495871648192406 tensor([0.0350, 0.0657, 0.0128, 0.3288, 0.5577], grad_fn=<SoftmaxBackward0>)\n",
      "4 7.329570962610887e-06 tensor([2.6895e-01, 7.3102e-01, 2.9197e-06, 1.6471e-05, 7.3296e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022796019911766052 tensor([0.5509, 0.3766, 0.0007, 0.0490, 0.0228], grad_fn=<SoftmaxBackward0>)\n",
      "1 7.072338803482126e-07 tensor([1.2617e-08, 7.0723e-07, 2.7081e-03, 2.5881e-03, 9.9470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.765312274277676e-05 tensor([2.7653e-05, 1.6535e-01, 8.1408e-01, 2.2412e-05, 2.0528e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0029481472447514534 tensor([8.3459e-01, 2.7396e-02, 1.1193e-05, 1.3506e-01, 2.9481e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.468889862138894e-07 tensor([9.8682e-01, 1.3002e-02, 4.0614e-09, 1.7545e-04, 7.4689e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.781699681188911e-05 tensor([5.5794e-07, 4.7817e-05, 3.0445e-02, 3.9134e-03, 9.6559e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004016630817204714 tensor([1.7709e-01, 4.0166e-03, 1.1530e-05, 7.9277e-01, 2.6108e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002445701975375414 tensor([4.9780e-01, 4.9466e-01, 1.4493e-04, 4.9409e-03, 2.4457e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00019400898599997163 tensor([2.6868e-03, 1.9401e-04, 6.8431e-05, 8.5404e-01, 1.4301e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006838692352175713 tensor([5.4219e-01, 4.5483e-01, 5.6474e-05, 2.2451e-03, 6.8387e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0022003382910043 tensor([6.6938e-05, 2.2433e-02, 4.8139e-01, 2.2003e-03, 4.9391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02116597257554531 tensor([0.0501, 0.8098, 0.0212, 0.0167, 0.1022], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002021375112235546 tensor([2.0214e-03, 7.8429e-01, 1.8141e-01, 4.0457e-04, 3.1882e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0023487266153097153 tensor([8.6101e-01, 2.5396e-02, 8.8465e-06, 1.1123e-01, 2.3487e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.214513218263164e-05 tensor([1.6777e-01, 8.3202e-01, 4.2174e-05, 8.8610e-05, 8.2145e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02581607922911644 tensor([0.0825, 0.7337, 0.0173, 0.0258, 0.1406], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.2639518394717015e-06 tensor([2.0777e-04, 3.2640e-06, 1.9902e-06, 9.0928e-01, 9.0507e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002001640386879444 tensor([1.0814e-05, 2.0016e-03, 1.5285e-01, 4.5102e-03, 8.4063e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012144320644438267 tensor([0.0052, 0.3641, 0.2369, 0.0121, 0.3817], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.691393663350027e-06 tensor([8.9655e-01, 1.0339e-01, 5.1590e-08, 5.6901e-05, 1.6914e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0548967111390084e-05 tensor([8.0428e-07, 7.9408e-03, 9.7341e-01, 1.0549e-05, 1.8637e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.8251265601065825e-06 tensor([7.0589e-04, 1.8251e-06, 1.0887e-07, 9.8858e-01, 1.0717e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003971025871578604 tensor([3.9710e-04, 4.1146e-01, 5.0382e-01, 2.9088e-04, 8.4033e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007373277912847698 tensor([7.3733e-04, 5.5102e-04, 1.5876e-03, 4.0387e-01, 5.9326e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006060390151105821 tensor([1.3421e-01, 8.6390e-01, 3.3587e-04, 6.0604e-04, 9.4977e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4246295904740691e-05 tensor([2.7127e-06, 2.3421e-02, 9.5052e-01, 1.4246e-05, 2.6047e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.1725221560918726e-06 tensor([1.2953e-03, 3.1725e-06, 1.5635e-07, 9.9088e-01, 7.8239e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.0218907909329573e-07 tensor([5.1266e-09, 6.9978e-04, 9.9396e-01, 3.0219e-07, 5.3442e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014017481356859207 tensor([0.0068, 0.0228, 0.0140, 0.1761, 0.7803], grad_fn=<SoftmaxBackward0>)\n",
      "4 3.716362698469311e-05 tensor([1.3067e-02, 9.8670e-01, 1.8898e-04, 4.8677e-06, 3.7164e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0028411666862666607 tensor([2.8412e-03, 8.4280e-01, 1.2027e-01, 4.4103e-04, 3.3645e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013007334200665355 tensor([0.0015, 0.0013, 0.0013, 0.3538, 0.6420], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0033787942957133055 tensor([2.5790e-05, 7.1643e-03, 3.2908e-01, 3.3788e-03, 6.6036e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0056955874897539616 tensor([7.3049e-01, 2.1434e-02, 1.4758e-05, 2.4236e-01, 5.6956e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007912983419373631 tensor([2.3247e-01, 7.6575e-01, 1.7741e-04, 8.1725e-04, 7.9130e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002689813729375601 tensor([5.2868e-05, 1.6432e-02, 4.5710e-01, 2.6898e-03, 5.2373e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009619841584935784 tensor([7.0576e-04, 9.6198e-04, 1.3378e-03, 2.5142e-01, 7.4557e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006502503529191017 tensor([6.5025e-04, 5.4558e-01, 3.9744e-01, 2.7078e-04, 5.6059e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.056221988052129745 tensor([0.0146, 0.2405, 0.1047, 0.0562, 0.5840], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2161141186952591 tensor([0.2238, 0.2884, 0.0054, 0.2663, 0.2161], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00014882472169119865 tensor([6.1222e-06, 1.7086e-02, 8.6655e-01, 1.4882e-04, 1.1621e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.662325530560338e-06 tensor([9.9632e-01, 1.0220e-03, 7.4080e-10, 2.6604e-03, 1.6623e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.078892066492699e-05 tensor([1.9767e-05, 1.3577e-01, 8.4136e-01, 2.0789e-05, 2.2831e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0034228807780891657 tensor([0.0012, 0.2684, 0.4409, 0.0034, 0.2861], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08084754645824432 tensor([0.4080, 0.1194, 0.0007, 0.3911, 0.0808], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0030409612227231264 tensor([0.0006, 0.1428, 0.4523, 0.0030, 0.4013], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.2359816941607278e-06 tensor([1.6988e-04, 3.2360e-06, 3.1101e-06, 8.6904e-01, 1.3078e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013591548195108771 tensor([2.0685e-04, 1.1268e-01, 6.3742e-01, 1.3592e-03, 2.4833e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05226145684719086 tensor([0.0063, 0.0523, 0.0532, 0.1214, 0.7668], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.053024117136374e-05 tensor([7.9268e-01, 2.0653e-01, 2.4630e-06, 7.2607e-04, 6.0530e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00024192218552343547 tensor([3.6846e-05, 2.4192e-04, 5.8210e-03, 3.8535e-02, 9.5537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00022957015607971698 tensor([3.4300e-04, 9.1055e-05, 2.2957e-04, 5.6384e-01, 4.3550e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.588189656511531e-06 tensor([1.6174e-06, 3.6044e-02, 9.5305e-01, 3.5882e-06, 1.0901e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006287672556936741 tensor([1.3420e-04, 2.5515e-02, 3.3912e-01, 6.2877e-03, 6.2894e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010965455323457718 tensor([0.0543, 0.9008, 0.0110, 0.0048, 0.0291], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007479418069124222 tensor([1.5165e-04, 1.3439e-02, 1.1574e-01, 7.4794e-03, 8.6319e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.497767753899097e-05 tensor([8.4894e-07, 6.4978e-05, 1.6696e-02, 4.7634e-03, 9.7847e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006581795169040561 tensor([9.3228e-07, 9.1402e-04, 4.8552e-01, 6.5818e-04, 5.1291e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10961532592773438 tensor([0.3736, 0.1978, 0.0022, 0.3168, 0.1096], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02160251885652542 tensor([0.0216, 0.8415, 0.0449, 0.0049, 0.0871], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04159395396709442 tensor([4.8055e-01, 4.5782e-02, 2.0813e-04, 4.3187e-01, 4.1594e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.944962817418855e-06 tensor([7.4368e-04, 7.9450e-06, 1.7529e-06, 9.6379e-01, 3.5461e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002762577496469021 tensor([3.1519e-04, 9.9683e-02, 4.1646e-01, 2.7626e-03, 4.8078e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.372053495491855e-05 tensor([9.6389e-01, 8.8009e-04, 8.6428e-09, 3.5202e-02, 2.3721e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010043532587587833 tensor([7.5575e-01, 1.5690e-01, 1.1335e-04, 7.7187e-02, 1.0044e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01267260778695345 tensor([0.0127, 0.8436, 0.0790, 0.0027, 0.0620], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010287869372405112 tensor([2.0113e-05, 1.0288e-04, 2.5757e-03, 5.9827e-02, 9.3747e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000123498888569884 tensor([1.2350e-04, 6.3209e-01, 3.5811e-01, 1.3703e-05, 9.6613e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006430693436414003 tensor([0.1139, 0.8523, 0.0057, 0.0064, 0.0217], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009311054833233356 tensor([5.7638e-01, 1.5168e-02, 1.4030e-05, 3.9913e-01, 9.3111e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006005954463034868 tensor([0.0833, 0.8862, 0.0060, 0.0039, 0.0206], grad_fn=<SoftmaxBackward0>)\n",
      "1 5.230780288911774e-07 tensor([1.1200e-04, 5.2308e-07, 2.4919e-07, 9.7733e-01, 2.2556e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.183374014450237e-07 tensor([5.1834e-07, 4.3140e-02, 9.5412e-01, 4.4824e-07, 2.7387e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00010524072422413155 tensor([9.7495e-01, 1.4397e-02, 4.6383e-07, 1.0551e-02, 1.0524e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006362093263305724 tensor([8.0971e-01, 1.8357e-01, 1.4639e-05, 6.0693e-03, 6.3621e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.597117365141457e-07 tensor([2.9967e-08, 2.3128e-03, 9.9194e-01, 5.5971e-07, 5.7416e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.713706509093754e-06 tensor([1.4244e-05, 8.5390e-07, 7.7137e-06, 5.9240e-01, 4.0757e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08602243661880493 tensor([0.0860, 0.3752, 0.0219, 0.1291, 0.3878], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08429492264986038 tensor([0.0843, 0.1574, 0.0130, 0.3002, 0.4450], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.121133770240704e-06 tensor([9.9117e-01, 7.7691e-03, 1.1427e-08, 1.0583e-03, 4.1211e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1529045877978206e-06 tensor([3.5336e-09, 2.1675e-04, 9.8343e-01, 1.1529e-06, 1.6356e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.09924749773927e-05 tensor([1.8786e-04, 3.0376e-05, 8.0992e-05, 6.2101e-01, 3.7869e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019097881158813834 tensor([8.4949e-06, 1.8250e-02, 8.4474e-01, 1.9098e-04, 1.3681e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.728438115213066e-05 tensor([8.7284e-05, 2.3505e-05, 1.9718e-04, 4.5868e-01, 5.4101e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00044419735786505044 tensor([3.1938e-03, 9.9161e-01, 4.7368e-03, 1.4097e-05, 4.4420e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004205535631626844 tensor([2.0382e-04, 4.2055e-03, 3.6553e-02, 3.2642e-02, 9.2640e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011505584698170424 tensor([2.2264e-01, 1.1506e-03, 1.3290e-06, 7.6978e-01, 6.4323e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.6498469348298386e-05 tensor([3.8691e-06, 1.4712e-02, 9.0276e-01, 5.6498e-05, 8.2467e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10179450362920761 tensor([0.3751, 0.1508, 0.0009, 0.3714, 0.1018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005178895080462098 tensor([4.3817e-03, 9.9104e-01, 4.0410e-03, 2.0380e-05, 5.1789e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005394738633185625 tensor([5.7575e-05, 5.3947e-03, 1.4753e-01, 7.3655e-03, 8.3965e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.669806680292822e-06 tensor([6.6698e-06, 5.7403e-06, 2.0014e-04, 1.4874e-01, 8.5105e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.732690629476565e-06 tensor([1.9326e-08, 5.4295e-04, 9.7119e-01, 3.7327e-06, 2.8267e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003950886311940849 tensor([3.5828e-06, 3.9509e-04, 8.7412e-02, 6.7738e-03, 9.0542e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.67501020769123e-05 tensor([9.9062e-01, 6.0966e-03, 1.9858e-08, 3.2644e-03, 1.6750e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00042796999332495034 tensor([2.9597e-05, 3.4333e-02, 7.7129e-01, 4.2797e-04, 1.9392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00985704269260168 tensor([1.1673e-04, 1.0484e-02, 1.1836e-01, 9.8570e-03, 8.6118e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.740009873989038e-05 tensor([2.6453e-05, 1.0729e-01, 8.4561e-01, 5.7400e-05, 4.7023e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17886722087860107 tensor([0.2620, 0.3431, 0.0061, 0.2099, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.5837984430363576e-07 tensor([9.8729e-01, 1.2614e-02, 2.1436e-09, 9.6147e-05, 4.5838e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006448124186135828 tensor([3.5199e-02, 6.4481e-04, 1.2986e-05, 9.3338e-01, 3.0767e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000748966820538044 tensor([1.5835e-02, 7.4897e-04, 3.6902e-05, 8.9583e-01, 8.7547e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009293066104874015 tensor([9.2931e-04, 8.9317e-01, 9.8075e-02, 5.8332e-05, 7.7672e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008839917136356235 tensor([3.8221e-03, 8.8399e-04, 4.0358e-04, 6.5861e-01, 3.3628e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06152253970503807 tensor([3.7462e-01, 6.1523e-02, 2.8416e-04, 4.9979e-01, 6.3782e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0577733746686135e-06 tensor([4.3545e-09, 2.6299e-04, 9.8653e-01, 1.0578e-06, 1.3210e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008005230338312685 tensor([2.7507e-06, 8.0052e-04, 1.3409e-01, 2.3894e-03, 8.6272e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00027634756406769156 tensor([1.3566e-06, 2.2720e-03, 6.5395e-01, 2.7635e-04, 3.4350e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006977146957069635 tensor([3.6901e-01, 6.9771e-03, 1.2157e-05, 6.1433e-01, 9.6721e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.372516736519174e-07 tensor([9.4913e-01, 5.0820e-02, 1.2192e-08, 4.8771e-05, 6.3725e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006920544430613518 tensor([6.3859e-01, 3.3157e-01, 1.9165e-04, 2.2735e-02, 6.9205e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0027159336023032665 tensor([1.4192e-01, 2.7159e-03, 1.0048e-05, 8.3375e-01, 2.1596e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0019319596467539668 tensor([4.4558e-06, 1.9653e-03, 2.6321e-01, 1.9320e-03, 7.3288e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007214655634015799 tensor([7.8801e-01, 1.0014e-01, 7.3393e-05, 1.0456e-01, 7.2147e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3206011317379307e-05 tensor([9.8162e-01, 1.6715e-02, 7.6716e-08, 1.6477e-03, 1.3206e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06115473061800003 tensor([0.0612, 0.3393, 0.0392, 0.1310, 0.4293], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005230483948253095 tensor([7.8877e-03, 5.2305e-04, 4.2197e-05, 8.3109e-01, 1.6046e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0750013142824173 tensor([0.1272, 0.0750, 0.0028, 0.4905, 0.3044], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.809826062526554e-05 tensor([1.2792e-08, 1.8098e-05, 2.3964e-01, 3.5400e-04, 7.5999e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002842231187969446 tensor([4.2109e-03, 9.9310e-01, 2.3950e-03, 1.2099e-05, 2.8422e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008095638826489449 tensor([0.0134, 0.0081, 0.0024, 0.5531, 0.4230], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00016828674415592104 tensor([4.8771e-01, 5.1158e-01, 1.9474e-05, 5.2860e-04, 1.6829e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0020447000861167908 tensor([4.3346e-02, 9.5210e-01, 2.0624e-03, 4.4851e-04, 2.0447e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00012162217899458483 tensor([1.2133e-05, 3.0948e-02, 9.0484e-01, 1.2162e-04, 6.4078e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.840137201725156e-06 tensor([3.7768e-02, 9.6222e-01, 5.4481e-06, 9.4968e-07, 1.8401e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.6096624234669434e-08 tensor([4.3334e-10, 1.9858e-04, 9.9749e-01, 5.6097e-08, 2.3087e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.212934724520892e-05 tensor([1.6999e-04, 2.7185e-05, 7.2129e-05, 5.6320e-01, 4.3654e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09460227936506271 tensor([0.1647, 0.0946, 0.0027, 0.4652, 0.2728], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1], [3, 2], [0, 1], [2, 0], [0, 3], [0, 1], [2, 4], [2, 4], [2, 1], [2, 4], [0, 1], [3, 4], [0, 3], [2, 1], [0, 3], [0, 3], [2, 3], [0, 1], [0, 1], [0, 3], [2, 3], [3, 2], [2, 0], [2, 1], [3, 0], [2, 1], [3, 0], [2, 1], [1, 2], [3, 4], [2, 1], [3, 4], [2, 1], [2, 1], [0, 3], [0, 1], [3, 4], [0, 3], [0, 1], [3, 0], [0, 3], [3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 4], [0, 1], [2, 1], [0, 3], [0, 1], [3, 4], [0, 3], [0, 1], [3, 2], [2, 0], [2, 3], [0, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 1], [0, 3], [2, 1], [2, 4], [0, 3], [0, 1], [2, 0], [2, 3], [3, 4], [0, 3], [2, 1], [3, 0], [2, 1], [2, 4], [2, 4], [2, 1], [3, 0], [3, 0], [0, 1], [0, 3], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [2, 1], [3, 0], [0, 3], [2, 4], [2, 1], [0, 1], [0, 3], [3, 0], [3, 4], [0, 3], [0, 1], [0, 3], [2, 1], [0, 3], [2, 1], [2, 1], [0, 3], [0, 1], [2, 1], [0, 2], [2, 1], [0, 1], [2, 0], [2, 4], [0, 3], [2, 1], [0, 3], [2, 4], [3, 4], [0, 3], [2, 1], [0, 3], [2, 0], [2, 4], [2, 4], [0, 1], [3, 0], [2, 4], [2, 4], [0, 1], [2, 1], [2, 4], [2, 1], [2, 4], [0, 3], [3, 2], [3, 0], [2, 4], [2, 4], [2, 3], [2, 1], [0, 1], [0, 3], [2, 4], [0, 3], [2, 1], [3, 0], [1, 0], [2, 3], [0, 3], [2, 1], [0, 3], [0, 2], [3, 4], [3, 0], [1, 2], [0, 3], [2, 4], [2, 4], [0, 3], [0, 1], [3, 0], [0, 3], [2], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 1], [0, 3], [0, 1], [2, 4], [0, 1], [1, 2], [0, 3], [0, 3], [3, 2], [0, 3], [0, 1], [0, 3], [2, 4], [3, 0], [2, 4], [2, 1], [0, 3], [2, 4], [2, 4], [3, 0], [0, 1], [3, 0], [2, 3], [2, 4], [3, 2], [2, 1], [3, 0], [2, 4], [2, 4], [0, 3], [1, 2], [2, 0], [2, 0], [2, 4], [0, 3], [1, 2], [0, 3], [1, 0], [3, 4], [0, 1], [2, 1], [0, 3], [2, 4], [3, 4], [0, 1], [1, 0], [0, 3], [0, 1], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [2, 1], [2, 1], [3, 0], [2, 1], [2, 1], [0, 3], [0, 1], [0, 3], [2, 1], [2, 4], [2, 4], [2, 1], [0, 3], [2, 4], [2, 4], [2, 0], [2, 1], [2, 1], [0, 1], [3, 4], [2, 1], [2, 4], [3, 4], [0, 3], [3, 4], [0, 3], [1, 2], [2, 1]]\n",
      "[[0, 3, 4], [0, 1, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 4], [2, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 4], [0, 1, 4], [1, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 2], [1, 2, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 2], [1, 2, 4], [2, 3, 4], [0, 1, 4], [1, 3, 4], [0, 1, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 3, 4], [0, 1, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [1, 3, 4], [0, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 4], [0, 1, 3], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 2], [2, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 2], [2, 3, 4], [2, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [0, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 2], [1, 2, 4], [0, 3, 4], [0, 3, 4]]\n",
      "NL_pred of 1th iteration [[2, 1], [3, 2], [0, 1], [2, 0], [0, 3], [0, 1], [2, 4], [2, 4], [2, 1], [2, 4], [0, 1], [3, 4], [0, 3], [2, 1], [0, 3], [0, 3], [2, 3], [0, 1], [0, 1], [0, 3], [2, 3], [3, 2], [2, 0], [2, 1], [3, 0], [2, 1], [3, 0], [2, 1], [1, 2], [3, 4], [2, 1], [3, 4], [2, 1], [2, 1], [0, 3], [0, 1], [3, 4], [0, 3], [0, 1], [3, 0], [0, 3], [3, 4], [0, 3], [2, 4], [2, 4], [0, 3], [2, 4], [0, 1], [2, 1], [0, 3], [0, 1], [3, 4], [0, 3], [0, 1], [3, 2], [2, 0], [2, 3], [0, 1], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 1], [0, 3], [2, 1], [2, 4], [0, 3], [0, 1], [2, 0], [2, 3], [3, 4], [0, 3], [2, 1], [3, 0], [2, 1], [2, 4], [2, 4], [2, 1], [3, 0], [3, 0], [0, 1], [0, 3], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [2, 1], [3, 0], [0, 3], [2, 4], [2, 1], [0, 1], [0, 3], [3, 0], [3, 4], [0, 3], [0, 1], [0, 3], [2, 1], [0, 3], [2, 1], [2, 1], [0, 3], [0, 1], [2, 1], [0, 2], [2, 1], [0, 1], [2, 0], [2, 4], [0, 3], [2, 1], [0, 3], [2, 4], [3, 4], [0, 3], [2, 1], [0, 3], [2, 0], [2, 4], [2, 4], [0, 1], [3, 0], [2, 4], [2, 4], [0, 1], [2, 1], [2, 4], [2, 1], [2, 4], [0, 3], [3, 2], [3, 0], [2, 4], [2, 4], [2, 3], [2, 1], [0, 1], [0, 3], [2, 4], [0, 3], [2, 1], [3, 0], [1, 0], [2, 3], [0, 3], [2, 1], [0, 3], [0, 2], [3, 4], [3, 0], [1, 2], [0, 3], [2, 4], [2, 4], [0, 3], [0, 1], [3, 0], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 1], [0, 3], [0, 1], [2, 4], [0, 1], [1, 2], [0, 3], [0, 3], [3, 2], [0, 3], [0, 1], [0, 3], [2, 4], [3, 0], [2, 4], [2, 1], [0, 3], [2, 4], [2, 4], [3, 0], [0, 1], [3, 0], [2, 3], [2, 4], [3, 2], [2, 1], [3, 0], [2, 4], [2, 4], [0, 3], [1, 2], [2, 0], [2, 0], [2, 4], [0, 3], [1, 2], [0, 3], [1, 0], [3, 4], [0, 1], [2, 1], [0, 3], [2, 4], [3, 4], [0, 1], [1, 0], [0, 3], [0, 1], [2, 4], [0, 3], [0, 3], [0, 3], [2, 4], [2, 4], [2, 1], [2, 1], [3, 0], [2, 1], [2, 1], [0, 3], [0, 1], [0, 3], [2, 1], [2, 4], [2, 4], [2, 1], [0, 3], [2, 4], [2, 4], [2, 0], [2, 1], [2, 1], [0, 1], [3, 4], [2, 1], [2, 4], [3, 4], [0, 3], [3, 4], [0, 3], [1, 2], [2, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004514653998685171  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004514645381146167  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004514631018581161  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0045146104322379855  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004514583143364474  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004514552503225794  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004514516596813278  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004514477339135595  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004514434730192743  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004514390206241225  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004514342809776704  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00451429301955135  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004514242750573829  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.00451419056658764  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.0045141383826014505  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004514085241110928  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004514031620868239  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004513977521873382  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004513923422878526  Accuracy on Support set:0.0\n",
      "torch.Size([249, 2048]) torch.Size([249])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004513869323883669  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.05198514088988304 tensor([1.0168e-01, 7.3168e-03, 1.0823e-04, 8.3891e-01, 5.1985e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010257754474878311 tensor([8.2007e-02, 9.1579e-01, 7.1602e-04, 4.5987e-04, 1.0258e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010612027253955603 tensor([7.1227e-07, 3.4341e-04, 2.3508e-01, 1.0612e-03, 7.6352e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.029495883733034134 tensor([0.0168, 0.0295, 0.0077, 0.2962, 0.6499], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00021744522382505238 tensor([3.9127e-08, 2.1745e-04, 7.6382e-01, 6.5890e-05, 2.3589e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00240973848849535 tensor([0.0007, 0.0008, 0.0024, 0.2919, 0.7041], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015896364348009229 tensor([9.3073e-01, 6.7634e-02, 6.6882e-07, 1.5896e-03, 4.7769e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003456979524344206 tensor([7.5999e-01, 2.3609e-01, 1.7522e-05, 3.4570e-03, 4.5074e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006922050379216671 tensor([1.6028e-01, 8.8253e-05, 2.4954e-08, 8.3894e-01, 6.9221e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016290029510855675 tensor([6.0044e-01, 1.6290e-02, 1.6020e-05, 3.7272e-01, 1.0538e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018727395683526993 tensor([4.8770e-05, 8.4111e-04, 1.8727e-02, 2.9165e-02, 9.5122e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010649660602211952 tensor([1.0650e-02, 9.6739e-01, 1.4463e-02, 3.5929e-04, 7.1343e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01253872737288475 tensor([6.5444e-05, 1.2539e-02, 3.4818e-01, 5.1680e-03, 6.3405e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00047834773431532085 tensor([4.7835e-04, 6.7044e-06, 1.7364e-06, 9.2465e-01, 7.4861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03563008829951286 tensor([4.6200e-04, 3.5630e-02, 1.5404e-01, 1.0706e-02, 7.9916e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18565484881401062 tensor([5.2753e-04, 1.8565e-01, 5.6005e-01, 2.3602e-03, 2.5140e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07419034838676453 tensor([0.1118, 0.7828, 0.0100, 0.0211, 0.0742], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014000564813613892 tensor([2.3775e-04, 1.2348e-02, 1.0349e-01, 1.4001e-02, 8.6992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01373421959578991 tensor([2.8505e-05, 9.4983e-04, 2.1183e-02, 1.3734e-02, 9.6410e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.033959876745939255 tensor([1.2071e-05, 3.3960e-02, 8.5548e-01, 1.4372e-04, 1.1041e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06677398085594177 tensor([0.0668, 0.5698, 0.0358, 0.0569, 0.2707], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001319576520472765 tensor([9.2804e-02, 9.0465e-01, 6.2515e-04, 5.9843e-04, 1.3196e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2384786754846573 tensor([0.1471, 0.2564, 0.0125, 0.2385, 0.3456], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018427006900310516 tensor([1.8427e-02, 4.1960e-03, 3.4288e-04, 6.9406e-01, 2.8298e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005233193282037973 tensor([2.9469e-05, 4.1257e-01, 5.8216e-01, 4.5613e-06, 5.2332e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08077365159988403 tensor([2.4605e-01, 3.7708e-02, 4.5674e-04, 6.3501e-01, 8.0774e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001996867824345827 tensor([6.3060e-04, 9.4235e-01, 5.5010e-02, 1.3760e-05, 1.9969e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012799816206097603 tensor([4.7137e-01, 1.0017e-02, 1.7131e-05, 5.0580e-01, 1.2800e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.991202488075942e-05 tensor([5.9912e-05, 2.9240e-06, 1.0511e-05, 7.4717e-01, 2.5276e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003517776494845748 tensor([1.4211e-02, 9.8064e-01, 3.5178e-03, 1.2024e-04, 1.5146e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10547801107168198 tensor([1.0548e-01, 1.1185e-02, 2.6118e-04, 7.7431e-01, 1.0877e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01716090366244316 tensor([0.0172, 0.9433, 0.0194, 0.0013, 0.0188], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003252550959587097 tensor([3.2526e-03, 3.3913e-06, 5.0492e-08, 9.9208e-01, 4.6637e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015556465368717909 tensor([1.5556e-03, 1.7218e-04, 8.9475e-05, 7.7643e-01, 2.2175e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1630222052335739 tensor([0.0011, 0.4804, 0.3543, 0.0012, 0.1630], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00896644126623869 tensor([1.4464e-05, 1.0932e-03, 8.3768e-02, 8.9664e-03, 9.0616e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009546689689159393 tensor([9.5467e-03, 9.7681e-01, 1.0052e-02, 1.8390e-04, 3.4115e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014148137997835875 tensor([7.7817e-08, 1.4148e-03, 9.6872e-01, 5.6958e-06, 2.9862e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009084077319130301 tensor([1.0405e-06, 6.3097e-06, 9.0841e-04, 2.5135e-02, 9.7395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017266634851694107 tensor([1.0546e-04, 4.1774e-01, 5.6485e-01, 3.5592e-05, 1.7267e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0058869896456599236 tensor([4.7710e-06, 5.8870e-03, 6.4034e-01, 4.5365e-04, 3.5331e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001298337709158659 tensor([2.1971e-02, 9.7614e-01, 1.2983e-03, 8.0534e-05, 5.0514e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002205478260293603 tensor([2.6184e-08, 4.4963e-03, 9.9330e-01, 1.6369e-07, 2.2055e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012586081866174936 tensor([9.9536e-01, 3.3830e-03, 2.8318e-09, 1.2586e-03, 2.1368e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005905699217692018 tensor([2.9940e-01, 6.9958e-01, 7.7654e-05, 5.9057e-04, 3.4926e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06476159393787384 tensor([0.0022, 0.0648, 0.1302, 0.0371, 0.7658], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001033077947795391 tensor([7.8521e-01, 2.1364e-01, 3.7627e-06, 1.0331e-03, 1.0672e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005313450004905462 tensor([8.1681e-04, 2.0784e-03, 5.3135e-03, 1.4192e-01, 8.4988e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00016661583504173905 tensor([1.6662e-04, 3.0506e-07, 5.5973e-08, 9.9238e-01, 7.4554e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06783627718687057 tensor([0.0108, 0.2214, 0.0678, 0.0420, 0.6581], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.035745542496442795 tensor([2.3084e-04, 4.1948e-03, 3.5746e-02, 3.7494e-02, 9.2234e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007166133727878332 tensor([2.1081e-02, 9.7784e-01, 7.1661e-04, 4.9491e-05, 3.1394e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002214504056610167 tensor([3.0358e-09, 2.2145e-04, 9.8622e-01, 9.0935e-07, 1.3556e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005387319717556238 tensor([2.4312e-07, 1.3953e-05, 1.0413e-02, 5.3873e-03, 9.8419e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0036858143284916878 tensor([4.3284e-02, 9.4976e-01, 2.6621e-03, 6.0422e-04, 3.6858e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07504144310951233 tensor([0.0554, 0.4201, 0.0394, 0.0750, 0.4101], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008846087148413062 tensor([2.0325e-01, 7.9499e-01, 1.9354e-04, 6.7817e-04, 8.8461e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02835637331008911 tensor([4.1615e-04, 4.8500e-03, 2.8356e-02, 4.4816e-02, 9.2156e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004218905698508024 tensor([6.2162e-01, 3.7301e-01, 3.9984e-05, 4.2189e-03, 1.1051e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023403998464345932 tensor([6.2623e-06, 4.6381e-02, 9.3019e-01, 1.9125e-05, 2.3404e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05699250474572182 tensor([2.5670e-05, 5.6993e-02, 8.6834e-01, 1.5913e-04, 7.4482e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.253783820080571e-05 tensor([9.4916e-01, 5.0787e-02, 1.3561e-08, 5.2538e-05, 8.3998e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01558163296431303 tensor([1.1340e-06, 1.5582e-02, 9.6611e-01, 7.0844e-06, 1.8301e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004058971244376153 tensor([4.0590e-04, 4.7892e-07, 3.0896e-08, 9.9398e-01, 5.6109e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05473102256655693 tensor([7.5505e-04, 5.4731e-02, 1.6440e-01, 1.1503e-02, 7.6861e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023271091282367706 tensor([6.1011e-02, 9.0518e-04, 9.8987e-06, 9.1480e-01, 2.3271e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00746537558734417 tensor([9.2724e-01, 7.4654e-03, 4.3925e-07, 6.4900e-02, 3.9347e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.164595444919541e-05 tensor([7.2793e-11, 7.1646e-05, 9.9816e-01, 2.1747e-08, 1.7693e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008422264829277992 tensor([9.6859e-07, 3.1158e-05, 8.6540e-03, 8.4223e-03, 9.8289e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1284196972846985 tensor([0.0469, 0.1987, 0.0196, 0.1284, 0.6064], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006120684091001749 tensor([0.0953, 0.8933, 0.0025, 0.0027, 0.0061], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00018837337847799063 tensor([4.5791e-02, 9.5383e-01, 1.8837e-04, 4.6422e-05, 1.4076e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009946083649992943 tensor([7.1436e-05, 9.9461e-03, 2.3081e-01, 5.7181e-03, 7.5345e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01809559017419815 tensor([1.8096e-02, 2.1125e-03, 1.1664e-04, 8.1808e-01, 1.6160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003685144241899252 tensor([2.6112e-05, 5.0993e-01, 4.8636e-01, 2.4828e-06, 3.6851e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10495149344205856 tensor([0.2963, 0.0926, 0.0012, 0.5049, 0.1050], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003672894963528961 tensor([9.4578e-01, 3.6729e-04, 1.8650e-09, 5.3836e-02, 1.4563e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06343087553977966 tensor([8.5339e-01, 7.9195e-02, 3.8905e-05, 6.3431e-02, 3.9422e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0027212819550186396 tensor([5.3575e-03, 4.7780e-06, 2.7597e-08, 9.9192e-01, 2.7213e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0783618912100792 tensor([0.0134, 0.7553, 0.0784, 0.0059, 0.1471], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08042753487825394 tensor([0.0206, 0.6642, 0.0804, 0.0143, 0.2204], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026826869696378708 tensor([0.0022, 0.0183, 0.0268, 0.0936, 0.8592], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004216121509671211 tensor([2.6698e-07, 4.2161e-03, 9.7231e-01, 6.9146e-06, 2.3464e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0012395589146763086 tensor([1.9942e-06, 1.0524e-05, 1.2396e-03, 3.6897e-02, 9.6185e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17617659270763397 tensor([0.0013, 0.1762, 0.3013, 0.0072, 0.5140], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08043979108333588 tensor([0.0040, 0.0804, 0.0813, 0.0455, 0.7887], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02483493834733963 tensor([4.8445e-01, 2.4835e-02, 5.1994e-05, 4.6419e-01, 2.6473e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011602836893871427 tensor([7.2195e-01, 2.7674e-01, 9.5971e-06, 1.1603e-03, 1.4225e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010524392127990723 tensor([0.0105, 0.0071, 0.0017, 0.4312, 0.5495], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.028751803562045097 tensor([2.7811e-03, 8.5254e-01, 1.1547e-01, 4.5628e-04, 2.8752e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015236146282404661 tensor([5.7316e-09, 1.8611e-03, 9.9662e-01, 7.2344e-08, 1.5236e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01823480799794197 tensor([6.3130e-01, 3.4404e-01, 1.5958e-04, 1.8235e-02, 6.2723e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07174757122993469 tensor([2.8808e-01, 3.1904e-02, 2.9020e-04, 6.0798e-01, 7.1748e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009788245661184192 tensor([7.8677e-06, 2.7277e-05, 9.7882e-04, 5.4008e-02, 9.4498e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06692838668823242 tensor([7.5356e-05, 1.7906e-01, 7.5381e-01, 1.2532e-04, 6.6928e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06845036894083023 tensor([0.0130, 0.8257, 0.0685, 0.0048, 0.0880], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002398405922576785 tensor([6.8474e-03, 9.9030e-01, 2.3984e-03, 2.2515e-05, 4.2851e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006959294551052153 tensor([1.5875e-08, 6.9593e-04, 9.8700e-01, 1.4844e-06, 1.2298e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004925123415887356 tensor([6.0234e-05, 4.0394e-04, 4.9251e-03, 5.8942e-02, 9.3567e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0035948152653872967 tensor([1.6932e-07, 1.6774e-02, 9.7963e-01, 3.8099e-07, 3.5948e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03186221048235893 tensor([3.1862e-02, 6.3299e-03, 5.5912e-04, 7.1105e-01, 2.5020e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13409362733364105 tensor([0.0010, 0.4268, 0.4369, 0.0012, 0.1341], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1308901160955429 tensor([2.0318e-01, 3.6260e-02, 5.2802e-04, 6.2914e-01, 1.3089e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007746550836600363 tensor([2.9637e-03, 6.8553e-07, 2.4328e-09, 9.9626e-01, 7.7466e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02737531065940857 tensor([1.3049e-05, 9.6283e-02, 8.7631e-01, 2.1914e-05, 2.7375e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01052865944802761 tensor([6.3365e-04, 2.1562e-03, 1.0529e-02, 1.7118e-01, 8.1550e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11870355904102325 tensor([0.1187, 0.0900, 0.0034, 0.5018, 0.2862], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.019942982122302055 tensor([0.0050, 0.0199, 0.0121, 0.1190, 0.8440], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026780283078551292 tensor([2.6780e-02, 5.3742e-04, 7.9081e-06, 9.4326e-01, 2.9413e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030234545469284058 tensor([5.2162e-04, 7.7915e-03, 3.0235e-02, 4.1387e-02, 9.2006e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18336689472198486 tensor([0.0904, 0.2922, 0.0249, 0.1834, 0.4091], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.036440517753362656 tensor([8.8622e-01, 3.6441e-02, 7.4392e-06, 7.5518e-02, 1.8146e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11552587896585464 tensor([3.3915e-04, 1.1553e-01, 5.4213e-01, 1.7731e-03, 3.4023e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006301157409325242 tensor([6.3012e-04, 1.3640e-05, 3.8573e-06, 9.0640e-01, 9.2953e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21842682361602783 tensor([0.0042, 0.4173, 0.2184, 0.0069, 0.3531], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008205740712583065 tensor([9.3285e-01, 5.8754e-02, 2.9155e-06, 8.2057e-03, 1.9243e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3854216376785189e-05 tensor([8.4116e-02, 9.1585e-01, 1.3854e-05, 8.7830e-06, 1.1162e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023320365697145462 tensor([4.1391e-06, 4.3584e-02, 9.3308e-01, 1.4769e-05, 2.3320e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03323809802532196 tensor([3.3238e-02, 3.5692e-03, 1.3181e-04, 8.3465e-01, 1.2841e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003836396150290966 tensor([1.2660e-05, 3.8364e-03, 2.1283e-01, 2.6463e-03, 7.8067e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06152217090129852 tensor([0.0315, 0.0615, 0.0132, 0.3217, 0.5721], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.691070428933017e-05 tensor([2.6672e-01, 7.3325e-01, 3.0502e-06, 1.6911e-05, 7.6136e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.051859140396118164 tensor([0.5402, 0.3820, 0.0008, 0.0519, 0.0251], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0025171160232275724 tensor([1.1775e-08, 6.7866e-07, 2.7387e-03, 2.5171e-03, 9.9474e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020515821874141693 tensor([2.5199e-05, 1.5508e-01, 8.2436e-01, 2.1703e-05, 2.0516e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02754330448806286 tensor([8.2523e-01, 2.7543e-02, 1.2117e-05, 1.4400e-01, 3.2130e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00018166056543122977 tensor([9.8683e-01, 1.2991e-02, 4.2020e-09, 1.8166e-04, 7.7409e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003794402116909623 tensor([5.1730e-07, 4.5703e-05, 3.0803e-02, 3.7944e-03, 9.6536e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.027090296149253845 tensor([1.6604e-01, 3.8280e-03, 1.1842e-05, 8.0303e-01, 2.7090e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0051667881198227406 tensor([4.9162e-01, 5.0043e-01, 1.5655e-04, 5.1668e-03, 2.6246e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002480822615325451 tensor([2.4808e-03, 1.8570e-04, 7.1320e-05, 8.4859e-01, 1.4867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0023531808983534575 tensor([5.3919e-01, 4.5767e-01, 6.0002e-05, 2.3532e-03, 7.2719e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.020731519907712936 tensor([6.0108e-05, 2.0732e-02, 4.8420e-01, 2.1274e-03, 4.9288e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04812148958444595 tensor([0.0481, 0.8013, 0.0229, 0.0174, 0.1103], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03377504646778107 tensor([1.9053e-03, 7.6887e-01, 1.9504e-01, 4.1084e-04, 3.3775e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0256233811378479 tensor([8.5328e-01, 2.5623e-02, 9.5929e-06, 1.1853e-01, 2.5602e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.183063957607374e-05 tensor([1.6609e-01, 8.3368e-01, 4.4413e-05, 9.1831e-05, 8.6426e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0780332013964653 tensor([0.0780, 0.7251, 0.0189, 0.0264, 0.1515], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001940244110301137 tensor([1.9402e-04, 3.1622e-06, 2.0849e-06, 9.0547e-01, 9.4330e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004344949033111334 tensor([9.6839e-06, 1.8539e-03, 1.5456e-01, 4.3449e-03, 8.3923e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24711944162845612 tensor([0.0047, 0.3444, 0.2471, 0.0119, 0.3919], grad_fn=<SoftmaxBackward0>)\n",
      "3 5.886032522539608e-05 tensor([8.9634e-01, 1.0360e-01, 5.3585e-08, 5.8860e-05, 1.7567e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007382573094218969 tensor([7.2745e-07, 7.3826e-03, 9.7417e-01, 1.0164e-05, 1.8435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006629000999964774 tensor([6.6290e-04, 1.7654e-06, 1.1335e-07, 9.8819e-01, 1.1150e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08616210520267487 tensor([3.6204e-04, 3.8982e-01, 5.2337e-01, 2.8613e-04, 8.6162e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0016098261112347245 tensor([6.7636e-04, 5.2016e-04, 1.6098e-03, 3.9478e-01, 6.0241e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010031562997028232 tensor([1.3197e-01, 8.6605e-01, 3.5656e-04, 6.2648e-04, 1.0032e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021625040099024773 tensor([2.4271e-06, 2.1625e-02, 9.5264e-01, 1.3637e-05, 2.5720e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00122087262570858 tensor([1.2209e-03, 3.0614e-06, 1.6123e-07, 9.9069e-01, 8.0816e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006611721473746002 tensor([4.7609e-09, 6.6117e-04, 9.9397e-01, 2.9770e-07, 5.3686e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02114897407591343 tensor([0.0060, 0.0211, 0.0142, 0.1699, 0.7887], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001978697400772944 tensor([1.2868e-02, 9.8689e-01, 1.9787e-04, 4.9781e-06, 3.8613e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.035880617797374725 tensor([2.6864e-03, 8.3067e-01, 1.3031e-01, 4.4967e-04, 3.5881e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013819545274600387 tensor([0.0014, 0.0013, 0.0013, 0.3449, 0.6512], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006612597964704037 tensor([2.3020e-05, 6.6126e-03, 3.3197e-01, 3.2500e-03, 6.5815e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021473664790391922 tensor([7.1537e-01, 2.1474e-02, 1.6036e-05, 2.5693e-01, 6.2102e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0008500319672748446 tensor([2.3057e-01, 7.6756e-01, 1.8696e-04, 8.5003e-04, 8.3462e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015236997976899147 tensor([4.7579e-05, 1.5237e-02, 4.6013e-01, 2.5966e-03, 5.2199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013617550721392035 tensor([6.3349e-04, 8.9829e-04, 1.3618e-03, 2.4245e-01, 7.5466e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05807274207472801 tensor([5.9812e-04, 5.2279e-01, 4.1827e-01, 2.6841e-04, 5.8073e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10864763706922531 tensor([0.0131, 0.2269, 0.1086, 0.0545, 0.5969], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21296760439872742 tensor([0.2130, 0.2799, 0.0057, 0.2735, 0.2279], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015710676088929176 tensor([5.4557e-06, 1.5711e-02, 8.6885e-01, 1.4277e-04, 1.1529e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001020584488287568 tensor([9.9621e-01, 1.0206e-03, 7.6760e-10, 2.7662e-03, 1.7294e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02273767814040184 tensor([1.7943e-05, 1.2697e-01, 8.5025e-01, 2.0088e-05, 2.2738e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2519552707672119 tensor([0.0011, 0.2520, 0.4530, 0.0033, 0.2906], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11597141623497009 tensor([0.3933, 0.1160, 0.0007, 0.4051, 0.0849], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13199086487293243 tensor([0.0005, 0.1320, 0.4599, 0.0029, 0.4047], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015931576490402222 tensor([1.5932e-04, 3.1408e-06, 3.2431e-06, 8.6426e-01, 1.3557e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10407824069261551 tensor([1.8416e-04, 1.0408e-01, 6.4517e-01, 1.3087e-03, 2.4926e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.053451601415872574 tensor([0.0057, 0.0484, 0.0535, 0.1183, 0.7741], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007585495477542281 tensor([7.9114e-01, 2.0804e-01, 2.6023e-06, 7.5855e-04, 6.3969e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005868231412023306 tensor([3.3078e-05, 2.2573e-04, 5.8682e-03, 3.6913e-02, 9.5696e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003156616003252566 tensor([3.1566e-04, 8.6321e-05, 2.3452e-04, 5.5429e-01, 4.4507e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01078039314597845 tensor([1.4498e-06, 3.3317e-02, 9.5590e-01, 3.4406e-06, 1.0780e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0234831590205431 tensor([1.1905e-04, 2.3483e-02, 3.4163e-01, 6.0237e-03, 6.2875e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03129344433546066 tensor([0.0537, 0.8983, 0.0117, 0.0050, 0.0313], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01248103380203247 tensor([1.3697e-04, 1.2481e-02, 1.1673e-01, 7.2573e-03, 8.6340e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00463233282789588 tensor([7.8889e-07, 6.1907e-05, 1.6833e-02, 4.6323e-03, 9.7847e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008530746563337743 tensor([8.4147e-07, 8.5307e-04, 4.8844e-01, 6.3221e-04, 5.1007e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19457578659057617 tensor([0.3594, 0.1946, 0.0023, 0.3272, 0.1165], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04882469028234482 tensor([0.0205, 0.8320, 0.0488, 0.0050, 0.0937], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04523960500955582 tensor([4.6152e-01, 4.5240e-02, 2.2396e-04, 4.4826e-01, 4.4761e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006947069778107107 tensor([6.9471e-04, 7.7022e-06, 1.8409e-06, 9.6223e-01, 3.7068e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09221325069665909 tensor([2.8216e-04, 9.2213e-02, 4.2240e-01, 2.6699e-03, 4.8243e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008890326716937125 tensor([9.6125e-01, 8.8903e-04, 9.3897e-09, 3.7832e-02, 2.5993e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08212115615606308 tensor([7.4925e-01, 1.5763e-01, 1.2188e-04, 8.2121e-02, 1.0882e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06642773002386093 tensor([0.0120, 0.8329, 0.0860, 0.0027, 0.0664], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002581800566986203 tensor([1.8109e-05, 9.5608e-05, 2.5818e-03, 5.7539e-02, 9.3977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010063214227557182 tensor([1.1519e-04, 6.1184e-01, 3.7797e-01, 1.3702e-05, 1.0063e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023479236289858818 tensor([0.1094, 0.8543, 0.0063, 0.0066, 0.0235], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014872376807034016 tensor([5.5825e-01, 1.4872e-02, 1.4774e-05, 4.1694e-01, 9.9166e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022377073764801025 tensor([0.0803, 0.8867, 0.0066, 0.0041, 0.0224], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010524086974328384 tensor([1.0524e-04, 5.0807e-07, 2.6052e-07, 9.7639e-01, 2.3506e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002716022776439786 tensor([4.7047e-07, 4.0214e-02, 9.5707e-01, 4.3254e-07, 2.7160e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01119158323854208 tensor([9.7399e-01, 1.4699e-02, 5.0626e-07, 1.1192e-02, 1.1431e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006435116287320852 tensor([8.0751e-01, 1.8535e-01, 1.5715e-05, 6.4351e-03, 6.8501e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002176106907427311 tensor([2.7599e-08, 2.1761e-03, 9.9209e-01, 5.4693e-07, 5.7377e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.332351348537486e-05 tensor([1.3324e-05, 8.2945e-07, 7.9963e-06, 5.8167e-01, 4.1831e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12871968746185303 tensor([0.0790, 0.3611, 0.0234, 0.1287, 0.4078], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15006422996520996 tensor([0.0776, 0.1501, 0.0135, 0.2979, 0.4610], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011022572871297598 tensor([9.9112e-01, 7.7698e-03, 1.1888e-08, 1.1023e-03, 4.2954e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00020705269707832485 tensor([3.3235e-09, 2.0705e-04, 9.8334e-01, 1.1392e-06, 1.6455e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017408441635780036 tensor([1.7408e-04, 2.8899e-05, 8.2601e-05, 6.1264e-01, 3.8708e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01685401238501072 tensor([7.6163e-06, 1.6854e-02, 8.4712e-01, 1.8373e-04, 1.3584e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00019987448467873037 tensor([8.0782e-05, 2.2314e-05, 1.9987e-04, 4.5029e-01, 5.4941e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0031295015942305326 tensor([3.1295e-03, 9.9138e-01, 5.0071e-03, 1.4450e-05, 4.6523e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.031270597130060196 tensor([1.8068e-04, 3.8635e-03, 3.6690e-02, 3.1271e-02, 9.2799e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006665403489023447 tensor([2.1039e-01, 1.1005e-03, 1.3617e-06, 7.8184e-01, 6.6654e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013609621673822403 tensor([3.4744e-06, 1.3610e-02, 9.0451e-01, 5.4395e-05, 8.1820e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14782966673374176 tensor([0.3600, 0.1478, 0.0010, 0.3833, 0.1079], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004279018845409155 tensor([4.2790e-03, 9.9087e-01, 4.2858e-03, 2.0850e-05, 5.4291e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007092310115695 tensor([5.1246e-05, 4.9711e-03, 1.4901e-01, 7.0923e-03, 8.3887e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00020280359603930265 tensor([6.1473e-06, 5.4695e-06, 2.0280e-04, 1.4375e-01, 8.5604e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005028201267123222 tensor([1.7415e-08, 5.0282e-04, 9.7142e-01, 3.6018e-06, 2.8072e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006559258326888084 tensor([3.2966e-06, 3.7413e-04, 8.8182e-02, 6.5593e-03, 9.0488e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0034285567235201597 tensor([9.9043e-01, 6.1232e-03, 2.0896e-08, 3.4286e-03, 1.7717e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03165886178612709 tensor([2.6423e-05, 3.1659e-02, 7.7532e-01, 4.1085e-04, 1.9258e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009630794636905193 tensor([1.0331e-04, 9.6308e-03, 1.1929e-01, 9.4424e-03, 8.6153e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04675757512450218 tensor([2.3925e-05, 9.9960e-02, 8.5320e-01, 5.5436e-05, 4.6758e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21505975723266602 tensor([0.2502, 0.3381, 0.0065, 0.2151, 0.1901], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.953054541256279e-05 tensor([9.8728e-01, 1.2624e-02, 2.2236e-09, 9.9531e-05, 4.7575e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03207749128341675 tensor([3.2639e-02, 6.1762e-04, 1.3556e-05, 9.3465e-01, 3.2077e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014710335060954094 tensor([1.4710e-02, 7.0904e-04, 3.7625e-05, 8.9468e-01, 8.9859e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008302184753119946 tensor([8.8202e-04, 8.8417e-01, 1.0659e-01, 5.9593e-05, 8.3022e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003513127099722624 tensor([3.5131e-03, 8.3975e-04, 4.1568e-04, 6.4939e-01, 3.4584e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06678597629070282 tensor([3.5816e-01, 5.9237e-02, 2.9289e-04, 5.1553e-01, 6.6786e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002506630844436586 tensor([4.0775e-09, 2.5066e-04, 9.8648e-01, 1.0419e-06, 1.3272e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0023010720033198595 tensor([2.4782e-06, 7.4539e-04, 1.3541e-01, 2.3011e-03, 8.6154e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002116105519235134 tensor([1.2293e-06, 2.1161e-03, 6.5587e-01, 2.6723e-04, 3.4175e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010288301855325699 tensor([3.4986e-01, 6.7818e-03, 1.2876e-05, 6.3306e-01, 1.0288e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.040270843892358e-05 tensor([9.4899e-01, 5.0957e-02, 1.2665e-08, 5.0403e-05, 6.6135e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.024273071438074112 tensor([6.3181e-01, 3.3609e-01, 2.1030e-04, 2.4273e-02, 7.6112e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02229285053908825 tensor([1.3289e-01, 2.5785e-03, 1.0257e-05, 8.4223e-01, 2.2293e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001817202428355813 tensor([3.9864e-06, 1.8172e-03, 2.6572e-01, 1.8598e-03, 7.3060e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10091231763362885 tensor([7.7952e-01, 1.0091e-01, 7.9759e-05, 1.1161e-01, 7.8853e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017214989056810737 tensor([9.8143e-01, 1.6831e-02, 8.0815e-08, 1.7215e-03, 1.3898e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.130441814661026 tensor([0.0560, 0.3238, 0.0413, 0.1304, 0.4484], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00726625882089138 tensor([7.2663e-03, 4.9817e-04, 4.3807e-05, 8.2585e-01, 1.6634e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11653656512498856 tensor([0.1165, 0.0709, 0.0029, 0.4914, 0.3182], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00034210202284157276 tensor([1.1866e-08, 1.7323e-05, 2.4175e-01, 3.4210e-04, 7.5789e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00254265358671546 tensor([4.1122e-03, 9.9303e-01, 2.5427e-03, 1.2389e-05, 2.9830e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012224957346916199 tensor([0.0122, 0.0076, 0.0025, 0.5453, 0.4325], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005487156449817121 tensor([4.8190e-01, 5.1735e-01, 2.0891e-05, 5.4872e-04, 1.7907e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00223421398550272 tensor([4.1728e-02, 9.5341e-01, 2.2342e-03, 4.5783e-04, 2.1739e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028661491349339485 tensor([1.0916e-05, 2.8661e-02, 9.0776e-01, 1.1701e-04, 6.3454e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.688225428457372e-06 tensor([3.7223e-02, 9.6277e-01, 5.6882e-06, 9.6800e-07, 1.9026e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00018820434343069792 tensor([4.0334e-10, 1.8820e-04, 9.9749e-01, 5.5175e-08, 2.3170e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00015588794485665858 tensor([1.5589e-04, 2.5984e-05, 7.4799e-05, 5.5091e-01, 4.4883e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15186113119125366 tensor([0.1519, 0.0902, 0.0029, 0.4682, 0.2868], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 4], [3, 2, 4], [0, 1, 3], [2, 0, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 4, 3], [2, 1, 4], [2, 4, 1], [0, 1, 2], [3, 4, 0], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 1], [2, 3, 0], [3, 2, 4], [2, 0], [2, 1, 0], [3, 0, 4], [2, 1, 4], [3, 0, 4], [2, 1, 4], [1, 2, 0], [3, 4, 2], [2, 1, 0], [3, 4, 0], [2, 1, 0], [2, 1, 0], [0, 3, 4], [0, 1, 3], [3, 4, 0], [0, 3, 1], [0, 1, 2], [3, 0, 4], [0, 3, 1], [3, 4, 2], [0, 3, 4], [2, 4, 3], [2, 4, 3], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 1, 0], [0, 3, 2], [0, 1, 2], [3, 4, 2], [0, 3, 1], [0, 1, 3], [3, 2, 4], [2, 0, 3], [2, 3, 4], [0, 1, 2], [2, 4, 3], [0, 3, 4], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [2, 1, 4], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 3], [2, 3, 4], [3, 4, 2], [0, 3, 1], [2, 1, 0], [3, 0, 4], [2, 1, 4], [2, 4, 1], [2, 4, 3], [2, 1, 4], [3, 0, 2], [3, 0, 2], [0, 1, 2], [0, 3, 1], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 1, 0], [3, 0, 4], [0, 3, 4], [2, 4, 3], [2, 1, 4], [0, 1, 2], [0, 3, 4], [3, 0, 2], [3, 4, 2], [0, 3, 1], [0, 1, 2], [0, 3, 4], [2, 1, 0], [0, 3, 4], [2, 1, 4], [2, 1, 4], [0, 3, 4], [0, 1, 2], [2, 1, 0], [0, 2, 1], [2, 1, 0], [0, 1, 2], [2, 0, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0], [0, 3], [2, 4, 3], [3, 4, 2], [0, 3, 4], [2, 1, 0], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3], [3, 0, 4], [2, 4, 1], [2, 4, 3], [0, 1, 3], [2, 1, 4], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 3, 1], [3, 2, 0], [3, 0, 4], [2, 4, 1], [2, 4, 3], [2, 3, 0], [2, 1, 0], [0, 1, 3], [0, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0], [3, 0, 4], [1, 0, 2], [2, 3, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 2, 1], [3, 4, 2], [3, 0, 4], [1, 2, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 1, 2], [3, 0, 4], [0, 3, 2], [2], [0, 3, 1], [2, 4, 1], [0, 3, 4], [0, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 1, 2], [2, 4, 3], [0, 1, 2], [1, 2, 0], [0, 3, 4], [0, 3, 1], [3, 2, 4], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [3, 0, 2], [2, 4, 1], [2, 1, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [3, 0, 4], [0, 1, 2], [3, 0, 4], [2, 3, 4], [2, 4, 1], [3, 2, 4], [2, 1, 0], [3, 0, 4], [2, 4, 3], [2, 4, 3], [0, 3, 1], [1, 2, 0], [2, 0, 3], [2, 0, 1], [2, 4, 3], [0, 3, 1], [1, 2, 0], [0, 3, 1], [1, 0, 2], [3, 4, 0], [0, 1, 3], [2, 1, 4], [0, 3, 1], [2, 4, 1], [3, 4, 0], [0, 1, 3], [1, 0, 2], [0, 3, 1], [0, 1, 3], [2, 4, 3], [0, 3, 1], [0, 3, 1], [0, 3, 4], [2, 4], [2, 4, 3], [2, 1, 4], [2, 1, 0], [3, 0, 4], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 1, 4], [2, 4, 3], [2, 4, 3], [2, 1, 4], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 0, 3], [2, 1, 0], [2, 1, 0], [0, 1, 3], [3, 4, 2], [2, 1, 0], [2, 4, 3], [3, 4, 2], [0, 3, 1], [3, 4, 2], [0, 3, 1], [1, 2, 0], [2, 1, 0]]\n",
      "[[0, 3], [0, 1], [2, 4], [3, 4], [2, 4], [3, 4], [0, 1], [0, 1], [0, 3], [0, 3], [3, 4], [1, 2], [2, 4], [3, 4], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [1, 4], [0, 1], [1, 3, 4], [3, 4], [1, 2], [0, 3], [1, 2], [0, 3], [3, 4], [0, 1], [3, 4], [1, 2], [3, 4], [3, 4], [1, 2], [2, 4], [1, 2], [2, 4], [3, 4], [1, 2], [2, 4], [0, 1], [1, 2], [0, 1], [0, 1], [2, 4], [0, 1], [3, 4], [3, 4], [1, 4], [3, 4], [0, 1], [2, 4], [2, 4], [0, 1], [1, 4], [0, 1], [3, 4], [0, 1], [1, 2], [2, 4], [0, 1], [2, 4], [3, 4], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [1, 4], [0, 1], [0, 1], [2, 4], [3, 4], [1, 2], [0, 3], [0, 3], [0, 1], [0, 3], [1, 4], [1, 4], [3, 4], [2, 4], [3, 4], [2, 4], [2, 4], [0, 3], [0, 1], [3, 4], [1, 2], [1, 2], [0, 1], [0, 3], [3, 4], [1, 2], [1, 4], [0, 1], [2, 4], [3, 4], [1, 2], [3, 4], [1, 2], [0, 3], [0, 3], [1, 2], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [1, 4], [0, 3], [2, 4], [3, 4], [1, 2, 4], [0, 1], [0, 1], [1, 2], [3, 4], [2, 4], [3, 4], [0, 1], [0, 1], [2, 4], [1, 2], [0, 3], [0, 1], [2, 4], [0, 3], [0, 1], [3, 4], [0, 1], [2, 4], [1, 4], [1, 2], [0, 3], [0, 1], [1, 4], [3, 4], [2, 4], [1, 2, 4], [0, 1], [2, 4], [3, 4], [1, 2], [3, 4], [0, 1], [2, 4], [3, 4], [2, 4], [3, 4], [0, 1], [1, 2], [3, 4], [2, 4], [0, 3], [0, 1], [2, 4], [3, 4], [1, 2], [1, 4], [0, 1, 3, 4], [2, 4], [0, 3], [1, 2], [1, 2, 4], [0, 3], [2, 4], [3, 4], [2, 4], [3, 4], [0, 1], [3, 4], [3, 4], [1, 2], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [0, 3], [1, 4], [0, 3], [3, 4], [2, 4], [0, 3], [0, 1], [1, 2], [3, 4], [1, 2], [0, 1], [0, 3], [0, 1], [3, 4], [1, 2], [0, 1], [0, 1], [2, 4], [3, 4], [1, 4], [3, 4], [0, 1], [2, 4], [3, 4], [2, 4], [3, 4], [1, 2], [2, 4], [0, 3], [2, 4], [0, 3], [1, 2], [2, 4], [3, 4], [2, 4], [2, 4], [0, 1], [2, 4], [2, 4], [1, 2], [0, 1, 3], [0, 1], [0, 3], [3, 4], [1, 2], [3, 4], [0, 3], [2, 4], [2, 4], [2, 4], [0, 3], [0, 1], [0, 1], [0, 3], [2, 4], [0, 3], [0, 1], [1, 4], [3, 4], [3, 4], [2, 4], [0, 1], [3, 4], [0, 1], [0, 1], [2, 4], [0, 1], [2, 4], [3, 4], [3, 4]]\n",
      "NL_pred of 2th iteration [[2, 1, 4], [3, 2, 4], [0, 1, 3], [2, 0, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3], [2, 4, 3], [2, 1, 4], [2, 4, 1], [0, 1, 2], [3, 4, 0], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 1], [2, 3, 0], [3, 2, 4], [2, 1, 0], [3, 0, 4], [2, 1, 4], [3, 0, 4], [2, 1, 4], [1, 2, 0], [3, 4, 2], [2, 1, 0], [3, 4, 0], [2, 1, 0], [2, 1, 0], [0, 3, 4], [0, 1, 3], [3, 4, 0], [0, 3, 1], [0, 1, 2], [3, 0, 4], [0, 3, 1], [3, 4, 2], [0, 3, 4], [2, 4, 3], [2, 4, 3], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 1, 0], [0, 3, 2], [0, 1, 2], [3, 4, 2], [0, 3, 1], [0, 1, 3], [3, 2, 4], [2, 0, 3], [2, 3, 4], [0, 1, 2], [2, 4, 3], [0, 3, 4], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 1], [2, 1, 4], [2, 4, 1], [0, 3, 1], [0, 1, 3], [2, 0, 3], [2, 3, 4], [3, 4, 2], [0, 3, 1], [2, 1, 0], [3, 0, 4], [2, 1, 4], [2, 4, 1], [2, 4, 3], [2, 1, 4], [3, 0, 2], [3, 0, 2], [0, 1, 2], [0, 3, 1], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 1, 0], [3, 0, 4], [0, 3, 4], [2, 4, 3], [2, 1, 4], [0, 1, 2], [0, 3, 4], [3, 0, 2], [3, 4, 2], [0, 3, 1], [0, 1, 2], [0, 3, 4], [2, 1, 0], [0, 3, 4], [2, 1, 4], [2, 1, 4], [0, 3, 4], [0, 1, 2], [2, 1, 0], [0, 2, 1], [2, 1, 0], [0, 1, 2], [2, 0, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0], [2, 4, 3], [3, 4, 2], [0, 3, 4], [2, 1, 0], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3], [3, 0, 4], [2, 4, 1], [2, 4, 3], [0, 1, 3], [2, 1, 4], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 3, 1], [3, 2, 0], [3, 0, 4], [2, 4, 1], [2, 4, 3], [2, 3, 0], [2, 1, 0], [0, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 0], [3, 0, 4], [1, 0, 2], [2, 3, 4], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 2, 1], [3, 4, 2], [3, 0, 4], [1, 2, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 1, 2], [3, 0, 4], [0, 3, 2], [0, 3, 1], [2, 4, 1], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 1, 2], [2, 4, 3], [0, 1, 2], [1, 2, 0], [0, 3, 4], [0, 3, 1], [3, 2, 4], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 4, 1], [3, 0, 2], [2, 4, 1], [2, 1, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [3, 0, 4], [0, 1, 2], [3, 0, 4], [2, 3, 4], [2, 4, 1], [3, 2, 4], [2, 1, 0], [3, 0, 4], [2, 4, 3], [2, 4, 3], [0, 3, 1], [1, 2, 0], [2, 0, 3], [2, 0, 1], [2, 4, 3], [0, 3, 1], [1, 2, 0], [0, 3, 1], [1, 0, 2], [3, 4, 0], [0, 1, 3], [2, 1, 4], [0, 3, 1], [2, 4, 1], [3, 4, 0], [0, 1, 3], [1, 0, 2], [0, 3, 1], [0, 1, 3], [2, 4, 3], [0, 3, 1], [0, 3, 1], [0, 3, 4], [2, 4, 3], [2, 1, 4], [2, 1, 0], [3, 0, 4], [2, 1, 0], [2, 1, 4], [0, 3, 1], [0, 1, 3], [0, 3, 1], [2, 1, 4], [2, 4, 3], [2, 4, 3], [2, 1, 4], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 0, 3], [2, 1, 0], [2, 1, 0], [0, 1, 3], [3, 4, 2], [2, 1, 0], [2, 4, 3], [3, 4, 2], [0, 3, 1], [3, 4, 2], [0, 3, 1], [1, 2, 0], [2, 1, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004690148791328805  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00469010091218792  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0046900129709087435  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004689886921741923  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004689729604564729  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.00468954541644112  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004689338754435055  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004689114992735816  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004688875108468728  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0046886254529483985  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004688367980425475  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0046881036680252825  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004687837401374442  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004687571134723601  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004687303890947436  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0046870410442352295  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004686781128898996  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004686525122064059  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0046862749779810675  Accuracy on Support set:0.0\n",
      "torch.Size([244, 2048]) torch.Size([244])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.0046860306966500205  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.08627913147211075 tensor([8.6279e-02, 6.2413e-03, 1.0885e-04, 8.5256e-01, 5.4809e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08229126036167145 tensor([8.2291e-02, 9.1525e-01, 7.9057e-04, 5.1007e-04, 1.1534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2334790974855423 tensor([5.9512e-07, 2.9755e-04, 2.3348e-01, 1.0052e-03, 7.6522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28588351607322693 tensor([0.0133, 0.0240, 0.0076, 0.2859, 0.6692], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2362622320652008 tensor([3.2512e-08, 1.8840e-04, 7.6349e-01, 6.2072e-05, 2.3626e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2791650593280792 tensor([5.9068e-04, 6.7948e-04, 2.3272e-03, 2.7917e-01, 7.1724e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06720253825187683 tensor([9.3096e-01, 6.7203e-02, 7.3726e-07, 1.7793e-03, 5.3896e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24058571457862854 tensor([7.5502e-01, 2.4059e-01, 2.0282e-05, 3.8517e-03, 5.2187e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1383027732372284 tensor([1.3830e-01, 7.5833e-05, 2.4823e-08, 8.6090e-01, 7.2604e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41461122035980225 tensor([5.5803e-01, 1.5213e-02, 1.7563e-05, 4.1461e-01, 1.2132e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026867065578699112 tensor([3.7710e-05, 6.8223e-04, 1.8280e-02, 2.6867e-02, 9.5413e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016814064234495163 tensor([1.0292e-02, 9.6417e-01, 1.6814e-02, 3.9788e-04, 8.3278e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3446759879589081 tensor([5.2177e-05, 1.0286e-02, 3.4468e-01, 4.9420e-03, 6.4004e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08091005682945251 tensor([4.0213e-04, 5.9109e-06, 1.8291e-06, 9.1868e-01, 8.0910e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15470118820667267 tensor([3.4811e-04, 2.8615e-02, 1.5470e-01, 9.8464e-03, 8.0649e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26284539699554443 tensor([4.1702e-04, 1.5435e-01, 5.8010e-01, 2.2858e-03, 2.6285e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11017753928899765 tensor([0.1102, 0.7674, 0.0113, 0.0242, 0.0870], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10168576240539551 tensor([1.8202e-04, 9.8736e-03, 1.0169e-01, 1.3050e-02, 8.7521e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.020901819691061974 tensor([2.2348e-05, 7.7390e-04, 2.0902e-02, 1.2868e-02, 9.6543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11078744381666183 tensor([9.5019e-06, 2.8000e-02, 8.6107e-01, 1.3501e-04, 1.1079e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3102501630783081 tensor([0.0586, 0.5303, 0.0409, 0.0599, 0.3103], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09258385747671127 tensor([9.2584e-02, 9.0459e-01, 6.9147e-04, 6.5821e-04, 1.4797e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22959135472774506 tensor([0.1292, 0.2296, 0.0132, 0.2488, 0.3792], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29851487278938293 tensor([1.4803e-02, 3.4825e-03, 3.4753e-04, 6.8285e-01, 2.9851e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3708602488040924 tensor([2.4917e-05, 3.7086e-01, 6.2365e-01, 4.4147e-06, 5.4563e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2130487561225891 tensor([2.1305e-01, 3.2568e-02, 4.6757e-04, 6.6652e-01, 8.7397e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06271348893642426 tensor([5.9791e-04, 9.3446e-01, 6.2713e-02, 1.4460e-05, 2.2189e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.42560267448425293 tensor([4.2560e-01, 9.1869e-03, 1.8701e-05, 5.5062e-01, 1.4573e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26804378628730774 tensor([5.0337e-05, 2.5726e-06, 1.0917e-05, 7.3189e-01, 2.6804e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013399251736700535 tensor([1.3399e-02, 9.8068e-01, 4.0957e-03, 1.2633e-04, 1.7013e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11679225414991379 tensor([8.7337e-02, 9.4300e-03, 2.6689e-04, 7.8617e-01, 1.1679e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02252761647105217 tensor([0.0168, 0.9369, 0.0225, 0.0015, 0.0223], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004891819786280394 tensor([2.8102e-03, 2.9827e-06, 5.1116e-08, 9.9230e-01, 4.8918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2358599305152893 tensor([1.2828e-03, 1.4809e-04, 9.2236e-05, 7.6262e-01, 2.3586e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.38843706250190735 tensor([0.0010, 0.4338, 0.3884, 0.0012, 0.1756], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08241304010152817 tensor([1.1507e-05, 9.0062e-04, 8.2413e-02, 8.4336e-03, 9.0824e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011570127680897713 tensor([9.2229e-03, 9.7509e-01, 1.1570e-02, 2.0055e-04, 3.9161e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030053289607167244 tensor([6.1385e-08, 1.1622e-03, 9.6878e-01, 5.3973e-06, 3.0053e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023441467434167862 tensor([8.5500e-07, 5.4194e-06, 9.0218e-04, 2.3441e-02, 9.7565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37311944365501404 tensor([8.8268e-05, 3.7312e-01, 6.0861e-01, 3.4490e-05, 1.8147e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35992109775543213 tensor([3.8065e-06, 4.8078e-03, 6.3483e-01, 4.3851e-04, 3.5992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.021387038752436638 tensor([2.1387e-02, 9.7652e-01, 1.4515e-03, 8.5575e-05, 5.5799e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0038841438945382833 tensor([2.1837e-08, 3.8841e-03, 9.9392e-01, 1.5610e-07, 2.1997e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0032762796618044376 tensor([9.9535e-01, 3.2763e-03, 2.9351e-09, 1.3725e-03, 2.2872e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29238244891166687 tensor([2.9238e-01, 7.0649e-01, 8.8869e-05, 6.3971e-04, 3.9640e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12962821125984192 tensor([0.0017, 0.0528, 0.1296, 0.0350, 0.7808], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2123764157295227 tensor([7.8636e-01, 2.1238e-01, 4.1175e-06, 1.1432e-03, 1.1933e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13348524272441864 tensor([6.4183e-04, 1.6954e-03, 5.2160e-03, 1.3349e-01, 8.5896e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0079084113240242 tensor([1.4472e-04, 2.7234e-07, 5.7589e-08, 9.9195e-01, 7.9084e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18935085833072662 tensor([0.0087, 0.1894, 0.0704, 0.0406, 0.6910], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03498142957687378 tensor([1.7526e-04, 3.2921e-03, 3.4394e-02, 3.4981e-02, 9.2716e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020808778703212738 tensor([2.0809e-02, 9.7800e-01, 7.9235e-04, 5.3294e-05, 3.4653e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01389882992953062 tensor([2.5816e-09, 1.9292e-04, 9.8591e-01, 8.9289e-07, 1.3899e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010327142663300037 tensor([2.0158e-07, 1.2002e-05, 1.0327e-02, 5.0663e-03, 9.8459e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04124555364251137 tensor([4.1246e-02, 9.5067e-01, 3.1369e-03, 6.5675e-04, 4.2909e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37748464941978455 tensor([0.0470, 0.3775, 0.0435, 0.0767, 0.4553], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2027953416109085 tensor([2.0280e-01, 7.9527e-01, 2.1226e-04, 7.3921e-04, 9.8188e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04171507805585861 tensor([3.1653e-04, 3.8320e-03, 2.7470e-02, 4.1715e-02, 9.2667e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37221410870552063 tensor([6.2174e-01, 3.7221e-01, 4.4604e-05, 4.7441e-03, 1.2596e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03881657496094704 tensor([4.9638e-06, 3.8817e-02, 9.3822e-01, 1.7621e-05, 2.2942e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07500361651182175 tensor([2.0587e-05, 4.7380e-02, 8.7744e-01, 1.5212e-04, 7.5004e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.049635205417871475 tensor([9.5031e-01, 4.9635e-02, 1.3995e-08, 5.5726e-05, 8.8268e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.017995081841945648 tensor([8.8895e-07, 1.2865e-02, 9.6913e-01, 6.5376e-06, 1.7995e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00593750411644578 tensor([3.5142e-04, 4.2511e-07, 3.1670e-08, 9.9371e-01, 5.9375e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16502781212329865 tensor([5.8397e-04, 4.4646e-02, 1.6503e-01, 1.0741e-02, 7.7900e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0505136176943779 tensor([5.0514e-02, 7.4705e-04, 9.7062e-06, 9.2437e-01, 2.4361e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07457002252340317 tensor([9.1772e-01, 7.2564e-03, 4.8237e-07, 7.4570e-02, 4.5430e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0017883495893329382 tensor([6.2542e-11, 6.3242e-05, 9.9815e-01, 2.1062e-08, 1.7883e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008581268601119518 tensor([8.0538e-07, 2.6863e-05, 8.5813e-03, 7.9321e-03, 9.8346e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1705211102962494 tensor([0.0377, 0.1705, 0.0207, 0.1242, 0.6468], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09103050827980042 tensor([0.0910, 0.8957, 0.0029, 0.0031, 0.0073], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04466436430811882 tensor([4.4664e-02, 9.5492e-01, 2.0944e-04, 4.9255e-05, 1.5478e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22943377494812012 tensor([5.5442e-05, 8.0415e-03, 2.2943e-01, 5.3829e-03, 7.5709e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16903121769428253 tensor([1.4945e-02, 1.7723e-03, 1.1666e-04, 8.1413e-01, 1.6903e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4643271565437317 tensor([2.2391e-05, 4.6433e-01, 5.3173e-01, 2.4492e-06, 3.9189e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26256081461906433 tensor([0.2626, 0.0832, 0.0013, 0.5367, 0.1163], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06289011985063553 tensor([9.3673e-01, 3.5875e-04, 2.0892e-09, 6.2890e-02, 1.7193e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07798154652118683 tensor([8.4322e-01, 7.7982e-02, 4.4374e-05, 7.4046e-02, 4.7051e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004615152720361948 tensor([4.6152e-03, 4.1595e-06, 2.7557e-08, 9.9255e-01, 2.8319e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1684158593416214 tensor([0.0122, 0.7231, 0.0900, 0.0063, 0.1684], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2516351640224457 tensor([0.0186, 0.6235, 0.0909, 0.0154, 0.2516], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08891287446022034 tensor([0.0017, 0.0147, 0.0260, 0.0889, 0.8687], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.023191208019852638 tensor([2.0745e-07, 3.4580e-03, 9.7334e-01, 6.3953e-06, 2.3191e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.034890372306108475 tensor([1.6674e-06, 9.0626e-06, 1.2174e-03, 3.4890e-02, 9.6388e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31163400411605835 tensor([0.0010, 0.1453, 0.3116, 0.0068, 0.5353], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07958793640136719 tensor([0.0031, 0.0638, 0.0796, 0.0435, 0.8100], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.44586247205734253 tensor([4.4586e-01, 2.2348e-02, 5.3207e-05, 5.0283e-01, 2.8906e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28085386753082275 tensor([7.1769e-01, 2.8085e-01, 1.0982e-05, 1.2822e-03, 1.6295e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4148402512073517 tensor([0.0083, 0.0059, 0.0017, 0.4148, 0.5693], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13576140999794006 tensor([2.5493e-03, 8.2786e-01, 1.3576e-01, 4.9031e-04, 3.3337e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016207004664465785 tensor([4.8259e-09, 1.6207e-03, 9.9686e-01, 6.9185e-08, 1.5210e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3408242166042328 tensor([6.3045e-01, 3.4082e-01, 1.8011e-04, 2.1154e-02, 7.3934e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2526674270629883 tensor([2.5267e-01, 2.8548e-02, 3.0962e-04, 6.3908e-01, 7.9393e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05009607970714569 tensor([6.2042e-06, 2.2575e-05, 9.6813e-04, 5.0096e-02, 9.4891e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15400472283363342 tensor([6.1368e-05, 1.5400e-01, 7.7854e-01, 1.1788e-04, 6.7276e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10267860442399979 tensor([0.0121, 0.8004, 0.0796, 0.0053, 0.1027], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0066781193017959595 tensor([6.6781e-03, 9.9016e-01, 2.6698e-03, 2.3843e-05, 4.7019e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012520360760390759 tensor([1.3211e-08, 5.9544e-04, 9.8688e-01, 1.4437e-06, 1.2520e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05454809591174126 tensor([4.6688e-05, 3.2703e-04, 4.7970e-03, 5.4548e-02, 9.4028e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014187857508659363 tensor([1.3607e-07, 1.4188e-02, 9.8230e-01, 3.5053e-07, 3.5079e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2640843093395233 tensor([2.5626e-02, 5.2170e-03, 5.6244e-04, 7.0451e-01, 2.6408e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.38638120889663696 tensor([0.0009, 0.3864, 0.4663, 0.0013, 0.1451], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17516322433948517 tensor([1.7516e-01, 3.1432e-02, 5.4081e-04, 6.5175e-01, 1.4112e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0025647322181612253 tensor([2.5647e-03, 6.0128e-07, 2.4449e-09, 9.9663e-01, 8.0822e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08194892853498459 tensor([1.0519e-05, 8.1949e-02, 8.9102e-01, 2.0339e-05, 2.6997e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1600549817085266 tensor([4.9477e-04, 1.7573e-03, 1.0348e-02, 1.6005e-01, 8.2735e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3036523163318634 tensor([0.1011, 0.0763, 0.0034, 0.5156, 0.3037], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1110391616821289 tensor([0.0038, 0.0160, 0.0120, 0.1110, 0.8571], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03112233802676201 tensor([2.2151e-02, 4.5222e-04, 7.9824e-06, 9.4627e-01, 3.1122e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03797532618045807 tensor([3.9560e-04, 6.2639e-03, 2.9974e-02, 3.7975e-02, 9.2539e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26142656803131104 tensor([0.0773, 0.2614, 0.0270, 0.1867, 0.4476], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08640505373477936 tensor([8.7601e-01, 3.5485e-02, 8.1823e-06, 8.6405e-02, 2.0924e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34976926445961 tensor([2.6742e-04, 9.5637e-02, 5.5263e-01, 1.6963e-03, 3.4977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10125216096639633 tensor([5.2972e-04, 1.2158e-05, 4.1308e-06, 8.9820e-01, 1.0125e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23795057833194733 tensor([0.0035, 0.3709, 0.2380, 0.0069, 0.3808], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.057960812002420425 tensor([9.3261e-01, 5.7961e-02, 3.1908e-06, 9.2053e-03, 2.1648e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08325342833995819 tensor([8.3253e-02, 9.1671e-01, 1.4954e-05, 9.2144e-06, 1.1926e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03634639084339142 tensor([3.2789e-06, 3.6346e-02, 9.4070e-01, 1.3679e-05, 2.2937e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13473103940486908 tensor([2.7413e-02, 2.9801e-03, 1.3154e-04, 8.3474e-01, 1.3473e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21367952227592468 tensor([9.7185e-06, 3.1029e-03, 2.1368e-01, 2.4613e-03, 7.8075e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3129558861255646 tensor([0.0251, 0.0506, 0.0133, 0.3130, 0.5981], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2674734890460968 tensor([2.6747e-01, 7.3250e-01, 3.2532e-06, 1.7944e-05, 8.1333e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3834071159362793 tensor([0.5253, 0.3834, 0.0010, 0.0597, 0.0306], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0027189028915017843 tensor([9.9144e-09, 5.9183e-07, 2.7189e-03, 2.3842e-03, 9.9490e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13324204087257385 tensor([2.0555e-05, 1.3324e-01, 8.4621e-01, 2.0313e-05, 2.0509e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1673218309879303 tensor([8.0253e-01, 2.6359e-02, 1.3421e-05, 1.6732e-01, 3.7722e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012568424455821514 tensor([9.8724e-01, 1.2568e-02, 4.3007e-09, 1.9430e-04, 8.1318e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030535316094756126 tensor([4.3134e-07, 3.9576e-05, 3.0535e-02, 3.5814e-03, 9.6584e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14267662167549133 tensor([1.4268e-01, 3.2803e-03, 1.1834e-05, 8.2537e-01, 2.8658e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4867551028728485 tensor([4.8676e-01, 5.0437e-01, 1.7737e-04, 5.7020e-03, 2.9939e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15853610634803772 tensor([2.0479e-03, 1.5924e-04, 7.3490e-05, 8.3918e-01, 1.5854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4537001848220825 tensor([5.4279e-01, 4.5370e-01, 6.5313e-05, 2.6310e-03, 8.1513e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.48247864842414856 tensor([4.7425e-05, 1.6931e-02, 4.8248e-01, 2.0270e-03, 4.9852e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13191993534564972 tensor([0.0451, 0.7768, 0.0267, 0.0195, 0.1319], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22511222958564758 tensor([1.7168e-03, 7.3443e-01, 2.2511e-01, 4.3235e-04, 3.8307e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13956744968891144 tensor([8.3261e-01, 2.4749e-02, 1.0850e-05, 1.3957e-01, 3.0681e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1667919158935547 tensor([1.6679e-01, 8.3296e-01, 4.8276e-05, 1.0012e-04, 9.5336e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1793767213821411 tensor([0.0703, 0.6993, 0.0224, 0.0285, 0.1794], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10197332501411438 tensor([1.6335e-04, 2.7991e-06, 2.2031e-06, 8.9786e-01, 1.0197e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.15556375682353973 tensor([7.6288e-06, 1.5406e-03, 1.5556e-01, 4.0326e-03, 8.3886e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2628600597381592 tensor([0.0039, 0.2997, 0.2629, 0.0120, 0.4215], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10113310068845749 tensor([8.9880e-01, 1.0113e-01, 5.5246e-08, 6.2668e-05, 1.8489e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.018161572515964508 tensor([5.8291e-07, 6.1856e-03, 9.7564e-01, 9.5021e-06, 1.8162e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012015527114272118 tensor([5.6584e-04, 1.5674e-06, 1.1876e-07, 9.8742e-01, 1.2016e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3430877923965454 tensor([2.9812e-04, 3.4309e-01, 5.6500e-01, 2.7882e-04, 9.1334e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.37882646918296814 tensor([5.4001e-04, 4.2621e-04, 1.5744e-03, 3.7883e-01, 6.1863e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1307457983493805 tensor([1.3075e-01, 8.6704e-01, 3.9796e-04, 6.8661e-04, 1.1286e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025399314239621162 tensor([1.9063e-06, 1.7843e-02, 9.5674e-01, 1.2654e-05, 2.5399e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008523832075297832 tensor([1.0582e-03, 2.7155e-06, 1.6481e-07, 9.9042e-01, 8.5238e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005426338408142328 tensor([4.0006e-09, 5.7302e-04, 9.9400e-01, 2.8758e-07, 5.4263e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15924161672592163 tensor([0.0047, 0.0171, 0.0142, 0.1592, 0.8048], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012680362910032272 tensor([1.2680e-02, 9.8706e-01, 2.1493e-04, 5.2245e-06, 4.1449e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1530078798532486 tensor([2.4444e-03, 8.0258e-01, 1.5301e-01, 4.8067e-04, 4.1490e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3285816013813019 tensor([0.0011, 0.0010, 0.0013, 0.3286, 0.6680], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33180439472198486 tensor([1.7884e-05, 5.3788e-03, 3.3180e-01, 3.0419e-03, 6.5976e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2920491099357605 tensor([6.8021e-01, 2.0459e-02, 1.7899e-05, 2.9205e-01, 7.2603e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23191401362419128 tensor([2.3191e-01, 7.6602e-01, 2.0362e-04, 9.3478e-04, 9.2669e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4586777985095978 tensor([3.7351e-05, 1.2418e-02, 4.5868e-01, 2.4622e-03, 5.2640e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22681491076946259 tensor([4.9208e-04, 7.3315e-04, 1.3536e-03, 2.2681e-01, 7.7061e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.46050316095352173 tensor([5.1084e-04, 4.7621e-01, 4.6050e-01, 2.6652e-04, 6.2507e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19225208461284637 tensor([0.0106, 0.1923, 0.1126, 0.0536, 0.6309], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19219841063022614 tensor([0.1922, 0.2542, 0.0061, 0.2940, 0.2535], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11585207283496857 tensor([4.2467e-06, 1.2783e-02, 8.7123e-01, 1.3426e-04, 1.1585e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003011360764503479 tensor([9.9600e-01, 9.8470e-04, 7.8935e-10, 3.0114e-03, 1.8429e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10861174762248993 tensor([1.4500e-05, 1.0861e-01, 8.6888e-01, 1.8616e-05, 2.2479e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21514330804347992 tensor([0.0009, 0.2151, 0.4738, 0.0033, 0.3069], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3625377118587494 tensor([0.3625, 0.1048, 0.0007, 0.4392, 0.0927], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4192562997341156 tensor([3.8679e-04, 1.0844e-01, 4.6912e-01, 2.7925e-03, 4.1926e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1449315994977951 tensor([1.3539e-04, 2.7919e-06, 3.3945e-06, 8.5493e-01, 1.4493e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25489455461502075 tensor([1.4272e-04, 8.5529e-02, 6.5820e-01, 1.2312e-03, 2.5489e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11437007784843445 tensor([0.0045, 0.0387, 0.0518, 0.1144, 0.7906], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20607495307922363 tensor([7.9302e-01, 2.0607e-01, 2.8015e-06, 8.3018e-04, 7.0304e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03411414846777916 tensor([2.5944e-05, 1.8633e-04, 5.7672e-03, 3.4114e-02, 9.5991e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4628327786922455 tensor([2.5800e-04, 7.3045e-05, 2.3566e-04, 5.3660e-01, 4.6283e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027732690796256065 tensor([1.1401e-06, 2.7733e-02, 9.6174e-01, 3.1450e-06, 1.0526e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3385853171348572 tensor([9.1160e-05, 1.8816e-02, 3.3859e-01, 5.6366e-03, 6.3687e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.053629614412784576 tensor([0.0536, 0.8905, 0.0132, 0.0058, 0.0368], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11501500755548477 tensor([1.0800e-04, 1.0132e-02, 1.1502e-01, 6.9015e-03, 8.6784e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016551468521356583 tensor([6.5584e-07, 5.2867e-05, 1.6551e-02, 4.3891e-03, 9.7901e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4897002577781677 tensor([6.7336e-07, 7.2012e-04, 4.8970e-01, 5.8760e-04, 5.0899e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3237901031970978 tensor([0.3238, 0.1758, 0.0025, 0.3635, 0.1344], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10945667326450348 tensor([0.0193, 0.8092, 0.0565, 0.0055, 0.1095], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.42013615369796753 tensor([4.2014e-01, 4.1975e-02, 2.4648e-04, 4.8660e-01, 5.1041e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04002884775400162 tensor([5.8758e-04, 6.8098e-06, 1.9346e-06, 9.5937e-01, 4.0029e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.43108177185058594 tensor([2.2249e-04, 7.6575e-02, 4.3108e-01, 2.5133e-03, 4.8961e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.044724829494953156 tensor([9.5438e-01, 8.6870e-04, 1.0592e-08, 4.4725e-02, 3.1040e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1534619927406311 tensor([7.3990e-01, 1.5346e-01, 1.3415e-04, 9.3942e-02, 1.2559e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1007513478398323 tensor([0.0111, 0.8079, 0.1008, 0.0030, 0.0773], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05347449705004692 tensor([1.4144e-05, 7.7549e-05, 2.5088e-03, 5.3474e-02, 9.4392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4210067689418793 tensor([9.9907e-05, 5.6800e-01, 4.2101e-01, 1.3647e-05, 1.0880e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10303197801113129 tensor([0.1030, 0.8545, 0.0075, 0.0073, 0.0277], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4563847780227661 tensor([5.1900e-01, 1.3612e-02, 1.5462e-05, 4.5638e-01, 1.0992e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07680612802505493 tensor([0.0768, 0.8842, 0.0078, 0.0046, 0.0267], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0253911055624485 tensor([9.0209e-05, 4.5573e-07, 2.7504e-07, 9.7452e-01, 2.5391e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.034150153398513794 tensor([3.7980e-07, 3.4150e-02, 9.6319e-01, 3.9865e-07, 2.6563e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014721284620463848 tensor([9.7253e-01, 1.4721e-02, 5.6985e-07, 1.2615e-02, 1.3140e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18285895884037018 tensor([8.0907e-01, 1.8286e-01, 1.7205e-05, 7.2766e-03, 7.7677e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005767429247498512 tensor([2.2817e-08, 1.8606e-03, 9.9237e-01, 5.2488e-07, 5.7674e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.43968719244003296 tensor([1.1278e-05, 7.4203e-07, 8.3184e-06, 5.6029e-01, 4.3969e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32830584049224854 tensor([0.0673, 0.3283, 0.0259, 0.1297, 0.4488], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30130627751350403 tensor([0.0630, 0.1248, 0.0138, 0.3013, 0.4971], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007519000209867954 tensor([9.9128e-01, 7.5190e-03, 1.2298e-08, 1.1975e-03, 4.5745e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016830243170261383 tensor([2.8445e-09, 1.8204e-04, 9.8299e-01, 1.1137e-06, 1.6830e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4026086926460266 tensor([1.4410e-04, 2.4586e-05, 8.2621e-05, 5.9714e-01, 4.0261e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1363936811685562 tensor([5.9932e-06, 1.3843e-02, 8.4958e-01, 1.7310e-04, 1.3639e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.43347790837287903 tensor([6.5634e-05, 1.8671e-05, 1.9766e-04, 4.3348e-01, 5.6624e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005552712827920914 tensor([3.0565e-03, 9.9087e-01, 5.5527e-03, 1.5281e-05, 5.0848e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.035893794149160385 tensor([1.3769e-04, 3.0869e-03, 3.5894e-02, 2.8894e-02, 9.3199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18395759165287018 tensor([1.8396e-01, 9.4854e-04, 1.3462e-06, 8.0809e-01, 7.0007e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08171331882476807 tensor([2.7718e-06, 1.1360e-02, 9.0687e-01, 5.1153e-05, 8.1713e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3253501057624817 tensor([0.3254, 0.1347, 0.0011, 0.4171, 0.1218], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004784188698977232 tensor([4.1556e-03, 9.9044e-01, 4.7842e-03, 2.2011e-05, 5.9545e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14746153354644775 tensor([3.9898e-05, 4.0190e-03, 1.4746e-01, 6.6846e-03, 8.4179e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13503152132034302 tensor([5.0950e-06, 4.7182e-06, 2.0173e-04, 1.3503e-01, 8.6476e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02785743586719036 tensor([1.3844e-08, 4.1934e-04, 9.7172e-01, 3.3588e-06, 2.7857e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08699291944503784 tensor([2.7056e-06, 3.1733e-04, 8.6993e-02, 6.1921e-03, 9.0650e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0059711080975830555 tensor([9.9019e-01, 5.9711e-03, 2.2203e-08, 3.8226e-03, 1.9581e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1956978738307953 tensor([2.0951e-05, 2.5986e-02, 7.7790e-01, 3.9349e-04, 1.9570e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11763450503349304 tensor([7.8876e-05, 7.6766e-03, 1.1763e-01, 8.7957e-03, 8.6581e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08485247939825058 tensor([1.9155e-05, 8.4852e-02, 8.6906e-01, 5.1175e-05, 4.6020e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22423891723155975 tensor([0.2242, 0.3142, 0.0074, 0.2337, 0.2205], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012259319424629211 tensor([9.8763e-01, 1.2259e-02, 2.2887e-09, 1.0637e-04, 5.0122e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.027151033282279968 tensor([2.7151e-02, 5.2912e-04, 1.3965e-05, 9.3802e-01, 3.4286e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09310365468263626 tensor([1.2245e-02, 5.9120e-04, 3.6948e-05, 8.9402e-01, 9.3104e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1254568248987198 tensor([8.0578e-04, 8.6416e-01, 1.2546e-01, 6.2902e-05, 9.5129e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3620862364768982 tensor([2.8737e-03, 7.0951e-04, 4.1954e-04, 6.3391e-01, 3.6209e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32365891337394714 tensor([3.2366e-01, 5.2761e-02, 2.9972e-04, 5.5087e-01, 7.2410e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013564224354922771 tensor([3.4941e-09, 2.2039e-04, 9.8621e-01, 1.0189e-06, 1.3564e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13359510898590088 tensor([1.9546e-06, 6.1044e-04, 1.3360e-01, 2.1568e-03, 8.6364e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3449891209602356 tensor([9.8911e-07, 1.7586e-03, 6.5300e-01, 2.5573e-04, 3.4499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3072911202907562 tensor([3.0729e-01, 6.0267e-03, 1.3662e-05, 6.7522e-01, 1.1445e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04978608712553978 tensor([9.5016e-01, 4.9786e-02, 1.3098e-08, 5.3582e-05, 6.9625e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3369768261909485 tensor([6.2536e-01, 3.3698e-01, 2.4555e-04, 2.8232e-02, 9.1842e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11469719558954239 tensor([1.1470e-01, 2.2002e-03, 1.0072e-05, 8.5990e-01, 2.3197e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26725587248802185 tensor([3.1034e-06, 1.4906e-03, 2.6726e-01, 1.7289e-03, 7.2952e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13020461797714233 tensor([7.6239e-01, 9.7950e-02, 9.0016e-05, 1.3020e-01, 9.3637e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016439273953437805 tensor([9.8166e-01, 1.6439e-02, 8.5097e-08, 1.8807e-03, 1.5017e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28914564847946167 tensor([0.0476, 0.2891, 0.0445, 0.1317, 0.4870], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1769537776708603 tensor([5.9271e-03, 4.2128e-04, 4.4978e-05, 8.1665e-01, 1.7695e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3454362154006958 tensor([0.0962, 0.0611, 0.0031, 0.4942, 0.3454], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23902392387390137 tensor([9.9524e-09, 1.5057e-05, 2.3902e-01, 3.2379e-04, 7.6064e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00398546876385808 tensor([3.9855e-03, 9.9283e-01, 2.8463e-03, 1.3063e-05, 3.2742e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44760021567344666 tensor([0.0099, 0.0062, 0.0024, 0.5339, 0.4476], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.47871294617652893 tensor([4.7871e-01, 5.2047e-01, 2.3232e-05, 5.9696e-04, 2.0020e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03963276743888855 tensor([3.9633e-02, 9.5479e-01, 2.6085e-03, 4.8930e-04, 2.4831e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06354101002216339 tensor([8.7586e-06, 2.3825e-02, 9.1251e-01, 1.1134e-04, 6.3541e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.036879975348711014 tensor([3.6880e-02, 9.6311e-01, 6.1315e-06, 1.0133e-06, 2.0254e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0023455549962818623 tensor([3.3925e-10, 1.6297e-04, 9.9749e-01, 5.3361e-08, 2.3456e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4717028737068176 tensor([1.2627e-04, 2.2237e-05, 7.7027e-05, 5.2807e-01, 4.7170e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31715676188468933 tensor([0.1263, 0.0793, 0.0031, 0.4741, 0.3172], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 4, 0], [3, 2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3, 1], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 1], [2, 3, 4, 0], [0, 1, 3, 2], [0, 1, 3, 2], [0, 3, 1, 4], [2, 3, 0], [3, 2, 4, 0], [2, 0], [2, 1, 0], [3, 0, 4], [2, 1, 4], [3, 0, 4, 2], [2, 1, 4], [1, 2, 0], [3, 4, 2, 0], [2, 1, 0, 4], [3, 4, 0, 2], [2, 1, 0, 4], [2, 1, 0], [0, 3, 4], [0, 1, 3, 2], [3, 4, 0, 2], [0, 3, 1, 4], [0, 1, 2, 3], [3, 0, 4], [0, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 3], [0, 3, 1, 2], [2, 4, 3], [0, 1, 2, 3], [2, 1, 0, 4], [0, 3, 2, 1], [0, 1, 2, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 3, 2], [3, 2, 4, 0], [2, 0, 3], [2, 3, 4], [0, 1, 2, 3], [2, 4, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 1], [2, 1, 0, 4], [3, 0, 4], [2, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 0], [3, 0, 2, 4], [3, 0, 2], [0, 1, 2, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1], [0, 3, 1, 2], [2, 4, 1], [2, 4, 3], [2, 1, 0], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 3], [2, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [3, 0, 2, 4], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 1, 0], [0, 3, 4], [2, 1, 4, 0], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 1, 0], [0, 2, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 3], [2, 4, 1, 3], [0, 3, 1], [2, 1, 0, 4], [0, 3], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3, 2], [3, 0, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 3], [2, 1, 0, 4], [2, 4, 3], [0, 3, 1], [3, 2, 0, 4], [3, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 3, 0, 4], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [3, 0, 4], [1, 0, 2], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 2, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 1, 2], [3, 0, 4], [0, 3, 2, 1], [2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1], [0, 1, 2, 3], [2, 4, 3], [0, 1, 2, 3], [1, 2, 0], [0, 3, 4, 1], [0, 3, 1], [3, 2, 4, 0], [0, 3, 1, 2], [0, 1, 3, 2], [0, 3, 1], [2, 4, 1], [3, 0, 2, 4], [2, 4, 1], [2, 1, 0, 4], [0, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [3, 0, 4], [2, 3, 4, 0], [2, 4, 1], [3, 2, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [2, 0, 3], [2, 0, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [0, 3, 1, 4], [1, 0, 2], [3, 4, 0, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1], [3, 4, 0, 2], [0, 1, 3, 2], [1, 0, 2, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 0], [2, 1, 4], [0, 3, 1, 4], [0, 1, 3, 2], [0, 3, 1], [2, 1, 4], [2, 4, 3, 1], [2, 4, 3], [2, 1, 4, 0], [0, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [2, 0, 3], [2, 1, 0, 4], [2, 1, 0], [0, 1, 3], [3, 4, 2, 0], [2, 1, 0], [2, 4, 3], [3, 4, 2, 0], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4], [1, 2, 0], [2, 1, 0]]\n",
      "[[3], [1], [2, 4], [3, 4], [2, 4], [3, 4], [0], [0, 1], [3], [0, 3], [4], [1], [2, 4], [3], [4], [2, 4], [1], [4], [4], [2], [1, 4], [1], [1, 3, 4], [3, 4], [1, 2], [0, 3], [1], [0, 3], [3, 4], [1], [3], [1], [3], [3, 4], [1, 2], [4], [1], [2], [4], [1, 2], [2, 4], [1], [2], [0], [0, 1], [4], [0, 1], [4], [3], [4], [4], [1], [2], [4], [1], [1, 4], [0, 1], [4], [0, 1], [2], [2], [0], [2], [3], [4], [3], [0], [2], [4], [4], [1], [1], [2, 4], [3], [1, 2], [0, 3], [0], [0], [3], [1], [1, 4], [4], [2], [4], [2, 4], [4], [0, 3], [0, 1], [3, 4], [1], [2], [0, 1], [0, 3], [4], [2], [1], [1], [2], [4], [2], [3, 4], [1, 2], [3], [3], [2], [4], [3, 4], [4], [3], [4], [1, 4], [0], [2, 4], [3], [1, 2, 4], [0], [1], [2], [3], [2, 4], [3, 4], [0, 1], [0, 1], [4], [2], [0], [0], [4], [3], [0, 1], [3], [0, 1], [2, 4], [1], [1, 2], [0], [1], [1], [3], [4], [1, 2, 4], [0], [2], [3], [1, 2], [3, 4], [1], [2], [3], [2], [4], [1], [1], [3, 4], [2, 4], [0, 3], [0, 1], [2, 4], [3, 4], [1, 2], [4], [1, 3, 4], [2], [0], [2], [1, 2, 4], [0, 3], [2, 4], [3], [2, 4], [4], [0, 1], [4], [3, 4], [2], [2, 4], [1], [4], [4], [2, 4], [0, 3], [1], [0, 3], [3], [2, 4], [0], [0], [1], [4], [1, 2], [1], [0, 3], [1], [3], [2], [0], [0], [2], [3, 4], [1, 4], [3, 4], [0], [2], [3, 4], [2], [3, 4], [1], [4], [3], [2], [0, 3], [1], [4], [4], [2], [4], [0], [2], [4], [2], [0, 1, 3], [0], [3], [3], [1], [3, 4], [0, 3], [2], [4], [2, 4], [0, 3], [0], [0, 1], [3], [2, 4], [0], [0], [1, 4], [3], [3, 4], [2, 4], [1], [3, 4], [0, 1], [1], [2], [1], [2], [3, 4], [3, 4]]\n",
      "NL_pred of 3th iteration [[2, 1, 4, 0], [3, 2, 4, 0], [2, 4, 3, 1], [2, 1, 4, 0], [0, 1, 2, 3], [3, 4, 0, 2], [2, 1, 0, 4], [0, 3, 1, 2], [2, 3, 4, 0], [0, 1, 3, 2], [0, 1, 3, 2], [0, 3, 1, 4], [3, 2, 4, 0], [3, 0, 4, 2], [3, 4, 2, 0], [2, 1, 0, 4], [3, 4, 0, 2], [2, 1, 0, 4], [0, 1, 3, 2], [3, 4, 0, 2], [0, 3, 1, 4], [0, 1, 2, 3], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 1, 2, 3], [2, 1, 0, 4], [0, 3, 2, 1], [0, 1, 2, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 3, 2], [3, 2, 4, 0], [0, 1, 2, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 0], [3, 0, 2, 4], [0, 1, 2, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 2], [3, 0, 4, 2], [0, 3, 4, 1], [0, 1, 2, 3], [0, 3, 4, 1], [3, 0, 2, 4], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [0, 2, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [3, 0, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [2, 1, 0, 4], [3, 2, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 3, 0, 4], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 2, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [0, 3, 2, 1], [2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 4, 1], [3, 2, 4, 0], [0, 3, 1, 2], [0, 1, 3, 2], [3, 0, 2, 4], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [2, 3, 4, 0], [3, 2, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 4], [3, 4, 0, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 3, 1, 4], [3, 4, 0, 2], [0, 1, 3, 2], [1, 0, 2, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 2], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 0, 4], [3, 4, 2, 0], [3, 4, 2, 0], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4]]\n",
      "Start of Epoch\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.007578924017132453  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.007578806307330821  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.007578584383118828  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.007578270240399822  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.007577875125333198  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.007577409534334387  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.007576883214074861  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.007576304411738174  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.007575682124251839  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.007575025348543371  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.007574337833332566  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.007573627076059018  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.007572896825442524  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.007572148580971004  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.007571389840084052  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00757061760380583  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.007569837870088013  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.007569052138418521  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.0075682611585413135  Accuracy on Support set:0.0\n",
      "torch.Size([159, 2048]) torch.Size([159])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.007567464930456389  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.22127403318881989 tensor([5.3594e-07, 2.6753e-04, 2.2127e-01, 1.0015e-03, 7.7746e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.28900912404060364 tensor([0.0116, 0.0206, 0.0070, 0.2890, 0.6718], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24389344453811646 tensor([2.9597e-08, 1.7478e-04, 7.5587e-01, 6.2136e-05, 2.4389e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27834105491638184 tensor([5.1689e-04, 5.8637e-04, 2.1477e-03, 2.7834e-01, 7.1841e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24304135143756866 tensor([7.5223e-01, 2.4304e-01, 2.1477e-05, 4.1377e-03, 5.6557e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4494333267211914 tensor([5.2413e-01, 1.3662e-02, 1.6703e-05, 4.4943e-01, 1.2759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3332839906215668 tensor([4.4687e-05, 8.9409e-03, 3.3328e-01, 4.8903e-03, 6.5284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2724010646343231 tensor([3.6664e-04, 1.4026e-01, 5.8470e-01, 2.2790e-03, 2.7240e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.33667290210723877 tensor([0.0541, 0.5035, 0.0428, 0.0629, 0.3367], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20661331713199615 tensor([0.1191, 0.2066, 0.0128, 0.2630, 0.3985], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29513394832611084 tensor([1.2894e-02, 2.9255e-03, 3.1096e-04, 6.8874e-01, 2.9513e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3443862795829773 tensor([2.1535e-05, 3.4439e-01, 6.5006e-01, 4.2021e-06, 5.5318e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.19428972899913788 tensor([1.9429e-01, 2.9257e-02, 4.4730e-04, 6.8631e-01, 8.9694e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3907296657562256 tensor([3.9073e-01, 8.0541e-03, 1.7420e-05, 5.8616e-01, 1.5034e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26216599345207214 tensor([4.4824e-05, 2.1873e-06, 9.6211e-06, 7.3778e-01, 2.6217e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23516245186328888 tensor([1.1222e-03, 1.2665e-04, 8.3928e-05, 7.6350e-01, 2.3516e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4033208191394806 tensor([0.0009, 0.4065, 0.4033, 0.0012, 0.1881], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.34910014271736145 tensor([7.7663e-05, 3.4910e-01, 6.3214e-01, 3.3464e-05, 1.8653e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3687731921672821 tensor([3.3724e-06, 4.3492e-03, 6.2644e-01, 4.3591e-04, 3.6877e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28452742099761963 tensor([2.8453e-01, 7.1429e-01, 9.5160e-05, 6.6526e-04, 4.2555e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20285306870937347 tensor([7.9576e-01, 2.0285e-01, 3.9664e-06, 1.2579e-03, 1.2483e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3519439101219177 tensor([0.0429, 0.3519, 0.0441, 0.0787, 0.4823], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20531824231147766 tensor([2.0532e-01, 7.9261e-01, 2.1703e-04, 8.0290e-04, 1.0535e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3644956946372986 tensor([6.2875e-01, 3.6450e-01, 4.5364e-05, 5.3373e-03, 1.3758e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21798613667488098 tensor([4.8297e-05, 7.0041e-03, 2.1799e-01, 5.3675e-03, 7.6959e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4395720362663269 tensor([2.0036e-05, 4.3957e-01, 5.5636e-01, 2.4005e-06, 4.0424e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2405446320772171 tensor([0.2405, 0.0758, 0.0013, 0.5607, 0.1216], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2694076597690582 tensor([0.0173, 0.6016, 0.0959, 0.0158, 0.2694], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.310920387506485 tensor([0.0009, 0.1307, 0.3109, 0.0067, 0.5508], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.41560882329940796 tensor([4.1561e-01, 1.9831e-02, 4.9370e-05, 5.3481e-01, 2.9696e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27997252345085144 tensor([7.1847e-01, 2.7997e-01, 1.1286e-05, 1.3733e-03, 1.7361e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41598591208457947 tensor([0.0072, 0.0050, 0.0016, 0.4160, 0.5703], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3275960087776184 tensor([6.3963e-01, 3.2760e-01, 1.8055e-04, 2.4409e-02, 8.1797e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.22832152247428894 tensor([2.2832e-01, 2.5119e-02, 2.9169e-04, 6.6461e-01, 8.1657e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26441991329193115 tensor([2.2597e-02, 4.5103e-03, 5.1893e-04, 7.0795e-01, 2.6442e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36343517899513245 tensor([0.0009, 0.3634, 0.4795, 0.0013, 0.1549], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3048301041126251 tensor([0.0912, 0.0654, 0.0030, 0.5355, 0.3048], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24413514137268066 tensor([0.0712, 0.2441, 0.0270, 0.1912, 0.4664], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3617803156375885 tensor([2.3860e-04, 8.7118e-02, 5.4916e-01, 1.7019e-03, 3.6178e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24467895925045013 tensor([0.0031, 0.3448, 0.2447, 0.0069, 0.4005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20579415559768677 tensor([8.3860e-06, 2.7168e-03, 2.0579e-01, 2.4209e-03, 7.8906e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30816560983657837 tensor([0.0218, 0.0453, 0.0130, 0.3082, 0.6117], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2723608613014221 tensor([2.7236e-01, 7.2761e-01, 3.2028e-06, 1.8903e-05, 8.3211e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37359651923179626 tensor([0.5241, 0.3736, 0.0010, 0.0672, 0.0341], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.48371756076812744 tensor([4.8372e-01, 5.0671e-01, 1.8692e-04, 6.1332e-03, 3.2510e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4414820969104767 tensor([5.5468e-01, 4.4148e-01, 6.4232e-05, 2.9142e-03, 8.6401e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4660172760486603 tensor([4.2346e-05, 1.5054e-02, 4.6602e-01, 2.0665e-03, 5.1682e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24302378296852112 tensor([1.5776e-03, 7.1380e-01, 2.4302e-01, 4.4028e-04, 4.1160e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26653480529785156 tensor([0.0035, 0.2762, 0.2665, 0.0121, 0.4416], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3207533061504364 tensor([2.6640e-04, 3.2075e-01, 5.8330e-01, 2.7754e-04, 9.5402e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3745972514152527 tensor([4.7056e-04, 3.7180e-04, 1.4801e-03, 3.7460e-01, 6.2308e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3275565207004547 tensor([0.0010, 0.0009, 0.0012, 0.3276, 0.6694], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32366228103637695 tensor([1.5148e-05, 4.6807e-03, 3.2366e-01, 2.9671e-03, 6.6867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3180566430091858 tensor([6.5499e-01, 1.9177e-02, 1.7706e-05, 3.1806e-01, 7.7586e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23886573314666748 tensor([2.3887e-01, 7.5891e-01, 2.0373e-04, 1.0306e-03, 9.9002e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.45069420337677 tensor([3.2786e-05, 1.1139e-02, 4.5069e-01, 2.4303e-03, 5.3570e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2310936152935028 tensor([4.3920e-04, 6.2750e-04, 1.2139e-03, 2.3109e-01, 7.6663e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.44915905594825745 tensor([4.5959e-04, 4.4916e-01, 4.8381e-01, 2.6870e-04, 6.6301e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22935546934604645 tensor([0.1772, 0.2294, 0.0060, 0.3169, 0.2706], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20015189051628113 tensor([0.0008, 0.2002, 0.4807, 0.0032, 0.3151], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3390456736087799 tensor([0.3390, 0.0924, 0.0007, 0.4725, 0.0954], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44280433654785156 tensor([3.5009e-04, 9.7374e-02, 4.5657e-01, 2.8993e-03, 4.4280e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26275622844696045 tensor([1.2467e-04, 7.7121e-02, 6.5878e-01, 1.2214e-03, 2.6276e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19882860779762268 tensor([8.0019e-01, 1.9883e-01, 2.7220e-06, 9.0049e-04, 7.3134e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.458208829164505 tensor([2.2669e-04, 6.1700e-05, 2.0987e-04, 5.4129e-01, 4.5821e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3336549401283264 tensor([7.7964e-05, 1.6753e-02, 3.3365e-01, 5.4284e-03, 6.4409e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.482473760843277 tensor([5.9678e-07, 6.5778e-04, 4.8247e-01, 5.7404e-04, 5.1629e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30229511857032776 tensor([0.3023, 0.1668, 0.0026, 0.3834, 0.1449], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3879690170288086 tensor([3.8797e-01, 3.7568e-02, 2.3688e-04, 5.2043e-01, 5.3792e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.42430734634399414 tensor([1.9492e-04, 6.8165e-02, 4.2431e-01, 2.5140e-03, 5.0482e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4422287940979004 tensor([9.0655e-05, 5.4637e-01, 4.4223e-01, 1.3436e-05, 1.1300e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.48680904507637024 tensor([4.8681e-01, 1.2137e-02, 1.4433e-05, 4.8969e-01, 1.1348e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4307248294353485 tensor([1.0261e-05, 6.4130e-07, 7.3275e-06, 5.6926e-01, 4.3072e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30685627460479736 tensor([0.0609, 0.3069, 0.0265, 0.1319, 0.4738], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30139535665512085 tensor([0.0559, 0.1136, 0.0138, 0.3014, 0.5153], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39985376596450806 tensor([1.2702e-04, 2.1019e-05, 7.4501e-05, 5.9992e-01, 3.9985e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4294463098049164 tensor([5.7097e-05, 1.6211e-05, 1.8479e-04, 4.2945e-01, 5.7030e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30242910981178284 tensor([0.3024, 0.1238, 0.0010, 0.4435, 0.1293], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.20996972918510437 tensor([0.2100, 0.3050, 0.0078, 0.2412, 0.2361], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3641098439693451 tensor([2.5048e-03, 6.1158e-04, 3.8874e-04, 6.3239e-01, 3.6411e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.29746872186660767 tensor([2.9747e-01, 4.6226e-02, 2.7658e-04, 5.8208e-01, 7.3947e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35271844267845154 tensor([8.6411e-07, 1.5783e-03, 6.4545e-01, 2.5243e-04, 3.5272e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.278768390417099 tensor([2.7877e-01, 5.3119e-03, 1.2836e-05, 7.0419e-01, 1.1716e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3287133276462555 tensor([6.2877e-01, 3.2871e-01, 2.5206e-04, 3.2056e-02, 1.0209e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26077204942703247 tensor([2.6302e-06, 1.3017e-03, 2.6077e-01, 1.6776e-03, 7.3625e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2656525671482086 tensor([0.0436, 0.2657, 0.0443, 0.1365, 0.5099], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.35515210032463074 tensor([0.0850, 0.0537, 0.0030, 0.5032, 0.3552], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22750964760780334 tensor([8.9114e-09, 1.3546e-05, 2.2751e-01, 3.2148e-04, 7.7216e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44525185227394104 tensor([0.0085, 0.0052, 0.0022, 0.5389, 0.4453], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.480575829744339 tensor([4.8058e-01, 5.1855e-01, 2.3789e-05, 6.4153e-04, 2.1364e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4693933129310608 tensor([1.1146e-04, 1.9092e-05, 6.9741e-05, 5.3041e-01, 4.6939e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.33242690563201904 tensor([0.1112, 0.0711, 0.0031, 0.4822, 0.3324], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 4, 0], [3, 2, 4, 0], [0, 1, 3], [2, 0, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3, 1], [2, 4, 3], [2, 1, 4, 0], [2, 4, 1], [0, 1, 2, 3], [3, 4, 0, 2], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 1], [2, 3, 4, 0], [0, 1, 3, 2], [0, 1, 3, 2], [0, 3, 1, 4], [2, 3, 0], [3, 2, 4, 0], [2, 0], [2, 1, 0], [3, 0, 4], [2, 1, 4, 0], [3, 0, 4, 2], [2, 1, 4], [1, 2, 0], [3, 4, 2, 0], [2, 1, 0, 4], [3, 4, 0, 2], [2, 1, 0, 4], [2, 1, 0], [0, 3, 4], [0, 1, 3, 2], [3, 4, 0, 2], [0, 3, 1, 4], [0, 1, 2, 3], [3, 0, 4], [0, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 3], [0, 3, 1, 2], [2, 4, 3], [0, 1, 2, 3], [2, 1, 0, 4], [0, 3, 2, 1], [0, 1, 2, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 3, 2], [3, 2, 4, 0], [2, 0, 3], [2, 3, 4], [0, 1, 2, 3], [2, 4, 3], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 1], [2, 1, 0, 4], [3, 0, 4], [2, 1, 4], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 0], [3, 0, 2, 4], [3, 0, 2], [0, 1, 2, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1], [0, 3, 1, 2], [2, 4, 1], [2, 4, 3], [2, 1, 0], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 3], [2, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [3, 0, 2, 4], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 1, 0], [0, 3, 4], [2, 1, 4, 0], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 1, 0], [0, 2, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 3], [2, 4, 1, 3], [0, 3, 1], [2, 1, 0, 4], [0, 3], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1], [2, 0, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3, 2], [3, 0, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 3], [2, 1, 0, 4], [2, 4, 3], [0, 3, 1], [3, 2, 0, 4], [3, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 3, 0, 4], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [3, 0, 4], [1, 0, 2], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 2, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0], [0, 3, 1], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 1, 2], [3, 0, 4], [0, 3, 2, 1], [2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1], [0, 1, 2, 3], [2, 4, 3, 1], [0, 1, 2, 3], [1, 2, 0], [0, 3, 4, 1], [0, 3, 1], [3, 2, 4, 0], [0, 3, 1, 2], [0, 1, 3, 2], [0, 3, 1], [2, 4, 1], [3, 0, 2, 4], [2, 4, 1], [2, 1, 0, 4], [0, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [3, 0, 4], [2, 3, 4, 0], [2, 4, 1], [3, 2, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [2, 0, 3], [2, 0, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [0, 3, 1, 4], [1, 0, 2], [3, 4, 0, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1], [3, 4, 0, 2], [0, 1, 3, 2], [1, 0, 2, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 0], [2, 1, 4], [0, 3, 1, 4], [0, 1, 3, 2], [0, 3, 1], [2, 1, 4], [2, 4, 3, 1], [2, 4, 3], [2, 1, 4, 0], [0, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [2, 0, 3], [2, 1, 0, 4], [2, 1, 0], [0, 1, 3], [3, 4, 2, 0], [2, 1, 0], [2, 4, 3], [3, 4, 2, 0], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4], [1, 2, 0], [2, 1, 0]]\n",
      "[[3], [1], [2, 4], [3, 4], [2, 4], [3, 4], [0], [0, 1], [3], [0, 3], [4], [1], [2, 4], [3], [4], [2, 4], [1], [4], [4], [2], [1, 4], [1], [1, 3, 4], [3, 4], [1, 2], [3], [1], [0, 3], [3, 4], [1], [3], [1], [3], [3, 4], [1, 2], [4], [1], [2], [4], [1, 2], [2, 4], [1], [2], [0], [0, 1], [4], [0, 1], [4], [3], [4], [4], [1], [2], [4], [1], [1, 4], [0, 1], [4], [0, 1], [2], [2], [0], [2], [3], [4], [3], [0], [2], [4], [4], [1], [1], [2, 4], [3], [1, 2], [0, 3], [0], [0], [3], [1], [1, 4], [4], [2], [4], [2, 4], [4], [0, 3], [0, 1], [3, 4], [1], [2], [0, 1], [0, 3], [4], [2], [1], [1], [2], [4], [2], [3, 4], [1, 2], [3], [3], [2], [4], [3, 4], [4], [3], [4], [1, 4], [0], [2, 4], [3], [1, 2, 4], [0], [1], [2], [3], [2, 4], [3, 4], [0, 1], [0, 1], [4], [2], [0], [0], [4], [3], [0, 1], [3], [0, 1], [2, 4], [1], [1, 2], [0], [1], [1], [3], [4], [1, 2, 4], [0], [2], [3], [1, 2], [3, 4], [1], [2], [3], [2], [4], [1], [1], [3, 4], [2, 4], [0, 3], [0, 1], [2, 4], [3, 4], [1, 2], [4], [1, 3, 4], [2], [0], [2], [1, 2, 4], [0, 3], [2, 4], [3], [2, 4], [4], [0], [4], [3, 4], [2], [2, 4], [1], [4], [4], [2, 4], [0, 3], [1], [0, 3], [3], [2, 4], [0], [0], [1], [4], [1, 2], [1], [0, 3], [1], [3], [2], [0], [0], [2], [3, 4], [1, 4], [3, 4], [0], [2], [3, 4], [2], [3, 4], [1], [4], [3], [2], [0, 3], [1], [4], [4], [2], [4], [0], [2], [4], [2], [0, 1, 3], [0], [3], [3], [1], [3, 4], [0, 3], [2], [4], [2, 4], [0, 3], [0], [0, 1], [3], [2, 4], [0], [0], [1, 4], [3], [3, 4], [2, 4], [1], [3, 4], [0, 1], [1], [2], [1], [2], [3, 4], [3, 4]]\n",
      "NL_pred of 4th iteration [[2, 1, 4, 0], [2, 4, 3, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.6615387201309204  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.6582400798797607  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.6522965431213379  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.6445317268371582  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.6357899904251099  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.6268205642700195  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.6181986331939697  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.6103110313415527  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.6033393740653992  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.597365140914917  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.5923042297363281  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.588103711605072  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.584673285484314  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.5818692445755005  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.5795803070068359  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.5776965022087097  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.5761364698410034  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.574847936630249  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.5737788677215576  Accuracy on Support set:0.0\n",
      "torch.Size([2, 2048]) torch.Size([2])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.5728830099105835  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.05274614319205284 tensor([2.3831e-07, 4.2834e-05, 5.2746e-02, 1.8743e-03, 9.4534e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.47217506170272827 tensor([0.0018, 0.0009, 0.0010, 0.4722, 0.5242], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.37856313586235046 tensor([2.6036e-08, 5.5471e-05, 3.7856e-01, 2.3944e-04, 6.2114e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3617277443408966 tensor([7.0649e-05, 3.2168e-05, 3.9766e-04, 3.6173e-01, 6.3777e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07174137979745865 tensor([9.0024e-01, 7.1741e-02, 1.2042e-05, 2.6669e-02, 1.3421e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09974528849124908 tensor([9.9745e-02, 7.1832e-04, 2.7241e-06, 8.8802e-01, 1.1511e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08662717044353485 tensor([1.3106e-05, 9.4456e-04, 8.6627e-02, 9.9558e-03, 9.0246e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2909657061100006 tensor([1.5218e-04, 2.1331e-02, 2.9097e-01, 8.3487e-03, 6.7920e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06717993319034576 tensor([0.0258, 0.0672, 0.0158, 0.2552, 0.6360], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.012275263667106628 tensor([0.0276, 0.0123, 0.0021, 0.5774, 0.3807], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17338044941425323 tensor([1.5210e-03, 9.9337e-05, 3.2164e-05, 8.2497e-01, 1.7338e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15609943866729736 tensor([1.5597e-05, 1.5610e-01, 8.3073e-01, 1.1259e-05, 1.3144e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06374921649694443 tensor([6.3749e-02, 3.8723e-04, 2.5037e-06, 9.2462e-01, 1.1242e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19280990958213806 tensor([1.1808e-05, 2.8215e-07, 2.3025e-06, 8.0718e-01, 1.9281e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15667535364627838 tensor([1.9829e-04, 8.6406e-06, 1.3493e-05, 8.4310e-01, 1.5668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09928218275308609 tensor([5.7564e-04, 9.9282e-02, 2.7748e-01, 5.7810e-03, 6.1688e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15781056880950928 tensor([7.6571e-05, 1.5781e-01, 7.7496e-01, 1.7097e-04, 6.6982e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24549442529678345 tensor([1.1838e-06, 5.7181e-04, 2.4549e-01, 1.2590e-03, 7.5267e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4164886474609375 tensor([5.7699e-01, 4.1649e-01, 9.2933e-05, 5.0323e-03, 1.3918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03745129704475403 tensor([9.5656e-01, 3.7451e-02, 8.0753e-07, 5.8472e-03, 1.3784e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02773999236524105 tensor([0.0107, 0.0277, 0.0127, 0.2114, 0.7375], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.374373197555542 tensor([6.1563e-01, 3.7437e-01, 9.6444e-05, 7.4089e-03, 2.4911e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09806255251169205 tensor([8.4870e-01, 9.8063e-02, 2.3375e-05, 4.9072e-02, 4.1373e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05157181993126869 tensor([1.0634e-05, 5.8689e-04, 5.1572e-02, 9.1007e-03, 9.3873e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22735081613063812 tensor([1.7099e-05, 2.2735e-01, 7.6221e-01, 7.1165e-06, 1.0412e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03229759261012077 tensor([3.2298e-02, 3.1685e-03, 2.0254e-04, 8.6797e-01, 9.6366e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09683551639318466 tensor([0.0079, 0.0968, 0.0556, 0.0706, 0.7690], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09597964584827423 tensor([2.3423e-04, 1.2786e-02, 9.5980e-02, 1.4996e-02, 8.7600e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07969143986701965 tensor([7.9691e-02, 7.6720e-04, 4.2110e-06, 9.0281e-01, 1.6723e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08083904534578323 tensor([9.1192e-01, 8.0839e-02, 4.8970e-06, 6.9342e-03, 3.0149e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3856680393218994 tensor([1.1119e-03, 2.0901e-04, 1.7873e-04, 6.1283e-01, 3.8567e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.056926898658275604 tensor([7.4078e-01, 5.6927e-02, 4.7911e-05, 1.8640e-01, 1.5847e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.031081557273864746 tensor([3.1082e-02, 1.0286e-03, 3.7660e-05, 9.1211e-01, 5.5746e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19046640396118164 tensor([2.0478e-03, 1.4980e-04, 7.2267e-05, 8.0726e-01, 1.9047e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08151697367429733 tensor([0.0006, 0.0815, 0.3238, 0.0083, 0.5858], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.16939769685268402 tensor([1.3279e-02, 2.0280e-03, 2.4303e-04, 8.1505e-01, 1.6940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.017042849212884903 tensor([0.0149, 0.0170, 0.0067, 0.4101, 0.5512], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20328497886657715 tensor([1.0130e-04, 1.1932e-02, 2.0328e-01, 5.8931e-03, 7.7879e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04798748344182968 tensor([0.0011, 0.0480, 0.1037, 0.0199, 0.8272], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04909301921725273 tensor([2.3008e-06, 2.7105e-04, 4.9093e-02, 4.3197e-03, 9.4631e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4529780149459839 tensor([0.0026, 0.0019, 0.0023, 0.4530, 0.5402], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3368616998195648 tensor([6.6304e-01, 3.3686e-01, 1.1569e-06, 8.5317e-05, 1.1302e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06828915327787399 tensor([0.4031, 0.0683, 0.0005, 0.4396, 0.0886], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21163608133792877 tensor([7.2060e-01, 2.1164e-01, 1.6924e-04, 5.4834e-02, 1.2759e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12148451060056686 tensor([8.5407e-01, 1.2148e-01, 2.6033e-05, 2.2532e-02, 1.8844e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13544560968875885 tensor([1.3603e-05, 1.6196e-03, 1.3545e-01, 5.3343e-03, 8.5759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3631576895713806 tensor([0.0023, 0.3938, 0.3632, 0.0038, 0.2369], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03175920993089676 tensor([0.0011, 0.0318, 0.1006, 0.0333, 0.8333], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09937988221645355 tensor([2.1767e-04, 9.9380e-02, 5.1997e-01, 1.6013e-03, 3.7883e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4604021906852722 tensor([6.1823e-05, 2.0273e-05, 2.7242e-04, 4.6040e-01, 5.3924e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.48918816447257996 tensor([1.8806e-04, 4.7878e-05, 1.5051e-04, 5.1043e-01, 4.8919e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08079435676336288 tensor([4.0411e-06, 4.4784e-04, 8.0794e-02, 5.8763e-03, 9.1288e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14038120210170746 tensor([1.4038e-01, 1.2662e-03, 4.5380e-06, 8.4794e-01, 1.0404e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.36105749011039734 tensor([6.2586e-01, 3.6106e-01, 1.1759e-04, 1.0094e-02, 2.8711e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13265497982501984 tensor([1.0034e-05, 1.1974e-03, 1.3265e-01, 5.8194e-03, 8.6032e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4069671034812927 tensor([1.0087e-04, 3.7165e-05, 1.5645e-04, 4.0697e-01, 5.9274e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18455380201339722 tensor([4.6566e-04, 1.8455e-01, 5.3761e-01, 1.6287e-03, 2.7575e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013619768433272839 tensor([0.0403, 0.0136, 0.0010, 0.6898, 0.2552], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.023599088191986084 tensor([2.2638e-04, 2.3599e-02, 2.2544e-01, 9.7391e-03, 7.4099e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07188306003808975 tensor([7.1883e-02, 3.5442e-03, 5.4052e-05, 8.7080e-01, 5.3719e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13963919878005981 tensor([1.1825e-04, 1.0440e-02, 1.3964e-01, 8.5925e-03, 8.4121e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3210749626159668 tensor([5.3626e-05, 1.1819e-02, 3.2107e-01, 4.5624e-03, 6.6249e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3181377947330475 tensor([4.5392e-05, 4.3089e-06, 3.2287e-05, 6.8178e-01, 3.1814e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0886678472161293 tensor([1.7302e-05, 1.3907e-03, 8.8668e-02, 1.0493e-02, 8.9943e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17900598049163818 tensor([2.8780e-07, 1.3333e-04, 1.7901e-01, 1.2479e-03, 8.1961e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04007907584309578 tensor([4.0079e-02, 7.7919e-03, 6.3090e-04, 7.7456e-01, 1.7694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07696190476417542 tensor([7.6962e-02, 2.0263e-03, 3.3049e-05, 8.8131e-01, 3.9672e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.14299426972866058 tensor([7.2771e-05, 8.9312e-03, 1.4299e-01, 6.4725e-03, 8.4153e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3258936107158661 tensor([1.0877e-04, 3.2589e-01, 6.3303e-01, 6.3624e-05, 4.0904e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10431050509214401 tensor([1.0431e-01, 5.4725e-04, 1.4438e-06, 8.8804e-01, 7.1007e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34284818172454834 tensor([3.1310e-06, 1.0171e-07, 2.0286e-06, 6.5715e-01, 3.4285e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027894577011466026 tensor([0.0167, 0.0279, 0.0076, 0.3200, 0.6277], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.46923935413360596 tensor([0.0058, 0.0043, 0.0026, 0.4692, 0.5180], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24406082928180695 tensor([2.5841e-05, 1.2897e-06, 9.0107e-06, 7.5590e-01, 2.4406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.46229398250579834 tensor([1.0026e-05, 1.5894e-06, 5.0074e-05, 4.6229e-01, 5.3764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05123515427112579 tensor([5.1235e-02, 4.8427e-03, 1.2154e-04, 8.4546e-01, 9.8342e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021175101399421692 tensor([0.0412, 0.0212, 0.0024, 0.5904, 0.3448], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2547508478164673 tensor([3.6394e-04, 3.1754e-05, 5.6343e-05, 7.4480e-01, 2.5475e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04797057434916496 tensor([4.7971e-02, 1.4494e-03, 2.0175e-05, 9.1203e-01, 3.8529e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2577505111694336 tensor([4.4186e-07, 2.9106e-04, 2.5775e-01, 7.9105e-04, 7.4117e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03811580315232277 tensor([3.8116e-02, 2.0776e-04, 1.5307e-06, 9.5431e-01, 7.3660e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07565019279718399 tensor([6.3096e-01, 7.5650e-02, 1.3758e-04, 2.6207e-01, 3.1185e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06989306211471558 tensor([8.2253e-07, 1.5951e-04, 6.9893e-02, 2.9533e-03, 9.2699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02051125094294548 tensor([0.0118, 0.0205, 0.0101, 0.3465, 0.6111], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2489805370569229 tensor([1.1839e-02, 2.1435e-03, 3.7882e-04, 7.3666e-01, 2.4898e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.058931317180395126 tensor([3.5670e-09, 2.1898e-06, 5.8931e-02, 5.6149e-04, 9.4051e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2859383523464203 tensor([1.2690e-03, 2.2369e-04, 2.4837e-04, 7.1232e-01, 2.8594e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1619221270084381 tensor([8.3436e-01, 1.6192e-01, 7.7478e-06, 3.3946e-03, 3.1806e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3396373391151428 tensor([2.3693e-05, 1.5402e-06, 1.2267e-05, 6.6033e-01, 3.3964e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2508297264575958 tensor([1.9259e-02, 3.8723e-03, 4.8951e-04, 7.2555e-01, 2.5083e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 1, 4, 0], [3, 2, 4, 0], [0, 1, 3, 2], [2, 0, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 1, 0], [0, 1, 2, 3], [3, 4, 0, 2], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 1], [2, 3, 4, 0], [0, 1, 3, 2], [0, 1, 3, 2], [0, 3, 1, 4], [2, 3, 0, 1], [3, 2, 4, 0], [2, 0, 1], [2, 1, 0, 4], [3, 0, 4, 1], [2, 1, 4, 0], [3, 0, 4, 2], [2, 1, 4, 0], [1, 2, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [3, 4, 0, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 4, 1], [0, 1, 3, 2], [3, 4, 0, 2], [0, 3, 1, 4], [0, 1, 2, 3], [3, 0, 4, 1], [0, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 3], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 3, 2, 1], [0, 1, 2, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 3, 2], [3, 2, 4, 0], [2, 0, 3, 1], [2, 3, 4], [0, 1, 2, 3], [2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 1, 2], [2, 1, 0, 4], [3, 0, 4], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 0], [3, 0, 2, 4], [3, 0, 2, 1], [0, 1, 2, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 1, 0], [2, 4, 3, 1], [2, 1, 0], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 3, 1], [2, 1, 4, 0], [0, 1, 2, 3], [0, 3, 4, 1], [3, 0, 2, 4], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 3, 1], [2, 4, 1, 3], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3], [2, 4, 3, 1], [0, 1, 3, 2], [3, 0, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [3, 2, 0, 4], [3, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 3, 0, 4], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [3, 0, 4, 1], [1, 0, 2], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 2, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0], [0, 3, 1, 2], [2, 4, 1, 0], [2, 4, 3], [0, 3, 1, 2], [0, 1, 2], [3, 0, 4, 1], [0, 3, 2, 1], [2, 0, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1], [0, 1, 2, 3], [2, 4, 3, 1], [0, 1, 2, 3], [1, 2, 0], [0, 3, 4, 1], [0, 3, 1, 2], [3, 2, 4, 0], [0, 3, 1, 2], [0, 1, 3, 2], [0, 3, 1, 2], [2, 4, 1, 0], [3, 0, 2, 4], [2, 4, 1, 0], [2, 1, 0, 4], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [3, 0, 4], [2, 3, 4, 0], [2, 4, 1, 0], [3, 2, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [2, 0, 3, 1], [2, 0, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [0, 3, 1, 4], [1, 0, 2], [3, 4, 0, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 0], [3, 4, 0, 2], [0, 1, 3, 2], [1, 0, 2, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 1], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 0], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [0, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [2, 0, 3, 1], [2, 1, 0, 4], [2, 1, 0], [0, 1, 3, 2], [3, 4, 2, 0], [2, 1, 0], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4], [1, 2, 0], [2, 1, 0]]\n",
      "[[3], [1], [4], [3, 4], [2, 4], [3, 4], [0], [0], [3], [3], [4], [1], [4], [3], [4], [2, 4], [1], [4], [4], [2], [4], [1], [3, 4], [3], [2], [3], [1], [3], [3], [1], [3], [1], [3], [3], [2], [4], [1], [2], [4], [2], [2, 4], [1], [2], [0], [0, 1], [4], [0], [4], [3], [4], [4], [1], [2], [4], [1], [4], [0, 1], [4], [0], [2], [2], [0], [2], [3], [4], [3], [0], [2], [4], [4], [1], [1], [4], [3], [1, 2], [3], [0], [0], [3], [1], [4], [4], [2], [4], [4], [4], [3], [0], [3, 4], [1], [2], [0], [3], [4], [2], [1], [1], [2], [4], [2], [3], [2], [3], [3], [2], [4], [3], [4], [3], [4], [4], [0], [2, 4], [3], [2, 4], [0], [1], [2], [3], [4], [3, 4], [0, 1], [0], [4], [2], [0], [0], [4], [3], [0, 1], [3], [0], [4], [1], [1, 2], [0], [1], [1], [3], [4], [2, 4], [0], [2], [3], [2], [3, 4], [1], [2], [3], [2], [4], [1], [1], [3, 4], [4], [3], [0, 1], [4], [3, 4], [2], [4], [3, 4], [2], [0], [2], [2, 4], [3], [4], [3], [2, 4], [4], [0], [4], [3, 4], [2], [4], [1], [4], [4], [4], [3], [1], [3], [3], [4], [0], [0], [1], [4], [1, 2], [1], [3], [1], [3], [2], [0], [0], [2], [3, 4], [4], [3, 4], [0], [2], [3, 4], [2], [3, 4], [1], [4], [3], [2], [3], [1], [4], [4], [2], [4], [0], [2], [4], [2], [0, 3], [0], [3], [3], [1], [3, 4], [3], [2], [4], [2, 4], [3], [0], [0], [3], [4], [0], [0], [4], [3], [3, 4], [4], [1], [3, 4], [0], [1], [2], [1], [2], [3, 4], [3, 4]]\n",
      "NL_pred of 5th iteration [[0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 3, 0, 1], [2, 0, 1], [2, 1, 0, 4], [3, 0, 4, 1], [2, 1, 4, 0], [1, 2, 0, 4], [2, 1, 0, 4], [0, 3, 4, 1], [3, 0, 4, 1], [2, 4, 3, 1], [2, 0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [2, 1, 4, 0], [3, 0, 2, 1], [0, 3, 1, 2], [2, 4, 1, 0], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 0, 4], [0, 3, 4, 1], [2, 1, 0, 4], [2, 0, 3, 1], [0, 3, 1], [0, 3, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 2], [0, 3, 1], [3, 0, 4, 1], [0, 3, 1, 2], [2, 4, 1, 0], [0, 3, 1, 2], [3, 0, 4, 1], [2, 0, 1], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 1, 0], [2, 4, 1, 0], [0, 3, 1, 2], [2, 4, 1, 0], [2, 0, 3, 1], [2, 4, 1, 0], [2, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 1, 2], [2, 0, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.020470241368827174  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.020452004368022338  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.020418225708654373  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.020371924012394276  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.02031625529467049  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.020254266464104085  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.020188608412015235  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.02012151580745891  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.020054855589139258  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.019990147170373947  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.019928330082004352  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.019870050882889054  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.019815622749975174  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.019765403311131365  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.019719358217918266  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.01967728340019614  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.019638956603357346  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.01960413536782992  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.019572552988084697  Accuracy on Support set:0.0\n",
      "torch.Size([59, 2048]) torch.Size([59])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.019543922553628176  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 1, 4, 0], [3, 2, 4, 0], [0, 1, 3, 2], [2, 0, 1], [0, 3, 1], [0, 1, 2], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4, 0], [2, 4, 1, 0], [0, 1, 2, 3], [3, 4, 0, 2], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1, 2], [0, 3, 1], [2, 3, 4, 0], [0, 1, 3, 2], [0, 1, 3, 2], [0, 3, 1, 4], [2, 3, 0, 1], [3, 2, 4, 0], [2, 0, 1], [2, 1, 0, 4], [3, 0, 4, 1], [2, 1, 4, 0], [3, 0, 4, 2], [2, 1, 4, 0], [1, 2, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [3, 4, 0, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 4, 1], [0, 1, 3, 2], [3, 4, 0, 2], [0, 3, 1, 4], [0, 1, 2, 3], [3, 0, 4, 1], [0, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 3], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 3, 2, 1], [0, 1, 2, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 3, 2], [3, 2, 4, 0], [2, 0, 3, 1], [2, 3, 4], [0, 1, 2, 3], [2, 4, 3, 1], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 2], [2, 1, 4, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 1, 2], [2, 1, 0, 4], [3, 0, 4], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 3, 1], [2, 1, 4, 0], [3, 0, 2, 4], [3, 0, 2, 1], [0, 1, 2, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 1, 0], [2, 4, 3, 1], [2, 1, 0], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 3, 1], [2, 1, 4, 0], [0, 1, 2, 3], [0, 3, 4, 1], [3, 0, 2, 4], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 4, 1], [2, 1, 4, 0], [2, 1, 4, 0], [0, 3, 4, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 3, 1], [2, 4, 1, 3], [0, 3, 1], [2, 1, 0, 4], [0, 3, 1], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3], [2, 4, 3, 1], [0, 1, 3, 2], [3, 0, 4, 1], [2, 4, 1, 3], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 3], [2, 1, 0, 4], [2, 4, 3, 1], [0, 3, 1, 2], [3, 2, 0, 4], [3, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 3, 0, 4], [2, 1, 0, 4], [0, 1, 3, 2], [0, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 1, 0, 4], [3, 0, 4, 1], [1, 0, 2], [2, 3, 4, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1, 4], [0, 2, 1, 3], [3, 4, 2, 0], [3, 0, 4, 2], [1, 2, 0], [0, 3, 1, 2], [2, 4, 1, 0], [2, 4, 3], [0, 3, 1, 2], [0, 1, 2], [3, 0, 4, 1], [0, 3, 2, 1], [2, 0, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [0, 3, 1], [2, 4, 1, 0], [0, 3, 1, 2], [2, 1, 0, 4], [0, 3, 1], [0, 1, 2, 3], [2, 4, 3, 1], [0, 1, 2, 3], [1, 2, 0], [0, 3, 4, 1], [0, 3, 1, 2], [3, 2, 4, 0], [0, 3, 1, 2], [0, 1, 3, 2], [0, 3, 1, 2], [2, 4, 1, 0], [3, 0, 2, 4], [2, 4, 1, 0], [2, 1, 0, 4], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 1, 2, 3], [3, 0, 4], [2, 3, 4, 0], [2, 4, 1, 0], [3, 2, 4, 0], [2, 1, 0, 4], [3, 0, 4, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [2, 0, 3, 1], [2, 0, 1], [2, 4, 3, 1], [0, 3, 1, 4], [1, 2, 0], [0, 3, 1, 4], [1, 0, 2], [3, 4, 0, 2], [0, 1, 3, 2], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 0], [3, 4, 0, 2], [0, 1, 3, 2], [1, 0, 2, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 4, 1], [2, 4, 1], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 0, 4], [3, 0, 4, 2], [2, 1, 0], [2, 1, 4, 0], [0, 3, 1, 4], [0, 1, 3, 2], [0, 3, 1], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 1], [2, 1, 4, 0], [0, 3, 1, 2], [2, 4, 1, 3], [2, 4, 3, 1], [2, 0, 3, 1], [2, 1, 0, 4], [2, 1, 0], [0, 1, 3, 2], [3, 4, 2, 0], [2, 1, 0], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 1, 4], [1, 2, 0], [2, 1, 0]]\n",
      "POSITION :  [[3], [1], [4], [3, 4], [2, 4], [3, 4], [0], [0], [3], [3], [4], [1], [4], [3], [4], [2, 4], [1], [4], [4], [2], [4], [1], [3, 4], [3], [2], [3], [1], [3], [3], [1], [3], [1], [3], [3], [2], [4], [1], [2], [4], [2], [2, 4], [1], [2], [0], [0, 1], [4], [0], [4], [3], [4], [4], [1], [2], [4], [1], [4], [0, 1], [4], [0], [2], [2], [0], [2], [3], [4], [3], [0], [2], [4], [4], [1], [1], [4], [3], [1, 2], [3], [0], [0], [3], [1], [4], [4], [2], [4], [4], [4], [3], [0], [3, 4], [1], [2], [0], [3], [4], [2], [1], [1], [2], [4], [2], [3], [2], [3], [3], [2], [4], [3], [4], [3], [4], [4], [0], [2, 4], [3], [2, 4], [0], [1], [2], [3], [4], [3, 4], [0, 1], [0], [4], [2], [0], [0], [4], [3], [0, 1], [3], [0], [4], [1], [1, 2], [0], [1], [1], [3], [4], [2, 4], [0], [2], [3], [2], [3, 4], [1], [2], [3], [2], [4], [1], [1], [3, 4], [4], [3], [0, 1], [4], [3, 4], [2], [4], [3, 4], [2], [0], [2], [2, 4], [3], [4], [3], [2, 4], [4], [0], [4], [3, 4], [2], [4], [1], [4], [4], [4], [3], [1], [3], [3], [4], [0], [0], [1], [4], [1, 2], [1], [3], [1], [3], [2], [0], [0], [2], [3, 4], [4], [3, 4], [0], [2], [3, 4], [2], [3, 4], [1], [4], [3], [2], [3], [1], [4], [4], [2], [4], [0], [2], [4], [2], [0, 3], [0], [3], [3], [1], [3, 4], [3], [2], [4], [2, 4], [3], [0], [0], [3], [4], [0], [0], [4], [3], [3, 4], [4], [1], [3, 4], [0], [1], [2], [1], [2], [3, 4], [3, 4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.396\n",
      "tensor([3, 1, 4, 0, 0, 3, 3, 4, 1, 4, 3, 4, 1, 4, 4, 2, 4, 1, 3, 2, 3, 1, 3, 3,\n",
      "        1, 3, 1, 3, 3, 2, 4, 1, 2, 4, 2, 1, 2, 0, 4, 0, 4, 3, 4, 4, 1, 2, 4, 1,\n",
      "        4, 4, 0, 2, 2, 0, 2, 3, 4, 3, 0, 2, 4, 4, 1, 1, 4, 3, 3, 0, 0, 3, 1, 4,\n",
      "        4, 2, 4, 4, 4, 3, 0, 1, 2, 0, 3, 4, 2, 1, 1, 2, 4, 2, 3, 2, 3, 3, 2, 4,\n",
      "        3, 4, 3, 4, 4, 0, 3, 0, 1, 2, 3, 4, 0, 4, 2, 0, 0, 4, 3, 3, 0, 4, 1, 0,\n",
      "        1, 1, 3, 4, 0, 2, 3, 2, 1, 2, 3, 2, 4, 1, 1, 4, 3, 4, 2, 4, 2, 0, 2, 3,\n",
      "        4, 3, 4, 0, 4, 2, 4, 1, 4, 4, 4, 3, 1, 3, 3, 4, 0, 0, 1, 4, 1, 3, 1, 3,\n",
      "        2, 0, 0, 2, 4, 0, 2, 2, 1, 4, 3, 2, 3, 1, 4, 4, 2, 4, 0, 2, 4, 2, 0, 3,\n",
      "        3, 1, 3, 2, 4, 3, 0, 0, 3, 4, 0, 0, 4, 3, 4, 1, 0, 1, 2, 1, 2])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 90.14084507042254\n",
      "Epoch: 1  Loss: 94.83568075117371\n",
      "Epoch: 2  Loss: 97.65258215962442\n",
      "Epoch: 3  Loss: 99.53051643192488\n",
      "Epoch: 4  Loss: 100.0\n",
      "Epoch: 5  Loss: 100.0\n",
      "Epoch: 6  Loss: 100.0\n",
      "Epoch: 7  Loss: 100.0\n",
      "Epoch: 8  Loss: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 13/15 [10:50<01:47, 53.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 100.0\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  30.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 6.124636462181806  Accuracy on Support set:40.0\n",
      "Train_Epoch: 1  Train_Loss: 2.785647881999612  Accuracy on Support set:52.0\n",
      "Train_Epoch: 2  Train_Loss: 1.4996177181601524  Accuracy on Support set:64.0\n",
      "Train_Epoch: 3  Train_Loss: 0.9683943492174149  Accuracy on Support set:76.0\n",
      "Train_Epoch: 4  Train_Loss: 0.5902735969424248  Accuracy on Support set:84.0\n",
      "Train_Epoch: 5  Train_Loss: 0.4177507632225752  Accuracy on Support set:96.0\n",
      "Train_Epoch: 6  Train_Loss: 0.3009062276780605  Accuracy on Support set:100.0\n",
      "Train_Epoch: 7  Train_Loss: 0.1920010793209076  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.14836258679628372  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.12317625053226948  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.10813193082809448  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.0966328277438879  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.08745617926120758  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.08011929221451282  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.07391974925994874  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.06870968744158745  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.0642835995554924  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.06036282770335674  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.05693477675318718  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.05389530695974827  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.05117840349674225  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.04872610371559858  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.04651381630450487  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.04447752356529236  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.0426358475908637  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.040944247283041475  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.0393713528290391  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.037935784719884395  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.0365990087389946  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.03535925712436438  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.03419774658977985  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.033121005296707154  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.03209805965423584  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.031142367422580718  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.030251991674304007  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.029398501589894294  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.028600866608321666  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.0278469243273139  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.02711951479315758  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.026430717706680297  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.025770855247974397  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.025145714953541756  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.024553578309714795  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.023980778940021992  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.023431406803429127  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.022908847369253634  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.02241063442081213  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.02193421419709921  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.02147642821073532  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.021042834147810937  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  32.666666666666664\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.0024577102158218622 tensor([0.0548, 0.9333, 0.0056, 0.0025, 0.0037], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00025453203124925494 tensor([2.3224e-01, 7.6600e-01, 2.5453e-04, 1.2520e-03, 2.5964e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008931676857173443 tensor([0.2290, 0.7637, 0.0009, 0.0047, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.366124918917194e-05 tensor([7.6870e-01, 2.0487e-01, 7.3661e-05, 2.5633e-02, 7.2499e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.991475842710315e-09 tensor([5.3599e-02, 1.2728e-05, 6.9915e-09, 9.4604e-01, 3.5241e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01597806252539158 tensor([0.1319, 0.2949, 0.0160, 0.3473, 0.2099], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.940927854273468e-05 tensor([3.2842e-04, 3.9409e-05, 7.7233e-05, 7.9884e-01, 2.0072e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2778625091414142e-07 tensor([2.0856e-03, 3.8532e-06, 1.2779e-07, 9.9471e-01, 3.2038e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.129979523284419e-06 tensor([9.2473e-01, 7.3167e-02, 1.1300e-06, 2.0836e-03, 1.6868e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3874307114747353e-05 tensor([6.7384e-01, 1.9649e-02, 1.3874e-05, 3.0341e-01, 3.0857e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.8142193286839756e-07 tensor([9.6187e-01, 3.6205e-02, 2.8142e-07, 1.9202e-03, 8.3199e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.033944208174943924 tensor([0.0399, 0.7509, 0.0578, 0.0339, 0.1175], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002137034432962537 tensor([0.0021, 0.1847, 0.3308, 0.0224, 0.4599], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008333927020430565 tensor([0.1832, 0.7118, 0.0083, 0.0531, 0.0435], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013940883800387383 tensor([0.0014, 0.1796, 0.3392, 0.0128, 0.4669], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.534481831797166e-06 tensor([6.2636e-01, 3.7333e-01, 4.5345e-06, 2.9696e-04, 1.4837e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.7678046535584144e-05 tensor([4.7372e-03, 2.1586e-04, 2.7678e-05, 9.3239e-01, 6.2627e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2601536525380652e-07 tensor([1.2602e-07, 1.5097e-05, 1.6607e-02, 3.7392e-03, 9.7964e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021639224141836166 tensor([0.0420, 0.8301, 0.0414, 0.0216, 0.0648], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8852460420836223e-09 tensor([2.8852e-09, 2.0113e-04, 9.8573e-01, 2.2320e-06, 1.4069e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1322112964128337e-09 tensor([9.5379e-01, 3.4723e-04, 2.1322e-09, 4.5854e-02, 7.3250e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02064158208668232 tensor([0.1641, 0.4879, 0.0206, 0.1883, 0.1391], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.013332187198102474 tensor([0.0133, 0.5499, 0.1874, 0.0284, 0.2210], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.548337872416596e-06 tensor([3.5483e-06, 7.4353e-03, 7.6093e-01, 4.9886e-04, 2.3113e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004007274750620127 tensor([0.0047, 0.6983, 0.2072, 0.0040, 0.0857], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000678854004945606 tensor([2.6793e-01, 7.2413e-01, 6.7885e-04, 5.5200e-03, 1.7450e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0978056707244832e-05 tensor([1.0978e-05, 8.4063e-04, 5.7460e-02, 1.2208e-02, 9.2948e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012067325413227081 tensor([0.0215, 0.8306, 0.0835, 0.0121, 0.0523], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2482164493121672e-05 tensor([1.9809e-05, 2.2104e-01, 7.7491e-01, 1.2482e-05, 4.0145e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.8115318855270743e-06 tensor([1.4772e-04, 2.8115e-06, 3.2894e-06, 9.3803e-01, 6.1816e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3635624327434925e-06 tensor([7.9944e-01, 8.1471e-03, 1.3636e-06, 1.9153e-01, 8.7950e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003569318214431405 tensor([5.7172e-01, 4.0682e-01, 3.5693e-04, 1.9666e-02, 1.4398e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.375977636859261e-08 tensor([4.2698e-02, 2.0525e-05, 2.3760e-08, 9.5670e-01, 5.8045e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00011285670916549861 tensor([7.1246e-02, 9.2810e-01, 3.5036e-04, 1.9197e-04, 1.1286e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.229555881669512e-07 tensor([5.2296e-07, 2.1973e-05, 6.6649e-03, 7.8395e-03, 9.8547e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.214755896711722e-05 tensor([6.6725e-01, 6.7617e-02, 7.2148e-05, 2.5809e-01, 6.9646e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.7949176001129672e-05 tensor([1.8910e-04, 6.8735e-01, 3.1051e-01, 2.7949e-05, 1.9192e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.86809653416276e-05 tensor([6.8681e-05, 1.6063e-04, 1.9008e-03, 1.9834e-01, 7.9953e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002496684028301388 tensor([2.6686e-01, 3.0684e-02, 2.4967e-04, 6.6027e-01, 4.1933e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.572219065972604e-05 tensor([8.5179e-01, 1.2331e-01, 3.5722e-05, 2.4187e-02, 6.6941e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00030486524337902665 tensor([1.7654e-02, 9.7691e-01, 4.1731e-03, 3.0487e-04, 9.5699e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.696316707646474e-05 tensor([7.8929e-01, 4.8001e-02, 3.6963e-05, 1.5871e-01, 3.9627e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012503105972427875 tensor([1.2503e-04, 6.0510e-03, 9.2373e-02, 3.1569e-02, 8.6988e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0026750292163342237 tensor([0.0027, 0.0854, 0.1382, 0.0616, 0.7121], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.281024933381559e-07 tensor([2.2810e-07, 1.9127e-06, 9.6691e-04, 2.9389e-02, 9.6964e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019371498376131058 tensor([0.0194, 0.1015, 0.0312, 0.3039, 0.5440], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002462651114910841 tensor([0.0255, 0.9448, 0.0208, 0.0025, 0.0065], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003490751550998539 tensor([6.0293e-03, 9.6464e-01, 2.6197e-02, 3.4908e-04, 2.7859e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4627572202007286e-05 tensor([2.4628e-05, 8.0987e-02, 8.8696e-01, 1.1843e-04, 3.1914e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.2047983040683903e-06 tensor([3.2048e-06, 1.2301e-05, 1.0812e-03, 6.7858e-02, 9.3105e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.531841125641421e-10 tensor([9.7480e-01, 1.3043e-04, 2.5318e-10, 2.5065e-02, 1.4906e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010134657844901085 tensor([0.1098, 0.8526, 0.0101, 0.0151, 0.0123], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007025043596513569 tensor([0.0007, 0.0008, 0.0022, 0.4187, 0.5775], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.7905381304881303e-06 tensor([4.7747e-06, 1.1331e-01, 8.8385e-01, 3.7905e-06, 2.8353e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.2653551329531183e-07 tensor([6.0188e-06, 3.2654e-07, 5.0957e-06, 6.4741e-01, 3.5258e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3999023167343694e-06 tensor([7.9996e-01, 8.1891e-03, 1.3999e-06, 1.9115e-01, 7.0369e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.3159543022520666e-07 tensor([3.3160e-07, 3.9213e-03, 9.6603e-01, 2.4877e-05, 3.0026e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00764043815433979 tensor([0.0076, 0.2854, 0.1787, 0.0498, 0.4785], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001451062853448093 tensor([0.4832, 0.4078, 0.0015, 0.0913, 0.0162], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7977636446175893e-07 tensor([1.7978e-07, 2.2203e-03, 9.6214e-01, 2.4291e-05, 3.5619e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.990079681490897e-06 tensor([8.5897e-01, 1.3970e-01, 1.9901e-06, 1.3041e-03, 2.1100e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.325727043877123e-05 tensor([1.3257e-05, 5.6366e-03, 3.7902e-01, 4.0922e-03, 6.1123e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0280063495192735e-07 tensor([2.0280e-07, 1.2922e-03, 9.0324e-01, 7.3649e-05, 9.5393e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010108058340847492 tensor([0.0161, 0.9592, 0.0185, 0.0010, 0.0052], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00032945213024504483 tensor([3.2945e-04, 4.1364e-02, 2.6645e-01, 1.3105e-02, 6.7876e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.200529292575084e-05 tensor([6.6161e-01, 3.3204e-01, 4.2005e-05, 5.9500e-03, 3.5889e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001919755944982171 tensor([0.0019, 0.0162, 0.0448, 0.1821, 0.7550], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.783569107123185e-05 tensor([1.7627e-02, 6.9177e-04, 2.7836e-05, 9.5631e-01, 2.5346e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017968518659472466 tensor([1.1779e-02, 9.8254e-01, 4.9138e-03, 1.7969e-04, 5.8501e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2380723092064727e-05 tensor([1.1697e-03, 3.6488e-05, 1.2381e-05, 9.3445e-01, 6.4331e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.343761131371139e-06 tensor([6.7032e-01, 1.0454e-02, 5.3438e-06, 3.1715e-01, 2.0755e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.005912960506975651 tensor([0.1231, 0.1234, 0.0059, 0.6120, 0.1356], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00033770487061701715 tensor([4.6918e-03, 9.4842e-01, 4.3244e-02, 3.3770e-04, 3.3074e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000990669010207057 tensor([0.1394, 0.8564, 0.0010, 0.0021, 0.0010], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.756745468157078e-09 tensor([8.9001e-03, 3.0493e-06, 7.7567e-09, 9.9035e-01, 7.4822e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.782577534001575e-08 tensor([9.8094e-01, 1.8065e-02, 4.7826e-08, 9.9265e-04, 2.8104e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9406023713818286e-06 tensor([2.9406e-06, 4.1409e-02, 9.5071e-01, 1.3915e-05, 7.8684e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007809475064277649 tensor([0.0403, 0.0563, 0.0078, 0.6302, 0.2654], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0071171969175338745 tensor([0.0071, 0.1040, 0.0877, 0.1092, 0.6920], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00857411790639162 tensor([0.0086, 0.0751, 0.0427, 0.1518, 0.7218], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.751423616193961e-09 tensor([9.9549e-01, 2.3434e-03, 3.7514e-09, 2.1656e-03, 1.1158e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008571451529860497 tensor([0.0086, 0.4685, 0.2306, 0.0307, 0.2616], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1331540501232666e-07 tensor([4.1332e-07, 3.5680e-04, 2.4704e-01, 1.3038e-03, 7.5130e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013568919384852052 tensor([0.0579, 0.9355, 0.0035, 0.0014, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006354032666422427 tensor([6.3540e-04, 2.1032e-03, 5.9086e-03, 1.9987e-01, 7.9148e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014815287431702018 tensor([0.0015, 0.2870, 0.4164, 0.0077, 0.2874], grad_fn=<SoftmaxBackward0>)\n",
      "0 8.401807463087607e-06 tensor([8.4018e-06, 6.9053e-02, 9.1995e-01, 2.6841e-05, 1.0965e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007217550883069634 tensor([7.2176e-04, 4.4934e-03, 1.9729e-02, 2.0569e-01, 7.6937e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.800760396188707e-06 tensor([3.8008e-06, 6.4092e-03, 7.8938e-01, 5.8534e-04, 2.0362e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006717966869473457 tensor([0.2682, 0.4641, 0.0067, 0.1729, 0.0881], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0063797286711633205 tensor([0.0064, 0.2926, 0.1811, 0.0293, 0.4907], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002560499415267259 tensor([2.5605e-04, 1.0755e-01, 7.0483e-01, 2.7294e-03, 1.8464e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.018417300656437874 tensor([0.1645, 0.4638, 0.0184, 0.2253, 0.1280], grad_fn=<SoftmaxBackward0>)\n",
      "2 3.033502025573398e-06 tensor([9.0019e-01, 9.6370e-02, 3.0335e-06, 3.3918e-03, 4.8834e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001973457692656666 tensor([1.9735e-04, 1.7392e-03, 1.4506e-02, 8.9519e-02, 8.9404e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.673902822105447e-06 tensor([6.5011e-01, 9.4739e-03, 4.6739e-06, 3.3819e-01, 2.2190e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.93463662173599e-05 tensor([2.0409e-04, 5.7160e-01, 4.2234e-01, 6.9346e-05, 5.7869e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8001855561777802e-10 tensor([1.8002e-10, 7.5344e-05, 9.9577e-01, 1.9469e-07, 4.1531e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.72932806587778e-05 tensor([1.1590e-02, 9.8616e-01, 2.0482e-03, 4.7293e-05, 1.5083e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.4602737792301923e-06 tensor([1.4603e-06, 9.8889e-04, 3.3520e-01, 1.8447e-03, 6.6196e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.028966518118977547 tensor([0.0616, 0.5470, 0.0290, 0.0932, 0.2693], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00031673270859755576 tensor([1.7590e-02, 2.4426e-03, 3.1673e-04, 8.5195e-01, 1.2771e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004194171924609691 tensor([9.4587e-03, 2.3060e-03, 4.1942e-04, 8.4447e-01, 1.4334e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.617534669814631e-05 tensor([7.5405e-01, 9.0785e-02, 9.6175e-05, 1.4970e-01, 5.3705e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.519667946733534e-05 tensor([9.5197e-05, 1.0342e-04, 7.2288e-04, 2.3522e-01, 7.6386e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.261313006281853e-05 tensor([3.3105e-01, 6.6831e-01, 4.2613e-05, 5.2910e-04, 6.5525e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001479125116020441 tensor([0.0058, 0.0031, 0.0015, 0.7143, 0.2753], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00039424787973985076 tensor([6.0541e-01, 3.4157e-01, 3.9425e-04, 4.9639e-02, 2.9808e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00035246534389443696 tensor([3.5247e-04, 1.7354e-02, 1.2063e-01, 2.4467e-02, 8.3720e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.6931262431826326e-08 tensor([4.6931e-08, 2.3310e-06, 3.2380e-03, 4.6907e-03, 9.9207e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5547686749162182e-10 tensor([9.9383e-01, 2.4128e-04, 1.5548e-10, 5.9284e-03, 4.9618e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006368185277096927 tensor([0.5926, 0.2343, 0.0006, 0.1619, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009064070181921124 tensor([0.0009, 0.0042, 0.0135, 0.2468, 0.7346], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00010681888670660555 tensor([1.0682e-04, 3.3801e-02, 5.5824e-01, 5.0359e-03, 4.0282e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005971692153252661 tensor([5.9717e-04, 5.1357e-02, 2.7002e-01, 2.6284e-02, 6.5175e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009151959675364196 tensor([0.2740, 0.7114, 0.0009, 0.0110, 0.0027], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011339517310261726 tensor([0.1310, 0.7939, 0.0113, 0.0391, 0.0245], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00025404084590263665 tensor([1.9838e-02, 9.7475e-01, 4.6418e-03, 2.5404e-04, 5.1590e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002902144333347678 tensor([0.0029, 0.2280, 0.3229, 0.0211, 0.4250], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3085229258867912e-05 tensor([6.3766e-03, 2.1623e-04, 2.3085e-05, 9.0166e-01, 9.1722e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6654514638503315e-06 tensor([1.6655e-06, 6.3907e-05, 1.1851e-02, 1.6620e-02, 9.7146e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009510897099971771 tensor([0.0156, 0.0261, 0.0095, 0.5753, 0.3734], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.583609501831234e-05 tensor([6.5836e-05, 1.0849e-02, 2.5985e-01, 8.8745e-03, 7.2036e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009384588338434696 tensor([0.3180, 0.0999, 0.0009, 0.5050, 0.0762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00019998200878035277 tensor([7.1920e-02, 8.8413e-03, 1.9998e-04, 8.5199e-01, 6.7052e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4490769695285621e-11 tensor([9.9928e-01, 2.7380e-04, 1.4491e-11, 4.4240e-04, 1.8294e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.1726742640312295e-06 tensor([9.0120e-01, 2.1123e-02, 5.1727e-06, 7.7253e-02, 4.1433e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009708101861178875 tensor([0.0264, 0.0088, 0.0010, 0.7736, 0.1902], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006022764835506678 tensor([0.0060, 0.6136, 0.2207, 0.0090, 0.1507], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0532706085086829e-07 tensor([1.0533e-07, 5.9231e-03, 9.9050e-01, 1.5373e-06, 3.5733e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.5112940877677374e-09 tensor([9.9407e-01, 4.9431e-03, 5.5113e-09, 9.8392e-04, 8.1174e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2639465012398432e-06 tensor([6.8547e-01, 3.8997e-03, 1.2639e-06, 3.0985e-01, 7.8335e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016832632245495915 tensor([0.0017, 0.0166, 0.0297, 0.1246, 0.8275], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.6974686988978647e-05 tensor([2.6975e-05, 9.9861e-04, 3.5435e-02, 2.3347e-02, 9.4019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.681716280989349e-06 tensor([6.3124e-04, 2.0489e-05, 8.6817e-06, 9.1713e-01, 8.2214e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1705633795600079e-08 tensor([9.9025e-01, 2.2796e-03, 1.1706e-08, 7.4616e-03, 3.9419e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006291742902249098 tensor([0.1629, 0.8015, 0.0063, 0.0195, 0.0097], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002128786873072386 tensor([0.0156, 0.9381, 0.0321, 0.0021, 0.0120], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.28827432997059e-06 tensor([8.6332e-01, 1.3117e-01, 8.2883e-06, 5.4115e-03, 8.9489e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.553025974018965e-07 tensor([4.5530e-07, 4.9601e-04, 3.6195e-01, 9.4410e-04, 6.3661e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010555010288953781 tensor([0.0168, 0.0367, 0.0106, 0.4375, 0.4985], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005526269669644535 tensor([5.5263e-04, 1.0484e-03, 3.8717e-03, 3.1938e-01, 6.7515e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00020359110203571618 tensor([2.6125e-03, 9.1698e-01, 7.6892e-02, 2.0359e-04, 3.3097e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004658227786421776 tensor([0.2142, 0.7258, 0.0047, 0.0391, 0.0162], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.8779034841572866e-05 tensor([7.2969e-04, 6.2047e-05, 4.8779e-05, 7.9935e-01, 1.9981e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015201998176053166 tensor([0.0243, 0.0139, 0.0015, 0.7545, 0.2058], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01716260425746441 tensor([0.0172, 0.7700, 0.1001, 0.0203, 0.0925], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7407521585587347e-08 tensor([2.7408e-08, 3.3598e-04, 9.0954e-01, 4.6032e-05, 9.0079e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001637879031477496 tensor([6.5300e-01, 3.2841e-01, 1.6379e-04, 1.7195e-02, 1.2329e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2737665858585387e-05 tensor([2.2738e-05, 3.1202e-03, 1.4229e-01, 8.6099e-03, 8.4595e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011468425393104553 tensor([0.0886, 0.8458, 0.0115, 0.0208, 0.0334], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014251037500798702 tensor([0.0014, 0.0291, 0.0717, 0.0698, 0.8280], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7621921870158985e-05 tensor([1.7622e-05, 5.7958e-02, 9.0140e-01, 1.5020e-04, 4.0473e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.119621422025375e-05 tensor([2.1196e-05, 2.2941e-03, 9.5726e-02, 1.0437e-02, 8.9152e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.747678599732353e-08 tensor([9.8768e-01, 6.9738e-03, 5.7477e-08, 5.3411e-03, 8.3087e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014039166271686554 tensor([0.0140, 0.6026, 0.1267, 0.0309, 0.2257], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00014668796211481094 tensor([7.2328e-01, 2.5328e-01, 1.4669e-04, 2.2166e-02, 1.1257e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00020261286408640444 tensor([6.8412e-01, 2.6630e-01, 2.0261e-04, 4.6804e-02, 2.5731e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009455093182623386 tensor([0.0641, 0.8942, 0.0126, 0.0095, 0.0196], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.846139876870438e-05 tensor([7.6945e-01, 8.0083e-02, 6.8461e-05, 1.4687e-01, 3.5205e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005574295064434409 tensor([1.3019e-01, 8.6765e-01, 5.5743e-04, 1.0116e-03, 5.9351e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.5730089231169586e-08 tensor([1.5730e-08, 2.5211e-06, 1.0975e-02, 2.1660e-03, 9.8686e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3727190637146123e-05 tensor([3.3862e-05, 2.4164e-01, 7.5168e-01, 2.3727e-05, 6.6166e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014458881923928857 tensor([0.3473, 0.1301, 0.0014, 0.4439, 0.0774], grad_fn=<SoftmaxBackward0>)\n",
      "1 7.029003609204665e-05 tensor([1.5361e-04, 7.0290e-05, 4.0569e-04, 5.1528e-01, 4.8409e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.606406894287375e-09 tensor([9.5237e-01, 5.8302e-04, 5.6064e-09, 4.7037e-02, 1.0633e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.9253855043643853e-06 tensor([1.9254e-06, 8.9344e-06, 9.6548e-04, 5.0281e-02, 9.4874e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.592426764953416e-07 tensor([4.8966e-02, 1.0308e-04, 3.5924e-07, 9.4941e-01, 1.5168e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.4230728740803897e-05 tensor([2.5823e-04, 2.4231e-05, 5.4177e-05, 7.7660e-01, 2.2306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.979463357012719e-06 tensor([9.9795e-06, 1.4550e-02, 7.8187e-01, 6.0764e-04, 2.0296e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1040505114578991e-06 tensor([9.5988e-01, 2.5324e-02, 1.1041e-06, 1.4713e-02, 8.3890e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.8479348657128867e-06 tensor([3.6526e-03, 3.8014e-05, 2.8479e-06, 9.8218e-01, 1.4124e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.613174897054705e-07 tensor([3.6132e-07, 1.5232e-03, 8.8861e-01, 1.7200e-04, 1.0969e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.477500861976296e-05 tensor([6.6514e-01, 3.2161e-01, 9.4775e-05, 1.2542e-02, 6.1293e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0251286539642024e-06 tensor([1.0251e-06, 3.4841e-04, 1.2958e-01, 2.7620e-03, 8.6731e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008440185920335352 tensor([0.0034, 0.0015, 0.0008, 0.6389, 0.3554], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.3265277781756595e-05 tensor([4.3265e-05, 3.4652e-03, 1.0412e-01, 1.8109e-02, 8.7427e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.372790721798083e-06 tensor([6.3728e-06, 1.4888e-02, 9.0196e-01, 2.8090e-04, 8.2864e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.6807143512996845e-05 tensor([2.6807e-05, 5.1783e-03, 2.2522e-01, 5.7403e-03, 7.6383e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0368170478614047e-05 tensor([1.0368e-05, 2.6935e-04, 1.5711e-02, 2.7434e-02, 9.5658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002125755709130317 tensor([4.0315e-01, 5.9089e-01, 2.1258e-04, 4.9414e-03, 8.0550e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007313332171179354 tensor([0.6131, 0.2392, 0.0007, 0.1356, 0.0114], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002571012591943145 tensor([2.9612e-02, 9.6776e-01, 1.9547e-03, 2.5710e-04, 4.1519e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03180966153740883 tensor([0.0640, 0.7905, 0.0318, 0.0423, 0.0713], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.09630243830361e-07 tensor([1.7698e-01, 2.8782e-04, 2.0963e-07, 8.2169e-01, 1.0371e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.503041130490601e-05 tensor([7.5872e-01, 2.3739e-01, 1.5030e-05, 3.7497e-03, 1.2624e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00863499753177166 tensor([0.0612, 0.8990, 0.0142, 0.0086, 0.0169], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.9115406025302946e-08 tensor([3.9115e-08, 1.5970e-03, 9.9025e-01, 3.2494e-06, 8.1518e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.8054217434837483e-05 tensor([6.4293e-04, 9.4376e-01, 5.4939e-02, 1.8054e-05, 6.4217e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007297976408153772 tensor([7.2980e-04, 2.4889e-02, 8.3246e-02, 4.6427e-02, 8.4471e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00025572077720426023 tensor([6.2837e-01, 3.1517e-01, 2.5572e-04, 5.1141e-02, 5.0597e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.988388435682282e-05 tensor([6.8841e-02, 4.3976e-03, 8.9884e-05, 8.9742e-01, 2.9250e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0005852513131685555 tensor([1.9758e-01, 4.2133e-02, 5.8525e-04, 6.9998e-01, 5.9725e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.947788203135133e-05 tensor([7.8832e-01, 1.7848e-01, 9.9478e-05, 3.1555e-02, 1.5458e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002950068796053529 tensor([0.0557, 0.0381, 0.0030, 0.6080, 0.2953], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0028466214425861835 tensor([0.0491, 0.0450, 0.0028, 0.6139, 0.2891], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005173258017748594 tensor([0.0052, 0.1353, 0.1642, 0.0691, 0.6263], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.000381824211217463 tensor([2.6032e-02, 9.6947e-01, 3.2472e-03, 3.8182e-04, 8.7236e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019841312314383686 tensor([6.2400e-03, 9.7524e-01, 1.6990e-02, 1.9841e-04, 1.3291e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.137571541993566e-08 tensor([2.1376e-08, 5.5602e-05, 3.2879e-01, 4.1230e-04, 6.7074e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00030755475745536387 tensor([7.0160e-04, 6.7704e-01, 3.0569e-01, 3.0755e-04, 1.6263e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002715532900765538 tensor([2.7155e-04, 1.0026e-02, 1.0262e-01, 4.4688e-02, 8.4240e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.514445052132942e-05 tensor([3.5144e-05, 3.7002e-03, 1.1199e-01, 1.4182e-02, 8.7009e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.6221557315438986e-05 tensor([9.1583e-03, 9.8754e-01, 3.0327e-03, 5.6222e-05, 2.1084e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.134753104764968e-06 tensor([7.1348e-06, 6.8109e-05, 3.3907e-03, 3.1360e-02, 9.6517e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4117036073457712e-07 tensor([9.2399e-01, 7.5791e-02, 1.4117e-07, 2.1912e-04, 1.4669e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.544515720932395e-07 tensor([6.6195e-03, 1.9894e-05, 3.5445e-07, 9.8969e-01, 3.6699e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010192703921347857 tensor([0.1316, 0.0394, 0.0010, 0.7476, 0.0803], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.750154327368364e-05 tensor([8.1529e-01, 1.2009e-01, 7.7502e-05, 6.2526e-02, 2.0200e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004485856625251472 tensor([4.4859e-04, 1.4296e-02, 6.7599e-02, 4.4076e-02, 8.7358e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.090667400509119e-05 tensor([2.0614e-01, 7.9342e-01, 7.5610e-05, 3.2113e-04, 5.0907e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.0965699216476423e-08 tensor([9.2834e-01, 1.4816e-03, 5.0966e-08, 7.0151e-02, 2.8879e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.275766940509129e-08 tensor([4.2758e-08, 1.8554e-03, 9.9097e-01, 2.8985e-06, 7.1691e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8482971427147277e-05 tensor([1.8483e-05, 4.6530e-03, 2.5703e-01, 5.1514e-03, 7.3314e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.040565949660959e-06 tensor([3.8626e-05, 6.0406e-06, 5.2711e-05, 6.1150e-01, 3.8840e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.8979217101295944e-06 tensor([6.1300e-01, 3.8663e-01, 5.8979e-06, 3.4920e-04, 1.2681e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.639129838324152e-05 tensor([4.6391e-05, 1.8201e-03, 5.4549e-02, 2.7854e-02, 9.1573e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009927553124725819 tensor([0.0099, 0.0462, 0.0264, 0.2854, 0.6321], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00043660562369041145 tensor([1.0751e-03, 6.5094e-01, 3.3167e-01, 4.3661e-04, 1.5873e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00040631607407703996 tensor([1.7873e-03, 8.3887e-01, 1.4720e-01, 4.0632e-04, 1.1738e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00030759855872020125 tensor([4.5890e-01, 5.2702e-01, 3.0760e-04, 1.1865e-02, 1.9031e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005356630426831543 tensor([5.3566e-04, 4.3115e-02, 2.4852e-01, 2.4910e-02, 6.8292e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0016891718842089176 tensor([0.4024, 0.1889, 0.0017, 0.3661, 0.0409], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007960691000334918 tensor([0.0020, 0.7428, 0.2334, 0.0008, 0.0211], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.264266558109739e-07 tensor([7.2808e-03, 1.2586e-05, 1.2643e-07, 9.8972e-01, 2.9905e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.41306963491661e-06 tensor([8.1734e-01, 1.8124e-01, 3.4131e-06, 1.3821e-03, 2.6954e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003388401528354734 tensor([3.3884e-04, 2.3109e-01, 6.8271e-01, 1.3130e-03, 8.4544e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.695159899232749e-09 tensor([5.6952e-09, 6.7438e-05, 7.3874e-01, 7.3018e-05, 2.6112e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.002431839006021619 tensor([0.2846, 0.6586, 0.0024, 0.0380, 0.0163], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003536812437232584 tensor([5.8578e-04, 3.5368e-04, 7.0769e-04, 4.7762e-01, 5.2073e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7196825865539722e-05 tensor([7.5718e-01, 2.6923e-02, 1.7197e-05, 2.1277e-01, 3.1066e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.823890383751859e-07 tensor([9.7346e-01, 2.2153e-02, 2.8239e-07, 4.3750e-03, 1.1935e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8342923624459218e-07 tensor([1.8343e-07, 3.2798e-03, 9.7806e-01, 1.0788e-05, 1.8647e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00016910674457903951 tensor([6.4051e-01, 1.0167e-01, 1.6911e-04, 2.4455e-01, 1.3109e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012969954404979944 tensor([0.0013, 0.0013, 0.0023, 0.4361, 0.5589], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.036844984511845e-06 tensor([7.9544e-01, 1.4011e-02, 4.0368e-06, 1.8811e-01, 2.4291e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017002286040224135 tensor([2.9655e-04, 4.8716e-01, 5.0090e-01, 1.7002e-04, 1.1476e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013992070453241467 tensor([0.3658, 0.1701, 0.0014, 0.3890, 0.0738], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.045301350506179e-08 tensor([8.5641e-01, 5.9919e-04, 2.0453e-08, 1.4294e-01, 5.0688e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.375586281137657e-08 tensor([5.3484e-01, 4.7903e-04, 6.3756e-08, 4.6429e-01, 3.9720e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001529466826468706 tensor([1.6732e-04, 1.5295e-04, 7.3461e-04, 3.6162e-01, 6.3733e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00771800521761179 tensor([0.2404, 0.6634, 0.0077, 0.0630, 0.0254], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7827727535623126e-06 tensor([2.7828e-06, 1.6609e-02, 9.5726e-01, 6.2024e-05, 2.6067e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.477298302343115e-05 tensor([5.3717e-01, 2.9007e-02, 6.4773e-05, 4.2198e-01, 1.1776e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006353746168315411 tensor([0.1670, 0.7712, 0.0064, 0.0292, 0.0262], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00021357527293730527 tensor([1.7367e-01, 2.1170e-02, 2.1358e-04, 7.6302e-01, 4.1922e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012708645954262465 tensor([1.2709e-04, 5.6580e-04, 6.6282e-03, 1.7542e-01, 8.1726e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9884226933063474e-06 tensor([2.9884e-06, 5.0585e-04, 1.0694e-01, 7.1416e-03, 8.8541e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00021801097318530083 tensor([1.5844e-02, 9.7855e-01, 4.5143e-03, 2.1801e-04, 8.7613e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.007524694316089e-06 tensor([4.0075e-06, 5.2499e-06, 3.2851e-04, 1.3343e-01, 8.6623e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[3], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [3], [0], [2], [0], [2], [2], [0], [3], [0], [2], [2], [0], [0], [3], [2], [0], [3], [3], [1], [2], [2], [2], [4], [0], [2], [3], [0], [2], [2], [3], [2], [0], [0], [0], [0], [3], [3], [0], [0], [2], [2], [0], [3], [1], [2], [0], [0], [2], [0], [2], [0], [0], [3], [0], [2], [0], [2], [3], [2], [2], [2], [3], [4], [2], [2], [0], [2], [0], [0], [2], [0], [0], [3], [0], [0], [0], [0], [0], [2], [0], [0], [2], [2], [0], [2], [3], [0], [3], [0], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [0], [0], [0], [2], [2], [3], [0], [2], [0], [2], [0], [2], [2], [2], [2], [2], [0], [0], [2], [2], [0], [0], [2], [2], [2], [3], [2], [0], [2], [0], [3], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [0], [2], [2], [3], [2], [2], [0], [3], [2], [1], [2], [0], [2], [1], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [0], [2], [2], [3], [2], [2], [2], [3], [0], [3], [0], [2], [2], [2], [2], [2], [2], [0], [3], [3], [0], [3], [0], [0], [3], [0], [2], [2], [2], [2], [0], [4], [2], [0], [0], [1], [2], [0], [0], [3], [3], [2], [0], [2], [3], [2], [2], [0], [0], [2], [1], [2], [2], [0], [2], [0], [2], [3], [2], [2], [2], [1], [2], [0], [2], [2], [2], [0], [0], [3], [0]]\n",
      "[[0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 3], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[3], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [3], [0], [2], [0], [2], [2], [0], [3], [0], [2], [2], [0], [0], [3], [2], [0], [3], [3], [1], [2], [2], [2], [4], [0], [2], [3], [0], [2], [2], [3], [2], [0], [0], [0], [0], [3], [3], [0], [0], [2], [2], [0], [3], [1], [2], [0], [0], [2], [0], [2], [0], [0], [3], [0], [2], [0], [2], [3], [2], [2], [2], [3], [4], [2], [2], [0], [2], [0], [0], [2], [0], [0], [3], [0], [0], [0], [0], [0], [2], [0], [0], [2], [2], [0], [2], [3], [0], [3], [0], [2], [2], [2], [2], [0], [2], [2], [2], [0], [0], [2], [2], [0], [0], [0], [2], [2], [3], [0], [2], [0], [2], [0], [2], [2], [2], [2], [2], [0], [0], [2], [2], [0], [0], [2], [2], [2], [3], [2], [0], [2], [0], [3], [2], [2], [2], [0], [0], [2], [0], [2], [0], [0], [0], [2], [0], [2], [2], [3], [2], [2], [0], [3], [2], [1], [2], [0], [2], [1], [0], [2], [2], [0], [2], [0], [2], [0], [0], [0], [0], [2], [2], [3], [2], [2], [2], [3], [0], [3], [0], [2], [2], [2], [2], [2], [2], [0], [3], [3], [0], [3], [0], [0], [3], [0], [2], [2], [2], [2], [0], [4], [2], [0], [0], [1], [2], [0], [0], [3], [3], [2], [0], [2], [3], [2], [2], [0], [0], [2], [1], [2], [2], [0], [2], [0], [2], [3], [2], [2], [2], [1], [2], [0], [2], [2], [2], [0], [0], [3], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004453732490539551  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0044537301063537595  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004453726291656494  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004453720569610596  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004453712940216064  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004453705310821533  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0044536952972412105  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0044536848068237305  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004453672409057617  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004453660011291504  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004453646659851074  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004453632831573486  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004453618049621582  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004453602790832519  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004453587532043457  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004453571796417236  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004453555107116699  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.00445353889465332  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004453522205352783  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004453505516052246  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 0.0038304219488054514 tensor([0.0530, 0.9349, 0.0059, 0.0024, 0.0038], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00026615854585543275 tensor([2.2634e-01, 7.7188e-01, 2.6611e-04, 1.2475e-03, 2.6616e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001798034179955721 tensor([0.2222, 0.7704, 0.0009, 0.0047, 0.0018], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007636463851667941 tensor([7.6197e-01, 2.1109e-01, 7.9186e-05, 2.6098e-02, 7.6365e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2844594493799377e-05 tensor([5.2463e-02, 1.2845e-05, 7.3204e-09, 9.4716e-01, 3.6366e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12747742235660553 tensor([0.1275, 0.2958, 0.0167, 0.3445, 0.2155], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.071883348748088e-05 tensor([3.2362e-04, 4.0138e-05, 8.0719e-05, 7.9446e-01, 2.0509e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.926783392671496e-06 tensor([2.0398e-03, 3.9268e-06, 1.3554e-07, 9.9463e-01, 3.3214e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.7651551388553344e-05 tensor([9.2212e-01, 7.5752e-02, 1.2123e-06, 2.1142e-03, 1.7652e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032533847261220217 tensor([6.6751e-01, 2.0236e-02, 1.4920e-05, 3.0899e-01, 3.2534e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.715303920325823e-06 tensor([9.6061e-01, 3.7422e-02, 3.0142e-07, 1.9557e-03, 8.7153e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.038233816623687744 tensor([0.0382, 0.7488, 0.0602, 0.0333, 0.1195], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021702513098716736 tensor([0.0020, 0.1809, 0.3375, 0.0217, 0.4578], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04482029378414154 tensor([0.1775, 0.7160, 0.0088, 0.0529, 0.0448], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01241086982190609 tensor([0.0013, 0.1755, 0.3459, 0.0124, 0.4648], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.5346073269029148e-05 tensor([6.1818e-01, 3.8150e-01, 4.7991e-06, 2.9857e-04, 1.5346e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00022147208801470697 tensor([4.6758e-03, 2.2147e-04, 2.9288e-05, 9.3053e-01, 6.4544e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5214464838209096e-05 tensor([1.2330e-07, 1.5214e-05, 1.7109e-02, 3.6732e-03, 9.7920e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04045853018760681 tensor([0.0405, 0.8290, 0.0431, 0.0214, 0.0661], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.1970745365251787e-06 tensor([2.8276e-09, 1.9956e-04, 9.8588e-01, 2.1971e-06, 1.3923e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.787685717630666e-06 tensor([9.5245e-01, 3.5881e-04, 2.3029e-09, 4.7187e-02, 7.7877e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14146746695041656 tensor([0.1602, 0.4900, 0.0213, 0.1869, 0.1415], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.027733247727155685 tensor([0.0127, 0.5427, 0.1937, 0.0277, 0.2232], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004859861801378429 tensor([3.4146e-06, 7.2977e-03, 7.6416e-01, 4.8599e-04, 2.2805e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0045142751187086105 tensor([0.0045, 0.6906, 0.2143, 0.0039, 0.0866], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0018043062882497907 tensor([2.6088e-01, 7.3107e-01, 7.1523e-04, 5.5285e-03, 1.8043e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008392497547902167 tensor([1.0639e-05, 8.3925e-04, 5.8880e-02, 1.1940e-02, 9.2833e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020512497052550316 tensor([0.0205, 0.8273, 0.0872, 0.0118, 0.0532], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8887698388425633e-05 tensor([1.8888e-05, 2.1531e-01, 7.8066e-01, 1.2207e-05, 4.0006e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.472086973488331e-06 tensor([1.4592e-04, 2.8821e-06, 3.4721e-06, 9.3622e-01, 6.3629e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009365671430714428 tensor([7.9413e-01, 8.4335e-03, 1.4809e-06, 1.9650e-01, 9.3657e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0014989470364525914 tensor([5.6279e-01, 4.1551e-01, 3.7906e-04, 1.9825e-02, 1.4989e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.075803786283359e-05 tensor([4.1713e-02, 2.0758e-05, 2.5013e-08, 9.5767e-01, 6.0071e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00019043694192077965 tensor([6.9018e-02, 9.3031e-01, 3.6513e-04, 1.9044e-04, 1.1523e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.2119664208730683e-05 tensor([5.1099e-07, 2.2120e-05, 6.8634e-03, 7.6941e-03, 9.8542e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007372495252639055 tensor([6.5947e-01, 6.9510e-02, 7.7772e-05, 2.6357e-01, 7.3725e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00018262266530655324 tensor([1.8262e-04, 6.7991e-01, 3.1794e-01, 2.7657e-05, 1.9397e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00016108601994346827 tensor([6.6532e-05, 1.6109e-04, 1.9587e-03, 1.9387e-01, 8.0395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.031250495463609695 tensor([2.6128e-01, 3.1250e-02, 2.6534e-04, 6.6357e-01, 4.3631e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007091608131304383 tensor([8.4696e-01, 1.2754e-01, 3.8630e-05, 2.4746e-02, 7.0916e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009756204090081155 tensor([1.7067e-02, 9.7730e-01, 4.3537e-03, 3.0220e-04, 9.7562e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004193299915641546 tensor([7.8401e-01, 4.9588e-02, 3.9899e-05, 1.6217e-01, 4.1933e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005979688372462988 tensor([1.1929e-04, 5.9797e-03, 9.4395e-02, 3.0605e-02, 8.6890e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.059629108756780624 tensor([0.0025, 0.0837, 0.1414, 0.0596, 0.7127], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9280003016319824e-06 tensor([2.2327e-07, 1.9280e-06, 9.9286e-04, 2.8804e-02, 9.7020e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03228481113910675 tensor([0.0184, 0.1004, 0.0323, 0.2974, 0.5516], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006605239585042 tensor([0.0247, 0.9446, 0.0216, 0.0024, 0.0066], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0028482822235673666 tensor([5.8172e-03, 9.6359e-01, 2.7395e-02, 3.4668e-04, 2.8483e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011462356633273885 tensor([2.3474e-05, 7.9040e-02, 8.8939e-01, 1.1462e-04, 3.1435e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2341251931502484e-05 tensor([3.1081e-06, 1.2341e-05, 1.1107e-03, 6.6172e-02, 9.3270e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.5533581745330594e-06 tensor([9.7438e-01, 1.3407e-04, 2.6831e-10, 2.5487e-02, 1.5534e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01261424832046032 tensor([0.1066, 0.8552, 0.0106, 0.0150, 0.0126], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008190838852897286 tensor([0.0007, 0.0008, 0.0023, 0.4116, 0.5846], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.563295988191385e-06 tensor([4.5633e-06, 1.1013e-01, 8.8704e-01, 3.7143e-06, 2.8173e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.3265707720129285e-06 tensor([5.9263e-06, 3.3312e-07, 5.3266e-06, 6.4100e-01, 3.5898e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007492497097700834 tensor([7.9460e-01, 8.4693e-03, 1.5186e-06, 1.9618e-01, 7.4925e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3858659915276803e-05 tensor([3.1156e-07, 3.7887e-03, 9.6683e-01, 2.3859e-05, 2.9359e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04827941581606865 tensor([0.0072, 0.2806, 0.1837, 0.0483, 0.4802], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016916396096348763 tensor([0.4739, 0.4155, 0.0015, 0.0921, 0.0169], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.3460435841116123e-05 tensor([1.7143e-07, 2.1668e-03, 9.6290e-01, 2.3460e-05, 3.4914e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.2067433746997267e-05 tensor([8.5457e-01, 1.4408e-01, 2.1288e-06, 1.3237e-03, 2.2067e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00397288054227829 tensor([1.2724e-05, 5.5659e-03, 3.8498e-01, 3.9729e-03, 6.0547e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.092156738508493e-05 tensor([1.9315e-07, 1.2626e-03, 9.0535e-01, 7.0922e-05, 9.3313e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005319694988429546 tensor([0.0155, 0.9588, 0.0193, 0.0010, 0.0053], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012682346627116203 tensor([3.1169e-04, 4.0511e-02, 2.7175e-01, 1.2682e-02, 6.7475e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003754081844817847 tensor([6.5276e-01, 3.4080e-01, 4.4964e-05, 6.0165e-03, 3.7541e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.016182653605937958 tensor([0.0019, 0.0162, 0.0459, 0.1782, 0.7579], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007049100240692496 tensor([1.7200e-02, 7.0491e-04, 2.9650e-05, 9.5574e-01, 2.6328e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005959087866358459 tensor([1.1407e-02, 9.8270e-01, 5.1162e-03, 1.7828e-04, 5.9591e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.731336983037181e-05 tensor([1.1486e-03, 3.7313e-05, 1.3127e-05, 9.3232e-01, 6.6479e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002191116800531745 tensor([6.6383e-01, 1.0774e-02, 5.7564e-06, 3.2320e-01, 2.1911e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12048881500959396 tensor([0.1205, 0.1250, 0.0062, 0.6092, 0.1391], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0033758715726435184 tensor([4.5233e-03, 9.4661e-01, 4.5153e-02, 3.3488e-04, 3.3759e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00108047341927886 tensor([0.1354, 0.8604, 0.0011, 0.0021, 0.0010], grad_fn=<SoftmaxBackward0>)\n",
      "1 3.0819055609754287e-06 tensor([8.7035e-03, 3.0819e-06, 8.1432e-09, 9.9052e-01, 7.7261e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.934538088084082e-06 tensor([9.8036e-01, 1.8630e-02, 5.1004e-08, 1.0103e-03, 2.9345e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.347493252978893e-05 tensor([2.7881e-06, 4.0129e-02, 9.5210e-01, 1.3475e-05, 7.7576e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.03888680040836334 tensor([0.0389, 0.0566, 0.0082, 0.6240, 0.2723], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08998236805200577 tensor([0.0068, 0.1028, 0.0900, 0.1061, 0.6943], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04391757398843765 tensor([0.0082, 0.0743, 0.0439, 0.1478, 0.7259], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1663034911180148e-06 tensor([9.9537e-01, 2.4228e-03, 4.0105e-09, 2.2026e-03, 1.1663e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.029960911720991135 tensor([0.0081, 0.4612, 0.2371, 0.0300, 0.2636], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00035636185202747583 tensor([4.0163e-07, 3.5636e-04, 2.5227e-01, 1.2742e-03, 7.4610e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001714636804535985 tensor([0.0560, 0.9373, 0.0037, 0.0013, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0020966152660548687 tensor([6.0989e-04, 2.0966e-03, 6.0951e-03, 1.9475e-01, 7.9644e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0074724904261529446 tensor([0.0014, 0.2810, 0.4241, 0.0075, 0.2861], grad_fn=<SoftmaxBackward0>)\n",
      "3 2.60826764133526e-05 tensor([8.0298e-06, 6.7373e-02, 9.2176e-01, 2.6083e-05, 1.0830e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0044600036926567554 tensor([6.9001e-04, 4.4600e-03, 2.0307e-02, 2.0026e-01, 7.7429e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005661197355948389 tensor([3.6403e-06, 6.2876e-03, 7.9316e-01, 5.6612e-04, 1.9999e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09130331873893738 tensor([0.2603, 0.4686, 0.0071, 0.1727, 0.0913], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02864360436797142 tensor([0.0061, 0.2875, 0.1856, 0.0286, 0.4922], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0026485619600862265 tensor([2.4508e-04, 1.0546e-01, 7.0930e-01, 2.6486e-03, 1.8235e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13096478581428528 tensor([0.1598, 0.4664, 0.0192, 0.2236, 0.1310], grad_fn=<SoftmaxBackward0>)\n",
      "4 5.1214337872806937e-05 tensor([8.9679e-01, 9.9708e-02, 3.2589e-06, 3.4475e-03, 5.1214e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017185477772727609 tensor([1.8767e-04, 1.7185e-03, 1.4886e-02, 8.6772e-02, 8.9644e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0023363051004707813 tensor([6.4397e-01, 9.7634e-03, 5.0255e-06, 3.4392e-01, 2.3363e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001952246529981494 tensor([1.9522e-04, 5.6194e-01, 4.3197e-01, 6.8125e-05, 5.8216e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.9297038988952409e-07 tensor([1.7776e-10, 7.4983e-05, 9.9580e-01, 1.9297e-07, 4.1249e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00015390146290883422 tensor([1.1219e-02, 9.8645e-01, 2.1349e-03, 4.6995e-05, 1.5390e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009809258626773953 tensor([1.4096e-06, 9.8093e-04, 3.4100e-01, 1.7981e-03, 6.5622e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.059180740267038345 tensor([0.0592, 0.5435, 0.0301, 0.0923, 0.2750], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0024766598362475634 tensor([1.7182e-02, 2.4767e-03, 3.3332e-04, 8.4858e-01, 1.3143e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002335058059543371 tensor([9.1947e-03, 2.3351e-03, 4.4305e-04, 8.4010e-01, 1.4793e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005675795953720808 tensor([7.4782e-01, 9.3572e-02, 1.0361e-04, 1.5283e-01, 5.6758e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010380425374023616 tensor([9.2381e-05, 1.0380e-04, 7.4536e-04, 2.3036e-01, 7.6870e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.738149386364967e-05 tensor([3.2362e-01, 6.7573e-01, 4.4722e-05, 5.2882e-04, 6.7381e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003157851053401828 tensor([0.0057, 0.0032, 0.0015, 0.7088, 0.2808], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0031299844849854708 tensor([5.9566e-01, 3.5056e-01, 4.2361e-04, 5.0227e-02, 3.1300e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.017071900889277458 tensor([3.3490e-04, 1.7072e-02, 1.2353e-01, 2.3768e-02, 8.3530e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.3537734250567155e-06 tensor([4.6037e-08, 2.3538e-06, 3.3336e-03, 4.6079e-03, 9.9206e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.160828209227475e-07 tensor([9.9373e-01, 2.4784e-04, 1.6448e-10, 6.0198e-03, 5.1608e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011052639223635197 tensor([0.5847, 0.2401, 0.0007, 0.1635, 0.0111], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0041857617907226086 tensor([0.0009, 0.0042, 0.0139, 0.2415, 0.7395], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004868485499173403 tensor([1.0117e-04, 3.2953e-02, 5.6365e-01, 4.8685e-03, 3.9842e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025476673617959023 tensor([5.6838e-04, 5.0532e-02, 2.7541e-01, 2.5477e-02, 6.4801e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0028332180809229612 tensor([0.2667, 0.7185, 0.0010, 0.0110, 0.0028], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02522917464375496 tensor([0.1269, 0.7971, 0.0119, 0.0389, 0.0252], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000526081828866154 tensor([1.9136e-02, 9.7525e-01, 4.8415e-03, 2.5105e-04, 5.2608e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020509643480181694 tensor([0.0027, 0.2229, 0.3296, 0.0205, 0.4242], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002202142495661974 tensor([6.2598e-03, 2.2021e-04, 2.4312e-05, 8.9918e-01, 9.4312e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.395534728653729e-05 tensor([1.6124e-06, 6.3955e-05, 1.2162e-02, 1.6190e-02, 9.7158e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01519694086164236 tensor([0.0152, 0.0263, 0.0099, 0.5687, 0.3800], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008580866269767284 tensor([6.2256e-05, 1.0622e-02, 2.6488e-01, 8.5809e-03, 7.1586e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07916058599948883 tensor([0.3111, 0.1016, 0.0010, 0.5072, 0.0792], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00897908117622137 tensor([7.0095e-02, 8.9791e-03, 2.1218e-04, 8.5121e-01, 6.9502e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.897432078123984e-08 tensor([9.9927e-01, 2.8034e-04, 1.5242e-11, 4.4933e-04, 1.8974e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004353376862127334 tensor([8.9891e-01, 2.1774e-02, 5.5268e-06, 7.8872e-02, 4.3534e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008866175077855587 tensor([0.0256, 0.0089, 0.0010, 0.7684, 0.1961], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008795959874987602 tensor([0.0057, 0.6050, 0.2281, 0.0088, 0.1524], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.5039428262753063e-06 tensor([1.0093e-07, 5.7689e-03, 9.9069e-01, 1.5039e-06, 3.5368e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.483959277327813e-07 tensor([9.9389e-01, 5.1086e-03, 5.8900e-09, 1.0009e-03, 8.4840e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008258435991592705 tensor([6.7947e-01, 4.0184e-03, 1.3602e-06, 3.1569e-01, 8.2584e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01648436300456524 tensor([0.0016, 0.0165, 0.0305, 0.1212, 0.8303], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009933279361575842 tensor([2.5973e-05, 9.9333e-04, 3.6270e-02, 2.2735e-02, 9.3998e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.091945134452544e-05 tensor([6.1895e-04, 2.0919e-05, 9.1954e-06, 9.1446e-01, 8.4892e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.124851784581551e-06 tensor([9.9004e-01, 2.3556e-03, 1.2521e-08, 7.6031e-03, 4.1249e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009960962459445 tensor([0.1583, 0.8057, 0.0066, 0.0194, 0.0100], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012245042249560356 tensor([0.0150, 0.9370, 0.0336, 0.0021, 0.0122], grad_fn=<SoftmaxBackward0>)\n",
      "4 9.416329703526571e-05 tensor([8.5884e-01, 1.3555e-01, 8.9210e-06, 5.5123e-03, 9.4163e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004943739622831345 tensor([4.4276e-07, 4.9437e-04, 3.6787e-01, 9.2258e-04, 6.3071e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.016110504046082497 tensor([0.0161, 0.0365, 0.0109, 0.4302, 0.5062], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010528855491429567 tensor([5.3752e-04, 1.0529e-03, 3.9898e-03, 3.1367e-01, 6.8075e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002522030845284462 tensor([2.5220e-03, 9.1376e-01, 8.0131e-02, 2.0250e-04, 3.3829e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01670113392174244 tensor([0.2083, 0.7310, 0.0049, 0.0391, 0.0167], grad_fn=<SoftmaxBackward0>)\n",
      "1 6.317425868473947e-05 tensor([7.1330e-04, 6.3174e-05, 5.1495e-05, 7.9373e-01, 2.0545e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014074640348553658 tensor([0.0238, 0.0141, 0.0016, 0.7499, 0.2107], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.019979704171419144 tensor([0.0164, 0.7656, 0.1039, 0.0200, 0.0941], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.438555333763361e-05 tensor([2.6231e-08, 3.2970e-04, 9.1140e-01, 4.4386e-05, 8.8224e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0012966139474883676 tensor([6.4391e-01, 3.3717e-01, 1.7596e-04, 1.7449e-02, 1.2966e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0030700531788170338 tensor([2.1642e-05, 3.0701e-03, 1.4551e-01, 8.3658e-03, 8.4303e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.020715130493044853 tensor([0.0855, 0.8473, 0.0120, 0.0207, 0.0344], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02895449288189411 tensor([0.0014, 0.0290, 0.0735, 0.0679, 0.8282], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00014422621461562812 tensor([1.6510e-05, 5.5916e-02, 9.0412e-01, 1.4423e-04, 3.9802e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022786152549088 tensor([2.0430e-05, 2.2786e-03, 9.7895e-02, 1.0191e-02, 8.8961e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.68157349032117e-06 tensor([9.8735e-01, 7.2102e-03, 6.1449e-08, 5.4314e-03, 8.6816e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03030604124069214 tensor([0.0134, 0.5959, 0.1313, 0.0303, 0.2292], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0011842612875625491 tensor([7.1590e-01, 2.6019e-01, 1.5729e-04, 2.2565e-02, 1.1843e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0027135685086250305 tensor([6.7542e-01, 2.7406e-01, 2.1844e-04, 4.7583e-02, 2.7136e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013180063106119633 tensor([0.0618, 0.8955, 0.0132, 0.0094, 0.0201], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0037163379602134228 tensor([7.6382e-01, 8.2751e-02, 7.3839e-05, 1.4964e-01, 3.7163e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006080671446397901 tensor([1.2619e-01, 8.7161e-01, 5.8323e-04, 1.0057e-03, 6.0807e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.547738404246047e-06 tensor([1.5492e-08, 2.5477e-06, 1.1282e-02, 2.1356e-03, 9.8658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.214896423742175e-05 tensor([3.2149e-05, 2.3489e-01, 7.5847e-01, 2.3151e-05, 6.5919e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08054985851049423 tensor([0.3397, 0.1324, 0.0015, 0.4458, 0.0805], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00014987780014052987 tensor([1.4988e-04, 7.1125e-05, 4.2247e-04, 5.0776e-01, 4.9160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1267795343883336e-05 tensor([9.5113e-01, 6.0331e-04, 6.0547e-09, 4.8253e-02, 1.1268e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 8.98256985237822e-06 tensor([1.8742e-06, 8.9826e-06, 9.9210e-04, 4.9138e-02, 9.4986e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010491289140190929 tensor([4.7887e-02, 1.0491e-04, 3.8094e-07, 9.5044e-01, 1.5715e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.697225788026117e-05 tensor([2.5340e-04, 2.4703e-05, 5.6972e-05, 7.7095e-01, 2.2871e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005871576140634716 tensor([9.4849e-06, 1.4181e-02, 7.8577e-01, 5.8716e-04, 1.9945e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.934141806093976e-05 tensor([9.5846e-01, 2.6319e-02, 1.2000e-06, 1.5126e-02, 8.9341e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.898076465702616e-05 tensor([3.6132e-03, 3.8981e-05, 3.0042e-06, 9.8180e-01, 1.4543e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00016715144738554955 tensor([3.4762e-07, 1.4951e-03, 8.9039e-01, 1.6715e-04, 1.0795e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006411717622540891 tensor([6.5637e-01, 3.3020e-01, 1.0147e-04, 1.2681e-02, 6.4117e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003493532713036984 tensor([9.9996e-07, 3.4935e-04, 1.3279e-01, 2.7082e-03, 8.6416e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015046821208670735 tensor([0.0033, 0.0015, 0.0009, 0.6308, 0.3635], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003444262780249119 tensor([4.1722e-05, 3.4443e-03, 1.0649e-01, 1.7671e-02, 8.7236e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00027077601407654583 tensor([6.0573e-06, 1.4506e-02, 9.0393e-01, 2.7078e-04, 8.1291e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005134526174515486 tensor([2.5779e-05, 5.1345e-03, 2.3003e-01, 5.5875e-03, 7.5922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00026905295089818537 tensor([1.0021e-05, 2.6905e-04, 1.6102e-02, 2.6728e-02, 9.5689e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008338073384948075 tensor([3.9443e-01, 5.9955e-01, 2.2469e-04, 4.9575e-03, 8.3381e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011818873696029186 tensor([0.6058, 0.2445, 0.0008, 0.1371, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004231402126606554 tensor([2.8536e-02, 9.6875e-01, 2.0390e-03, 2.5378e-04, 4.2314e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04198436439037323 tensor([0.0620, 0.7902, 0.0330, 0.0420, 0.0728], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00029368489049375057 tensor([1.7362e-01, 2.9368e-04, 2.2275e-07, 8.2501e-01, 1.0777e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0001323592005064711 tensor([7.5162e-01, 2.4443e-01, 1.6119e-05, 3.8039e-03, 1.3236e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014848912134766579 tensor([0.0592, 0.9001, 0.0148, 0.0086, 0.0172], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.167527893310762e-06 tensor([3.7701e-08, 1.5666e-03, 9.9039e-01, 3.1675e-06, 8.0402e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006235886248759925 tensor([6.2359e-04, 9.4161e-01, 5.7093e-02, 1.7987e-05, 6.5555e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02448994480073452 tensor([6.9039e-04, 2.4490e-02, 8.5359e-02, 4.4905e-02, 8.4456e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005337190348654985 tensor([6.1910e-01, 3.2325e-01, 2.7513e-04, 5.2033e-02, 5.3372e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004470891319215298 tensor([6.7412e-02, 4.4709e-03, 9.4966e-05, 8.9777e-01, 3.0247e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.042783115059137344 tensor([1.9289e-01, 4.2783e-02, 6.2093e-04, 7.0169e-01, 6.2022e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0016347350319847465 tensor([7.8190e-01, 1.8413e-01, 1.0738e-04, 3.2227e-02, 1.6347e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03833167627453804 tensor([0.0537, 0.0383, 0.0031, 0.6014, 0.3034], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04503311589360237 tensor([0.0473, 0.0450, 0.0030, 0.6082, 0.2965], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06703455746173859 tensor([0.0049, 0.1335, 0.1681, 0.0670, 0.6265], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008890702156350017 tensor([2.5136e-02, 9.7021e-01, 3.3857e-03, 3.7761e-04, 8.8907e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0013585594715550542 tensor([6.0589e-03, 9.7471e-01, 1.7676e-02, 1.9793e-04, 1.3586e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.567090556723997e-05 tensor([2.0944e-08, 5.5671e-05, 3.3389e-01, 4.0457e-04, 6.6565e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006712128524668515 tensor([6.7121e-04, 6.6788e-01, 3.1472e-01, 3.0263e-04, 1.6419e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009927810169756413 tensor([2.6038e-04, 9.9278e-03, 1.0467e-01, 4.3446e-02, 8.4170e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.003677957458421588 tensor([3.3918e-05, 3.6780e-03, 1.1448e-01, 1.3849e-02, 8.6796e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002152420929633081 tensor([8.8805e-03, 9.8769e-01, 3.1570e-03, 5.5959e-05, 2.1524e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.82654426782392e-05 tensor([6.9115e-06, 6.8265e-05, 3.4817e-03, 3.0548e-02, 9.6590e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.5271594975274638e-06 tensor([9.2175e-01, 7.8027e-02, 1.5018e-07, 2.2243e-04, 1.5272e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.0306482838350348e-05 tensor([6.4936e-03, 2.0306e-05, 3.7531e-07, 9.8969e-01, 3.7959e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04008693993091583 tensor([0.1285, 0.0401, 0.0011, 0.7471, 0.0832], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0021402304992079735 tensor([8.0966e-01, 1.2420e-01, 8.3888e-05, 6.3920e-02, 2.1402e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014103692956268787 tensor([4.2668e-04, 1.4104e-02, 6.9181e-02, 4.2712e-02, 8.7358e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.924848614493385e-05 tensor([2.0042e-01, 7.9912e-01, 7.9248e-05, 3.1993e-04, 5.2279e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.0467019314528443e-05 tensor([9.2669e-01, 1.5332e-03, 5.4895e-08, 7.1745e-02, 3.0467e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.8250503874005517e-06 tensor([4.1187e-08, 1.8189e-03, 9.9111e-01, 2.8251e-06, 7.0688e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004591073375195265 tensor([1.7715e-05, 4.5911e-03, 2.6195e-01, 5.0129e-03, 7.2843e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.78950426238589e-05 tensor([3.7895e-05, 6.1487e-06, 5.5104e-05, 6.0432e-01, 3.9558e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3118005881551653e-05 tensor([6.0461e-01, 3.9502e-01, 6.2464e-06, 3.5107e-04, 1.3118e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00181474385317415 tensor([4.4890e-05, 1.8147e-03, 5.5737e-02, 2.7183e-02, 9.1522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02729063108563423 tensor([0.0094, 0.0457, 0.0273, 0.2783, 0.6393], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0010264561278745532 tensor([1.0265e-03, 6.4143e-01, 3.4112e-01, 4.2851e-04, 1.5997e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0017216179985553026 tensor([1.7216e-03, 8.3334e-01, 1.5260e-01, 4.0249e-04, 1.1931e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.001991152996197343 tensor([4.4957e-01, 5.3611e-01, 3.2781e-04, 1.2001e-02, 1.9912e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02429705485701561 tensor([5.1547e-04, 4.2659e-02, 2.5293e-01, 2.4297e-02, 6.7960e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04259147867560387 tensor([0.3946, 0.1925, 0.0018, 0.3685, 0.0426], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0018823269056156278 tensor([0.0019, 0.7349, 0.2411, 0.0008, 0.0213], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2821087693737354e-05 tensor([7.1261e-03, 1.2821e-05, 1.3390e-07, 9.8976e-01, 3.0971e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.815163134073373e-05 tensor([8.1198e-01, 1.8658e-01, 3.6449e-06, 1.4014e-03, 2.8152e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012657640036195517 tensor([3.1856e-04, 2.2432e-01, 6.9043e-01, 1.2658e-03, 8.3661e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.713031325489283e-05 tensor([5.5693e-09, 6.7130e-05, 7.4227e-01, 7.1454e-05, 2.5759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016838589683175087 tensor([0.2771, 0.6654, 0.0026, 0.0381, 0.0168], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005657298024743795 tensor([5.6573e-04, 3.5554e-04, 7.3806e-04, 4.6895e-01, 5.2939e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032873356249183416 tensor([7.5146e-01, 2.7776e-02, 1.8547e-05, 2.1746e-01, 3.2873e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.2480427358241286e-05 tensor([9.7262e-01, 2.2918e-02, 3.0224e-07, 4.4483e-03, 1.2480e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0465709237905685e-05 tensor([1.7586e-07, 3.2084e-03, 9.7847e-01, 1.0466e-05, 1.8309e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01382665615528822 tensor([6.3273e-01, 1.0469e-01, 1.8229e-04, 2.4857e-01, 1.3827e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0013463853392750025 tensor([0.0012, 0.0013, 0.0024, 0.4272, 0.5678], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002583195688202977 tensor([7.9007e-01, 1.4498e-02, 4.3807e-06, 1.9284e-01, 2.5832e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002804039977490902 tensor([2.8040e-04, 4.7627e-01, 5.1181e-01, 1.6525e-04, 1.1472e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07698462903499603 tensor([0.3575, 0.1734, 0.0015, 0.3906, 0.0770], grad_fn=<SoftmaxBackward0>)\n",
      "4 5.376858462113887e-05 tensor([8.5269e-01, 6.1817e-04, 2.2053e-08, 1.4664e-01, 5.3769e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0004159780219197273 tensor([5.2816e-01, 4.8959e-04, 6.7882e-08, 4.7093e-01, 4.1598e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0001606352161616087 tensor([1.6064e-04, 1.5245e-04, 7.5988e-04, 3.5393e-01, 6.4499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02597488835453987 tensor([0.2353, 0.6679, 0.0080, 0.0628, 0.0260], grad_fn=<SoftmaxBackward0>)\n",
      "3 5.9338952269172296e-05 tensor([2.6053e-06, 1.6007e-02, 9.5843e-01, 5.9339e-05, 2.5498e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.012389740906655788 tensor([5.2971e-01, 2.9799e-02, 6.9612e-05, 4.2803e-01, 1.2390e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.026955917477607727 tensor([0.1616, 0.7757, 0.0067, 0.0291, 0.0270], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021570727229118347 tensor([1.7004e-01, 2.1571e-02, 2.2659e-04, 7.6468e-01, 4.3489e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005666203214786947 tensor([1.2359e-04, 5.6662e-04, 6.7944e-03, 1.7204e-01, 8.2047e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005054110661149025 tensor([2.8953e-06, 5.0541e-04, 1.0982e-01, 6.9770e-03, 8.8269e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000891608651727438 tensor([1.5328e-02, 9.7886e-01, 4.7029e-03, 2.1596e-04, 8.9161e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.286739451548783e-06 tensor([3.9046e-06, 5.2867e-06, 3.3814e-04, 1.3052e-01, 8.6913e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4], [2, 4], [2, 4], [2, 4], [2, 1], [2, 0], [1, 2], [2, 1], [2, 4], [2, 4], [2, 4], [3, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 1], [0, 1], [3, 0], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [3, 0], [2, 4], [0, 1], [3, 0], [3, 0], [1, 2], [2, 4], [2, 4], [2, 1], [4, 3], [0, 1], [2, 4], [3, 0], [0, 1], [2, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 3], [0, 1], [0, 2], [3, 4], [3, 4], [0, 3], [0, 1], [2, 4], [2, 4], [0, 1], [3, 0], [1, 2], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [0, 3], [3, 4], [0, 3], [2, 4], [0, 1], [2, 1], [3, 4], [2, 1], [2, 4], [2, 0], [3, 4], [4, 2], [2, 1], [2, 4], [0, 3], [2, 0], [0, 2], [0, 2], [2, 4], [0, 3], [0, 1], [3, 4], [0, 1], [0, 3], [0, 3], [0, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [2, 4], [3, 0], [0, 3], [3, 4], [0, 1], [2, 0], [2, 1], [2, 1], [2, 4], [0, 1], [2, 4], [2, 1], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [3, 4], [0, 3], [2, 1], [0, 1], [2, 0], [0, 3], [2, 4], [2, 1], [2, 4], [2, 4], [2, 1], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [0, 1], [2, 1], [2, 4], [2, 4], [3, 4], [2, 4], [0, 1], [2, 0], [0, 1], [3, 0], [2, 4], [2, 1], [2, 1], [0, 3], [0, 3], [2, 4], [0, 1], [2, 3], [0, 1], [0, 3], [0, 1], [2, 4], [0, 3], [2, 4], [2, 4], [3, 2], [2, 4], [2, 4], [0, 1], [3, 0], [2, 4], [1, 0], [2, 4], [0, 1], [2, 1], [1, 2], [0, 3], [2, 4], [2, 1], [0, 3], [2, 4], [0, 1], [2, 1], [0, 1], [0, 3], [0, 1], [0, 1], [2, 4], [2, 4], [3, 4], [2, 3], [2, 1], [2, 4], [3, 2], [0, 3], [3, 0], [0, 1], [2, 4], [2, 1], [2, 1], [2, 4], [2, 1], [2, 1], [0, 3], [3, 4], [3, 4], [0, 1], [3, 0], [0, 1], [0, 1], [3, 4], [0, 1], [2, 4], [2, 1], [2, 1], [2, 4], [0, 1], [4, 2], [2, 4], [0, 3], [0, 1], [1, 0], [2, 4], [0, 1], [0, 2], [3, 0], [3, 0], [2, 4], [0, 3], [2, 4], [3, 0], [2, 1], [2, 4], [0, 3], [0, 1], [2, 4], [1, 0], [2, 4], [2, 4], [0, 3], [2, 4], [0, 1], [2, 4], [3, 0], [2, 4], [2, 4], [2, 4], [1, 0], [2, 4], [0, 3], [2, 4], [2, 4], [2, 1], [0, 1], [0, 1], [3, 4], [0, 1]]\n",
      "[[0, 1, 2], [0, 1, 3], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 3, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [0, 1, 2], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 3, 4], [1, 2, 4], [2, 3, 4], [1, 3, 4], [0, 1, 2], [0, 1, 2], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 1, 2], [0, 3, 4], [0, 1, 3], [1, 3, 4], [0, 1, 2], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 3, 4], [1, 3, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 2], [2, 3, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 2], [2, 3, 4], [1, 3, 4], [0, 3, 4], [0, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [2, 3, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 2], [0, 1, 3], [2, 3, 4], [1, 3, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 4], [2, 3, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 3, 4], [2, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [0, 1, 2], [0, 1, 4], [0, 3, 4], [0, 1, 3], [0, 1, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 2], [0, 1, 2], [2, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 2], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 3], [0, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 2], [2, 3, 4]]\n",
      "NL_pred of 1th iteration [[3, 4], [2, 4], [2, 4], [2, 4], [2, 1], [2, 0], [1, 2], [2, 1], [2, 4], [2, 4], [2, 4], [3, 0], [0, 3], [2, 4], [0, 3], [2, 4], [2, 1], [0, 1], [3, 0], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [3, 0], [2, 4], [0, 1], [3, 0], [3, 0], [1, 2], [2, 4], [2, 4], [2, 1], [4, 3], [0, 1], [2, 4], [3, 0], [0, 1], [2, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 3], [0, 1], [0, 2], [3, 4], [3, 4], [0, 3], [0, 1], [2, 4], [2, 4], [0, 1], [3, 0], [1, 2], [2, 4], [0, 3], [0, 3], [2, 4], [0, 3], [2, 4], [0, 3], [0, 3], [3, 4], [0, 3], [2, 4], [0, 1], [2, 1], [3, 4], [2, 1], [2, 4], [2, 0], [3, 4], [4, 2], [2, 1], [2, 4], [0, 3], [2, 0], [0, 2], [0, 2], [2, 4], [0, 3], [0, 1], [3, 4], [0, 1], [0, 3], [0, 3], [0, 1], [0, 3], [2, 4], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [2, 4], [3, 0], [0, 3], [3, 4], [0, 1], [2, 0], [2, 1], [2, 1], [2, 4], [0, 1], [2, 4], [2, 1], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [3, 4], [0, 3], [2, 1], [0, 1], [2, 0], [0, 3], [2, 4], [2, 1], [2, 4], [2, 4], [2, 1], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [0, 1], [2, 1], [2, 4], [2, 4], [3, 4], [2, 4], [0, 1], [2, 0], [0, 1], [3, 0], [2, 4], [2, 1], [2, 1], [0, 3], [0, 3], [2, 4], [0, 1], [2, 3], [0, 1], [0, 3], [0, 1], [2, 4], [0, 3], [2, 4], [2, 4], [3, 2], [2, 4], [2, 4], [0, 1], [3, 0], [2, 4], [1, 0], [2, 4], [0, 1], [2, 1], [1, 2], [0, 3], [2, 4], [2, 1], [0, 3], [2, 4], [0, 1], [2, 1], [0, 1], [0, 3], [0, 1], [0, 1], [2, 4], [2, 4], [3, 4], [2, 3], [2, 1], [2, 4], [3, 2], [0, 3], [3, 0], [0, 1], [2, 4], [2, 1], [2, 1], [2, 4], [2, 1], [2, 1], [0, 3], [3, 4], [3, 4], [0, 1], [3, 0], [0, 1], [0, 1], [3, 4], [0, 1], [2, 4], [2, 1], [2, 1], [2, 4], [0, 1], [4, 2], [2, 4], [0, 3], [0, 1], [1, 0], [2, 4], [0, 1], [0, 2], [3, 0], [3, 0], [2, 4], [0, 3], [2, 4], [3, 0], [2, 1], [2, 4], [0, 3], [0, 1], [2, 4], [1, 0], [2, 4], [2, 4], [0, 3], [2, 4], [0, 1], [2, 4], [3, 0], [2, 4], [2, 4], [2, 4], [1, 0], [2, 4], [0, 3], [2, 4], [2, 4], [2, 1], [0, 1], [0, 1], [3, 4], [0, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004492129325866699  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004492124557495117  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004492116928100586  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004492105007171631  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004492089748382568  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004492072105407715  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0044920511245727535  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004492029666900634  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.0044920058250427245  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004491979598999023  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004491951942443848  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004491922378540039  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004491892814636231  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004491862297058106  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004491829872131347  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00449179744720459  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004491764068603516  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004491730690002442  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004491697311401368  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004491661548614502  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 0.005934579763561487 tensor([0.0514, 0.9366, 0.0059, 0.0023, 0.0037], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012102426262572408 tensor([2.2080e-01, 7.7746e-01, 2.7070e-04, 1.2102e-03, 2.6144e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004488667007535696 tensor([0.2170, 0.7758, 0.0010, 0.0045, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.025609159842133522 tensor([7.5666e-01, 2.1689e-01, 8.1949e-05, 2.5609e-02, 7.5912e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003641584189608693 tensor([5.3347e-02, 1.3311e-05, 7.5685e-09, 9.4628e-01, 3.6416e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21474689245224 tensor([0.1261, 0.3049, 0.0175, 0.3368, 0.2147], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003292150213383138 tensor([3.2922e-04, 4.2035e-05, 8.4712e-05, 7.9331e-01, 2.0623e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0020706283394247293 tensor([2.0706e-03, 4.1161e-06, 1.4290e-07, 9.9457e-01, 3.3517e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0020974185317754745 tensor([9.1965e-01, 7.8230e-02, 1.2653e-06, 2.0974e-03, 1.7747e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.020960820838809013 tensor([6.7084e-01, 2.0961e-02, 1.5470e-05, 3.0495e-01, 3.2355e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0019417256116867065 tensor([9.5928e-01, 3.8770e-02, 3.1583e-07, 1.9417e-03, 8.7765e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06117328628897667 tensor([0.0371, 0.7536, 0.0612, 0.0320, 0.1162], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18459248542785645 tensor([0.0020, 0.1846, 0.3455, 0.0210, 0.4469], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05065448209643364 tensor([0.1743, 0.7232, 0.0088, 0.0507, 0.0431], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1790652871131897 tensor([0.0013, 0.1791, 0.3540, 0.0120, 0.4537], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002925496955867857 tensor([6.1000e-01, 3.8969e-01, 4.9453e-06, 2.9255e-04, 1.5230e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004749166779220104 tensor([4.7492e-03, 2.3235e-04, 3.0898e-05, 9.2986e-01, 6.5130e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036316239275038242 tensor([1.2368e-07, 1.5763e-05, 1.7906e-02, 3.6316e-03, 9.7845e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04353402927517891 tensor([0.0393, 0.8330, 0.0435, 0.0204, 0.0639], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00020053668413311243 tensor([2.7916e-09, 2.0054e-04, 9.8628e-01, 2.1315e-06, 1.3513e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00037027319194749 tensor([9.5312e-01, 3.7027e-04, 2.3866e-09, 4.6499e-02, 7.7507e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1599903106689453 tensor([0.1600, 0.5016, 0.0215, 0.1804, 0.1365], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19654539227485657 tensor([0.0123, 0.5483, 0.1965, 0.0265, 0.2162], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.007393216248601675 tensor([3.3764e-06, 7.3932e-03, 7.7113e-01, 4.6940e-04, 2.2101e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08312982320785522 tensor([0.0044, 0.6935, 0.2152, 0.0038, 0.0831], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005333264824002981 tensor([2.5513e-01, 7.3706e-01, 7.2359e-04, 5.3333e-03, 1.7569e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011846404522657394 tensor([1.0799e-05, 8.7671e-04, 6.1525e-02, 1.1846e-02, 9.2574e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0516451895236969 tensor([0.0198, 0.8290, 0.0882, 0.0113, 0.0516], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0039086248725652695 tensor([1.8679e-05, 2.1619e-01, 7.7987e-01, 1.1918e-05, 3.9086e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00014832752640359104 tensor([1.4833e-04, 3.0178e-06, 3.6493e-06, 9.3575e-01, 6.4100e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008766583167016506 tensor([7.9572e-01, 8.7666e-03, 1.5546e-06, 1.9457e-01, 9.3975e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01930699497461319 tensor([5.5446e-01, 4.2436e-01, 3.9033e-04, 1.9307e-02, 1.4817e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006050963420420885 tensor([4.2314e-02, 2.1610e-05, 2.6135e-08, 9.5706e-01, 6.0510e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00036640846519730985 tensor([6.7365e-02, 9.3197e-01, 3.6641e-04, 1.8388e-04, 1.1198e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0071863592602312565 tensor([5.1372e-07, 2.2957e-05, 7.1864e-03, 7.6145e-03, 9.8518e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07199450582265854 tensor([6.6030e-01, 7.1995e-02, 8.1100e-05, 2.6026e-01, 7.3612e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0018924244213849306 tensor([1.7901e-04, 6.7976e-01, 3.1814e-01, 2.6858e-05, 1.8924e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0020610683131963015 tensor([6.6982e-05, 1.6822e-04, 2.0611e-03, 1.9167e-01, 8.0604e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04360496252775192 tensor([2.6432e-01, 3.2516e-02, 2.7623e-04, 6.5928e-01, 4.3605e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0242815800011158 tensor([8.4341e-01, 1.3156e-01, 4.0080e-05, 2.4282e-02, 7.0494e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004397035110741854 tensor([1.6550e-02, 9.7781e-01, 4.3970e-03, 2.9094e-04, 9.4969e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.051384128630161285 tensor([7.8522e-01, 5.1384e-02, 4.1428e-05, 1.5920e-01, 4.1594e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.030181018635630608 tensor([1.2029e-04, 6.2289e-03, 9.8488e-02, 3.0181e-02, 8.6498e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08664629608392715 tensor([0.0025, 0.0866, 0.1472, 0.0582, 0.7055], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001033584587275982 tensor([2.2478e-07, 1.9954e-06, 1.0336e-03, 2.8579e-02, 9.7039e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1040516123175621 tensor([0.0185, 0.1041, 0.0337, 0.2935, 0.5503], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021515043452382088 tensor([0.0241, 0.9457, 0.0215, 0.0023, 0.0064], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005646157078444958 tensor([5.6462e-03, 9.6373e-01, 2.7541e-02, 3.3212e-04, 2.7541e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.030222607776522636 tensor([2.2926e-05, 7.9267e-02, 8.9038e-01, 1.0969e-04, 3.0223e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0011586518958210945 tensor([3.1225e-06, 1.2780e-05, 1.1587e-03, 6.5540e-02, 9.3329e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001379873719997704 tensor([9.7467e-01, 1.3799e-04, 2.7753e-10, 2.5188e-02, 1.5492e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01435132697224617 tensor([0.1040, 0.8589, 0.0106, 0.0144, 0.0122], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0024297514464706182 tensor([0.0007, 0.0009, 0.0024, 0.4088, 0.5872], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0027620960026979446 tensor([4.5468e-06, 1.1101e-01, 8.8622e-01, 3.6507e-06, 2.7621e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.961147508060094e-06 tensor([5.9611e-06, 3.4559e-07, 5.5875e-06, 6.3834e-01, 3.6164e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008792927488684654 tensor([7.9637e-01, 8.7929e-03, 1.5895e-06, 1.9409e-01, 7.5035e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0037853019312024117 tensor([3.0214e-07, 3.7853e-03, 9.6809e-01, 2.2650e-05, 2.8101e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1887579709291458 tensor([0.0071, 0.2877, 0.1888, 0.0467, 0.4697], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08928173035383224 tensor([0.4693, 0.4233, 0.0016, 0.0893, 0.0165], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0021716065239161253 tensor([1.6754e-07, 2.1716e-03, 9.6424e-01, 2.2441e-05, 3.3569e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013101774966344237 tensor([8.4998e-01, 1.4869e-01, 2.2243e-06, 1.3102e-03, 2.2174e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005707466043531895 tensor([1.2621e-05, 5.7075e-03, 3.9657e-01, 3.8596e-03, 5.9386e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012664172099903226 tensor([1.8802e-07, 1.2664e-03, 9.0923e-01, 6.7448e-05, 8.9439e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.015106674283742905 tensor([0.0151, 0.9595, 0.0193, 0.0010, 0.0051], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04169675335288048 tensor([3.0971e-04, 4.1697e-02, 2.8131e-01, 1.2347e-02, 6.6434e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0059055788442492485 tensor([6.4484e-01, 3.4884e-01, 4.6554e-05, 5.9056e-03, 3.7371e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.047903962433338165 tensor([0.0019, 0.0169, 0.0479, 0.1768, 0.7565], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0174932349473238 tensor([1.7493e-02, 7.4316e-04, 3.1442e-05, 9.5508e-01, 2.6651e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0051621017046272755 tensor([1.1063e-02, 9.8302e-01, 5.1621e-03, 1.7157e-04, 5.7965e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011688022641465068 tensor([1.1688e-03, 3.9258e-05, 1.3877e-05, 9.3165e-01, 6.7124e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011204618029296398 tensor([6.6636e-01, 1.1205e-02, 6.0366e-06, 3.2024e-01, 2.1980e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12944085896015167 tensor([0.1199, 0.1294, 0.0065, 0.6035, 0.1406], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0043848552741110325 tensor([4.3849e-03, 9.4665e-01, 4.5390e-02, 3.2012e-04, 3.2592e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002011452103033662 tensor([0.1320, 0.8640, 0.0011, 0.0020, 0.0010], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007754871621727943 tensor([8.8533e-03, 3.2030e-06, 8.4607e-09, 9.9037e-01, 7.7549e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010048630647361279 tensor([9.7970e-01, 1.9292e-02, 5.3433e-08, 1.0049e-03, 2.9598e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007535865530371666 tensor([2.7284e-06, 4.0119e-02, 9.5233e-01, 1.3011e-05, 7.5359e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05922280251979828 tensor([0.0392, 0.0592, 0.0086, 0.6190, 0.2739], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10499222576618195 tensor([0.0069, 0.1066, 0.0928, 0.1050, 0.6888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07716280221939087 tensor([0.0082, 0.0772, 0.0456, 0.1462, 0.7227], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0021872101351618767 tensor([9.9530e-01, 2.5143e-03, 4.2103e-09, 2.1872e-03, 1.1756e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24130983650684357 tensor([0.0079, 0.4665, 0.2413, 0.0287, 0.2556], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0012481948360800743 tensor([4.0101e-07, 3.6726e-04, 2.6150e-01, 1.2482e-03, 7.3688e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0037084363866597414 tensor([0.0544, 0.9389, 0.0037, 0.0013, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006388214882463217 tensor([6.1660e-04, 2.1887e-03, 6.3882e-03, 1.9322e-01, 7.9759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2776145339012146 tensor([0.0014, 0.2835, 0.4303, 0.0072, 0.2776], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01051452849060297 tensor([7.9100e-06, 6.7651e-02, 9.2180e-01, 2.5255e-05, 1.0515e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.021296001970767975 tensor([6.9790e-04, 4.6608e-03, 2.1296e-02, 1.9857e-01, 7.7478e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0063342321664094925 tensor([3.5624e-06, 6.3342e-03, 8.0017e-01, 5.4225e-04, 1.9295e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16680678725242615 tensor([0.2585, 0.4790, 0.0072, 0.1668, 0.0885], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.19069193303585052 tensor([0.0060, 0.2939, 0.1907, 0.0277, 0.4816], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10684682428836823 tensor([2.4232e-04, 1.0685e-01, 7.1425e-01, 2.5519e-03, 1.7611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.15866544842720032 tensor([0.1587, 0.4779, 0.0196, 0.2162, 0.1276], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0034049060195684433 tensor([8.9370e-01, 1.0284e-01, 3.3860e-06, 3.4049e-03, 5.1230e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015543489716947079 tensor([1.8986e-04, 1.7900e-03, 1.5543e-02, 8.6167e-02, 8.9631e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01017575990408659 tensor([6.4698e-01, 1.0176e-02, 5.2788e-06, 3.4049e-01, 2.3439e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005656023509800434 tensor([1.8929e-04, 5.5996e-01, 4.3413e-01, 6.5513e-05, 5.6560e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.543894025729969e-05 tensor([1.7678e-10, 7.5439e-05, 9.9589e-01, 1.8940e-07, 4.0381e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0021463208831846714 tensor([1.0936e-02, 9.8672e-01, 2.1463e-03, 4.5454e-05, 1.5002e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017571491189301014 tensor([1.4039e-06, 1.0059e-03, 3.5112e-01, 1.7571e-03, 6.4611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08924548327922821 tensor([0.0578, 0.5517, 0.0310, 0.0892, 0.2703], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017516331747174263 tensor([1.7516e-02, 2.6124e-03, 3.5246e-04, 8.4682e-01, 1.3270e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009295416995882988 tensor([9.2954e-03, 2.4488e-03, 4.6937e-04, 8.3797e-01, 1.4982e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09666468948125839 tensor([7.4794e-01, 9.6665e-02, 1.0708e-04, 1.4968e-01, 5.6114e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007798037258908153 tensor([9.3567e-05, 1.0829e-04, 7.7980e-04, 2.2901e-01, 7.7001e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005147664924152195 tensor([3.1665e-01, 6.8272e-01, 4.5636e-05, 5.1477e-04, 6.6344e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005844812374562025 tensor([0.0058, 0.0033, 0.0016, 0.7072, 0.2820], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04894014447927475 tensor([5.8796e-01, 3.5956e-01, 4.3884e-04, 4.8940e-02, 3.0995e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023436548188328743 tensor([3.3716e-04, 1.7717e-02, 1.2848e-01, 2.3437e-02, 8.3003e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003477612277492881 tensor([4.6250e-08, 2.4347e-06, 3.4776e-03, 4.5640e-03, 9.9196e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00025555150932632387 tensor([9.9378e-01, 2.5555e-04, 1.7096e-10, 5.9667e-03, 5.1756e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15961937606334686 tensor([0.5814, 0.2473, 0.0007, 0.1596, 0.0109], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.014514518901705742 tensor([0.0009, 0.0044, 0.0145, 0.2400, 0.7402], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.033497538417577744 tensor([9.9999e-05, 3.3498e-02, 5.7332e-01, 4.7138e-03, 3.8836e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05195164307951927 tensor([5.6519e-04, 5.1952e-02, 2.8448e-01, 2.4839e-02, 6.3817e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010679531842470169 tensor([0.2607, 0.7249, 0.0010, 0.0107, 0.0028], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03728361055254936 tensor([0.1236, 0.8026, 0.0120, 0.0373, 0.0245], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004883647430688143 tensor([1.8565e-02, 9.7580e-01, 4.8836e-03, 2.4177e-04, 5.1300e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22721999883651733 tensor([0.0027, 0.2272, 0.3364, 0.0199, 0.4138], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006385778542608023 tensor([6.3858e-03, 2.3206e-04, 2.5666e-05, 8.9821e-01, 9.5149e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012738988734781742 tensor([1.6163e-06, 6.6361e-05, 1.2739e-02, 1.5985e-02, 9.7121e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027678631246089935 tensor([0.0155, 0.0277, 0.0104, 0.5652, 0.3813], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01095083449035883 tensor([6.2119e-05, 1.0951e-02, 2.7426e-01, 8.3887e-03, 7.0633e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10529506206512451 tensor([0.3151, 0.1053, 0.0010, 0.5005, 0.0781], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07000745087862015 tensor([7.1248e-02, 9.4285e-03, 2.2347e-04, 8.4909e-01, 7.0007e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002888157614506781 tensor([9.9926e-01, 2.8882e-04, 1.5852e-11, 4.4757e-04, 1.9088e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022496197372674942 tensor([8.9986e-01, 2.2496e-02, 5.6994e-06, 7.7205e-02, 4.3015e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.02602233737707138 tensor([0.0260, 0.0093, 0.0011, 0.7657, 0.1978], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14700108766555786 tensor([0.0056, 0.6090, 0.2299, 0.0084, 0.1470], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0034593951422721148 tensor([9.9876e-08, 5.7908e-03, 9.9075e-01, 1.4711e-06, 3.4594e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009942944161593914 tensor([9.9371e-01, 5.2965e-03, 6.1683e-09, 9.9429e-04, 8.5485e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004179440438747406 tensor([6.8286e-01, 4.1794e-03, 1.4223e-06, 3.1214e-01, 8.2630e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03183051943778992 tensor([0.0016, 0.0172, 0.0318, 0.1199, 0.8295], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022578198462724686 tensor([2.6334e-05, 1.0353e-03, 3.7817e-02, 2.2578e-02, 9.3854e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000628532434348017 tensor([6.2853e-04, 2.1942e-05, 9.7003e-06, 9.1369e-01, 8.5648e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0024448237381875515 tensor([9.9002e-01, 2.4448e-03, 1.3135e-08, 7.5356e-03, 4.1514e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.018624039366841316 tensor([0.1544, 0.8108, 0.0066, 0.0186, 0.0096], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01458055805414915 tensor([0.0146, 0.9378, 0.0338, 0.0020, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0054416838102042675 tensor([8.5447e-01, 1.3998e-01, 9.3091e-06, 5.4417e-03, 9.4396e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009015777613967657 tensor([4.4068e-07, 5.0609e-04, 3.7820e-01, 9.0158e-04, 6.2039e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03807668015360832 tensor([0.0162, 0.0381, 0.0115, 0.4257, 0.5086], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00416580168530345 tensor([5.4869e-04, 1.1029e-03, 4.1658e-03, 3.1296e-01, 6.8122e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032773741986602545 tensor([2.4567e-03, 9.1371e-01, 8.0360e-02, 1.9483e-04, 3.2774e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03746231272816658 tensor([0.2037, 0.7378, 0.0049, 0.0375, 0.0161], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.000723557430319488 tensor([7.2356e-04, 6.6267e-05, 5.4370e-05, 7.9189e-01, 2.0727e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.024045797064900398 tensor([0.0240, 0.0147, 0.0017, 0.7471, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0911484882235527 tensor([0.0159, 0.7685, 0.1054, 0.0190, 0.0911], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00032924985862337053 tensor([2.5421e-08, 3.2925e-04, 9.1494e-01, 4.2261e-05, 8.4690e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01696697808802128 tensor([6.3653e-01, 3.4505e-01, 1.8068e-04, 1.6967e-02, 1.2756e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00826180912554264 tensor([2.1885e-05, 3.1950e-03, 1.5128e-01, 8.2618e-03, 8.3724e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0332491472363472 tensor([0.0828, 0.8520, 0.0122, 0.0197, 0.0332], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06720402091741562 tensor([0.0014, 0.0301, 0.0764, 0.0672, 0.8248], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.038360320031642914 tensor([1.6023e-05, 5.5785e-02, 9.0570e-01, 1.3795e-04, 3.8360e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010102570988237858 tensor([2.0674e-05, 2.3680e-03, 1.0177e-01, 1.0103e-02, 8.8574e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005381996743381023 tensor([9.8714e-01, 7.4715e-03, 6.4303e-08, 5.3820e-03, 8.7248e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13376130163669586 tensor([0.0130, 0.6017, 0.1338, 0.0290, 0.2225], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02200852707028389 tensor([7.1009e-01, 2.6657e-01, 1.6146e-04, 2.2009e-02, 1.1669e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04633400961756706 tensor([6.6971e-01, 2.8106e-01, 2.2469e-04, 4.6334e-02, 2.6721e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019398918375372887 tensor([0.0604, 0.8980, 0.0132, 0.0090, 0.0194], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08557962626218796 tensor([7.6378e-01, 8.5580e-02, 7.6566e-05, 1.4688e-01, 3.6882e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009698967915028334 tensor([1.2260e-01, 8.7524e-01, 5.9126e-04, 9.6990e-04, 5.9378e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0021146140061318874 tensor([1.5514e-08, 2.6266e-06, 1.1753e-02, 2.1146e-03, 9.8613e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006436787545681 tensor([3.1724e-05, 2.3545e-01, 7.5806e-01, 2.2582e-05, 6.4368e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13761086761951447 tensor([0.3423, 0.1376, 0.0016, 0.4387, 0.0798], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0004440517514012754 tensor([1.5177e-04, 7.4378e-05, 4.4405e-04, 5.0519e-01, 4.9414e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006240773946046829 tensor([9.5190e-01, 6.2408e-04, 6.2856e-09, 4.7465e-02, 1.1209e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010351300006732345 tensor([1.8862e-06, 9.3125e-06, 1.0351e-03, 4.8702e-02, 9.5025e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015836432576179504 tensor([4.8804e-02, 1.1043e-04, 4.0163e-07, 9.4950e-01, 1.5836e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00025730347260832787 tensor([2.5730e-04, 2.5850e-05, 5.9851e-05, 7.6943e-01, 2.3022e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014325310476124287 tensor([9.3263e-06, 1.4325e-02, 7.9242e-01, 5.6418e-04, 1.9268e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.014924907125532627 tensor([9.5773e-01, 2.7252e-02, 1.2524e-06, 1.4925e-02, 8.9388e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003681114176288247 tensor([3.6811e-03, 4.0996e-05, 3.1688e-06, 9.8161e-01, 1.4663e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015020834980532527 tensor([3.4067e-07, 1.5021e-03, 8.9412e-01, 1.6053e-04, 1.0422e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012428711168467999 tensor([6.4844e-01, 3.3838e-01, 1.0511e-04, 1.2429e-02, 6.3795e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0026678070425987244 tensor([1.0030e-06, 3.6158e-04, 1.3835e-01, 2.6678e-03, 8.5862e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0033053893130272627 tensor([0.0033, 0.0016, 0.0009, 0.6268, 0.3674], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017479417845606804 tensor([4.2271e-05, 3.5966e-03, 1.1120e-01, 1.7479e-02, 8.6769e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.014517522417008877 tensor([5.8808e-06, 1.4518e-02, 9.0719e-01, 2.5757e-04, 7.8029e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005495916586369276 tensor([2.5928e-05, 5.3109e-03, 2.3787e-01, 5.4959e-03, 7.5129e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016799001023173332 tensor([1.0156e-05, 2.8039e-04, 1.6799e-02, 2.6558e-02, 9.5635e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004829959943890572 tensor([3.8667e-01, 6.0745e-01, 2.3000e-04, 4.8300e-03, 8.2263e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.13346102833747864 tensor([0.6031, 0.2510, 0.0008, 0.1335, 0.0116], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0020567397587001324 tensor([2.7645e-02, 9.6964e-01, 2.0567e-03, 2.4358e-04, 4.1113e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.060061436146497726 tensor([0.0601, 0.7956, 0.0336, 0.0401, 0.0707], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00108423235360533 tensor([1.7656e-01, 3.0884e-04, 2.3463e-07, 8.2205e-01, 1.0842e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00374584156088531 tensor([7.4481e-01, 2.5129e-01, 1.6771e-05, 3.7458e-03, 1.3232e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.016686776652932167 tensor([0.0577, 0.9025, 0.0149, 0.0082, 0.0167], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0015723496908321977 tensor([3.7076e-08, 1.5723e-03, 9.9063e-01, 3.0583e-06, 7.7938e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006424722960218787 tensor([6.0954e-04, 9.4130e-01, 5.7426e-02, 1.7507e-05, 6.4247e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0441223569214344 tensor([6.9007e-04, 2.5364e-02, 8.9059e-02, 4.4122e-02, 8.4077e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05071413516998291 tensor([6.1227e-01, 3.3146e-01, 2.8425e-04, 5.0714e-02, 5.2693e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0305045023560524 tensor([6.8730e-02, 4.7140e-03, 1.0029e-04, 8.9595e-01, 3.0505e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.062057141214609146 tensor([1.9525e-01, 4.4623e-02, 6.4853e-04, 6.9742e-01, 6.2057e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.031493864953517914 tensor([7.7737e-01, 1.8940e-01, 1.1081e-04, 3.1494e-02, 1.6166e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05438235402107239 tensor([0.0544, 0.0402, 0.0033, 0.5972, 0.3049], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04755019396543503 tensor([0.0476, 0.0468, 0.0031, 0.6045, 0.2981], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1373804658651352 tensor([0.0049, 0.1374, 0.1745, 0.0650, 0.6182], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0034150995779782534 tensor([2.4323e-02, 9.7104e-01, 3.4151e-03, 3.6170e-04, 8.6245e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005904855206608772 tensor([5.9049e-03, 9.7492e-01, 1.7676e-02, 1.8981e-04, 1.3100e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003964352945331484 tensor([2.0847e-08, 5.6900e-05, 3.4287e-01, 3.9644e-04, 6.5668e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01584584265947342 tensor([6.4845e-04, 6.6663e-01, 3.1658e-01, 2.8866e-04, 1.5846e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04287451505661011 tensor([2.6246e-04, 1.0316e-02, 1.0889e-01, 4.2875e-02, 8.3766e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.013643707148730755 tensor([3.4097e-05, 3.8203e-03, 1.1943e-01, 1.3644e-02, 8.6307e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003169591072946787 tensor([8.6709e-03, 9.8790e-01, 3.1696e-03, 5.4179e-05, 2.0980e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0036359017249196768 tensor([6.9531e-06, 7.0830e-05, 3.6359e-03, 3.0242e-02, 9.6604e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00022076796449255198 tensor([9.1916e-01, 8.0621e-02, 1.5694e-07, 2.2077e-04, 1.5357e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003826126456260681 tensor([6.6035e-03, 2.1310e-05, 3.9532e-07, 9.8955e-01, 3.8261e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08349800854921341 tensor([0.1302, 0.0419, 0.0011, 0.7433, 0.0835], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06253638118505478 tensor([8.0740e-01, 1.2786e-01, 8.6453e-05, 6.2536e-02, 2.1138e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04207814857363701 tensor([4.2745e-04, 1.4605e-02, 7.2095e-02, 4.2078e-02, 8.7079e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00031020556343719363 tensor([1.9496e-01, 8.0460e-01, 8.0843e-05, 3.1021e-04, 5.1410e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001588867511600256 tensor([9.2777e-01, 1.5889e-03, 5.7093e-08, 7.0614e-02, 3.0358e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0018260799115523696 tensor([4.0532e-08, 1.8261e-03, 9.9131e-01, 2.7299e-06, 6.8585e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004904200788587332 tensor([1.7652e-05, 4.7207e-03, 2.7084e-01, 4.9042e-03, 7.1951e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.7780081988312304e-05 tensor([3.8345e-05, 6.4107e-06, 5.7780e-05, 6.0229e-01, 3.9760e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00034449281520210207 tensor([5.9536e-01, 4.0428e-01, 6.4870e-06, 3.4449e-04, 1.3090e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026926076039671898 tensor([4.5424e-05, 1.8911e-03, 5.8143e-02, 2.6926e-02, 9.1299e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.047533195465803146 tensor([0.0095, 0.0475, 0.0286, 0.2752, 0.6393], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015403218567371368 tensor([1.0017e-03, 6.4231e-01, 3.4087e-01, 4.1031e-04, 1.5403e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011518609710037708 tensor([1.6686e-03, 8.3304e-01, 1.5339e-01, 3.8472e-04, 1.1519e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01164504885673523 tensor([4.4112e-01, 5.4495e-01, 3.3602e-04, 1.1645e-02, 1.9578e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.044133782386779785 tensor([5.1613e-04, 4.4134e-02, 2.6186e-01, 2.3743e-02, 6.6974e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1993960738182068 tensor([0.3955, 0.1994, 0.0019, 0.3611, 0.0421], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02051231823861599 tensor([0.0018, 0.7347, 0.2422, 0.0007, 0.0205], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003125294577330351 tensor([7.2393e-03, 1.3439e-05, 1.4098e-07, 9.8962e-01, 3.1253e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013846587389707565 tensor([8.0603e-01, 1.9255e-01, 3.8146e-06, 1.3847e-03, 2.8286e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08053456991910934 tensor([3.0906e-04, 2.2442e-01, 6.9353e-01, 1.2077e-03, 8.0535e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 6.899765139678493e-05 tensor([5.4766e-09, 6.7741e-05, 7.4982e-01, 6.8998e-05, 2.5004e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03663131967186928 tensor([0.2717, 0.6728, 0.0026, 0.0366, 0.0163], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00077526003587991 tensor([5.7238e-04, 3.7150e-04, 7.7526e-04, 4.6634e-01, 5.3195e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028814081102609634 tensor([7.5361e-01, 2.8814e-02, 1.9314e-05, 2.1428e-01, 3.2725e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0044158268719911575 tensor([9.7182e-01, 2.3749e-02, 3.1652e-07, 4.4158e-03, 1.2572e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0032259575091302395 tensor([1.7257e-07, 3.2260e-03, 9.7918e-01, 1.0011e-05, 1.7586e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10821548104286194 tensor([6.3392e-01, 1.0822e-01, 1.8845e-04, 2.4400e-01, 1.3677e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0025368055794388056 tensor([0.0013, 0.0014, 0.0025, 0.4252, 0.5696], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015067610889673233 tensor([7.9216e-01, 1.5068e-02, 4.5807e-06, 1.9019e-01, 2.5784e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01107268687337637 tensor([2.7141e-04, 4.7518e-01, 5.1332e-01, 1.5775e-04, 1.1073e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1799149513244629 tensor([0.3587, 0.1799, 0.0016, 0.3836, 0.0763], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006377437966875732 tensor([8.5481e-01, 6.3774e-04, 2.2780e-08, 1.4449e-01, 5.3402e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005084689473733306 tensor([5.3255e-01, 5.0847e-04, 7.0658e-08, 4.6653e-01, 4.1563e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007979171350598335 tensor([1.6156e-04, 1.5865e-04, 7.9792e-04, 3.5106e-01, 6.4782e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.060313936322927475 tensor([0.2314, 0.6751, 0.0081, 0.0603, 0.0251], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01598825678229332 tensor([2.5255e-06, 1.5988e-02, 9.5951e-01, 5.6401e-05, 2.4444e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.030896021053195 tensor([5.3405e-01, 3.0896e-02, 7.2035e-05, 4.2268e-01, 1.2303e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.027752913534641266 tensor([0.1578, 0.7818, 0.0067, 0.0278, 0.0259], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04386242479085922 tensor([1.7188e-01, 2.2595e-02, 2.3913e-04, 7.6142e-01, 4.3862e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00709309009835124 tensor([1.2602e-04, 5.9291e-04, 7.0931e-03, 1.7159e-01, 8.2060e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006876179948449135 tensor([2.9036e-06, 5.2345e-04, 1.1456e-01, 6.8762e-03, 8.7804e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0047372798435389996 tensor([1.4882e-02, 9.7931e-01, 4.7373e-03, 2.0775e-04, 8.6609e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0003524562926031649 tensor([3.9284e-06, 5.4740e-06, 3.5246e-04, 1.2953e-01, 8.7011e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4, 2], [2, 4, 3], [2, 4, 3], [2, 4, 3], [2, 1, 4], [2, 0], [1, 2, 0], [2, 1, 0], [2, 4, 3], [2, 4, 1], [2, 4, 3], [3, 0, 2], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 4, 3], [2, 1, 0], [0, 1, 3], [3, 0, 2], [0, 3, 1], [2, 4, 1], [2, 4, 0], [0, 3, 2], [0, 3, 1], [3, 0, 4], [2, 4, 3], [0, 1, 3], [3, 0, 4], [3, 0, 4], [1, 2, 0], [2, 4, 1], [2, 4, 3], [2, 1, 4], [4, 3, 2], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 1, 2], [2, 1, 4], [2, 4, 3], [3, 4, 2], [2, 4, 1], [0, 1, 3], [0, 3, 1], [0, 1, 2], [0, 2, 1], [3, 4, 2], [3, 4, 0], [0, 3, 4], [0, 1, 2], [2, 4, 1], [2, 4, 3], [0, 1, 2], [3, 0, 4], [1, 2, 0], [2, 4, 1], [0, 3, 1], [0, 3, 2], [2, 4, 3], [0, 3, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [3, 4, 0], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 1, 0], [3, 4, 2], [2, 1, 0], [2, 4, 1], [2, 0, 1], [3, 4, 0], [4, 2, 3], [2, 1, 4], [2, 4, 3], [0, 3, 4], [2, 0, 1], [0, 2, 3], [0, 2, 1], [2, 4, 3], [0, 3], [0, 1, 3], [3, 4, 2], [0, 1, 2], [0, 3], [0, 3, 4], [0, 1, 2], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 4, 0], [2, 4, 3], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [3, 4, 2], [0, 1, 3], [2, 0, 3], [2, 1, 0], [2, 1, 0], [2, 4, 1], [0, 1, 2], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3], [0, 1, 2], [2, 4, 1], [2, 4, 3], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3], [3, 4, 2], [0, 3], [2, 1, 0], [0, 1, 2], [2, 0, 1], [0, 3, 1], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 1], [2, 1, 0], [0, 3, 4], [0, 3, 4], [2, 4, 3], [2, 4, 1], [0, 1, 2], [0, 1, 3], [2, 1, 0], [2, 4, 1], [2, 4, 3], [3, 4, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 1, 2], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 0], [0, 3, 4], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [2, 4, 3], [0, 3, 2], [2, 4, 3], [2, 4, 3], [3, 2, 4], [2, 4, 1], [2, 4, 3], [0, 1, 3], [3, 0, 4], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 1, 2], [2, 1, 4], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 1, 0], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 1, 0], [0, 1, 3], [0, 3, 1], [0, 1, 3], [0, 1, 2], [2, 4, 3], [2, 4, 3], [3, 4, 2], [2, 3, 0], [2, 1, 4], [2, 4, 3], [3, 2, 4], [0, 3, 1], [3, 0, 4], [0, 1, 3], [2, 4, 3], [2, 1, 4], [2, 1, 4], [2, 4, 3], [2, 1, 0], [2, 1, 0], [0, 3, 1], [3, 4, 2], [3, 4, 0], [0, 1, 3], [3, 0, 4], [0, 1, 3], [0, 1, 3], [3, 4, 2], [0, 1, 2], [2, 4, 3], [2, 1, 4], [2, 1, 4], [2, 4, 3], [0, 1, 3], [4, 2, 3], [2, 4, 1], [0, 3, 1], [0, 1, 3], [1, 0, 2], [2, 4, 3], [0, 1, 3], [0, 2, 1], [3, 0, 4], [3, 0, 4], [2, 4, 3], [0, 3, 1], [2, 4, 1], [3, 0, 4], [2, 1, 4], [2, 4, 3], [0, 3, 4], [0, 1, 3], [2, 4, 3], [1, 0, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [3, 0, 4], [2, 4, 1], [2, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 1, 4], [0, 1, 2], [0, 1, 3], [3, 4, 2], [0, 1, 2]]\n",
      "[[0, 1], [0, 1], [0, 1], [0, 1], [0, 3], [1, 3, 4], [3, 4], [3, 4], [0, 1], [0, 3], [0, 1], [1, 4], [2, 4], [0, 1], [2, 4], [0, 1], [3, 4], [2, 4], [1, 4], [2, 4], [0, 3], [1, 3], [1, 4], [2, 4], [1, 2], [0, 1], [2, 4], [1, 2], [1, 2], [3, 4], [0, 3], [0, 1], [0, 3], [0, 1], [3, 4], [0, 3], [1, 2], [3, 4], [0, 3], [0, 1], [0, 1], [0, 3], [2, 4], [2, 4], [3, 4], [3, 4], [0, 1], [1, 2], [1, 2], [3, 4], [0, 3], [0, 1], [3, 4], [1, 2], [3, 4], [0, 3], [2, 4], [1, 4], [0, 1], [2, 4], [0, 1], [2, 4], [2, 4], [1, 2], [2, 4], [0, 1], [3, 4], [3, 4], [0, 1], [3, 4], [0, 3], [3, 4], [1, 2], [0, 1], [0, 3], [0, 1], [1, 2], [3, 4], [1, 4], [3, 4], [0, 1], [1, 2, 4], [2, 4], [0, 1], [3, 4], [1, 2, 4], [1, 2], [3, 4], [2, 4], [0, 1], [1, 4], [2, 4], [1, 3], [0, 1], [3, 4], [0, 3], [1, 2], [2, 4], [0, 1], [2, 4], [1, 4], [3, 4], [3, 4], [0, 3], [3, 4], [0, 1], [3, 4], [0, 1], [2, 4], [3, 4], [0, 3], [0, 1], [3, 4], [2, 4], [2, 4], [0, 1], [0, 1], [0, 1], [1, 2, 4], [3, 4], [3, 4], [3, 4], [2, 4], [0, 3], [0, 3], [0, 3], [0, 3], [3, 4], [1, 2], [1, 2], [0, 1], [0, 3], [3, 4], [2, 4], [3, 4], [0, 3], [0, 1], [1, 2], [0, 1], [2, 4], [3, 4], [3, 4], [1, 2], [0, 1], [3, 4], [3, 4], [1, 2], [2, 4], [0, 1], [2, 4], [0, 1], [2, 4], [1, 2], [2, 4], [0, 1], [1, 4], [0, 1], [0, 1], [0, 1], [0, 3], [0, 1], [2, 4], [1, 2], [0, 3], [3, 4], [0, 3], [3, 4], [0, 3], [3, 4], [2, 4], [0, 1], [3, 4], [2, 4], [0, 1], [2, 4], [3, 4], [2, 4], [2, 4], [2, 4], [3, 4], [0, 1], [0, 1], [0, 1], [1, 4], [0, 3], [0, 1], [0, 1], [2, 4], [1, 2], [2, 4], [0, 1], [0, 3], [0, 3], [0, 1], [3, 4], [3, 4], [2, 4], [0, 1], [1, 2], [2, 4], [1, 2], [2, 4], [2, 4], [0, 1], [3, 4], [0, 1], [0, 3], [0, 3], [0, 1], [2, 4], [0, 1], [0, 3], [2, 4], [2, 4], [3, 4], [0, 1], [2, 4], [3, 4], [1, 2], [1, 2], [0, 1], [2, 4], [0, 3], [1, 2], [0, 3], [0, 1], [1, 2], [2, 4], [0, 1], [3, 4], [0, 3], [0, 1], [2, 4], [0, 3], [3, 4], [0, 3], [1, 2], [0, 3], [0, 3], [0, 3], [3, 4], [0, 1], [2, 4], [0, 3], [0, 1], [0, 3], [3, 4], [2, 4], [0, 1], [3, 4]]\n",
      "NL_pred of 2th iteration [[3, 4, 2], [2, 4, 3], [2, 4, 3], [2, 4, 3], [2, 1, 4], [1, 2, 0], [2, 1, 0], [2, 4, 3], [2, 4, 1], [2, 4, 3], [3, 0, 2], [0, 3, 1], [2, 4, 3], [0, 3, 1], [2, 4, 3], [2, 1, 0], [0, 1, 3], [3, 0, 2], [0, 3, 1], [2, 4, 1], [2, 4, 0], [0, 3, 2], [0, 3, 1], [3, 0, 4], [2, 4, 3], [0, 1, 3], [3, 0, 4], [3, 0, 4], [1, 2, 0], [2, 4, 1], [2, 4, 3], [2, 1, 4], [4, 3, 2], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 1, 2], [2, 1, 4], [2, 4, 3], [3, 4, 2], [2, 4, 1], [0, 1, 3], [0, 3, 1], [0, 1, 2], [0, 2, 1], [3, 4, 2], [3, 4, 0], [0, 3, 4], [0, 1, 2], [2, 4, 1], [2, 4, 3], [0, 1, 2], [3, 0, 4], [1, 2, 0], [2, 4, 1], [0, 3, 1], [0, 3, 2], [2, 4, 3], [0, 3, 1], [2, 4, 3], [0, 3, 1], [0, 3, 1], [3, 4, 0], [0, 3, 1], [2, 4, 3], [0, 1, 2], [2, 1, 0], [3, 4, 2], [2, 1, 0], [2, 4, 1], [2, 0, 1], [3, 4, 0], [4, 2, 3], [2, 1, 4], [2, 4, 3], [0, 3, 4], [2, 0, 1], [0, 2, 3], [0, 2, 1], [2, 4, 3], [0, 1, 3], [3, 4, 2], [0, 1, 2], [0, 3, 4], [0, 1, 2], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 4, 0], [2, 4, 3], [0, 1, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [3, 4, 2], [0, 1, 3], [2, 0, 3], [2, 1, 0], [2, 1, 0], [2, 4, 1], [0, 1, 2], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3], [0, 1, 2], [2, 4, 1], [2, 4, 3], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3], [3, 4, 2], [2, 1, 0], [0, 1, 2], [2, 0, 1], [0, 3, 1], [2, 4, 1], [2, 1, 4], [2, 4, 1], [2, 4, 1], [2, 1, 0], [0, 3, 4], [0, 3, 4], [2, 4, 3], [2, 4, 1], [0, 1, 2], [0, 1, 3], [2, 1, 0], [2, 4, 1], [2, 4, 3], [3, 4, 0], [2, 4, 3], [0, 1, 3], [2, 0, 1], [0, 1, 2], [3, 0, 4], [2, 4, 3], [2, 1, 0], [2, 1, 0], [0, 3, 4], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 3, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [2, 4, 3], [0, 3, 2], [2, 4, 3], [2, 4, 3], [3, 2, 4], [2, 4, 1], [2, 4, 3], [0, 1, 3], [3, 0, 4], [2, 4, 1], [1, 0, 2], [2, 4, 1], [0, 1, 2], [2, 1, 4], [1, 2, 0], [0, 3, 1], [2, 4, 3], [2, 1, 0], [0, 3, 1], [2, 4, 3], [0, 1, 3], [2, 1, 0], [0, 1, 3], [0, 3, 1], [0, 1, 3], [0, 1, 2], [2, 4, 3], [2, 4, 3], [3, 4, 2], [2, 3, 0], [2, 1, 4], [2, 4, 3], [3, 2, 4], [0, 3, 1], [3, 0, 4], [0, 1, 3], [2, 4, 3], [2, 1, 4], [2, 1, 4], [2, 4, 3], [2, 1, 0], [2, 1, 0], [0, 3, 1], [3, 4, 2], [3, 4, 0], [0, 1, 3], [3, 0, 4], [0, 1, 3], [0, 1, 3], [3, 4, 2], [0, 1, 2], [2, 4, 3], [2, 1, 4], [2, 1, 4], [2, 4, 3], [0, 1, 3], [4, 2, 3], [2, 4, 1], [0, 3, 1], [0, 1, 3], [1, 0, 2], [2, 4, 3], [0, 1, 3], [0, 2, 1], [3, 0, 4], [3, 0, 4], [2, 4, 3], [0, 3, 1], [2, 4, 1], [3, 0, 4], [2, 1, 4], [2, 4, 3], [0, 3, 4], [0, 1, 3], [2, 4, 3], [1, 0, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 4, 1], [0, 1, 2], [2, 4, 1], [3, 0, 4], [2, 4, 1], [2, 4, 1], [2, 4, 1], [1, 0, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [2, 4, 3], [2, 1, 4], [0, 1, 2], [0, 1, 3], [3, 4, 2], [0, 1, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004652757470200702  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004652734694442128  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.0046526905966968075  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004652629538280208  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004652552003782939  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004652460900748648  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004652357198358551  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004652244288746904  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004652123625685529  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.004651996662946251  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004651864369710286  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.00465172771515885  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.00465158960683559  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004651449075559291  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004651306605920559  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.00465116365169122  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0046510211820524885  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004650879681594972  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004650737211956241  Accuracy on Support set:0.0\n",
      "torch.Size([246, 2048]) torch.Size([246])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004650596680679942  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.05632997676730156 tensor([0.0563, 0.9313, 0.0058, 0.0026, 0.0039], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23900723457336426 tensor([2.3901e-01, 7.5914e-01, 2.5578e-04, 1.3306e-03, 2.6980e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23496903479099274 tensor([0.2350, 0.7574, 0.0009, 0.0050, 0.0018], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.20119595527648926 tensor([7.7077e-01, 2.0120e-01, 7.5192e-05, 2.7195e-02, 7.6381e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05115228518843651 tensor([5.1152e-02, 1.2051e-05, 6.8730e-09, 9.4848e-01, 3.5344e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21635004878044128 tensor([0.1283, 0.2804, 0.0159, 0.3591, 0.2164], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20197005569934845 tensor([3.1319e-04, 3.7234e-05, 7.5799e-05, 7.9760e-01, 2.0197e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0032002043444663286 tensor([1.9786e-03, 3.5909e-06, 1.2342e-07, 9.9482e-01, 3.2002e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07137618213891983 tensor([9.2647e-01, 7.1376e-02, 1.1164e-06, 2.1397e-03, 1.7034e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3141602873802185 tensor([6.6371e-01, 1.8943e-02, 1.3804e-05, 3.1416e-01, 3.1706e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03507783263921738 tensor([9.6293e-01, 3.5078e-02, 2.7635e-07, 1.9792e-03, 8.3944e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1248447522521019 tensor([0.0404, 0.7395, 0.0590, 0.0363, 0.1248], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32664787769317627 tensor([0.0021, 0.1739, 0.3266, 0.0230, 0.4744], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1867142766714096 tensor([0.1867, 0.7034, 0.0084, 0.0562, 0.0453], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33568233251571655 tensor([0.0013, 0.1694, 0.3357, 0.0131, 0.4805], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3657167851924896 tensor([6.3396e-01, 3.6572e-01, 4.4873e-06, 3.0817e-04, 1.5084e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06300199031829834 tensor([4.4893e-03, 2.0209e-04, 2.7040e-05, 9.3228e-01, 6.3002e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01615200750529766 tensor([1.2225e-07, 1.4379e-05, 1.6152e-02, 3.7470e-03, 9.8009e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06910674273967743 tensor([0.0425, 0.8227, 0.0426, 0.0231, 0.0691], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014551014639437199 tensor([2.9027e-09, 1.9755e-04, 9.8525e-01, 2.3156e-06, 1.4551e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04850618541240692 tensor([9.5115e-01, 3.3855e-04, 2.1573e-09, 4.8506e-02, 7.6717e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19455265998840332 tensor([0.1625, 0.4779, 0.0209, 0.1946, 0.1442], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23101429641246796 tensor([0.0136, 0.5385, 0.1867, 0.0302, 0.2310], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2401401549577713 tensor([3.5185e-06, 7.1327e-03, 7.5220e-01, 5.2161e-04, 2.4014e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21153976023197174 tensor([0.0047, 0.6897, 0.2115, 0.0042, 0.0899], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2744457423686981 tensor([2.7445e-01, 7.1718e-01, 6.8524e-04, 5.8720e-03, 1.8220e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.055845957249403 tensor([1.0430e-05, 7.8796e-04, 5.5846e-02, 1.2142e-02, 9.3121e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08499541878700256 tensor([0.0220, 0.8253, 0.0850, 0.0128, 0.0549], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2164938598871231 tensor([1.9751e-05, 2.1649e-01, 7.7935e-01, 1.2830e-05, 4.1187e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06250951439142227 tensor([1.4084e-04, 2.6716e-06, 3.2627e-06, 9.3734e-01, 6.2510e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20210692286491394 tensor([7.8918e-01, 7.7969e-03, 1.3572e-06, 2.0211e-01, 9.1663e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4035191237926483 tensor([5.7383e-01, 4.0352e-01, 3.6928e-04, 2.0762e-02, 1.5219e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04090624675154686 tensor([4.0906e-02, 1.9470e-05, 2.3303e-08, 9.5849e-01, 5.8127e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07332047075033188 tensor([7.3320e-02, 9.2601e-01, 3.5291e-04, 2.0207e-04, 1.1672e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007852266542613506 tensor([5.0551e-07, 2.0886e-05, 6.4915e-03, 7.8523e-03, 9.8563e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.27094587683677673 tensor([6.5748e-01, 6.4310e-02, 7.1019e-05, 2.7095e-01, 7.1900e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3187587261199951 tensor([1.8854e-04, 6.7902e-01, 3.1876e-01, 2.8975e-05, 2.0025e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19862382113933563 tensor([6.4975e-05, 1.4849e-04, 1.8270e-03, 1.9862e-01, 7.9934e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2584548890590668 tensor([2.5845e-01, 2.8565e-02, 2.3715e-04, 6.7110e-01, 4.1643e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.12041577696800232 tensor([8.5340e-01, 1.2042e-01, 3.5946e-05, 2.5449e-02, 6.9614e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01813771389424801 tensor([1.8138e-02, 9.7630e-01, 4.2415e-03, 3.2229e-04, 1.0007e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16633521020412445 tensor([7.8338e-01, 4.6153e-02, 3.6582e-05, 1.6634e-01, 4.0934e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08947093784809113 tensor([1.1960e-04, 5.6566e-03, 8.9471e-02, 3.1798e-02, 8.7295e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13517288863658905 tensor([0.0025, 0.0797, 0.1352, 0.0621, 0.7205], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.029711691662669182 tensor([2.2337e-07, 1.8239e-06, 9.3777e-04, 2.9712e-02, 9.6935e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31079402565956116 tensor([0.0186, 0.0937, 0.0298, 0.3108, 0.5471], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.025878097862005234 tensor([0.0259, 0.9433, 0.0214, 0.0026, 0.0068], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026870589703321457 tensor([6.1689e-03, 9.6364e-01, 2.6871e-02, 3.7134e-04, 2.9490e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0778714269399643 tensor([2.4088e-05, 7.7871e-02, 8.8891e-01, 1.2185e-04, 3.3068e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06846913695335388 tensor([3.1184e-06, 1.1667e-05, 1.0469e-03, 6.8469e-02, 9.3047e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02600562945008278 tensor([9.7387e-01, 1.2749e-04, 2.5243e-10, 2.6006e-02, 1.5240e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.11185986548662186 tensor([0.1119, 0.8487, 0.0104, 0.0161, 0.0130], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41994279623031616 tensor([0.0007, 0.0008, 0.0021, 0.4199, 0.5765], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11059514433145523 tensor([4.7238e-06, 1.1060e-01, 8.8651e-01, 3.8544e-06, 2.8856e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.353791743516922 tensor([5.8090e-06, 3.1276e-07, 5.0229e-06, 6.4620e-01, 3.5379e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20182135701179504 tensor([7.8957e-01, 7.8710e-03, 1.4039e-06, 2.0182e-01, 7.3645e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.031158823519945145 tensor([3.2841e-07, 3.7911e-03, 9.6502e-01, 2.5880e-05, 3.1159e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2733399569988251 tensor([0.0077, 0.2733, 0.1739, 0.0525, 0.4926], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4016343951225281 tensor([0.4851, 0.4016, 0.0015, 0.0951, 0.0168], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03700984641909599 tensor([1.7933e-07, 2.1572e-03, 9.6081e-01, 2.5337e-05, 3.7010e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1365506500005722 tensor([8.6208e-01, 1.3655e-01, 1.9814e-06, 1.3501e-03, 2.1505e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3714667856693268 tensor([1.2621e-05, 5.2885e-03, 3.7147e-01, 4.1082e-03, 6.1912e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09921259433031082 tensor([2.0208e-07, 1.2514e-03, 8.9946e-01, 7.6946e-05, 9.9213e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.019035089761018753 tensor([0.0164, 0.9580, 0.0190, 0.0011, 0.0055], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2599799335002899 tensor([3.1407e-04, 3.8673e-02, 2.5998e-01, 1.3227e-02, 6.8781e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3255586624145508 tensor([6.6778e-01, 3.2556e-01, 4.2175e-05, 6.2440e-03, 3.7075e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1835480034351349 tensor([0.0018, 0.0151, 0.0432, 0.1835, 0.7563], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.025091880932450294 tensor([1.6719e-02, 6.3764e-04, 2.6490e-05, 9.5753e-01, 2.5092e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.012125717476010323 tensor([1.2126e-02, 9.8210e-01, 4.9744e-03, 1.8972e-04, 6.0973e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06488650292158127 tensor([1.1149e-03, 3.4554e-05, 1.2218e-05, 9.3395e-01, 6.4887e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.33150920271873474 tensor([6.5637e-01, 9.9656e-03, 5.3083e-06, 3.3151e-01, 2.1516e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1385575532913208 tensor([0.1161, 0.1136, 0.0058, 0.6259, 0.1386], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.044360630214214325 tensor([4.7955e-03, 9.4698e-01, 4.4361e-02, 3.5887e-04, 3.5007e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.14333254098892212 tensor([0.1433, 0.8524, 0.0010, 0.0022, 0.0010], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008473011665046215 tensor([8.4730e-03, 2.8734e-06, 7.5794e-09, 9.9078e-01, 7.4809e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.01761675626039505 tensor([9.8136e-01, 1.7617e-02, 4.7410e-08, 1.0215e-03, 2.8466e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04020645469427109 tensor([2.9021e-06, 4.0206e-02, 9.5173e-01, 1.4229e-05, 8.0423e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2646592855453491 tensor([0.0383, 0.0517, 0.0074, 0.6379, 0.2647], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09779107570648193 tensor([0.0068, 0.0978, 0.0856, 0.1106, 0.6992], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15400183200836182 tensor([0.0082, 0.0694, 0.0412, 0.1540, 0.7273], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0022718084510415792 tensor([9.9549e-01, 2.2718e-03, 3.6954e-09, 2.2404e-03, 1.1315e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2320035696029663 tensor([0.0085, 0.4553, 0.2320, 0.0320, 0.2722], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24050253629684448 tensor([4.0171e-07, 3.3921e-04, 2.4050e-01, 1.3159e-03, 7.5784e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05906069278717041 tensor([0.0591, 0.9341, 0.0036, 0.0014, 0.0018], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20077863335609436 tensor([6.0219e-04, 1.9407e-03, 5.6630e-03, 2.0078e-01, 7.9102e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27539417147636414 tensor([0.0015, 0.2754, 0.4155, 0.0080, 0.2996], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06718391180038452 tensor([8.2868e-06, 6.7184e-02, 9.2158e-01, 2.7365e-05, 1.1203e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20852844417095184 tensor([6.9927e-04, 4.2013e-03, 1.8846e-02, 2.0853e-01, 7.6773e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21033638715744019 tensor([3.7174e-06, 6.1237e-03, 7.8293e-01, 6.0398e-04, 2.1034e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2697581946849823 tensor([0.2698, 0.4537, 0.0067, 0.1797, 0.0902], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.27988773584365845 tensor([0.0063, 0.2799, 0.1781, 0.0304, 0.5054], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19075405597686768 tensor([2.4821e-04, 1.0270e-01, 7.0350e-01, 2.7975e-03, 1.9075e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23605474829673767 tensor([0.1628, 0.4491, 0.0186, 0.2361, 0.1334], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09414313733577728 tensor([9.0232e-01, 9.4143e-02, 3.0046e-06, 3.4877e-03, 4.9420e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09075257927179337 tensor([1.9119e-04, 1.6294e-03, 1.3905e-02, 9.0753e-02, 8.9352e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3522210717201233 tensor([6.3644e-01, 9.0412e-03, 4.6363e-06, 3.5222e-01, 2.2918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.43104100227355957 tensor([2.0392e-04, 5.6264e-01, 4.3104e-01, 7.2267e-05, 6.0460e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00427589938044548 tensor([1.8030e-10, 7.3870e-05, 9.9565e-01, 2.0062e-07, 4.2759e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01181891467422247 tensor([1.1819e-02, 9.8591e-01, 2.0709e-03, 4.9100e-05, 1.5529e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32779794931411743 tensor([1.4132e-06, 9.3834e-04, 3.2780e-01, 1.8638e-03, 6.6940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2835901975631714 tensor([0.0609, 0.5282, 0.0293, 0.0981, 0.2836], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.12805220484733582 tensor([1.6665e-02, 2.2802e-03, 3.0847e-04, 8.5269e-01, 1.2805e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14278818666934967 tensor([8.8734e-03, 2.1082e-03, 4.0079e-04, 8.4583e-01, 1.4279e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15664200484752655 tensor([7.5055e-01, 8.7204e-02, 9.4576e-05, 1.5664e-01, 5.5113e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23547248542308807 tensor([9.0590e-05, 9.6493e-05, 6.9944e-04, 2.3547e-01, 7.6364e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3381894528865814 tensor([3.3819e-01, 6.6114e-01, 4.2897e-05, 5.5811e-04, 6.7866e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2774856686592102 tensor([0.0055, 0.0029, 0.0015, 0.7126, 0.2775], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.33580952882766724 tensor([6.0823e-01, 3.3581e-01, 4.0051e-04, 5.2439e-02, 3.1219e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11707966774702072 tensor([3.3574e-04, 1.6221e-02, 1.1708e-01, 2.4527e-02, 8.4184e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00472799688577652 tensor([4.5842e-08, 2.2264e-06, 3.1467e-03, 4.7280e-03, 9.9212e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00615853164345026 tensor([9.9361e-01, 2.3542e-04, 1.5495e-10, 6.1585e-03, 5.0673e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23153942823410034 tensor([0.5890, 0.2315, 0.0007, 0.1678, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24966539442539215 tensor([0.0009, 0.0039, 0.0129, 0.2497, 0.7327], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41517361998558044 tensor([1.0429e-04, 3.1993e-02, 5.4752e-01, 5.2104e-03, 4.1517e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26343730092048645 tensor([5.6710e-04, 4.7833e-02, 2.6344e-01, 2.6548e-02, 6.6161e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27961426973342896 tensor([0.2796, 0.7048, 0.0009, 0.0118, 0.0029], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1349363923072815 tensor([0.1349, 0.7857, 0.0114, 0.0421, 0.0258], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020518062636256218 tensor([2.0518e-02, 9.7399e-01, 4.6872e-03, 2.6959e-04, 5.3770e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21959297358989716 tensor([0.0028, 0.2196, 0.3216, 0.0215, 0.4345], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09189329296350479 tensor([5.9723e-03, 1.9901e-04, 2.2322e-05, 9.0191e-01, 9.1893e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01660752296447754 tensor([1.5874e-06, 5.9696e-05, 1.1448e-02, 1.6608e-02, 9.7188e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.37350353598594666 tensor([0.0148, 0.0242, 0.0092, 0.5783, 0.3735], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2526510953903198 tensor([6.3571e-05, 1.0228e-02, 2.5265e-01, 8.9764e-03, 7.2808e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31224414706230164 tensor([0.3122, 0.0939, 0.0009, 0.5171, 0.0759], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06907837092876434 tensor([6.9078e-02, 8.2286e-03, 1.9052e-04, 8.5622e-01, 6.6279e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00045825919369235635 tensor([9.9928e-01, 2.6585e-04, 1.4343e-11, 4.5826e-04, 1.8583e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08069947361946106 tensor([8.9798e-01, 2.0883e-02, 5.3151e-06, 8.0699e-02, 4.3378e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18940667808055878 tensor([0.0250, 0.0081, 0.0009, 0.7766, 0.1894], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22363007068634033 tensor([0.0060, 0.6043, 0.2236, 0.0093, 0.1567], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005703927483409643 tensor([1.0259e-07, 5.7039e-03, 9.9065e-01, 1.5581e-06, 3.6415e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004788979422301054 tensor([9.9420e-01, 4.7890e-03, 5.4042e-09, 1.0123e-03, 8.1715e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32195571064949036 tensor([6.7346e-01, 3.7694e-03, 1.2716e-06, 3.2196e-01, 8.1183e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12590286135673523 tensor([0.0016, 0.0153, 0.0284, 0.1259, 0.8288], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03413177281618118 tensor([2.5776e-05, 9.3131e-04, 3.4132e-02, 2.3452e-02, 9.4146e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08224592357873917 tensor([6.0307e-04, 1.9267e-05, 8.4482e-06, 9.1712e-01, 8.2246e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007730285637080669 tensor([9.9004e-01, 2.2298e-03, 1.1696e-08, 7.7303e-03, 4.0274e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16598622500896454 tensor([0.1660, 0.7966, 0.0064, 0.0208, 0.0102], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.033018190413713455 tensor([0.0160, 0.9359, 0.0330, 0.0023, 0.0128], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1283976286649704 tensor([8.6589e-01, 1.2840e-01, 8.2962e-06, 5.6146e-03, 9.1690e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.35305267572402954 tensor([4.4771e-07, 4.7497e-04, 3.5305e-01, 9.6148e-04, 6.4551e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.44149866700172424 tensor([0.0159, 0.0337, 0.0101, 0.4415, 0.4989], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.32085371017456055 tensor([5.2963e-04, 9.7952e-04, 3.7250e-03, 3.2085e-01, 6.7391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07902005314826965 tensor([2.6534e-03, 9.1462e-01, 7.9020e-02, 2.1469e-04, 3.4890e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21899369359016418 tensor([0.2190, 0.7178, 0.0047, 0.0417, 0.0169], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20114029943943024 tensor([6.9216e-04, 5.8330e-05, 4.7880e-05, 7.9806e-01, 2.0114e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20670653879642487 tensor([0.0227, 0.0128, 0.0015, 0.7563, 0.2067], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10256792604923248 tensor([0.0173, 0.7609, 0.1026, 0.0216, 0.0977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09454469382762909 tensor([2.7930e-08, 3.2958e-04, 9.0508e-01, 4.8799e-05, 9.4545e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3233907222747803 tensor([6.5712e-01, 3.2339e-01, 1.6586e-04, 1.8036e-02, 1.2823e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13771919906139374 tensor([2.2183e-05, 2.9600e-03, 1.3772e-01, 8.7145e-03, 8.5058e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09063389152288437 tensor([0.0906, 0.8395, 0.0118, 0.0224, 0.0357], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06945784389972687 tensor([0.0014, 0.0271, 0.0695, 0.0704, 0.8317], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0562068410217762 tensor([1.7564e-05, 5.6207e-02, 9.0153e-01, 1.5675e-04, 4.2091e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09197034686803818 tensor([2.0309e-05, 2.1411e-03, 9.1970e-02, 1.0494e-02, 8.9537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006803229916840792 tensor([9.8765e-01, 6.8032e-03, 5.7225e-08, 5.5382e-03, 8.4812e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23785419762134552 tensor([0.0141, 0.5875, 0.1276, 0.0329, 0.2379], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2495860457420349 tensor([7.2593e-01, 2.4959e-01, 1.4880e-04, 2.3163e-02, 1.1688e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25939294695854187 tensor([6.8854e-01, 2.5939e-01, 2.0151e-04, 4.9215e-02, 2.6527e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06562992930412292 tensor([0.0656, 0.8911, 0.0128, 0.0100, 0.0205], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15383002161979675 tensor([7.6452e-01, 7.7920e-02, 6.8966e-05, 1.5383e-01, 3.6616e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13327571749687195 tensor([1.3328e-01, 8.6447e-01, 5.6573e-04, 1.0675e-03, 6.1815e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.010700467973947525 tensor([1.5276e-08, 2.4086e-06, 1.0700e-02, 2.1682e-03, 9.8713e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2378370314836502 tensor([3.4130e-05, 2.3784e-01, 7.5529e-01, 2.4620e-05, 6.8192e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3381531238555908 tensor([0.3382, 0.1223, 0.0014, 0.4592, 0.0789], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4837053716182709 tensor([1.4600e-04, 6.5451e-05, 3.9158e-04, 5.1569e-01, 4.8371e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.049679581075906754 tensor([9.4974e-01, 5.6922e-04, 5.6806e-09, 4.9680e-02, 1.1134e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05050336569547653 tensor([1.8667e-06, 8.4922e-06, 9.3927e-04, 5.0503e-02, 9.4855e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.046579938381910324 tensor([4.6580e-02, 9.6121e-05, 3.4675e-07, 9.5181e-01, 1.5127e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22408133745193481 tensor([2.4678e-04, 2.2925e-05, 5.3149e-05, 7.7560e-01, 2.2408e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21093213558197021 tensor([9.8827e-06, 1.3952e-02, 7.7447e-01, 6.3570e-04, 2.1093e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024535881355404854 tensor([9.5993e-01, 2.4536e-02, 1.0970e-06, 1.5449e-02, 8.6443e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01419055461883545 tensor([3.4750e-03, 3.5717e-05, 2.7821e-06, 9.8230e-01, 1.4191e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1145009845495224 tensor([3.5860e-07, 1.4672e-03, 8.8385e-01, 1.8014e-04, 1.1450e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3158102035522461 tensor([6.7039e-01, 3.1581e-01, 9.4979e-05, 1.3072e-02, 6.2962e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12647324800491333 tensor([9.8545e-07, 3.3021e-04, 1.2647e-01, 2.7601e-03, 8.7044e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3562333285808563 tensor([0.0032, 0.0014, 0.0008, 0.6384, 0.3562], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10100147128105164 tensor([4.1419e-05, 3.2570e-03, 1.0100e-01, 1.8125e-02, 8.7757e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08642173558473587 tensor([6.2517e-06, 1.4226e-02, 8.9905e-01, 2.9296e-04, 8.6422e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2196386456489563 tensor([2.5670e-05, 4.8733e-03, 2.1964e-01, 5.7619e-03, 7.6970e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02767953649163246 tensor([9.9496e-06, 2.5087e-04, 1.5074e-02, 2.7680e-02, 9.5699e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4096481502056122 tensor([4.0965e-01, 5.8409e-01, 2.1498e-04, 5.2060e-03, 8.3699e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.236199289560318 tensor([0.6108, 0.2362, 0.0007, 0.1405, 0.0118], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030774937942624092 tensor([3.0775e-02, 9.6654e-01, 1.9743e-03, 2.7546e-04, 4.3488e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07638798654079437 tensor([0.0645, 0.7808, 0.0329, 0.0454, 0.0764], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1711779534816742 tensor([1.7118e-01, 2.7322e-04, 2.0415e-07, 8.2751e-01, 1.0371e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23340268433094025 tensor([7.6255e-01, 2.3340e-01, 1.5157e-05, 3.9066e-03, 1.3015e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06229144707322121 tensor([0.0623, 0.8961, 0.0147, 0.0092, 0.0178], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00849427655339241 tensor([3.9233e-08, 1.5553e-03, 9.8995e-01, 3.4081e-06, 8.4943e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05624329671263695 tensor([6.4938e-04, 9.4242e-01, 5.6243e-02, 1.8687e-05, 6.6537e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08034984767436981 tensor([6.8794e-04, 2.2930e-02, 8.0350e-02, 4.6636e-02, 8.4940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3084670305252075 tensor([6.3159e-01, 3.0847e-01, 2.5926e-04, 5.4359e-02, 5.3260e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0652468279004097 tensor([6.5247e-02, 4.0518e-03, 8.5833e-05, 9.0155e-01, 2.9067e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1905520260334015 tensor([1.9055e-01, 3.8770e-02, 5.4866e-04, 7.1111e-01, 5.9023e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17490631341934204 tensor([7.9043e-01, 1.7491e-01, 9.9991e-05, 3.2967e-02, 1.5981e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29766523838043213 tensor([0.0528, 0.0356, 0.0029, 0.6111, 0.2977], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2905244529247284 tensor([0.0463, 0.0414, 0.0027, 0.6190, 0.2905], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.16026198863983154 tensor([0.0049, 0.1253, 0.1603, 0.0703, 0.6393], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026585452258586884 tensor([2.6585e-02, 9.6878e-01, 3.3177e-03, 4.0176e-04, 9.1133e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017412615939974785 tensor([6.3722e-03, 9.7460e-01, 1.7413e-02, 2.0996e-04, 1.4009e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3203929364681244 tensor([2.1082e-08, 5.3348e-05, 3.2039e-01, 4.2006e-04, 6.7913e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31241917610168457 tensor([7.0744e-04, 6.6946e-01, 3.1242e-01, 3.2341e-04, 1.7094e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10002217441797256 tensor([2.5787e-04, 9.3799e-03, 1.0002e-01, 4.4714e-02, 8.4563e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10796459764242172 tensor([3.3090e-05, 3.4147e-03, 1.0796e-01, 1.4187e-02, 8.7440e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009368008933961391 tensor([9.3680e-03, 9.8729e-01, 3.0673e-03, 5.8691e-05, 2.1789e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.031442757695913315 tensor([6.8213e-06, 6.3684e-05, 3.2709e-03, 3.1443e-02, 9.6522e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07385217398405075 tensor([9.2592e-01, 7.3852e-02, 1.3990e-07, 2.2624e-04, 1.4894e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006324268411844969 tensor([6.3243e-03, 1.8731e-05, 3.4522e-07, 9.8999e-01, 3.6710e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.12678271532058716 tensor([0.1268, 0.0366, 0.0010, 0.7560, 0.0796], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.11737226694822311 tensor([8.1541e-01, 1.1737e-01, 7.7572e-05, 6.5059e-02, 2.0772e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0655733197927475 tensor([4.2387e-04, 1.3263e-02, 6.5573e-02, 4.4163e-02, 8.7658e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.21098382771015167 tensor([2.1098e-01, 7.8855e-01, 7.6608e-05, 3.3960e-04, 5.2985e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07298704981803894 tensor([9.2553e-01, 1.4519e-03, 5.1397e-08, 7.2987e-02, 2.9814e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007399751804769039 tensor([4.2101e-08, 1.7906e-03, 9.9081e-01, 2.9933e-06, 7.3998e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25026237964630127 tensor([1.7782e-05, 4.3801e-03, 2.5026e-01, 5.1968e-03, 7.4014e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38856714963912964 tensor([3.7013e-05, 5.6925e-06, 5.1216e-05, 6.1134e-01, 3.8857e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.38018566370010376 tensor([6.1943e-01, 3.8019e-01, 5.8851e-06, 3.6223e-04, 1.2945e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.053122155368328094 tensor([4.4150e-05, 1.7095e-03, 5.3122e-02, 2.7745e-02, 9.1738e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2897535264492035 tensor([0.0095, 0.0429, 0.0253, 0.2898, 0.6326], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3379136919975281 tensor([1.0807e-03, 6.4401e-01, 3.3791e-01, 4.5465e-04, 1.6540e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1511010080575943 tensor([1.8123e-03, 8.3427e-01, 1.5110e-01, 4.2926e-04, 1.2386e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.46508902311325073 tensor([4.6509e-01, 5.2000e-01, 3.1294e-04, 1.2602e-02, 1.9995e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24330784380435944 tensor([5.0562e-04, 4.0198e-02, 2.4331e-01, 2.4948e-02, 6.9104e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38218018412590027 tensor([0.3976, 0.1772, 0.0016, 0.3822, 0.0414], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23895512521266937 tensor([0.0020, 0.7359, 0.2390, 0.0008, 0.0223], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0069271354004740715 tensor([6.9271e-03, 1.1818e-05, 1.2318e-07, 9.9007e-01, 2.9932e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.17745566368103027 tensor([8.2108e-01, 1.7746e-01, 3.4136e-06, 1.4349e-03, 2.7591e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22482454776763916 tensor([3.3673e-04, 2.2482e-01, 6.8576e-01, 1.3622e-03, 8.7713e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27009081840515137 tensor([5.7342e-09, 6.5683e-05, 7.2977e-01, 7.6125e-05, 2.7009e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.289776086807251 tensor([0.2898, 0.6501, 0.0025, 0.0406, 0.0171], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4786936342716217 tensor([5.5874e-04, 3.2994e-04, 6.8268e-04, 4.7869e-01, 5.1973e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22316880524158478 tensor([7.4785e-01, 2.5756e-02, 1.7016e-05, 2.2317e-01, 3.2121e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.021630290895700455 tensor([9.7384e-01, 2.1630e-02, 2.8135e-07, 4.5192e-03, 1.2150e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019436480477452278 tensor([1.8274e-07, 3.1784e-03, 9.7737e-01, 1.1297e-05, 1.9436e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.25459545850753784 tensor([6.3447e-01, 9.7356e-02, 1.6595e-04, 2.5460e-01, 1.3409e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4388275146484375 tensor([0.0013, 0.0013, 0.0022, 0.4388, 0.5564], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19719143211841583 tensor([7.8674e-01, 1.3538e-02, 4.0477e-06, 1.9719e-01, 2.5282e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4799882173538208 tensor([2.9783e-04, 4.7999e-01, 5.0761e-01, 1.7706e-04, 1.1927e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3588447868824005 tensor([0.3588, 0.1599, 0.0014, 0.4049, 0.0751], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14982566237449646 tensor([8.4954e-01, 5.8463e-04, 2.0722e-08, 1.4983e-01, 5.2886e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.47700732946395874 tensor([5.2212e-01, 4.6165e-04, 6.3743e-08, 4.7701e-01, 4.0760e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3637691140174866 tensor([1.5978e-04, 1.4184e-04, 7.0279e-04, 3.6377e-01, 6.3523e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.24213291704654694 tensor([0.2421, 0.6571, 0.0079, 0.0662, 0.0266], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02719874307513237 tensor([2.7787e-06, 1.6087e-02, 9.5665e-01, 6.5082e-05, 2.7199e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.43280476331710815 tensor([5.2716e-01, 2.7957e-02, 6.4284e-05, 4.3280e-01, 1.2016e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17030717432498932 tensor([0.1703, 0.7645, 0.0065, 0.0312, 0.0276], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16524533927440643 tensor([1.6525e-01, 1.9542e-02, 2.0479e-04, 7.7302e-01, 4.1989e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17569640278816223 tensor([1.2133e-04, 5.2881e-04, 6.4014e-03, 1.7570e-01, 8.1725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10322045534849167 tensor([2.8798e-06, 4.7539e-04, 1.0322e-01, 7.1889e-03, 8.8911e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01621873676776886 tensor([1.6219e-02, 9.7805e-01, 4.5898e-03, 2.2909e-04, 9.1316e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1345432996749878 tensor([3.9028e-06, 4.9849e-06, 3.1813e-04, 1.3454e-01, 8.6513e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4, 2, 0], [2, 4, 3], [2, 4, 3], [2, 4, 3], [2, 1, 4, 0], [2, 0], [1, 2, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 1], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 4, 3], [2, 1, 0, 4], [0, 1, 3, 2], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 0, 3], [0, 3, 2], [0, 3, 1], [3, 0, 4], [2, 4, 3], [0, 1, 3, 2], [3, 0, 4, 2], [3, 0, 4], [1, 2, 0, 4], [2, 4, 1], [2, 4, 3], [2, 1, 4, 0], [4, 3, 2, 0], [0, 1, 2, 3], [2, 4, 1], [3, 0, 4], [0, 1, 2, 3], [2, 1, 4], [2, 4, 3, 1], [3, 4, 2, 0], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 1, 2], [0, 1, 2, 3], [0, 2, 1], [3, 4, 2, 0], [3, 4, 0, 2], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 2], [3, 0, 4, 1], [1, 2, 0], [2, 4, 1], [0, 3, 1, 4], [0, 3, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1, 4], [3, 4, 0, 2], [0, 3, 1], [2, 4, 3], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 1], [2, 0, 1, 4], [3, 4, 0, 2], [4, 2, 3, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4, 1], [2, 0, 1], [0, 2, 3, 1], [0, 2, 1, 3], [2, 4, 3, 1], [0, 3], [0, 1, 3], [3, 4, 2, 0], [0, 1, 2], [0, 3], [0, 3, 4, 1], [0, 1, 2], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1, 4], [2, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1], [3, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 1, 3], [2, 0, 3], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 1, 2], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3, 0], [3, 4, 2, 0], [0, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [0, 3, 4], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 1], [0, 1, 2, 3], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 3], [2, 0, 1], [0, 1, 2], [3, 0, 4, 2], [2, 4, 3], [2, 1, 0], [2, 1, 0], [0, 3, 4, 2], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 3, 4, 0], [0, 1, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 2], [2, 4, 3], [2, 4, 3], [3, 2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 3, 2], [3, 0, 4], [2, 4, 1], [1, 0, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 1, 4, 0], [1, 2, 0], [0, 3, 1], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 1, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 3], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 4, 3], [3, 2, 4, 0], [0, 3, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 0], [2, 1, 0], [0, 3, 1, 2], [3, 4, 2, 0], [3, 4, 0, 2], [0, 1, 3], [3, 0, 4], [0, 1, 3, 2], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 3, 2], [4, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3], [1, 0, 2], [2, 4, 3], [0, 1, 3, 2], [0, 2, 1], [3, 0, 4], [3, 0, 4, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [3, 0, 4], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4], [0, 1, 3], [2, 4, 3], [1, 0, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 1, 2], [2, 4, 1, 3], [3, 0, 4], [2, 4, 1], [2, 4, 1, 3], [2, 4, 1], [1, 0, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 1], [2, 4, 3, 0], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3]]\n",
      "[[1], [0, 1], [0, 1], [0, 1], [3], [1, 3, 4], [3, 4], [3], [0], [0, 3], [0], [1], [2, 4], [1], [2, 4], [0, 1], [3], [4], [1], [2], [0], [1], [1, 4], [2, 4], [1, 2], [0, 1], [4], [1], [1, 2], [3], [0, 3], [0, 1], [3], [1], [4], [0, 3], [1, 2], [4], [0, 3], [0], [1], [0], [4], [4], [4], [3, 4], [1], [1], [2], [4], [0], [1], [3, 4], [2], [3, 4], [0, 3], [2], [1, 4], [0, 1], [2], [0], [2, 4], [2], [1], [2, 4], [0, 1], [4], [3], [1], [3], [0, 3], [3], [1], [1], [3], [0], [2], [3, 4], [4], [4], [0], [1, 2, 4], [2, 4], [1], [3, 4], [1, 2, 4], [2], [3, 4], [2, 4], [0, 1], [1, 4], [2], [1, 3], [0], [4], [0, 3], [1, 2], [2], [1], [2, 4], [1, 4], [3], [3], [0], [3, 4], [0, 1], [3, 4], [0, 1], [4], [4], [0], [0, 1], [3, 4], [2, 4], [2, 4], [0, 1], [1], [1], [1, 2, 4], [3], [4], [3, 4], [2, 4], [0, 3], [3], [0], [0], [3], [1, 2], [2], [0], [0, 3], [4], [4], [3], [0], [1], [1], [0], [2, 4], [3, 4], [3, 4], [1], [0, 1], [3, 4], [3, 4], [1], [2], [0, 1], [4], [1], [4], [2], [4], [0], [1, 4], [0, 1], [0, 1], [1], [0], [1], [4], [1, 2], [0, 3], [3, 4], [0], [4], [3], [3, 4], [2, 4], [0], [3], [2], [0, 1], [4], [3, 4], [4], [2], [2, 4], [4], [0, 1], [0, 1], [1], [1], [3], [0, 1], [1], [2], [1], [4], [0, 1], [3], [3], [0], [3, 4], [3, 4], [4], [1], [1], [2, 4], [1, 2], [4], [4], [1], [4], [0], [3], [3], [0], [4], [0, 1], [0], [2], [2, 4], [3, 4], [0, 1], [4], [3, 4], [1, 2], [1], [0, 1], [2, 4], [0, 3], [1, 2], [3], [0], [1, 2], [2, 4], [0, 1], [3, 4], [0, 3], [0], [2], [0, 3], [3, 4], [0], [1, 2], [0, 3], [0], [0, 3], [3, 4], [0, 1], [2], [0, 3], [1], [3], [4], [4], [1], [4]]\n",
      "NL_pred of 3th iteration [[3, 4, 2, 0], [2, 1, 4, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 3, 1], [3, 0, 2, 4], [2, 4, 3, 0], [2, 1, 0, 4], [0, 1, 3, 2], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 0, 3], [0, 1, 3, 2], [3, 0, 4, 2], [1, 2, 0, 4], [2, 1, 4, 0], [4, 3, 2, 0], [0, 1, 2, 3], [0, 1, 2, 3], [2, 4, 3, 1], [3, 4, 2, 0], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 1, 2], [0, 1, 2, 3], [3, 4, 2, 0], [3, 4, 0, 2], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1, 4], [3, 4, 0, 2], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 0, 1, 4], [3, 4, 0, 2], [4, 2, 3, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4, 1], [0, 2, 3, 1], [0, 2, 1, 3], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 4, 1], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [0, 3, 1, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [3, 4, 2, 0], [2, 1, 0, 4], [0, 1, 2, 3], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [0, 3, 4, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [3, 4, 0, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 2], [0, 3, 1, 4], [0, 1, 3, 2], [2, 3, 4, 0], [0, 1, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 3, 1], [3, 2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 3, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [0, 1, 3, 2], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 2, 3], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [3, 2, 4, 0], [0, 3, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 1, 2], [3, 4, 2, 0], [3, 4, 0, 2], [0, 1, 3, 2], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 3, 2], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 3, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 0], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.008448654955083673  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.008448534078531331  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.00844830482989758  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.008447979713653351  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.008447567899743994  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.008447077724483464  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.008446522525974087  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.008445906472372842  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.008445238733625078  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00844452431151917  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.008443770708737674  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.008442982927068963  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.008442165968301413  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.008441324000591999  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.008440461192097697  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.0084395792100813  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.00843868138906839  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.008437771063584548  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.008436849900892565  Accuracy on Support set:0.0\n",
      "torch.Size([143, 2048]) torch.Size([143])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00843591790099244  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.24268370866775513 tensor([2.4268e-01, 7.5557e-01, 2.3072e-04, 1.2665e-03, 2.5349e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2403596192598343 tensor([0.2404, 0.7524, 0.0008, 0.0047, 0.0017], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1979052573442459 tensor([7.7618e-01, 1.9791e-01, 6.6064e-05, 2.5149e-02, 6.9450e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21002517640590668 tensor([0.1346, 0.2871, 0.0146, 0.3536, 0.2100], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20856930315494537 tensor([3.1258e-04, 3.8017e-05, 7.5465e-05, 7.9100e-01, 2.0857e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29940351843833923 tensor([6.7904e-01, 1.8609e-02, 1.2137e-05, 2.9940e-01, 2.9378e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31535467505455017 tensor([0.0022, 0.1825, 0.3154, 0.0232, 0.4767], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3271304666996002 tensor([0.0014, 0.1785, 0.3271, 0.0130, 0.4799], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3598196506500244 tensor([6.3987e-01, 3.5982e-01, 4.0227e-06, 2.9391e-04, 1.4183e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22608622908592224 tensor([0.0144, 0.5555, 0.1739, 0.0301, 0.2261], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2409314066171646 tensor([3.4505e-06, 7.2768e-03, 7.5128e-01, 5.0376e-04, 2.4093e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2006462663412094 tensor([0.0048, 0.7033, 0.2006, 0.0040, 0.0872], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27852386236190796 tensor([2.7852e-01, 7.1356e-01, 6.1825e-04, 5.5895e-03, 1.7134e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22732049226760864 tensor([2.0423e-05, 2.2732e-01, 7.6853e-01, 1.2695e-05, 4.1172e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19056980311870575 tensor([8.0098e-01, 7.6094e-03, 1.1801e-06, 1.9057e-01, 8.4295e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3975675106048584 tensor([5.8122e-01, 3.9757e-01, 3.2694e-04, 1.9484e-02, 1.4002e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2584519684314728 tensor([6.7046e-01, 6.4226e-02, 6.4155e-05, 2.5845e-01, 6.7963e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3059970736503601 tensor([1.8875e-04, 6.9184e-01, 3.0600e-01, 2.7792e-05, 1.9417e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2697964012622833 tensor([2.6980e-01, 2.9344e-02, 2.2247e-04, 6.5982e-01, 4.0819e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31567928194999695 tensor([0.0201, 0.0969, 0.0275, 0.3157, 0.5398], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41987845301628113 tensor([0.0007, 0.0008, 0.0020, 0.4199, 0.5767], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3578892648220062 tensor([5.7417e-06, 3.0673e-07, 4.7481e-06, 6.4210e-01, 3.5789e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19004692137241364 tensor([8.0156e-01, 7.7101e-03, 1.2261e-06, 1.9005e-01, 6.7769e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2831311523914337 tensor([0.0081, 0.2831, 0.1642, 0.0530, 0.4916], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.401496559381485 tensor([0.4913, 0.4015, 0.0013, 0.0901, 0.0158], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3627224564552307 tensor([1.2651e-05, 5.3563e-03, 3.6272e-01, 4.0824e-03, 6.2783e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25101524591445923 tensor([3.3457e-04, 4.0661e-02, 2.5102e-01, 1.3415e-02, 6.9457e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3198241889476776 tensor([6.7386e-01, 3.1982e-01, 3.7508e-05, 5.9312e-03, 3.4696e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.31651148200035095 tensor([6.7150e-01, 9.9517e-03, 4.7891e-06, 3.1651e-01, 2.0319e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26110613346099854 tensor([0.0407, 0.0537, 0.0070, 0.6374, 0.2611], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2202298492193222 tensor([0.0089, 0.4710, 0.2202, 0.0315, 0.2684], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2334546595811844 tensor([3.9652e-07, 3.3892e-04, 2.3345e-01, 1.2977e-03, 7.6491e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1986304223537445 tensor([6.1116e-04, 1.9651e-03, 5.4052e-03, 1.9863e-01, 7.9339e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28715115785598755 tensor([0.0015, 0.2872, 0.4009, 0.0081, 0.3023], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2100401371717453 tensor([7.3146e-04, 4.2998e-03, 1.7788e-02, 2.1004e-01, 7.6714e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21676979959011078 tensor([3.7762e-06, 6.2784e-03, 7.7634e-01, 6.1088e-04, 2.1677e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27743157744407654 tensor([0.2774, 0.4572, 0.0061, 0.1732, 0.0861], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.295699805021286 tensor([0.0067, 0.2957, 0.1699, 0.0302, 0.4975], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22925734519958496 tensor([0.1684, 0.4567, 0.0171, 0.2293, 0.1285], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.33885398507118225 tensor([6.5009e-01, 8.8974e-03, 4.1209e-06, 3.3885e-01, 2.1588e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40983185172080994 tensor([2.1189e-04, 5.8400e-01, 4.0983e-01, 7.0762e-05, 5.8904e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3194231688976288 tensor([1.4191e-06, 9.4821e-04, 3.1942e-01, 1.8566e-03, 6.7777e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27353495359420776 tensor([0.0638, 0.5397, 0.0269, 0.0961, 0.2735], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.23127630352973938 tensor([8.9170e-05, 9.5092e-05, 6.6350e-04, 2.3128e-01, 7.6788e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3415428102016449 tensor([3.4154e-01, 6.5782e-01, 3.8930e-05, 5.3118e-04, 6.4200e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2843325734138489 tensor([0.0056, 0.0030, 0.0014, 0.7056, 0.2843], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.32679155468940735 tensor([6.2051e-01, 3.2679e-01, 3.4464e-04, 4.9510e-02, 2.8469e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23220546543598175 tensor([0.5963, 0.2322, 0.0006, 0.1603, 0.0105], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24777893722057343 tensor([0.0009, 0.0039, 0.0122, 0.2478, 0.7353], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.42337632179260254 tensor([1.1049e-04, 3.3726e-02, 5.3748e-01, 5.3062e-03, 4.2338e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2567328214645386 tensor([5.7284e-04, 4.8753e-02, 2.5673e-01, 2.6349e-02, 6.6759e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28407275676727295 tensor([0.2841, 0.7012, 0.0008, 0.0112, 0.0027], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22988289594650269 tensor([0.0030, 0.2299, 0.3105, 0.0216, 0.4351], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3778778314590454 tensor([0.0151, 0.0249, 0.0089, 0.5731, 0.3779], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24446575343608856 tensor([6.6712e-05, 1.0648e-02, 2.4447e-01, 9.0674e-03, 7.3575e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.32421189546585083 tensor([0.3242, 0.0974, 0.0009, 0.5030, 0.0745], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2093074470758438 tensor([0.0063, 0.6235, 0.2093, 0.0091, 0.1518], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3055943250656128 tensor([6.8993e-01, 3.7258e-03, 1.1241e-06, 3.0559e-01, 7.5208e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3479447662830353 tensor([4.3665e-07, 4.7579e-04, 3.4794e-01, 9.3750e-04, 6.5064e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.44338080286979675 tensor([0.0169, 0.0349, 0.0095, 0.4434, 0.4954], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3145487308502197 tensor([5.2463e-04, 9.8248e-04, 3.6030e-03, 3.1455e-01, 6.8034e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.222391739487648 tensor([0.2224, 0.7183, 0.0042, 0.0393, 0.0158], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2016148865222931 tensor([7.0797e-04, 5.8653e-05, 4.5102e-05, 7.9757e-01, 2.0161e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2098313421010971 tensor([0.0232, 0.0131, 0.0014, 0.7525, 0.2098], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3189144730567932 tensor([6.6276e-01, 3.1891e-01, 1.4783e-04, 1.6983e-02, 1.1922e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23124447464942932 tensor([0.0149, 0.6033, 0.1181, 0.0325, 0.2312], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2492627650499344 tensor([7.2811e-01, 2.4926e-01, 1.3472e-04, 2.1415e-02, 1.0819e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2533773183822632 tensor([6.9752e-01, 2.5338e-01, 1.7613e-04, 4.6480e-02, 2.4455e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25070372223854065 tensor([3.5632e-05, 2.5070e-01, 7.4241e-01, 2.4515e-05, 6.8296e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3498237729072571 tensor([0.3498, 0.1234, 0.0013, 0.4491, 0.0764], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.487795889377594 tensor([1.4571e-04, 6.5041e-05, 3.7254e-04, 5.1162e-01, 4.8780e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22561313211917877 tensor([2.4748e-04, 2.2680e-05, 5.0170e-05, 7.7407e-01, 2.2561e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21486641466617584 tensor([1.0146e-05, 1.4566e-02, 7.6992e-01, 6.3397e-04, 2.1487e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3076294958591461 tensor([6.7926e-01, 3.0763e-01, 8.3031e-05, 1.2440e-02, 5.8353e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3520847260951996 tensor([0.0034, 0.0014, 0.0008, 0.6424, 0.3521], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.21255537867546082 tensor([2.5731e-05, 4.9078e-03, 2.1256e-01, 5.7157e-03, 7.7680e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4146338105201721 tensor([4.1463e-01, 5.7944e-01, 1.9343e-04, 4.9504e-03, 7.8726e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24221409857273102 tensor([0.6149, 0.2422, 0.0007, 0.1309, 0.0112], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23064206540584564 tensor([7.6555e-01, 2.3064e-01, 1.3589e-05, 3.6720e-03, 1.2148e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.30272483825683594 tensor([6.4082e-01, 3.0272e-01, 2.2745e-04, 5.1304e-02, 4.9221e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29398414492607117 tensor([0.0562, 0.0370, 0.0027, 0.6101, 0.2940], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2882400453090668 tensor([0.0490, 0.0432, 0.0026, 0.6169, 0.2882], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31592246890068054 tensor([2.0223e-08, 5.2879e-05, 3.1592e-01, 4.0695e-04, 6.8362e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29270094633102417 tensor([7.3496e-04, 6.8973e-01, 2.9270e-01, 3.1534e-04, 1.6517e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2146328091621399 tensor([2.1463e-01, 7.8492e-01, 6.8956e-05, 3.2384e-04, 4.9785e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2407616227865219 tensor([1.8596e-05, 4.5148e-03, 2.4076e-01, 5.2792e-03, 7.4943e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39208927750587463 tensor([3.6932e-05, 5.6438e-06, 4.8645e-05, 6.0782e-01, 3.9209e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3719449043273926 tensor([6.2769e-01, 3.7194e-01, 5.1999e-06, 3.4586e-04, 1.2084e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2953786849975586 tensor([0.0105, 0.0453, 0.0235, 0.2954, 0.6253], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31897738575935364 tensor([1.1191e-03, 6.6342e-01, 3.1898e-01, 4.4384e-04, 1.6043e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4722020924091339 tensor([4.7220e-01, 5.1381e-01, 2.7640e-04, 1.1863e-02, 1.8464e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2374410629272461 tensor([5.0908e-04, 4.0952e-02, 2.3744e-01, 2.4649e-02, 6.9645e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3701733946800232 tensor([0.4091, 0.1794, 0.0015, 0.3702, 0.0399], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.22303268313407898 tensor([0.0021, 0.7524, 0.2230, 0.0008, 0.0216], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23967483639717102 tensor([3.6210e-04, 2.3967e-01, 6.6972e-01, 1.3878e-03, 8.8858e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2780107855796814 tensor([5.6166e-09, 6.5507e-05, 7.2185e-01, 7.5940e-05, 2.7801e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2951578199863434 tensor([0.2952, 0.6486, 0.0022, 0.0382, 0.0159], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4786778688430786 tensor([5.7478e-04, 3.3409e-04, 6.4483e-04, 4.7868e-01, 5.1977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.21299993991851807 tensor([7.5805e-01, 2.5865e-02, 1.5688e-05, 2.1300e-01, 3.0741e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2421266734600067 tensor([6.4751e-01, 9.7555e-02, 1.5048e-04, 2.4213e-01, 1.2656e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4413849711418152 tensor([0.0013, 0.0013, 0.0021, 0.4414, 0.5539], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.48251157999038696 tensor([3.2110e-04, 5.0512e-01, 4.8251e-01, 1.8001e-04, 1.1872e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.372679740190506 tensor([0.3727, 0.1605, 0.0012, 0.3940, 0.0716], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.45872974395751953 tensor([5.4042e-01, 4.6746e-04, 5.8351e-08, 4.5873e-01, 3.8724e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.36508557200431824 tensor([1.6526e-04, 1.4343e-04, 6.5832e-04, 3.6509e-01, 6.3395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23815283179283142 tensor([0.2382, 0.6676, 0.0075, 0.0612, 0.0255], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.41632357239723206 tensor([5.4414e-01, 2.8082e-02, 5.8262e-05, 4.1632e-01, 1.1395e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4, 2, 0], [2, 4, 3], [2, 4, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 0], [1, 2, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 1], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1], [2, 4, 3, 0], [0, 3, 1], [2, 4, 3], [2, 1, 0, 4], [0, 1, 3, 2], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 0, 3], [0, 3, 2], [0, 3, 1], [3, 0, 4], [2, 4, 3], [0, 1, 3, 2], [3, 0, 4, 2], [3, 0, 4], [1, 2, 0, 4], [2, 4, 1, 3], [2, 4, 3], [2, 1, 4, 0], [4, 3, 2, 0], [0, 1, 2, 3], [2, 4, 1], [3, 0, 4], [0, 1, 2, 3], [2, 1, 4], [2, 4, 3, 1], [3, 4, 2, 0], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 1, 2], [0, 1, 2, 3], [0, 2, 1], [3, 4, 2, 0], [3, 4, 0, 2], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 2], [3, 0, 4, 1], [1, 2, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1, 4], [3, 4, 0, 2], [0, 3, 1], [2, 4, 3], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 1], [2, 0, 1, 4], [3, 4, 0, 2], [4, 2, 3, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4, 1], [2, 0, 1], [0, 2, 3, 1], [0, 2, 1, 3], [2, 4, 3, 1], [0, 3], [0, 1, 3], [3, 4, 2, 0], [0, 1, 2, 3], [0, 3], [0, 3, 4, 1], [0, 1, 2], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1, 4], [2, 4, 0], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1], [3, 0, 4], [0, 3, 1, 4], [3, 4, 2, 0], [0, 1, 3], [2, 0, 3], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 1, 2], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3, 0], [3, 4, 2, 0], [0, 3], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [0, 3, 4], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 1], [0, 1, 2, 3], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 3], [2, 0, 1], [0, 1, 2], [3, 0, 4, 2], [2, 4, 3], [2, 1, 0], [2, 1, 0], [0, 3, 4, 2], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 3, 4, 0], [0, 1, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 2], [2, 4, 3], [2, 4, 3], [3, 2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 3, 2], [3, 0, 4], [2, 4, 1], [1, 0, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 1, 4, 0], [1, 2, 0], [0, 3, 1], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 1, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 3], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 4, 3], [3, 2, 4, 0], [0, 3, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 0], [2, 1, 0], [0, 3, 1, 2], [3, 4, 2, 0], [3, 4, 0, 2], [0, 1, 3], [3, 0, 4], [0, 1, 3, 2], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 3, 2], [4, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3], [1, 0, 2], [2, 4, 3], [0, 1, 3, 2], [0, 2, 1], [3, 0, 4], [3, 0, 4, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1], [3, 0, 4], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4], [0, 1, 3], [2, 4, 3], [1, 0, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1], [0, 1, 2], [2, 4, 1, 3], [3, 0, 4], [2, 4, 1], [2, 4, 1, 3], [2, 4, 1], [1, 0, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 1], [2, 4, 3, 0], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3]]\n",
      "[[1], [0, 1], [0, 1], [0], [3], [1, 3, 4], [3, 4], [3], [0], [0, 3], [0], [1], [2, 4], [1], [2, 4], [0, 1], [3], [4], [1], [2], [0], [1], [1, 4], [2, 4], [1, 2], [0, 1], [4], [1], [1, 2], [3], [0], [0, 1], [3], [1], [4], [0, 3], [1, 2], [4], [0, 3], [0], [1], [0], [4], [4], [4], [3, 4], [1], [1], [2], [4], [0], [1], [3, 4], [2], [3, 4], [0], [2], [1, 4], [0, 1], [2], [0], [2, 4], [2], [1], [2, 4], [0, 1], [4], [3], [1], [3], [0, 3], [3], [1], [1], [3], [0], [2], [3, 4], [4], [4], [0], [1, 2, 4], [2, 4], [1], [4], [1, 2, 4], [2], [3, 4], [2, 4], [0, 1], [1, 4], [2], [1, 3], [0], [4], [0, 3], [1, 2], [2], [1], [2, 4], [1, 4], [3], [3], [0], [3, 4], [0, 1], [3, 4], [0, 1], [4], [4], [0], [0, 1], [3, 4], [2, 4], [2, 4], [0, 1], [1], [1], [1, 2, 4], [3], [4], [3, 4], [2, 4], [0, 3], [3], [0], [0], [3], [1, 2], [2], [0], [0, 3], [4], [4], [3], [0], [1], [1], [0], [2, 4], [3, 4], [3, 4], [1], [0, 1], [3, 4], [3, 4], [1], [2], [0, 1], [4], [1], [4], [2], [4], [0], [1, 4], [0, 1], [0, 1], [1], [0], [1], [4], [1, 2], [0, 3], [3, 4], [0], [4], [3], [3, 4], [2, 4], [0], [3], [2], [0, 1], [4], [3, 4], [4], [2], [2, 4], [4], [0, 1], [0, 1], [1], [1], [3], [0, 1], [1], [2], [1], [4], [0, 1], [3], [3], [0], [3, 4], [3, 4], [4], [1], [1], [2, 4], [1, 2], [4], [4], [1], [4], [0], [3], [3], [0], [4], [0, 1], [0], [2], [2, 4], [3, 4], [0, 1], [4], [3, 4], [1, 2], [1], [0, 1], [2, 4], [0, 3], [1, 2], [3], [0], [1, 2], [2, 4], [0, 1], [3, 4], [0, 3], [0], [2], [0, 3], [3, 4], [0], [1, 2], [0, 3], [0], [0, 3], [3, 4], [0, 1], [2], [0, 3], [1], [3], [4], [4], [1], [4]]\n",
      "NL_pred of 4th iteration [[2, 4, 3, 1], [2, 4, 1, 3], [2, 4, 1, 3], [0, 1, 2, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.3316880464553833  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.3305903673171997  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.32862260937690735  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.32605716586112976  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.32315248250961304  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.3201807141304016  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.31733229756355286  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.314683735370636  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.31229013204574585  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.3101327121257782  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.308202862739563  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.30646812915802  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.3048977851867676  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.30347689986228943  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.30217444896698  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.3009783625602722  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.2998814582824707  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.29887154698371887  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.2979426980018616  Accuracy on Support set:0.0\n",
      "torch.Size([4, 2048]) torch.Size([4])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.2970947027206421  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "0 0.2916998267173767 tensor([2.9170e-01, 7.0778e-01, 7.1897e-05, 3.7685e-04, 6.8485e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2649437487125397 tensor([2.6494e-01, 7.3335e-01, 2.4586e-04, 1.0870e-03, 3.7482e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.040537428110837936 tensor([0.3097, 0.5614, 0.0054, 0.0829, 0.0405], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22598841786384583 tensor([1.3108e-03, 1.6439e-04, 1.4353e-04, 7.7239e-01, 2.2599e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.048154376447200775 tensor([9.2914e-01, 2.2256e-02, 3.4218e-06, 4.8154e-02, 4.4497e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1823893040418625 tensor([0.0071, 0.5571, 0.2427, 0.0107, 0.1824], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1888306885957718 tensor([0.0040, 0.5245, 0.2770, 0.0057, 0.1888], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3822023570537567 tensor([6.1771e-01, 3.8220e-01, 2.0237e-06, 8.4076e-05, 5.2238e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.043741557747125626 tensor([0.0257, 0.8618, 0.0614, 0.0074, 0.0437], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1468045562505722 tensor([1.2343e-05, 2.6002e-02, 8.2685e-01, 3.3171e-04, 1.4680e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07948929816484451 tensor([5.6522e-03, 8.9734e-01, 7.9489e-02, 8.3201e-04, 1.6689e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.306252658367157 tensor([3.0625e-01, 6.9197e-01, 1.8324e-04, 1.2261e-03, 3.6470e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.48120489716529846 tensor([5.9671e-05, 5.1671e-01, 4.8120e-01, 8.0353e-06, 2.0131e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.37024354934692383 tensor([6.2444e-01, 3.7024e-01, 1.0501e-04, 4.8598e-03, 3.5598e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02370823733508587 tensor([9.1068e-01, 6.5131e-02, 8.9343e-06, 2.3708e-02, 4.6806e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12483307719230652 tensor([3.1496e-04, 8.7416e-01, 1.2483e-01, 1.2146e-05, 6.7901e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.18536017835140228 tensor([7.4150e-01, 6.3740e-02, 9.3519e-05, 1.8536e-01, 9.3035e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15289737284183502 tensor([0.1124, 0.4924, 0.0242, 0.1529, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4278357923030853 tensor([0.0043, 0.0045, 0.0037, 0.4278, 0.5598], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.40085574984550476 tensor([1.7379e-05, 9.3703e-07, 8.2514e-06, 5.9912e-01, 4.0086e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13751459121704102 tensor([0.0211, 0.7246, 0.0996, 0.0171, 0.1375], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4019397795200348 tensor([5.8191e-01, 4.0194e-01, 3.0407e-04, 1.3724e-02, 2.1246e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.44544288516044617 tensor([4.6409e-05, 2.1186e-02, 5.3033e-01, 2.9938e-03, 4.4544e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.33577585220336914 tensor([0.0017, 0.2052, 0.3358, 0.0099, 0.4474], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29432395100593567 tensor([7.0452e-01, 2.9432e-01, 9.6543e-06, 1.0874e-03, 6.3410e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03821362555027008 tensor([9.5015e-01, 1.1425e-02, 9.6972e-07, 3.8214e-02, 2.0644e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13512413203716278 tensor([0.2225, 0.2575, 0.0070, 0.3779, 0.1351], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.05521726608276367 tensor([0.0185, 0.8345, 0.0834, 0.0084, 0.0552], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3692544102668762 tensor([1.2065e-06, 1.1381e-03, 3.6925e-01, 1.0414e-03, 6.2856e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0900513231754303 tensor([0.0043, 0.6801, 0.2224, 0.0032, 0.0901], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.22831034660339355 tensor([0.0061, 0.0309, 0.0302, 0.2283, 0.7046], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13012143969535828 tensor([1.0527e-05, 1.8390e-02, 8.5110e-01, 3.7331e-04, 1.3012e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.37652042508125305 tensor([0.3765, 0.5835, 0.0017, 0.0266, 0.0117], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14541098475456238 tensor([0.0170, 0.7216, 0.1054, 0.0106, 0.1454], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04803652688860893 tensor([0.2910, 0.6338, 0.0050, 0.0480, 0.0221], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04983069375157356 tensor([9.3733e-01, 1.2511e-02, 1.3244e-06, 4.9831e-02, 3.2566e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1670052409172058 tensor([3.6629e-04, 8.3092e-01, 1.6701e-01, 2.4854e-05, 1.6855e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4861287772655487 tensor([3.9568e-06, 3.2973e-03, 5.0935e-01, 1.2171e-03, 4.8613e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02741178311407566 tensor([0.1100, 0.8430, 0.0066, 0.0130, 0.0274], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.17883598804473877 tensor([3.4645e-04, 5.0479e-04, 1.5888e-03, 1.7884e-01, 8.1872e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.36487454175949097 tensor([3.6487e-01, 6.3498e-01, 1.1968e-05, 1.2029e-04, 1.4718e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2748164236545563 tensor([0.0313, 0.0159, 0.0024, 0.6756, 0.2748], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.29574060440063477 tensor([6.9571e-01, 2.9574e-01, 7.5733e-05, 8.0469e-03, 4.2409e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22765180468559265 tensor([7.4191e-01, 2.2765e-01, 1.4774e-04, 2.8622e-02, 1.6724e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2521807551383972 tensor([0.0064, 0.0265, 0.0218, 0.2522, 0.6932], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23249810934066772 tensor([4.7076e-04, 1.4364e-01, 6.2002e-01, 3.3721e-03, 2.3250e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.372301310300827 tensor([0.0021, 0.2142, 0.3723, 0.0151, 0.3963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3462419807910919 tensor([3.4624e-01, 6.5083e-01, 1.9423e-04, 2.2621e-03, 4.6969e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14641033113002777 tensor([0.0090, 0.6292, 0.2062, 0.0091, 0.1464], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2789546549320221 tensor([0.0994, 0.1521, 0.0128, 0.4567, 0.2790], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3936265707015991 tensor([4.0464e-04, 6.6317e-02, 3.9363e-01, 7.5480e-03, 5.3210e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11397607624530792 tensor([7.0285e-01, 1.6923e-01, 2.8892e-04, 1.1398e-01, 1.3652e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06331085413694382 tensor([0.0108, 0.8997, 0.0633, 0.0020, 0.0242], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04753055423498154 tensor([9.4818e-01, 4.1828e-03, 2.7674e-07, 4.7531e-02, 1.0498e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4266439974308014 tensor([7.6411e-07, 1.2531e-03, 5.7162e-01, 4.8224e-04, 4.2664e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3137575387954712 tensor([0.1220, 0.2182, 0.0117, 0.3343, 0.3138], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30936962366104126 tensor([0.0026, 0.0048, 0.0064, 0.3094, 0.6767], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27744942903518677 tensor([0.2774, 0.7127, 0.0009, 0.0066, 0.0023], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2031603455543518 tensor([4.3501e-03, 3.6298e-04, 9.0066e-05, 7.9204e-01, 2.0316e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15577459335327148 tensor([0.1267, 0.0544, 0.0016, 0.6616, 0.1558], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.321026086807251 tensor([6.7508e-01, 3.2103e-01, 5.0902e-05, 3.5679e-03, 2.7935e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.02834274061024189 tensor([0.0259, 0.9094, 0.0310, 0.0054, 0.0283], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22118999063968658 tensor([7.7382e-01, 2.2119e-01, 3.6090e-05, 4.7287e-03, 2.2831e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23706725239753723 tensor([7.5547e-01, 2.3707e-01, 4.0856e-05, 7.0685e-03, 3.5764e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.40215787291526794 tensor([1.1912e-04, 5.9477e-01, 4.0216e-01, 1.5134e-05, 2.9352e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09192563593387604 tensor([6.6602e-01, 2.2703e-01, 5.2040e-04, 9.1926e-02, 1.4495e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.487283855676651 tensor([7.2547e-04, 3.4561e-04, 7.5820e-04, 4.8728e-01, 5.1089e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21069791913032532 tensor([1.3106e-03, 1.0528e-04, 7.9303e-05, 7.8781e-01, 2.1070e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.107622891664505 tensor([3.1338e-05, 5.1901e-02, 8.4013e-01, 3.1890e-04, 1.0762e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2763720750808716 tensor([7.2086e-01, 2.7637e-01, 2.2519e-05, 2.6228e-03, 1.2293e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3360995948314667 tensor([0.0295, 0.0124, 0.0015, 0.6204, 0.3361], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.34052926301956177 tensor([1.0414e-04, 2.1180e-02, 3.4053e-01, 4.8209e-03, 6.3337e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4610964059829712 tensor([4.6110e-01, 5.3780e-01, 4.6112e-05, 9.2654e-04, 1.3503e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2622990012168884 tensor([7.1287e-01, 2.6230e-01, 2.1203e-04, 2.2676e-02, 1.9413e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19680407643318176 tensor([8.0228e-01, 1.9680e-01, 3.8470e-06, 8.8165e-04, 2.9239e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.28231728076934814 tensor([7.1171e-01, 2.8232e-01, 3.8965e-05, 5.4862e-03, 4.5139e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1621384471654892 tensor([0.2909, 0.1777, 0.0029, 0.3664, 0.1621], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.10750623047351837 tensor([0.3415, 0.2421, 0.0021, 0.3069, 0.1075], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.49853259325027466 tensor([3.4434e-08, 1.2202e-04, 4.9853e-01, 2.3979e-04, 5.0111e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09409201145172119 tensor([1.1610e-03, 9.0124e-01, 9.4092e-02, 8.3574e-05, 3.4230e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23596055805683136 tensor([2.3596e-01, 7.6391e-01, 2.4052e-05, 8.7669e-05, 1.3871e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.37038546800613403 tensor([1.0428e-04, 2.4328e-02, 3.7039e-01, 4.9558e-03, 6.0023e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.41413789987564087 tensor([1.6263e-04, 2.5541e-05, 9.2096e-05, 5.8558e-01, 4.1414e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.35088247060775757 tensor([6.4902e-01, 3.5088e-01, 1.8664e-06, 9.4224e-05, 3.6624e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.20710863173007965 tensor([0.0644, 0.2759, 0.0327, 0.2071, 0.4200], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1253747195005417 tensor([1.5783e-03, 8.6906e-01, 1.2537e-01, 1.2384e-04, 3.8679e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.49218106269836426 tensor([5.0570e-01, 4.9218e-01, 6.2496e-05, 1.7977e-03, 2.6285e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32880738377571106 tensor([0.0021, 0.1746, 0.3288, 0.0181, 0.4764], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07456490397453308 tensor([6.4785e-01, 2.6907e-01, 5.6078e-04, 7.4565e-02, 7.9536e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0752662792801857 tensor([2.5941e-03, 9.1793e-01, 7.5266e-02, 1.7673e-04, 4.0375e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.41460350155830383 tensor([8.6804e-04, 5.5435e-01, 4.1460e-01, 5.4434e-04, 2.9637e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1555349975824356 tensor([8.1230e-09, 1.2094e-04, 8.4431e-01, 3.6481e-05, 1.5553e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3685761094093323 tensor([3.6858e-01, 6.2330e-01, 4.3170e-04, 5.8062e-03, 1.8871e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.45941615104675293 tensor([0.0033, 0.0021, 0.0013, 0.4594, 0.5339], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023725317791104317 tensor([9.4895e-01, 2.7036e-02, 2.9908e-06, 2.3725e-02, 2.8927e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0343007892370224 tensor([8.5081e-01, 1.1323e-01, 3.8001e-05, 3.4301e-02, 1.6196e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42850610613822937 tensor([0.0100, 0.0105, 0.0045, 0.4285, 0.5465], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1940416842699051 tensor([6.8057e-04, 8.0163e-01, 1.9404e-01, 7.3016e-05, 3.5792e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06992514431476593 tensor([6.6945e-01, 2.4948e-01, 3.6873e-04, 6.9925e-02, 1.0782e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08956407010555267 tensor([9.0965e-01, 7.1603e-04, 2.1776e-08, 8.9564e-02, 7.3829e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4047178030014038 tensor([0.0015, 0.0012, 0.0013, 0.4047, 0.5913], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.25337085127830505 tensor([0.2534, 0.7282, 0.0025, 0.0110, 0.0049], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07296228408813477 tensor([8.8960e-01, 3.5815e-02, 1.4901e-05, 7.2962e-02, 1.6055e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[3, 4, 2, 0], [2, 4, 3], [2, 4, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 0, 4], [1, 2, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 3], [2, 1, 0, 4], [0, 1, 3, 2], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 0, 3], [0, 3, 2, 4], [0, 3, 1, 4], [3, 0, 4, 2], [2, 4, 3], [0, 1, 3, 2], [3, 0, 4, 2], [3, 0, 4], [1, 2, 0, 4], [2, 4, 1, 3], [2, 4, 3], [2, 1, 4, 0], [4, 3, 2, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 3], [2, 4, 3, 1], [3, 4, 2, 0], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 1, 2], [0, 1, 2, 3], [0, 2, 1, 3], [3, 4, 2, 0], [3, 4, 0, 2], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 2], [3, 0, 4, 1], [1, 2, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 2, 4], [2, 4, 3], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1, 4], [3, 4, 0, 2], [0, 3, 1], [2, 4, 3], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 1, 3], [2, 0, 1, 4], [3, 4, 0, 2], [4, 2, 3, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4, 1], [2, 0, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [2, 4, 3, 1], [0, 3, 4], [0, 1, 3], [3, 4, 2, 0], [0, 1, 2, 3], [0, 3, 4], [0, 3, 4, 1], [0, 1, 2], [0, 3, 1, 4], [2, 4, 3], [0, 3, 2, 4], [0, 3, 1, 4], [2, 4, 0, 3], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [3, 4, 2, 0], [0, 1, 3], [2, 0, 3, 4], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3, 0], [3, 4, 2, 0], [0, 3, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1, 3], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [0, 3, 4, 2], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 3], [2, 0, 1], [0, 1, 2], [3, 0, 4, 2], [2, 4, 3], [2, 1, 0], [2, 1, 0, 4], [0, 3, 4, 2], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 3, 4, 0], [0, 1, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 2, 4], [2, 4, 3], [2, 4, 3], [3, 2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 3, 2], [3, 0, 4], [2, 4, 1, 3], [1, 0, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 1, 4, 0], [1, 2, 0], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 1, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 3], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 4, 3, 1], [3, 2, 4, 0], [0, 3, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1, 2], [3, 4, 2, 0], [3, 4, 0, 2], [0, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 3, 2], [4, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3], [1, 0, 2], [2, 4, 3], [0, 1, 3, 2], [0, 2, 1], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1, 3], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4], [0, 1, 3, 4], [2, 4, 3], [1, 0, 2], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2], [2, 4, 1, 3], [3, 0, 4, 2], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [1, 0, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3]]\n",
      "[[1], [0, 1], [0, 1], [0], [3], [1, 3], [3, 4], [3], [0], [0], [0], [1], [2], [1], [2], [0, 1], [3], [4], [1], [2], [0], [1], [1], [2], [1], [0, 1], [4], [1], [1, 2], [3], [0], [0, 1], [3], [1], [4], [0], [1], [4], [0], [0], [1], [0], [4], [4], [4], [4], [1], [1], [2], [4], [0], [1], [3, 4], [2], [3, 4], [0], [2], [1], [0, 1], [2], [0], [2, 4], [2], [1], [2, 4], [0, 1], [4], [3], [1], [3], [0], [3], [1], [1], [3], [0], [2], [3], [4], [4], [0], [1, 2], [2, 4], [1], [4], [1, 2], [2], [3, 4], [2], [0, 1], [1], [2], [1], [0], [4], [0], [1], [2], [1], [2, 4], [1], [3], [3], [0], [4], [0, 1], [3, 4], [0, 1], [4], [4], [0], [0, 1], [3, 4], [2, 4], [2, 4], [0, 1], [1], [1], [1, 2], [3], [4], [3, 4], [2, 4], [0], [3], [0], [0], [3], [1], [2], [0], [0], [4], [4], [3], [0], [1], [1], [0], [2, 4], [3, 4], [3, 4], [1], [0, 1], [3, 4], [3], [1], [2], [0, 1], [4], [1], [4], [2], [4], [0], [1], [0, 1], [0, 1], [1], [0], [1], [4], [1, 2], [0], [3, 4], [0], [4], [3], [3, 4], [2], [0], [3], [2], [0, 1], [4], [3, 4], [4], [2], [2, 4], [4], [0, 1], [0, 1], [1], [1], [3], [0], [1], [2], [1], [4], [0, 1], [3], [3], [0], [3], [3], [4], [1], [1], [2, 4], [1], [4], [4], [1], [4], [0], [3], [3], [0], [4], [0, 1], [0], [2], [2, 4], [3, 4], [0, 1], [4], [3, 4], [1], [1], [0, 1], [2, 4], [0], [1], [3], [0], [1, 2], [2], [0, 1], [3, 4], [0], [0], [2], [0], [3, 4], [0], [1], [0], [0], [0], [3, 4], [0, 1], [2], [0], [1], [3], [4], [4], [1], [4]]\n",
      "NL_pred of 5th iteration [[2, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 2, 4], [0, 3, 1, 4], [3, 0, 4, 2], [2, 4, 1, 3], [3, 0, 4, 2], [2, 1, 4, 3], [0, 2, 1, 3], [0, 3, 2, 4], [2, 4, 1, 3], [2, 0, 1, 4], [0, 3, 4], [0, 3, 4], [0, 3, 1, 4], [0, 3, 2, 4], [2, 4, 0, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 0, 3, 4], [0, 1, 2, 3], [0, 3, 4], [2, 4, 1, 3], [0, 3, 4, 2], [2, 4, 1, 3], [2, 1, 0, 4], [0, 3, 2, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 3, 4], [2, 4, 1, 3], [2, 4, 1, 3], [3, 0, 4, 2], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.027169667349921333  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.027117551697625055  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.027022973696390788  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.026897586716545952  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.026753510369194877  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.02660178608364529  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.02645138104756673  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.02630917231241862  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.02617918915218777  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.026063723034328884  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.025962318314446342  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.02587512599097358  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.025801833470662436  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.025739468468560112  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.025686266687181263  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.025640998946295845  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.025602343347337513  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.025569274690416123  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.025540839301215278  Accuracy on Support set:0.0\n",
      "torch.Size([45, 2048]) torch.Size([45])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.025516255696614584  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[3, 4, 2, 0], [2, 4, 3], [2, 4, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 0, 4], [1, 2, 0], [2, 1, 0, 4], [2, 4, 3, 1], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 3], [2, 1, 0, 4], [0, 1, 3, 2], [3, 0, 2, 4], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 0, 3], [0, 3, 2, 4], [0, 3, 1, 4], [3, 0, 4, 2], [2, 4, 3], [0, 1, 3, 2], [3, 0, 4, 2], [3, 0, 4], [1, 2, 0, 4], [2, 4, 1, 3], [2, 4, 3], [2, 1, 4, 0], [4, 3, 2, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 1, 2, 3], [2, 1, 4, 3], [2, 4, 3, 1], [3, 4, 2, 0], [2, 4, 1, 3], [0, 1, 3, 2], [0, 3, 1, 2], [0, 1, 2, 3], [0, 2, 1, 3], [3, 4, 2, 0], [3, 4, 0, 2], [0, 3, 4, 1], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 2], [3, 0, 4, 1], [1, 2, 0], [2, 4, 1, 3], [0, 3, 1, 4], [0, 3, 2, 4], [2, 4, 3], [0, 3, 1, 4], [2, 4, 3, 1], [0, 3, 1], [0, 3, 1, 4], [3, 4, 0, 2], [0, 3, 1], [2, 4, 3], [0, 1, 2, 3], [2, 1, 0, 4], [3, 4, 2, 0], [2, 1, 0, 4], [2, 4, 1, 3], [2, 0, 1, 4], [3, 4, 0, 2], [4, 2, 3, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4, 1], [2, 0, 1, 4], [0, 2, 3, 1], [0, 2, 1, 3], [2, 4, 3, 1], [0, 3, 4], [0, 1, 3], [3, 4, 2, 0], [0, 1, 2, 3], [0, 3, 4], [0, 3, 4, 1], [0, 1, 2], [0, 3, 1, 4], [2, 4, 3], [0, 3, 2, 4], [0, 3, 1, 4], [2, 4, 0, 3], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 1, 3], [3, 0, 4, 2], [0, 3, 1, 4], [3, 4, 2, 0], [0, 1, 3], [2, 0, 3, 4], [2, 1, 0, 4], [2, 1, 0, 4], [2, 4, 1, 3], [0, 1, 2, 3], [2, 4, 3], [2, 1, 0], [2, 4, 3], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 3], [0, 1, 2], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3, 0], [3, 4, 2, 0], [0, 3, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 0, 1], [0, 3, 1], [2, 4, 1, 3], [2, 1, 4, 0], [2, 4, 1, 3], [2, 4, 1, 3], [2, 1, 0, 4], [0, 3, 4, 2], [0, 3, 4, 1], [2, 4, 3, 1], [2, 4, 1, 3], [0, 1, 2, 3], [0, 1, 3, 2], [2, 1, 0, 4], [2, 4, 1, 3], [2, 4, 3, 0], [3, 4, 0, 2], [2, 4, 3, 1], [0, 1, 3], [2, 0, 1], [0, 1, 2], [3, 0, 4, 2], [2, 4, 3], [2, 1, 0], [2, 1, 0, 4], [0, 3, 4, 2], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 3, 4, 0], [0, 1, 3, 2], [0, 3, 4, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 3, 2, 4], [2, 4, 3], [2, 4, 3], [3, 2, 4, 0], [2, 4, 1, 3], [2, 4, 3, 0], [0, 1, 3, 2], [3, 0, 4], [2, 4, 1, 3], [1, 0, 2], [2, 4, 1, 3], [0, 1, 2, 3], [2, 1, 4, 0], [1, 2, 0], [0, 3, 1, 4], [2, 4, 3, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3], [0, 1, 3, 2], [2, 1, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 3], [0, 1, 2, 3], [2, 4, 3], [2, 4, 3], [3, 4, 2, 0], [2, 3, 0, 4], [2, 1, 4, 0], [2, 4, 3, 1], [3, 2, 4, 0], [0, 3, 1, 4], [3, 0, 4, 2], [0, 1, 3, 2], [2, 4, 3], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1, 2], [3, 4, 2, 0], [3, 4, 0, 2], [0, 1, 3], [3, 0, 4, 2], [0, 1, 3, 2], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3], [2, 4, 3, 1], [2, 1, 4, 0], [2, 1, 4, 0], [2, 4, 3, 1], [0, 1, 3, 2], [4, 2, 3], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3], [1, 0, 2], [2, 4, 3], [0, 1, 3, 2], [0, 2, 1], [3, 0, 4, 2], [3, 0, 4, 2], [2, 4, 3], [0, 3, 1], [2, 4, 1, 3], [3, 0, 4, 2], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4], [0, 1, 3, 4], [2, 4, 3], [1, 0, 2], [2, 4, 1, 3], [2, 4, 3, 1], [0, 3, 1, 4], [2, 4, 1, 3], [0, 1, 2], [2, 4, 1, 3], [3, 0, 4, 2], [2, 4, 1, 3], [2, 4, 1, 3], [2, 4, 1, 3], [1, 0, 2], [2, 4, 3], [0, 3, 1, 4], [2, 4, 1, 3], [2, 4, 3, 0], [2, 1, 4, 0], [0, 1, 2, 3], [0, 1, 3, 2], [3, 4, 2, 0], [0, 1, 2, 3]]\n",
      "POSITION :  [[1], [0, 1], [0, 1], [0], [3], [1, 3], [3, 4], [3], [0], [0], [0], [1], [2], [1], [2], [0, 1], [3], [4], [1], [2], [0], [1], [1], [2], [1], [0, 1], [4], [1], [1, 2], [3], [0], [0, 1], [3], [1], [4], [0], [1], [4], [0], [0], [1], [0], [4], [4], [4], [4], [1], [1], [2], [4], [0], [1], [3, 4], [2], [3, 4], [0], [2], [1], [0, 1], [2], [0], [2, 4], [2], [1], [2, 4], [0, 1], [4], [3], [1], [3], [0], [3], [1], [1], [3], [0], [2], [3], [4], [4], [0], [1, 2], [2, 4], [1], [4], [1, 2], [2], [3, 4], [2], [0, 1], [1], [2], [1], [0], [4], [0], [1], [2], [1], [2, 4], [1], [3], [3], [0], [4], [0, 1], [3, 4], [0, 1], [4], [4], [0], [0, 1], [3, 4], [2, 4], [2, 4], [0, 1], [1], [1], [1, 2], [3], [4], [3, 4], [2, 4], [0], [3], [0], [0], [3], [1], [2], [0], [0], [4], [4], [3], [0], [1], [1], [0], [2, 4], [3, 4], [3, 4], [1], [0, 1], [3, 4], [3], [1], [2], [0, 1], [4], [1], [4], [2], [4], [0], [1], [0, 1], [0, 1], [1], [0], [1], [4], [1, 2], [0], [3, 4], [0], [4], [3], [3, 4], [2], [0], [3], [2], [0, 1], [4], [3, 4], [4], [2], [2, 4], [4], [0, 1], [0, 1], [1], [1], [3], [0], [1], [2], [1], [4], [0, 1], [3], [3], [0], [3], [3], [4], [1], [1], [2, 4], [1], [4], [4], [1], [4], [0], [3], [3], [0], [4], [0, 1], [0], [2], [2, 4], [3, 4], [0, 1], [4], [3, 4], [1], [1], [0, 1], [2, 4], [0], [1], [3], [0], [1, 2], [2], [0, 1], [3, 4], [0], [0], [2], [0], [3, 4], [0], [1], [0], [0], [0], [3, 4], [0, 1], [2], [0], [1], [3], [4], [4], [1], [4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.372\n",
      "tensor([1, 0, 3, 3, 0, 0, 0, 1, 2, 1, 2, 3, 4, 1, 2, 0, 1, 1, 2, 1, 4, 1, 3, 0,\n",
      "        3, 1, 4, 0, 1, 4, 0, 0, 1, 0, 4, 4, 4, 4, 1, 1, 2, 4, 0, 1, 2, 0, 2, 1,\n",
      "        2, 0, 2, 1, 4, 3, 1, 3, 0, 3, 1, 1, 3, 0, 2, 3, 4, 4, 0, 1, 4, 2, 2, 1,\n",
      "        2, 1, 0, 4, 0, 1, 2, 1, 1, 3, 3, 0, 4, 4, 4, 0, 1, 1, 3, 4, 0, 3, 0, 0,\n",
      "        3, 1, 2, 0, 0, 4, 4, 3, 0, 1, 1, 0, 1, 3, 1, 2, 4, 1, 4, 2, 4, 0, 1, 1,\n",
      "        0, 1, 4, 0, 0, 4, 3, 2, 0, 3, 2, 4, 4, 2, 4, 1, 1, 3, 0, 1, 2, 1, 4, 3,\n",
      "        3, 0, 3, 3, 4, 1, 1, 1, 4, 4, 1, 4, 0, 3, 3, 0, 4, 0, 2, 4, 1, 1, 0, 1,\n",
      "        3, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 1, 3, 4, 4, 1, 4])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 95.2127659574468\n",
      "Epoch: 1  Loss: 96.27659574468085\n",
      "Epoch: 2  Loss: 92.02127659574468\n",
      "Epoch: 3  Loss: 92.02127659574468\n",
      "Epoch: 4  Loss: 93.08510638297872\n",
      "Epoch: 5  Loss: 96.27659574468085\n",
      "Epoch: 6  Loss: 97.87234042553192\n",
      "Epoch: 7  Loss: 98.93617021276596\n",
      "Epoch: 8  Loss: 99.46808510638297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 14/15 [11:41<00:52, 52.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 99.46808510638297\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  29.333333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1945504018.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************  Initial training the model on Support set\n",
      "Train_Epoch: 0  Train_Loss: 4.505109608802013  Accuracy on Support set:40.0\n",
      "Train_Epoch: 1  Train_Loss: 2.3873238318413494  Accuracy on Support set:64.0\n",
      "Train_Epoch: 2  Train_Loss: 1.2472521519102155  Accuracy on Support set:84.0\n",
      "Train_Epoch: 3  Train_Loss: 0.6771572354622185  Accuracy on Support set:92.0\n",
      "Train_Epoch: 4  Train_Loss: 0.27322623796761036  Accuracy on Support set:100.0\n",
      "Train_Epoch: 5  Train_Loss: 0.12875954296439887  Accuracy on Support set:100.0\n",
      "Train_Epoch: 6  Train_Loss: 0.09219950824975967  Accuracy on Support set:100.0\n",
      "Train_Epoch: 7  Train_Loss: 0.07541103355586529  Accuracy on Support set:100.0\n",
      "Train_Epoch: 8  Train_Loss: 0.06552890658378602  Accuracy on Support set:100.0\n",
      "Train_Epoch: 9  Train_Loss: 0.058152373470366  Accuracy on Support set:100.0\n",
      "Train_Epoch: 10  Train_Loss: 0.05258164731785655  Accuracy on Support set:100.0\n",
      "Train_Epoch: 11  Train_Loss: 0.04805322829633951  Accuracy on Support set:100.0\n",
      "Train_Epoch: 12  Train_Loss: 0.04435195993632078  Accuracy on Support set:100.0\n",
      "Train_Epoch: 13  Train_Loss: 0.0412356430105865  Accuracy on Support set:100.0\n",
      "Train_Epoch: 14  Train_Loss: 0.03858076514676213  Accuracy on Support set:100.0\n",
      "Train_Epoch: 15  Train_Loss: 0.03627504112198949  Accuracy on Support set:100.0\n",
      "Train_Epoch: 16  Train_Loss: 0.03423715205863118  Accuracy on Support set:100.0\n",
      "Train_Epoch: 17  Train_Loss: 0.032440380323678256  Accuracy on Support set:100.0\n",
      "Train_Epoch: 18  Train_Loss: 0.030872140005230905  Accuracy on Support set:100.0\n",
      "Train_Epoch: 19  Train_Loss: 0.029455204550176858  Accuracy on Support set:100.0\n",
      "Train_Epoch: 20  Train_Loss: 0.028175848945975305  Accuracy on Support set:100.0\n",
      "Train_Epoch: 21  Train_Loss: 0.02701475051231682  Accuracy on Support set:100.0\n",
      "Train_Epoch: 22  Train_Loss: 0.02594228271394968  Accuracy on Support set:100.0\n",
      "Train_Epoch: 23  Train_Loss: 0.0249699373729527  Accuracy on Support set:100.0\n",
      "Train_Epoch: 24  Train_Loss: 0.024071402670815586  Accuracy on Support set:100.0\n",
      "Train_Epoch: 25  Train_Loss: 0.023240607865154743  Accuracy on Support set:100.0\n",
      "Train_Epoch: 26  Train_Loss: 0.022470877282321455  Accuracy on Support set:100.0\n",
      "Train_Epoch: 27  Train_Loss: 0.021753526432439683  Accuracy on Support set:100.0\n",
      "Train_Epoch: 28  Train_Loss: 0.021079673040658234  Accuracy on Support set:100.0\n",
      "Train_Epoch: 29  Train_Loss: 0.020450457641854883  Accuracy on Support set:100.0\n",
      "Train_Epoch: 30  Train_Loss: 0.019860080499202012  Accuracy on Support set:100.0\n",
      "Train_Epoch: 31  Train_Loss: 0.019303684569895268  Accuracy on Support set:100.0\n",
      "Train_Epoch: 32  Train_Loss: 0.01877927554771304  Accuracy on Support set:100.0\n",
      "Train_Epoch: 33  Train_Loss: 0.018293985528871418  Accuracy on Support set:100.0\n",
      "Train_Epoch: 34  Train_Loss: 0.017826179768890144  Accuracy on Support set:100.0\n",
      "Train_Epoch: 35  Train_Loss: 0.01738623050041497  Accuracy on Support set:100.0\n",
      "Train_Epoch: 36  Train_Loss: 0.01697227269411087  Accuracy on Support set:100.0\n",
      "Train_Epoch: 37  Train_Loss: 0.01657584177330136  Accuracy on Support set:100.0\n",
      "Train_Epoch: 38  Train_Loss: 0.016198825854808092  Accuracy on Support set:100.0\n",
      "Train_Epoch: 39  Train_Loss: 0.01583983926102519  Accuracy on Support set:100.0\n",
      "Train_Epoch: 40  Train_Loss: 0.015497825387865306  Accuracy on Support set:100.0\n",
      "Train_Epoch: 41  Train_Loss: 0.015172183318063616  Accuracy on Support set:100.0\n",
      "Train_Epoch: 42  Train_Loss: 0.014859071904793382  Accuracy on Support set:100.0\n",
      "Train_Epoch: 43  Train_Loss: 0.01456007287837565  Accuracy on Support set:100.0\n",
      "Train_Epoch: 44  Train_Loss: 0.014273640252649784  Accuracy on Support set:100.0\n",
      "Train_Epoch: 45  Train_Loss: 0.0140003003180027  Accuracy on Support set:100.0\n",
      "Train_Epoch: 46  Train_Loss: 0.013735588965937495  Accuracy on Support set:100.0\n",
      "Train_Epoch: 47  Train_Loss: 0.013483197633177043  Accuracy on Support set:100.0\n",
      "Train_Epoch: 48  Train_Loss: 0.013238722179085016  Accuracy on Support set:100.0\n",
      "Train_Epoch: 49  Train_Loss: 0.013004874903708696  Accuracy on Support set:100.0\n",
      "Testing after training on support set\n",
      "Accuracy of testing on Query Set:  56.666666666666664\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "2 1.5720680313080493e-09 tensor([9.9120e-01, 2.2011e-03, 1.5721e-09, 6.5980e-03, 2.3853e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0010477312607690692 tensor([0.0084, 0.0279, 0.0010, 0.3927, 0.5699], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.758541429808247e-06 tensor([1.7585e-06, 6.2486e-02, 9.2044e-01, 1.1732e-05, 1.7060e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.279188528424129e-05 tensor([6.2792e-05, 1.2362e-04, 1.1248e-04, 3.3778e-01, 6.6192e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002723921788856387 tensor([0.0038, 0.8711, 0.0511, 0.0027, 0.0713], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004033872683066875 tensor([6.0143e-03, 9.8276e-01, 6.8545e-03, 4.0339e-04, 3.9632e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007447098614647985 tensor([0.0647, 0.0778, 0.0007, 0.6329, 0.2238], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005657403962686658 tensor([0.0006, 0.5631, 0.2560, 0.0014, 0.1789], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.0097364483717683e-07 tensor([2.0097e-07, 2.8291e-04, 4.2851e-02, 1.2266e-03, 9.5564e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.4984483388834633e-05 tensor([2.4984e-05, 1.1548e-04, 2.8279e-04, 1.7956e-01, 8.2002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2811316082661506e-05 tensor([1.2811e-05, 1.6368e-01, 7.6941e-01, 8.6363e-05, 6.6812e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001994532998651266 tensor([0.0020, 0.4620, 0.1294, 0.0104, 0.3963], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004858621396124363 tensor([0.0049, 0.7439, 0.0862, 0.0102, 0.1548], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.71982267211024e-08 tensor([3.7198e-08, 5.0908e-05, 2.1094e-02, 1.2060e-03, 9.7765e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2631736353796441e-05 tensor([1.2632e-05, 5.8352e-04, 3.6006e-03, 3.4257e-02, 9.6155e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.27666818122907e-08 tensor([9.4763e-01, 5.1962e-02, 3.2767e-08, 4.1071e-04, 1.8803e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7012786202030838e-06 tensor([8.6361e-01, 4.7115e-02, 1.7013e-06, 8.8612e-02, 6.6080e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.777785805876178e-12 tensor([4.7778e-12, 1.3426e-04, 9.9953e-01, 2.0477e-09, 3.3355e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6311490824705288e-08 tensor([7.7568e-06, 9.1744e-08, 1.6311e-08, 9.8507e-01, 1.4921e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.782597872894257e-05 tensor([5.7826e-05, 3.7552e-04, 6.0829e-04, 2.0299e-01, 7.9597e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.1999346497759689e-05 tensor([2.0796e-03, 3.5847e-04, 1.1999e-05, 9.2512e-01, 7.2432e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.2602983992546797e-06 tensor([9.4933e-03, 5.7454e-04, 3.2603e-06, 9.4640e-01, 4.3533e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.001443732064217329 tensor([0.0014, 0.5316, 0.2148, 0.0060, 0.2461], grad_fn=<SoftmaxBackward0>)\n",
      "2 4.370217254745512e-08 tensor([4.9632e-05, 7.5642e-07, 4.3702e-08, 9.8310e-01, 1.6848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006271493039093912 tensor([6.2715e-04, 1.1224e-02, 6.4257e-03, 8.5212e-02, 8.9651e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.770005029326342e-10 tensor([3.5905e-04, 3.4674e-07, 5.7700e-10, 9.9903e-01, 6.0894e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.583038031886645e-09 tensor([3.0457e-01, 3.1811e-04, 5.5830e-09, 6.9491e-01, 2.0505e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.845583128134436e-11 tensor([6.8456e-11, 3.9600e-04, 9.9885e-01, 1.3491e-08, 7.5482e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1048158739868086e-05 tensor([1.1048e-05, 9.4924e-03, 9.1681e-02, 3.6605e-03, 8.9515e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0013910495908930898 tensor([0.0529, 0.1004, 0.0014, 0.5014, 0.3439], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.718138946671388e-07 tensor([9.0678e-01, 9.2430e-02, 1.7181e-07, 7.7678e-04, 8.1495e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012711004819720984 tensor([1.2711e-04, 3.2534e-03, 3.3571e-03, 8.0043e-02, 9.1322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.515165149612585e-05 tensor([1.5152e-05, 5.9480e-02, 6.2910e-01, 6.1713e-04, 3.1079e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.792247636942193e-06 tensor([4.6908e-01, 2.3048e-02, 4.7922e-06, 5.0182e-01, 6.0449e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9628107111202553e-05 tensor([8.5949e-04, 2.4220e-04, 1.9628e-05, 7.0837e-01, 2.9050e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.9434551745689532e-09 tensor([9.9340e-01, 4.4171e-03, 1.9435e-09, 2.1808e-03, 1.1775e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0011834303149953485 tensor([0.0012, 0.7879, 0.1309, 0.0016, 0.0784], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004421349614858627 tensor([0.0044, 0.7463, 0.0647, 0.0073, 0.1774], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.000713936984539032 tensor([0.0710, 0.0918, 0.0007, 0.5601, 0.2763], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014716156758368015 tensor([0.0147, 0.8073, 0.0215, 0.0162, 0.1403], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0283952178724576e-06 tensor([5.0289e-01, 1.0126e-02, 1.0284e-06, 4.8288e-01, 4.1088e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0245462362945545e-06 tensor([1.2300e-06, 1.8315e-01, 8.1430e-01, 1.0245e-06, 2.5555e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.353958781346591e-08 tensor([6.3540e-08, 4.2833e-03, 9.5816e-01, 1.0619e-05, 3.7550e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00011826817353721708 tensor([1.1827e-04, 2.8099e-02, 6.8006e-02, 1.5534e-02, 8.8824e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.322433182504028e-05 tensor([6.3224e-05, 1.5563e-02, 5.6981e-02, 1.0680e-02, 9.1671e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.245099130694712e-10 tensor([9.9526e-01, 2.9954e-03, 9.2451e-10, 1.7466e-03, 1.1418e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.584652977726364e-06 tensor([1.4860e-04, 9.8423e-01, 1.5463e-02, 1.5847e-06, 1.5434e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0167354957957286e-07 tensor([1.0167e-07, 3.8566e-02, 9.6046e-01, 2.0139e-07, 9.7262e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.683724340506565e-12 tensor([3.3710e-01, 4.0888e-06, 1.6837e-12, 6.6289e-01, 1.3941e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.854359296288749e-08 tensor([7.8544e-08, 2.6775e-03, 8.7597e-01, 3.3229e-05, 1.2132e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.375304267043248e-05 tensor([3.6563e-01, 6.2785e-01, 7.3753e-05, 5.5306e-03, 9.1904e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.9470751289627515e-06 tensor([5.9245e-04, 9.9578e-01, 3.5001e-03, 3.9471e-06, 1.2169e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0025643750559538603 tensor([0.0026, 0.2936, 0.0702, 0.0251, 0.6086], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.874865204371417e-09 tensor([4.9998e-06, 4.2826e-08, 7.8749e-09, 9.8526e-01, 1.4732e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005361976800486445 tensor([5.3620e-04, 1.9748e-03, 8.4571e-04, 2.8724e-01, 7.0940e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0287313223500405e-09 tensor([9.9220e-01, 7.4402e-03, 1.0287e-09, 3.5562e-04, 3.0676e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.768787438384607e-07 tensor([1.3077e-04, 9.8964e-01, 1.0130e-02, 8.7688e-07, 9.7857e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.135215085440905e-09 tensor([7.1352e-09, 1.2599e-03, 9.8898e-01, 1.4215e-06, 9.7543e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003060505725443363 tensor([0.0279, 0.9573, 0.0039, 0.0031, 0.0079], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2518289622676093e-06 tensor([1.2518e-06, 1.5774e-04, 3.8207e-03, 7.9296e-03, 9.8809e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0035898771602660418 tensor([0.0671, 0.3446, 0.0036, 0.3361, 0.2486], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.803890336304903e-05 tensor([1.1419e-02, 9.8837e-01, 1.4978e-04, 1.8039e-05, 4.1648e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7656562079082505e-07 tensor([1.7657e-07, 4.4448e-04, 1.6105e-01, 1.0793e-03, 8.3743e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.2507292030127246e-08 tensor([6.1958e-05, 5.8629e-07, 2.2507e-08, 9.9171e-01, 8.2322e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7331906576600886e-07 tensor([1.7332e-07, 1.2861e-04, 2.4184e-02, 2.1719e-03, 9.7352e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.864855185360284e-08 tensor([9.3561e-01, 6.4050e-02, 4.8649e-08, 3.3727e-04, 2.6016e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012201855861349031 tensor([3.3094e-01, 6.5365e-01, 1.2202e-04, 1.2810e-02, 2.4758e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.513415327281109e-06 tensor([1.5134e-06, 1.6873e-03, 7.9876e-02, 2.1565e-03, 9.1628e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.50316825232278e-10 tensor([9.5032e-10, 3.7418e-04, 9.8034e-01, 9.3908e-07, 1.9281e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1850454839645863e-08 tensor([1.1850e-08, 2.0815e-04, 2.8711e-01, 1.3626e-04, 7.1254e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.340168177601299e-07 tensor([9.2199e-01, 7.4253e-02, 3.3402e-07, 3.7276e-03, 2.6590e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.2355990936826515e-10 tensor([9.9098e-01, 8.8934e-03, 4.2356e-10, 1.2285e-04, 8.6994e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8978445481820927e-08 tensor([1.8978e-08, 6.5512e-06, 2.3063e-03, 2.1055e-03, 9.9558e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2626915513180847e-08 tensor([2.9743e-02, 5.9936e-05, 1.2627e-08, 9.6924e-01, 9.5322e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3092508172339876e-07 tensor([2.3093e-07, 6.1658e-04, 9.2217e-02, 7.8314e-04, 9.0638e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.141807534731925e-05 tensor([1.6410e-01, 8.3518e-01, 6.1418e-05, 4.9567e-04, 1.6392e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0015453569358214736 tensor([0.0015, 0.2069, 0.0688, 0.0221, 0.7007], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1011854405759691e-12 tensor([1.1012e-12, 1.0103e-05, 9.8629e-01, 3.7849e-08, 1.3701e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3224661777065947e-11 tensor([2.4501e-06, 1.3762e-09, 2.3225e-11, 9.9919e-01, 8.1058e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.059983181927464e-09 tensor([6.0600e-09, 1.8177e-04, 4.3473e-01, 7.8002e-05, 5.6501e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.515041680657305e-05 tensor([3.5150e-05, 1.1385e-01, 6.0495e-01, 9.7926e-04, 2.8019e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002324816072359681 tensor([0.0090, 0.9556, 0.0123, 0.0023, 0.0208], grad_fn=<SoftmaxBackward0>)\n",
      "0 5.53121317636851e-08 tensor([5.5312e-08, 1.0794e-02, 9.8173e-01, 1.1291e-06, 7.4718e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.8992849707718555e-12 tensor([9.1896e-06, 1.5431e-09, 4.8993e-12, 9.9988e-01, 1.0980e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.2898349579020305e-09 tensor([7.2898e-09, 6.3868e-04, 9.0348e-01, 1.0859e-05, 9.5870e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8619678281844854e-12 tensor([9.9886e-01, 1.1198e-03, 1.8620e-12, 2.3171e-05, 1.2254e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.340349720332597e-07 tensor([1.1054e-05, 7.3902e-01, 2.6013e-01, 9.3403e-07, 8.4698e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012066785711795092 tensor([1.2067e-04, 1.9437e-03, 2.9419e-03, 1.1108e-01, 8.8392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0452644261249588e-07 tensor([1.8121e-05, 4.9184e-07, 1.0453e-07, 9.5619e-01, 4.3796e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1817195539639584e-11 tensor([4.1817e-11, 4.5197e-07, 1.3039e-02, 1.0714e-04, 9.8685e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.13156142234061e-08 tensor([9.8603e-01, 1.1606e-02, 1.1316e-08, 2.3577e-03, 4.0898e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.7368336557410657e-05 tensor([1.0354e-02, 9.8941e-01, 1.8164e-04, 1.7368e-05, 4.1965e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.935311188030255e-09 tensor([3.9353e-09, 1.6772e-03, 9.9403e-01, 3.7079e-07, 4.2923e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.507202001567066e-09 tensor([9.8119e-01, 1.7210e-03, 2.5072e-09, 1.7082e-02, 3.6624e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3759328787443792e-08 tensor([1.3759e-08, 6.0048e-04, 8.0569e-01, 4.0696e-05, 1.9367e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.17647523629239e-08 tensor([3.7413e-01, 1.5680e-03, 7.1765e-08, 6.2358e-01, 7.2044e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.431592853710754e-06 tensor([2.2913e-03, 9.9700e-01, 6.4601e-04, 5.4316e-06, 5.2481e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.6637590053724125e-05 tensor([2.6638e-05, 3.0770e-01, 6.6979e-01, 4.4776e-05, 2.2431e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.669796898175264e-07 tensor([9.1064e-01, 3.2788e-02, 8.6698e-07, 5.6089e-02, 4.7893e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0014762763166800141 tensor([0.0585, 0.0926, 0.0015, 0.5656, 0.2819], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.529306246084161e-06 tensor([8.2884e-01, 1.6803e-01, 1.5293e-06, 3.0616e-03, 6.4803e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.35595829912927e-05 tensor([7.1714e-04, 9.7069e-01, 2.6363e-02, 4.3560e-05, 2.1901e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.426922887683759e-07 tensor([3.4269e-07, 3.8622e-02, 9.5839e-01, 1.2958e-06, 2.9843e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00017241940076928586 tensor([1.3974e-03, 1.4228e-03, 1.7242e-04, 5.8294e-01, 4.1406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0014637297217632e-07 tensor([1.0015e-07, 1.4476e-03, 6.4629e-01, 1.6966e-04, 3.5210e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.9837939880271733e-07 tensor([2.8605e-02, 3.6798e-04, 3.9838e-07, 9.6504e-01, 5.9827e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.240241757244803e-06 tensor([3.2392e-01, 6.7570e-01, 7.2402e-06, 3.3923e-04, 3.5528e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003635574597865343 tensor([0.0036, 0.0715, 0.0157, 0.1464, 0.7627], grad_fn=<SoftmaxBackward0>)\n",
      "2 9.249851586901059e-07 tensor([1.1917e-05, 1.2103e-06, 9.2499e-07, 8.3564e-01, 1.6435e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.006495662499219179 tensor([0.0065, 0.6079, 0.0715, 0.0234, 0.2907], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.2477475649919079e-13 tensor([9.4370e-01, 7.1414e-06, 1.2477e-13, 5.6289e-02, 4.0511e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.739958477439359e-05 tensor([3.2908e-01, 6.6211e-01, 9.7400e-05, 7.1359e-03, 1.5735e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.081066774437204e-06 tensor([5.0811e-06, 5.2656e-02, 8.7364e-01, 9.7450e-05, 7.3603e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.75828869639372e-08 tensor([4.0649e-04, 3.9860e-06, 4.7583e-08, 9.8831e-01, 1.1284e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014923596754670143 tensor([0.0015, 0.2589, 0.1103, 0.0155, 0.6138], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.472213958692464e-09 tensor([1.9725e-01, 2.4107e-04, 7.4722e-09, 8.0219e-01, 3.2304e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017444869445171207 tensor([9.7055e-04, 9.3292e-01, 5.6150e-02, 1.7445e-04, 9.7869e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.0874656406231225e-05 tensor([4.0875e-05, 6.6681e-03, 3.4900e-02, 1.7714e-02, 9.4068e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.268696092272876e-07 tensor([3.1905e-02, 7.4399e-04, 8.2687e-07, 9.5813e-01, 9.2232e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.505553304918642e-11 tensor([4.0722e-01, 3.0524e-05, 5.5056e-11, 5.9274e-01, 8.8997e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.445941862555983e-12 tensor([9.9882e-01, 1.0705e-03, 8.4459e-12, 1.1082e-04, 8.2264e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007425858057104051 tensor([0.0007, 0.6641, 0.1830, 0.0019, 0.1503], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.702664563842717e-11 tensor([3.7027e-11, 1.2650e-04, 9.9667e-01, 4.6680e-08, 3.2029e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.789143422887719e-07 tensor([2.7891e-07, 3.1714e-06, 1.1589e-04, 4.3230e-02, 9.5665e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.556828101456631e-05 tensor([1.5568e-05, 1.9326e-01, 7.7629e-01, 4.5273e-05, 3.0387e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5462058716719795e-12 tensor([9.1758e-01, 2.0630e-05, 1.5462e-12, 8.2401e-02, 2.6909e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0015595908043906093 tensor([0.1043, 0.2243, 0.0016, 0.4364, 0.2334], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0573605102592865e-08 tensor([1.0574e-08, 3.0933e-03, 9.9171e-01, 5.7330e-07, 5.1920e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.465350832150161e-12 tensor([4.5572e-05, 5.2107e-09, 4.4654e-12, 9.9988e-01, 7.2035e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0002264289796585217 tensor([2.2643e-04, 4.9660e-04, 3.1171e-04, 3.5680e-01, 6.4217e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.7618013714582048e-07 tensor([6.8606e-01, 3.1389e-01, 1.7618e-07, 4.5629e-05, 1.0777e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004300448577851057 tensor([0.0777, 0.4150, 0.0043, 0.2503, 0.2527], grad_fn=<SoftmaxBackward0>)\n",
      "0 3.349111648276448e-05 tensor([3.3491e-05, 1.9157e-01, 7.2709e-01, 1.8410e-04, 8.1118e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.007344043406192e-06 tensor([1.6649e-03, 1.7220e-04, 5.0073e-06, 9.2345e-01, 7.4709e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.2276137769949855e-06 tensor([2.2276e-06, 1.0375e-03, 3.5984e-02, 5.0386e-03, 9.5794e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.698717160853903e-08 tensor([9.6023e-01, 3.9188e-02, 2.6987e-08, 5.8419e-04, 2.0192e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.810789975337684e-05 tensor([3.4501e-03, 9.9198e-01, 3.7766e-03, 7.8108e-05, 7.1380e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.8561327408406214e-07 tensor([1.8561e-07, 2.8223e-03, 6.5679e-01, 1.4982e-04, 3.4024e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.8811892321668466e-10 tensor([7.4503e-01, 1.4195e-04, 1.8812e-10, 2.5482e-01, 8.6528e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3276600305256459e-11 tensor([1.3277e-11, 2.6743e-05, 9.7160e-01, 2.6256e-07, 2.8375e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9502873033493415e-08 tensor([1.2086e-02, 5.2655e-05, 2.9503e-08, 9.8546e-01, 2.3993e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.556598807743285e-06 tensor([1.6914e-04, 9.6507e-01, 3.4113e-02, 5.5566e-06, 6.3986e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.792019353772048e-06 tensor([4.7920e-06, 1.8176e-01, 8.1239e-01, 5.5064e-06, 5.8401e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0019474411383271217 tensor([0.0019, 0.1173, 0.0275, 0.0474, 0.8059], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.3731809284963674e-09 tensor([4.3732e-09, 2.2157e-05, 4.3207e-02, 3.2564e-04, 9.5645e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.1117718684848796e-09 tensor([9.9031e-01, 9.1680e-03, 2.1118e-09, 5.1934e-04, 4.3154e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5900808964252064e-07 tensor([4.9166e-01, 5.0831e-01, 2.5901e-07, 3.0801e-05, 9.0865e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 5.067642518952198e-07 tensor([5.0676e-07, 8.4041e-04, 1.2656e-01, 1.3993e-03, 8.7120e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.394548654332084e-08 tensor([1.3945e-08, 1.4762e-07, 2.7189e-05, 3.8286e-02, 9.6169e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004840623587369919 tensor([4.8406e-04, 1.0366e-02, 6.4758e-03, 1.0232e-01, 8.8036e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4076480432667626e-11 tensor([9.9890e-01, 8.1022e-04, 1.4076e-11, 2.9237e-04, 2.2400e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.170015110467219e-11 tensor([9.9708e-01, 5.3984e-04, 5.1700e-11, 2.3759e-03, 2.4653e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.189978086710511e-10 tensor([4.1900e-10, 8.7932e-04, 9.9766e-01, 4.6205e-08, 1.4644e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 7.119112410691741e-07 tensor([8.9569e-07, 7.1191e-07, 6.6165e-06, 3.9487e-01, 6.0512e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.401861459195743e-09 tensor([3.4019e-09, 2.7308e-05, 7.4093e-02, 2.6102e-04, 9.2562e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.880385955947531e-08 tensor([8.4851e-01, 4.9165e-03, 5.8804e-08, 1.4642e-01, 1.5786e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.715137270570267e-05 tensor([1.3358e-02, 9.8634e-01, 1.9882e-04, 2.7151e-05, 7.1427e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1104592090305232e-07 tensor([1.1105e-07, 3.3617e-03, 8.8160e-01, 4.3877e-05, 1.1499e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3438818971044384e-05 tensor([3.8292e-04, 1.4226e-04, 2.3439e-05, 7.1451e-01, 2.8495e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003456771082710475 tensor([3.4568e-04, 4.8796e-03, 3.3695e-03, 1.3203e-01, 8.5938e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.872738670906983e-05 tensor([5.4181e-02, 1.0807e-02, 3.8727e-05, 8.5114e-01, 8.3833e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0459226373882302e-08 tensor([8.9694e-01, 2.1481e-03, 1.0459e-08, 1.0087e-01, 3.9587e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0038095745258033276 tensor([0.0207, 0.9464, 0.0079, 0.0038, 0.0212], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.37672779235254e-08 tensor([7.0093e-06, 1.0608e-07, 2.3767e-08, 9.8389e-01, 1.6099e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7346695813102997e-07 tensor([1.7347e-07, 9.3908e-04, 2.6765e-01, 4.8170e-04, 7.3093e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0028523437213152647 tensor([0.0402, 0.9391, 0.0029, 0.0051, 0.0128], grad_fn=<SoftmaxBackward0>)\n",
      "3 8.55029782087513e-07 tensor([8.8758e-04, 9.9844e-01, 6.5173e-04, 8.5503e-07, 1.5702e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.7309893084748182e-06 tensor([2.7310e-06, 8.4110e-02, 8.9771e-01, 1.2415e-05, 1.8167e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2896467893597219e-08 tensor([1.2896e-08, 3.5773e-05, 3.1340e-02, 5.3591e-04, 9.6809e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0021925526671111584 tensor([0.0022, 0.8173, 0.0958, 0.0025, 0.0822], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0528977689716612e-09 tensor([1.4763e-03, 1.5498e-06, 1.0529e-09, 9.9779e-01, 7.3126e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00021934045071247965 tensor([2.1934e-04, 1.5218e-01, 2.6613e-01, 7.6256e-03, 5.7385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006360923871397972 tensor([6.3609e-04, 4.0687e-02, 4.0393e-02, 5.5930e-02, 8.6235e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.789028715393215e-06 tensor([1.7890e-06, 2.5744e-06, 2.2780e-05, 2.8240e-01, 7.1757e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.026191576158908e-09 tensor([1.0262e-09, 1.4608e-05, 8.3338e-02, 1.2676e-04, 9.1652e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.9800361289744615e-07 tensor([4.2963e-01, 5.7035e-01, 2.9800e-07, 1.4057e-05, 7.0559e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.3308161770692095e-06 tensor([6.3308e-06, 9.1117e-06, 2.3548e-05, 2.8113e-01, 7.1883e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.1915898529865103e-10 tensor([4.1916e-10, 3.1703e-04, 9.9328e-01, 2.8813e-07, 6.3981e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.253260340192355e-05 tensor([1.9327e-01, 3.2683e-02, 4.2533e-05, 7.0714e-01, 6.6869e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0021239689085632563 tensor([0.0021, 0.6173, 0.1114, 0.0063, 0.2628], grad_fn=<SoftmaxBackward0>)\n",
      "2 8.28502197691705e-06 tensor([1.5040e-01, 8.4953e-01, 8.2850e-06, 5.2203e-05, 9.9033e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.45846380898729e-05 tensor([4.4585e-05, 7.4916e-02, 3.3367e-01, 2.2161e-03, 5.8916e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.6249300794443116e-05 tensor([8.1452e-04, 9.7216e-01, 2.5025e-02, 4.6249e-05, 1.9596e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.941346999956295e-05 tensor([5.4145e-03, 9.9376e-01, 7.1530e-04, 1.9413e-05, 9.5714e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0025066768284887075 tensor([0.0084, 0.9469, 0.0155, 0.0025, 0.0268], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.82294801476246e-09 tensor([9.8407e-01, 1.7794e-03, 1.8229e-09, 1.4147e-02, 5.3109e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1446671123849228e-05 tensor([2.0721e-02, 9.7921e-01, 4.0395e-05, 1.1447e-05, 1.3952e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018342936411499977 tensor([0.0183, 0.3954, 0.0239, 0.1310, 0.4313], grad_fn=<SoftmaxBackward0>)\n",
      "2 2.8324203427132488e-08 tensor([4.5831e-01, 9.8755e-04, 2.8324e-08, 5.4031e-01, 3.9084e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.200823013889021e-07 tensor([4.2008e-07, 1.7330e-04, 1.2444e-02, 3.2662e-03, 9.8412e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.842126261588419e-06 tensor([4.8337e-01, 5.1627e-01, 3.8421e-06, 3.2869e-04, 2.5196e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.689938451629132e-08 tensor([9.6899e-08, 9.9194e-03, 9.7621e-01, 3.2538e-06, 1.3864e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00011639382864814252 tensor([1.1639e-04, 9.4280e-04, 1.2650e-03, 1.4458e-01, 8.5310e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.884721677924972e-05 tensor([1.8847e-05, 1.1325e-01, 7.3943e-01, 2.8951e-04, 1.4701e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.5203554539766628e-05 tensor([5.0353e-01, 4.9425e-01, 1.5204e-05, 1.9945e-03, 2.0651e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.644577009296544e-11 tensor([9.9828e-01, 4.1920e-04, 1.6446e-11, 1.2970e-03, 6.4382e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3571501767728478e-05 tensor([1.3572e-05, 1.3195e-01, 7.7318e-01, 1.4651e-04, 9.4714e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017212204402312636 tensor([0.0124, 0.9652, 0.0101, 0.0017, 0.0106], grad_fn=<SoftmaxBackward0>)\n",
      "0 2.8979146859953175e-10 tensor([2.8979e-10, 1.1110e-06, 7.1124e-03, 2.5866e-04, 9.9263e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011020863428711891 tensor([0.0110, 0.7183, 0.0336, 0.0268, 0.2102], grad_fn=<SoftmaxBackward0>)\n",
      "2 6.57553846394876e-06 tensor([3.2917e-01, 2.1648e-02, 6.5755e-06, 6.3488e-01, 1.4292e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.3054071814622148e-06 tensor([8.7238e-01, 1.2044e-01, 1.3054e-06, 7.0540e-03, 1.3008e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.887317324644755e-07 tensor([1.8873e-07, 3.3670e-03, 7.8121e-01, 1.1727e-04, 2.1530e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0035025866236537695 tensor([0.0035, 0.2352, 0.0415, 0.0410, 0.6788], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007996502332389355 tensor([0.0199, 0.9288, 0.0123, 0.0080, 0.0310], grad_fn=<SoftmaxBackward0>)\n",
      "2 7.487877155654132e-05 tensor([2.8243e-01, 7.0105e-02, 7.4879e-05, 5.9555e-01, 5.1837e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.929832473659189e-06 tensor([3.9298e-06, 1.0187e-04, 9.4036e-04, 5.2923e-02, 9.4603e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.672644430072978e-05 tensor([3.0695e-05, 4.7609e-01, 5.1619e-01, 1.6726e-05, 7.6699e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012892835366073996 tensor([1.2893e-04, 5.8167e-03, 7.2269e-03, 4.9822e-02, 9.3701e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.849920024862513e-05 tensor([9.8499e-05, 3.7949e-02, 1.0189e-01, 9.1125e-03, 8.5095e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.4760428712179419e-05 tensor([6.2913e-01, 3.6492e-01, 1.4760e-05, 5.5537e-03, 3.8423e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.713230398716405e-06 tensor([3.7132e-06, 1.1394e-03, 2.3296e-02, 7.2545e-03, 9.6831e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.5816137824440375e-06 tensor([3.5816e-06, 4.9273e-02, 8.9481e-01, 5.8594e-05, 5.5851e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.545555374439573e-08 tensor([8.5456e-08, 2.7545e-03, 8.8227e-01, 3.3587e-05, 1.1494e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.711823820002792e-08 tensor([4.7118e-08, 8.8665e-04, 6.6448e-01, 1.1957e-04, 3.3452e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 3.816584467131179e-06 tensor([4.8017e-01, 5.1948e-01, 3.8166e-06, 3.2955e-04, 1.9827e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.004391738213598728 tensor([0.0044, 0.6073, 0.0448, 0.0153, 0.3282], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.535934016227429e-10 tensor([6.5359e-10, 1.0080e-03, 9.9699e-01, 7.6469e-08, 2.0019e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.76610146904477e-08 tensor([2.9874e-05, 4.8515e-07, 4.7661e-08, 9.7876e-01, 2.1214e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.0732592272688635e-05 tensor([1.0733e-05, 5.0183e-04, 3.1185e-03, 3.0494e-02, 9.6588e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.333568451320933e-12 tensor([9.9062e-01, 6.3108e-05, 2.3336e-12, 9.3145e-03, 9.8019e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 9.113051419262774e-06 tensor([7.3864e-01, 8.0304e-02, 9.1131e-06, 1.7703e-01, 4.0145e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0004914721357636154 tensor([4.9147e-04, 1.3738e-02, 8.3203e-03, 6.6158e-02, 9.1129e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9468511709419545e-06 tensor([3.7243e-06, 1.9469e-06, 7.5669e-06, 5.3012e-01, 4.6987e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0507031220186036e-05 tensor([4.2080e-01, 2.8631e-02, 1.0507e-05, 5.4238e-01, 8.1812e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.6257126251039153e-08 tensor([9.2161e-01, 7.8264e-02, 2.6257e-08, 1.2860e-04, 6.7165e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.103381333057769e-06 tensor([8.1034e-06, 2.6244e-02, 4.4912e-01, 1.3436e-03, 5.2328e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.004936358891427517 tensor([0.0744, 0.8676, 0.0049, 0.0216, 0.0315], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.6353109144606748e-10 tensor([5.2838e-04, 2.1060e-07, 1.6353e-10, 9.9922e-01, 2.5534e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00030868101748637855 tensor([3.0868e-04, 2.3963e-01, 3.0222e-01, 4.1123e-03, 4.5372e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.5335988732347325e-13 tensor([9.9363e-01, 2.6678e-05, 2.5336e-13, 6.3483e-03, 1.6531e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003838936798274517 tensor([0.0038, 0.1749, 0.0304, 0.0642, 0.7267], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.3885360772292188e-07 tensor([1.3885e-07, 2.9817e-03, 7.8030e-01, 9.3950e-05, 2.1662e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.4403507015667856e-05 tensor([3.4404e-05, 9.8294e-03, 4.2579e-02, 8.3476e-03, 9.3921e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.819452133041468e-12 tensor([3.8195e-12, 1.9254e-05, 9.8134e-01, 8.0856e-08, 1.8639e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0111488109032507e-06 tensor([2.7644e-01, 5.4272e-03, 1.0111e-06, 7.1434e-01, 3.7918e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4409202719889436e-07 tensor([4.6902e-04, 9.9925e-01, 2.7603e-04, 1.4409e-07, 2.5957e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0006478765280917287 tensor([6.4788e-04, 1.5503e-01, 9.6724e-02, 1.0593e-02, 7.3700e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.2597300812776666e-06 tensor([2.2013e-05, 3.7020e-06, 2.2597e-06, 7.2446e-01, 2.7551e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7986168998618268e-08 tensor([1.7986e-08, 5.0458e-06, 2.0796e-03, 2.9026e-03, 9.9501e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 5.827075801045911e-12 tensor([9.9866e-01, 2.8280e-04, 5.8271e-12, 1.0613e-03, 3.0913e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.82435671201165e-08 tensor([3.5365e-03, 9.9646e-01, 7.3417e-06, 8.8244e-08, 2.2687e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.277424517975305e-07 tensor([6.2774e-07, 9.9644e-03, 8.5393e-01, 9.1280e-05, 1.3601e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.562919902033173e-07 tensor([8.5629e-07, 5.8152e-06, 1.0374e-04, 8.5615e-02, 9.1427e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.6952965609107196e-07 tensor([1.6953e-07, 2.1573e-03, 5.8909e-01, 1.7315e-04, 4.0858e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00741030927747488 tensor([0.0175, 0.1554, 0.0074, 0.2165, 0.6032], grad_fn=<SoftmaxBackward0>)\n",
      "2 1.0369574283686234e-06 tensor([6.1544e-01, 3.8432e-01, 1.0370e-06, 2.3391e-04, 7.7673e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1329721928632353e-05 tensor([1.1330e-05, 2.3153e-02, 3.6171e-01, 1.7329e-03, 6.1340e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.907979826526912e-12 tensor([1.0740e-04, 1.3102e-08, 7.9080e-12, 9.9982e-01, 7.4103e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0012834846274927258 tensor([0.0013, 0.3032, 0.1140, 0.0125, 0.5691], grad_fn=<SoftmaxBackward0>)\n",
      "[[2], [2], [0], [0], [3], [3], [2], [0], [0], [0], [0], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [3], [0], [0], [0], [2], [3], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [3], [0], [2], [3], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [0], [3], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [2], [2], [2], [3], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [3], [0], [2], [2], [2], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [0], [1], [0], [2], [3], [0], [2], [0], [2], [2], [3], [2], [0], [2], [3], [0], [0], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [3], [3], [3], [2], [3], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [3], [0], [0], [2], [2], [0], [0], [3], [2], [0], [3], [0], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [1], [2], [2], [0], [2], [2], [0], [2], [0], [0], [0], [0], [2], [3], [0], [2], [0], [2], [3], [0], [0], [0], [2], [2], [0], [2], [0]]\n",
      "[[0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 2, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4], [0, 1, 3, 4], [1, 2, 3, 4]]\n",
      "NL_pred of 0th iteration [[2], [2], [0], [0], [3], [3], [2], [0], [0], [0], [0], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [0], [2], [2], [0], [0], [2], [2], [2], [0], [0], [2], [0], [2], [3], [0], [0], [0], [2], [3], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [3], [0], [2], [3], [0], [2], [0], [2], [2], [0], [0], [0], [2], [2], [0], [2], [0], [2], [0], [0], [2], [0], [0], [3], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [2], [2], [2], [3], [0], [2], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [3], [0], [2], [2], [2], [0], [0], [0], [0], [2], [2], [0], [2], [0], [2], [2], [0], [2], [0], [2], [3], [0], [2], [0], [2], [3], [0], [0], [0], [2], [2], [0], [0], [0], [2], [2], [0], [1], [0], [2], [3], [0], [2], [0], [2], [2], [3], [2], [0], [2], [3], [0], [0], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [0], [3], [3], [3], [2], [3], [0], [2], [0], [2], [0], [0], [0], [2], [2], [0], [3], [0], [0], [2], [2], [0], [0], [3], [2], [0], [3], [0], [0], [2], [0], [0], [0], [0], [2], [0], [0], [2], [0], [2], [2], [0], [1], [2], [2], [0], [2], [2], [0], [2], [0], [0], [0], [0], [2], [3], [0], [2], [0], [2], [3], [0], [0], [0], [2], [2], [0], [2], [0]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.0044711589813232425  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004471157550811768  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004471156120300293  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.0044711527824401855  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004471148490905762  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004471144199371338  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004471138954162597  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.0044711332321166995  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004471127510070801  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00447111988067627  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004471112728118896  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004471105575561523  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004471097469329834  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004471089363098145  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004471081256866455  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004471071720123291  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004471062660217285  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004471053600311279  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.004471044540405273  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.00447103500366211  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "4 2.3477182367059868e-06 tensor([9.9130e-01, 2.2107e-03, 1.5638e-09, 6.4892e-03, 2.3477e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.008672798983752728 tensor([0.0087, 0.0288, 0.0011, 0.3945, 0.5670], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1879806152137462e-05 tensor([1.8174e-06, 6.3951e-02, 9.1899e-01, 1.1880e-05, 1.7048e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00011355709284543991 tensor([6.4936e-05, 1.2739e-04, 1.1356e-04, 3.4026e-01, 6.5943e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0038612126372754574 tensor([0.0039, 0.8738, 0.0502, 0.0027, 0.0695], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0038649807684123516 tensor([6.0322e-03, 9.8298e-01, 6.7308e-03, 3.9638e-04, 3.8650e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06628002226352692 tensor([0.0663, 0.0797, 0.0007, 0.6312, 0.2220], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014356808969751 tensor([0.0006, 0.5692, 0.2533, 0.0014, 0.1754], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002916619705501944 tensor([2.0775e-07, 2.9166e-04, 4.3330e-02, 1.2368e-03, 9.5514e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00011916000221390277 tensor([2.5925e-05, 1.1916e-04, 2.8541e-04, 1.8151e-01, 8.1806e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.648060611449182e-05 tensor([1.3160e-05, 1.6712e-01, 7.6665e-01, 8.6481e-05, 6.6136e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010314353741705418 tensor([0.0020, 0.4690, 0.1286, 0.0103, 0.3901], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0100608691573143 tensor([0.0049, 0.7479, 0.0853, 0.0101, 0.1518], grad_fn=<SoftmaxBackward0>)\n",
      "1 5.226943176239729e-05 tensor([3.8264e-08, 5.2269e-05, 2.1337e-02, 1.2151e-03, 9.7740e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006024731555953622 tensor([1.3078e-05, 6.0247e-04, 3.6464e-03, 3.4593e-02, 9.6115e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.8680127595871454e-06 tensor([9.4737e-01, 5.2221e-02, 3.2777e-08, 4.0710e-04, 1.8680e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000644810264930129 tensor([8.6533e-01, 4.7294e-02, 1.6823e-06, 8.6732e-02, 6.4481e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.068540450039791e-09 tensor([4.8972e-12, 1.3645e-04, 9.9953e-01, 2.0685e-09, 3.3340e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.381752619219696e-08 tensor([7.9382e-06, 9.3818e-08, 1.6422e-08, 9.8517e-01, 1.4818e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003873081295751035 tensor([5.9968e-05, 3.8731e-04, 6.1383e-04, 2.0509e-01, 7.9385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003682822280097753 tensor([2.1293e-03, 3.6828e-04, 1.2159e-05, 9.2529e-01, 7.2200e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005921608535572886 tensor([9.7783e-03, 5.9216e-04, 3.2934e-06, 9.4636e-01, 4.3263e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005997005850076675 tensor([0.0015, 0.5378, 0.2127, 0.0060, 0.2420], grad_fn=<SoftmaxBackward0>)\n",
      "1 7.763931648696598e-07 tensor([5.0915e-05, 7.7639e-07, 4.4133e-08, 9.8320e-01, 1.6752e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006487742066383362 tensor([6.5086e-04, 1.1595e-02, 6.4877e-03, 8.6055e-02, 8.9521e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.5581274460128043e-07 tensor([3.6782e-04, 3.5581e-07, 5.8380e-10, 9.9903e-01, 6.0659e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002032787015195936 tensor([3.1006e-01, 3.2508e-04, 5.6226e-09, 6.8941e-01, 2.0328e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3611229476850895e-08 tensor([7.0064e-11, 4.0221e-04, 9.9884e-01, 1.3611e-08, 7.5406e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0036805756390094757 tensor([1.1418e-05, 9.8140e-03, 9.2890e-02, 3.6806e-03, 8.9360e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05424128845334053 tensor([0.0542, 0.1029, 0.0014, 0.5005, 0.3410], grad_fn=<SoftmaxBackward0>)\n",
      "4 8.082715794444084e-06 tensor([9.0633e-01, 9.2895e-02, 1.7180e-07, 7.6876e-04, 8.0827e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0033602463081479073 tensor([1.3188e-04, 3.3602e-03, 3.3927e-03, 8.0886e-02, 9.1223e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0006213768501766026 tensor([1.5630e-05, 6.0938e-02, 6.2903e-01, 6.2138e-04, 3.0939e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005952035542577505 tensor([4.7484e-01, 2.3374e-02, 4.7853e-06, 4.9582e-01, 5.9520e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002486980229150504 tensor([8.8334e-04, 2.4870e-04, 1.9795e-05, 7.1011e-01, 2.8874e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.1702809388225432e-06 tensor([9.9339e-01, 4.4537e-03, 1.9532e-09, 2.1556e-03, 1.1703e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0015493150567635894 tensor([0.0012, 0.7913, 0.1291, 0.0015, 0.0768], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007177507970482111 tensor([0.0045, 0.7507, 0.0639, 0.0072, 0.1737], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07271666079759598 tensor([0.0727, 0.0941, 0.0007, 0.5585, 0.2740], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01598869077861309 tensor([0.0148, 0.8111, 0.0212, 0.0160, 0.1369], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004046939779073 tensor([5.0920e-01, 1.0293e-02, 1.0297e-06, 4.7646e-01, 4.0469e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.2624949476958136e-06 tensor([1.2625e-06, 1.8645e-01, 8.1100e-01, 1.0326e-06, 2.5470e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0697221114241984e-05 tensor([6.5394e-08, 4.3777e-03, 9.5818e-01, 1.0697e-05, 3.7432e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01567724719643593 tensor([1.2231e-04, 2.8910e-02, 6.8475e-02, 1.5677e-02, 8.8682e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010796360671520233 tensor([6.5640e-05, 1.6072e-02, 5.7493e-02, 1.0796e-02, 9.1557e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.133132627728628e-06 tensor([9.9525e-01, 3.0188e-03, 9.2800e-10, 1.7270e-03, 1.1331e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00014963370631448925 tensor([1.4963e-04, 9.8446e-01, 1.5236e-02, 1.5785e-06, 1.5246e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.0339527395663026e-07 tensor([1.0439e-07, 3.9285e-02, 9.5974e-01, 2.0340e-07, 9.7339e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3855579936716822e-06 tensor([3.4238e-01, 4.1751e-06, 1.6991e-12, 6.5762e-01, 1.3856e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.335236760904081e-05 tensor([8.0514e-08, 2.7307e-03, 8.7659e-01, 3.3352e-05, 1.2065e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008963444270193577 tensor([3.6651e-01, 6.2709e-01, 7.2421e-05, 5.4350e-03, 8.9634e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00012013864034088328 tensor([5.9601e-04, 9.9583e-01, 3.4502e-03, 3.9272e-06, 1.2014e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.02509492076933384 tensor([0.0026, 0.2998, 0.0701, 0.0251, 0.6023], grad_fn=<SoftmaxBackward0>)\n",
      "1 4.3779404990118564e-08 tensor([5.1189e-06, 4.3779e-08, 7.9204e-09, 9.8538e-01, 1.4615e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0008522827411070466 tensor([5.5499e-04, 2.0333e-03, 8.5228e-04, 2.8977e-01, 7.0679e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.0562898700736696e-07 tensor([9.9215e-01, 7.5012e-03, 1.0342e-09, 3.5265e-04, 3.0563e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.658608178142458e-05 tensor([1.3164e-04, 9.8979e-01, 9.9787e-03, 8.7280e-07, 9.6586e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4287645626609446e-06 tensor([7.3092e-09, 1.2824e-03, 9.8900e-01, 1.4288e-06, 9.7143e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0038205257151275873 tensor([0.0280, 0.9574, 0.0038, 0.0030, 0.0077], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00016240592231042683 tensor([1.2933e-06, 1.6241e-04, 3.8589e-03, 8.0049e-03, 9.8797e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06814993172883987 tensor([0.0681, 0.3499, 0.0036, 0.3332, 0.2451], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.114857074455358e-05 tensor([1.1462e-02, 9.8833e-01, 1.4806e-04, 1.7921e-05, 4.1149e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00045454231440089643 tensor([1.8128e-07, 4.5454e-04, 1.6211e-01, 1.0871e-03, 8.3635e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.99988254634809e-07 tensor([6.3523e-05, 5.9999e-07, 2.2626e-08, 9.9177e-01, 8.1652e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00013186305295675993 tensor([1.7817e-07, 1.3186e-04, 2.4419e-02, 2.1895e-03, 9.7326e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.580338332336396e-06 tensor([9.3536e-01, 6.4302e-02, 4.8558e-08, 3.3417e-04, 2.5803e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0024162246845662594 tensor([3.3113e-01, 6.5378e-01, 1.2025e-04, 1.2556e-02, 2.4162e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0017400550423189998 tensor([1.5664e-06, 1.7401e-03, 8.0639e-02, 2.1759e-03, 9.1544e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.363895969727309e-07 tensor([9.6788e-10, 3.8033e-04, 9.8053e-01, 9.3639e-07, 1.9090e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00013669802865479141 tensor([1.2135e-08, 2.1304e-04, 2.8972e-01, 1.3670e-04, 7.0993e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.6198440536973067e-05 tensor([9.2179e-01, 7.4512e-02, 3.3218e-07, 3.6709e-03, 2.6198e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.662215122967609e-08 tensor([9.9092e-01, 8.9547e-03, 4.2501e-10, 1.2190e-04, 8.6622e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.721710178680951e-06 tensor([1.9517e-08, 6.7217e-06, 2.3317e-03, 2.1226e-03, 9.9554e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.162351201055571e-05 tensor([3.0527e-02, 6.1624e-05, 1.2770e-08, 9.6846e-01, 9.4971e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000635156873613596 tensor([2.3841e-07, 6.3516e-04, 9.3185e-02, 7.8938e-04, 9.0539e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00016139696526806802 tensor([1.6456e-01, 8.3472e-01, 6.0619e-05, 4.9097e-04, 1.6140e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.022103048861026764 tensor([0.0016, 0.2126, 0.0691, 0.0221, 0.6946], grad_fn=<SoftmaxBackward0>)\n",
      "3 3.802581716172426e-08 tensor([1.1229e-12, 1.0243e-05, 9.8636e-01, 3.8026e-08, 1.3628e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4076403376250823e-09 tensor([2.5065e-06, 1.4076e-09, 2.3428e-11, 9.9919e-01, 8.0542e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 7.830914546502754e-05 tensor([6.1990e-09, 1.8545e-04, 4.3717e-01, 7.8309e-05, 5.6257e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0009780509863048792 tensor([3.5912e-05, 1.1607e-01, 6.0506e-01, 9.7805e-04, 2.7786e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009016894735395908 tensor([0.0090, 0.9564, 0.0121, 0.0023, 0.0202], grad_fn=<SoftmaxBackward0>)\n",
      "3 1.1401866686355788e-06 tensor([5.7013e-08, 1.1040e-02, 9.8150e-01, 1.1402e-06, 7.4599e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5776481232521178e-09 tensor([9.3888e-06, 1.5776e-09, 4.9475e-12, 9.9988e-01, 1.0926e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.0873217433982063e-05 tensor([7.4679e-09, 6.5143e-04, 9.0420e-01, 1.0873e-05, 9.5141e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.2259004922299255e-09 tensor([9.9885e-01, 1.1300e-03, 1.8787e-12, 2.3043e-05, 1.2259e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.1186388292117044e-05 tensor([1.1186e-05, 7.4238e-01, 2.5677e-01, 9.3177e-07, 8.3674e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002004181733354926 tensor([1.2512e-04, 2.0042e-03, 2.9687e-03, 1.1232e-01, 8.8259e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.042675184085965e-07 tensor([1.8583e-05, 5.0427e-07, 1.0544e-07, 9.5646e-01, 4.3521e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.627295879799931e-07 tensor([4.2898e-11, 4.6273e-07, 1.3183e-02, 1.0795e-04, 9.8671e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.04674847231945e-06 tensor([9.8599e-01, 1.1676e-02, 1.1317e-08, 2.3278e-03, 4.0467e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.146762512391433e-05 tensor([1.0409e-02, 9.8935e-01, 1.7933e-04, 1.7279e-05, 4.1468e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.7371441408140527e-07 tensor([4.0437e-09, 1.7096e-03, 9.9401e-01, 3.7371e-07, 4.2809e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.6139635994913988e-06 tensor([9.8143e-01, 1.7274e-03, 2.4956e-09, 1.6841e-02, 3.6140e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.102031380170956e-05 tensor([1.4153e-08, 6.1304e-04, 8.0634e-01, 4.1020e-05, 1.9301e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007119880756363273 tensor([3.8022e-01, 1.5987e-03, 7.2047e-08, 6.1747e-01, 7.1199e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 5.1741695642704144e-05 tensor([2.3005e-03, 9.9701e-01, 6.3744e-04, 5.3879e-06, 5.1742e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.498608177527785e-05 tensor([2.7385e-05, 3.1321e-01, 6.6450e-01, 4.4986e-05, 2.2216e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00046805344754830003 tensor([9.1169e-01, 3.2914e-02, 8.5877e-07, 5.4931e-02, 4.6805e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05991267412900925 tensor([0.0599, 0.0946, 0.0015, 0.5649, 0.2791], grad_fn=<SoftmaxBackward0>)\n",
      "4 6.414744711946696e-05 tensor([8.2794e-01, 1.6897e-01, 1.5297e-06, 3.0215e-03, 6.4147e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0007218613172881305 tensor([7.2186e-04, 9.7117e-01, 2.5912e-02, 4.3134e-05, 2.1483e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.3144642707629828e-06 tensor([3.5377e-07, 3.9471e-02, 9.5754e-01, 1.3145e-06, 2.9917e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0014380353968590498 tensor([1.4380e-03, 1.4641e-03, 1.7430e-04, 5.8473e-01, 4.1220e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001710597425699234 tensor([1.0308e-07, 1.4790e-03, 6.4728e-01, 1.7106e-04, 3.5107e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003779070684686303 tensor([2.9390e-02, 3.7791e-04, 4.0125e-07, 9.6429e-01, 5.9403e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.494784323265776e-05 tensor([3.2421e-01, 6.7542e-01, 7.1523e-06, 3.3512e-04, 3.4948e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01574251428246498 tensor([0.0038, 0.0735, 0.0157, 0.1474, 0.7596], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.2418755659382441e-06 tensor([1.2239e-05, 1.2419e-06, 9.3259e-07, 8.3669e-01, 1.6329e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.023120714351534843 tensor([0.0066, 0.6139, 0.0709, 0.0231, 0.2855], grad_fn=<SoftmaxBackward0>)\n",
      "4 4.013425325410935e-08 tensor([9.4457e-01, 7.2143e-06, 1.2549e-13, 5.5426e-02, 4.0134e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015287032583728433 tensor([3.3014e-01, 6.6124e-01, 9.5409e-05, 6.9966e-03, 1.5287e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.812174539547414e-05 tensor([5.2443e-06, 5.3940e-02, 8.7272e-01, 9.8122e-05, 7.3234e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.091943992534652e-06 tensor([4.1691e-04, 4.0919e-06, 4.8085e-08, 9.8835e-01, 1.1226e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015491772443056107 tensor([0.0015, 0.2649, 0.1104, 0.0155, 0.6077], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002468853781465441 tensor([2.0158e-01, 2.4689e-04, 7.5258e-09, 7.9786e-01, 3.2033e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009776023216545582 tensor([9.7760e-04, 9.3414e-01, 5.5134e-02, 1.7242e-04, 9.5754e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.006871893536299467 tensor([4.2288e-05, 6.8719e-03, 3.5225e-02, 1.7881e-02, 9.3998e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007664518197998405 tensor([3.2677e-02, 7.6645e-04, 8.4069e-07, 9.5734e-01, 9.2173e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.800517207419034e-06 tensor([4.1298e-01, 3.1036e-05, 5.5208e-11, 5.8698e-01, 8.8005e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.214673385964488e-09 tensor([9.9881e-01, 1.0794e-03, 8.5041e-12, 1.1010e-04, 8.2147e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0018462450243532658 tensor([0.0008, 0.6688, 0.1811, 0.0018, 0.1475], grad_fn=<SoftmaxBackward0>)\n",
      "3 4.696360278444445e-08 tensor([3.7859e-11, 1.2850e-04, 9.9668e-01, 4.6964e-08, 3.1917e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.2587070109002525e-06 tensor([2.8672e-07, 3.2587e-06, 1.1740e-04, 4.3528e-02, 9.5635e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.559852823149413e-05 tensor([1.6013e-05, 1.9706e-01, 7.7266e-01, 4.5599e-05, 3.0220e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.6589486878947355e-07 tensor([9.1889e-01, 2.0814e-05, 1.5499e-12, 8.1090e-02, 2.6589e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.10610086470842361 tensor([0.1061, 0.2285, 0.0016, 0.4332, 0.2306], grad_fn=<SoftmaxBackward0>)\n",
      "3 5.78402421069768e-07 tensor([1.0878e-08, 3.1575e-03, 9.9166e-01, 5.7840e-07, 5.1839e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.337880359945757e-09 tensor([4.6642e-05, 5.3379e-09, 4.5115e-12, 9.9988e-01, 7.1688e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00031399179715663195 tensor([2.3406e-04, 5.1082e-04, 3.1399e-04, 3.5960e-01, 6.3934e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.0724521644078777e-06 tensor([6.8464e-01, 3.1532e-01, 1.7655e-07, 4.5264e-05, 1.0725e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.07869067788124084 tensor([0.0787, 0.4204, 0.0043, 0.2478, 0.2488], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00018485335749574006 tensor([3.4426e-05, 1.9538e-01, 7.2393e-01, 1.8485e-04, 8.0473e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0001768048241501674 tensor([1.7086e-03, 1.7680e-04, 5.0530e-06, 9.2388e-01, 7.4226e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0010704334126785398 tensor([2.3074e-06, 1.0704e-03, 3.6334e-02, 5.0902e-03, 9.5750e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.0073327959835296e-06 tensor([9.5999e-01, 3.9429e-02, 2.7049e-08, 5.7887e-04, 2.0073e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006999755860306323 tensor([3.4646e-03, 9.9204e-01, 3.7161e-03, 7.7158e-05, 6.9998e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00015075637202244252 tensor([1.9146e-07, 2.8946e-03, 6.5828e-01, 1.5076e-04, 3.3867e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 8.507875463692471e-06 tensor([7.4925e-01, 1.4325e-04, 1.8755e-10, 2.5060e-01, 8.5079e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.628611355248722e-07 tensor([1.3512e-11, 2.7111e-05, 9.7181e-01, 2.6286e-07, 2.8159e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.419969238573685e-05 tensor([1.2428e-02, 5.4200e-05, 2.9832e-08, 9.8513e-01, 2.3888e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00017049758753273636 tensor([1.7050e-04, 9.6571e-01, 3.3487e-02, 5.5087e-06, 6.2796e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.560393674386432e-06 tensor([4.9266e-06, 1.8512e-01, 8.0905e-01, 5.5604e-06, 5.8274e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02779202163219452 tensor([0.0020, 0.1207, 0.0278, 0.0474, 0.8020], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.2717405954608694e-05 tensor([4.5011e-09, 2.2717e-05, 4.3596e-02, 3.2857e-04, 9.5605e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 4.301138289974915e-07 tensor([9.9023e-01, 9.2512e-03, 2.1250e-09, 5.1462e-04, 4.3011e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.009602877085854e-07 tensor([4.9111e-01, 5.0886e-01, 2.5784e-07, 3.0552e-05, 9.0096e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008640246815048158 tensor([5.2265e-07, 8.6402e-04, 1.2771e-01, 1.4096e-03, 8.7002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5149407772696577e-07 tensor([1.4321e-08, 1.5149e-07, 2.7530e-05, 3.8552e-02, 9.6142e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006538011599332094 tensor([5.0181e-04, 1.0697e-02, 6.5380e-03, 1.0333e-01, 8.7894e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.2338701555213447e-08 tensor([9.9889e-01, 8.1729e-04, 1.4174e-11, 2.9002e-04, 2.2339e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.4464816306135617e-07 tensor([9.9711e-01, 5.4361e-04, 5.1821e-11, 2.3501e-03, 2.4465e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.661554697804604e-08 tensor([4.3022e-10, 8.9558e-04, 9.9764e-01, 4.6616e-08, 1.4623e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.185494036501041e-07 tensor([9.1855e-07, 7.3030e-07, 6.6972e-06, 3.9657e-01, 6.0342e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.7997532015433535e-05 tensor([3.4943e-09, 2.7998e-05, 7.4882e-02, 2.6287e-04, 9.2483e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00015509127115365118 tensor([8.5107e-01, 4.9551e-03, 5.8612e-08, 1.4382e-01, 1.5509e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.039109914330766e-05 tensor([1.3411e-02, 9.8630e-01, 1.9615e-04, 2.6925e-05, 7.0391e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.4283809984335676e-05 tensor([1.1457e-07, 3.4406e-03, 8.8178e-01, 4.4284e-05, 1.1474e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00014649159857071936 tensor([3.9326e-04, 1.4649e-04, 2.3761e-05, 7.1553e-01, 2.8391e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003400515066459775 tensor([3.5867e-04, 5.0365e-03, 3.4005e-03, 1.3339e-01, 8.5781e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.011105746030807495 tensor([5.5646e-02, 1.1106e-02, 3.9039e-05, 8.4998e-01, 8.3232e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.8807567761978135e-05 tensor([8.9880e-01, 2.1596e-03, 1.0389e-08, 9.9001e-02, 3.8808e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007801579777151346 tensor([0.0208, 0.9470, 0.0078, 0.0037, 0.0207], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.0868266286934158e-07 tensor([7.1925e-06, 1.0868e-07, 2.3916e-08, 9.8402e-01, 1.5969e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00048448581947013736 tensor([1.7831e-07, 9.6253e-04, 2.6961e-01, 4.8449e-04, 7.2894e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004960446152836084 tensor([0.0403, 0.9395, 0.0028, 0.0050, 0.0125], grad_fn=<SoftmaxBackward0>)\n",
      "4 1.5513793186983094e-05 tensor([8.9396e-04, 9.9845e-01, 6.4205e-04, 8.5179e-07, 1.5514e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.2544280252768658e-05 tensor([2.8206e-06, 8.6082e-02, 8.9579e-01, 1.2544e-05, 1.8110e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.6711244320031255e-05 tensor([1.3232e-08, 3.6711e-05, 3.1747e-02, 5.3897e-04, 9.6768e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0024734565522521734 tensor([0.0022, 0.8207, 0.0944, 0.0025, 0.0802], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.5900242260613595e-06 tensor([1.5141e-03, 1.5900e-06, 1.0632e-09, 9.9776e-01, 7.2716e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00764113012701273 tensor([2.2503e-04, 1.5556e-01, 2.6661e-01, 7.6411e-03, 5.6997e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04070526361465454 tensor([6.5806e-04, 4.1894e-02, 4.0705e-02, 5.6418e-02, 8.6032e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.642030949573382e-06 tensor([1.8409e-06, 2.6420e-06, 2.3014e-05, 2.8442e-01, 7.1555e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.4975658814364579e-05 tensor([1.0538e-09, 1.4976e-05, 8.4253e-02, 1.2763e-04, 9.1560e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.011668117229419e-07 tensor([4.2904e-01, 5.7095e-01, 2.9699e-07, 1.3968e-05, 7.0117e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 9.390967534272932e-06 tensor([6.5526e-06, 9.3910e-06, 2.3771e-05, 2.8359e-01, 7.1637e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.8990234568482265e-07 tensor([4.2977e-10, 3.2272e-04, 9.9330e-01, 2.8990e-07, 6.3754e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.033537138253450394 tensor([1.9767e-01, 3.3537e-02, 4.2865e-05, 7.0254e-01, 6.6211e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0062498729676008224 tensor([0.0022, 0.6234, 0.1103, 0.0062, 0.2579], grad_fn=<SoftmaxBackward0>)\n",
      "4 9.811311429075431e-06 tensor([1.5035e-01, 8.4958e-01, 8.2340e-06, 5.1822e-05, 9.8113e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0022255810908973217 tensor([4.5967e-05, 7.6941e-02, 3.3485e-01, 2.2256e-03, 5.8594e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008216368732973933 tensor([8.2164e-04, 9.7271e-01, 2.4509e-02, 4.5781e-05, 1.9182e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.434721869183704e-05 tensor([5.4498e-03, 9.9373e-01, 7.0410e-04, 1.9288e-05, 9.4347e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00841178186237812 tensor([0.0084, 0.9478, 0.0152, 0.0025, 0.0261], grad_fn=<SoftmaxBackward0>)\n",
      "4 5.197298378334381e-06 tensor([9.8435e-01, 1.7868e-03, 1.8090e-09, 1.3853e-02, 5.1973e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.3780258086626418e-05 tensor([2.0788e-02, 9.7915e-01, 3.9940e-05, 1.1366e-05, 1.3780e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.023860257118940353 tensor([0.0187, 0.4015, 0.0239, 0.1303, 0.4257], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003853855305351317 tensor([4.6449e-01, 1.0029e-03, 2.8319e-08, 5.3412e-01, 3.8539e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00017838891653809696 tensor([4.3360e-07, 1.7839e-04, 1.2574e-02, 3.2948e-03, 9.8395e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.4862931240932085e-05 tensor([4.8307e-01, 5.1658e-01, 3.8118e-06, 3.2501e-04, 2.4863e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.2805974115035497e-06 tensor([9.9686e-08, 1.0135e-02, 9.7602e-01, 3.2806e-06, 1.3838e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009730574674904346 tensor([1.2074e-04, 9.7306e-04, 1.2775e-03, 1.4613e-01, 8.5150e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002903538988903165 tensor([1.9380e-05, 1.1576e-01, 7.3805e-01, 2.9035e-04, 1.4587e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0002029734750976786 tensor([5.0324e-01, 4.9458e-01, 1.5048e-05, 1.9657e-03, 2.0297e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.411188024912917e-08 tensor([9.9829e-01, 4.2354e-04, 1.6577e-11, 1.2828e-03, 6.4112e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00014675658894702792 tensor([1.3945e-05, 1.3480e-01, 7.7117e-01, 1.4676e-04, 9.3868e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009947557002305984 tensor([0.0124, 0.9656, 0.0099, 0.0017, 0.0104], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.1391859970899532e-06 tensor([2.9745e-10, 1.1392e-06, 7.1975e-03, 2.6045e-04, 9.9254e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.026471780613064766 tensor([0.0111, 0.7229, 0.0333, 0.0265, 0.2062], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01411159336566925 tensor([3.3514e-01, 2.2118e-02, 6.6072e-06, 6.2862e-01, 1.4112e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00012779813550878316 tensor([8.7206e-01, 1.2088e-01, 1.2976e-06, 6.9290e-03, 1.2780e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001185255023301579 tensor([1.9495e-07, 3.4477e-03, 7.8143e-01, 1.1853e-04, 2.1500e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04108627885580063 tensor([0.0036, 0.2411, 0.0416, 0.0411, 0.6726], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.012097918428480625 tensor([0.0200, 0.9298, 0.0121, 0.0079, 0.0303], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0510471872985363 tensor([2.8780e-01, 7.1520e-02, 7.4929e-05, 5.8956e-01, 5.1047e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00010521942022023723 tensor([4.0726e-06, 1.0522e-04, 9.5198e-04, 5.3474e-02, 9.4546e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.1383257010020316e-05 tensor([3.1383e-05, 4.8182e-01, 5.1054e-01, 1.6779e-05, 7.5938e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0059988852590322495 tensor([1.3282e-04, 5.9989e-03, 7.3297e-03, 5.0078e-02, 9.3646e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.009172619320452213 tensor([1.0183e-04, 3.9128e-02, 1.0281e-01, 9.1726e-03, 8.4879e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003768360475078225 tensor([6.2912e-01, 3.6503e-01, 1.4592e-05, 5.4645e-03, 3.7684e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0011766059324145317 tensor([3.8439e-06, 1.1766e-03, 2.3565e-02, 7.3212e-03, 9.6793e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 5.914058783673681e-05 tensor([3.7027e-06, 5.0496e-02, 8.9378e-01, 5.9141e-05, 5.5662e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.374753214302473e-05 tensor([8.7972e-08, 2.8184e-03, 8.8285e-01, 3.3748e-05, 1.1430e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00012025768228340894 tensor([4.8347e-08, 9.0469e-04, 6.6586e-01, 1.2026e-04, 3.3311e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.9564035028452054e-05 tensor([4.7993e-01, 5.1972e-01, 3.7848e-06, 3.2581e-04, 1.9564e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015143993310630322 tensor([0.0045, 0.6137, 0.0444, 0.0151, 0.3222], grad_fn=<SoftmaxBackward0>)\n",
      "3 7.705472881980313e-08 tensor([6.7126e-10, 1.0274e-03, 9.9698e-01, 7.7055e-08, 1.9968e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.973725253876182e-07 tensor([3.0671e-05, 4.9737e-07, 4.7973e-08, 9.7892e-01, 2.1046e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005180363077670336 tensor([1.1125e-05, 5.1804e-04, 3.1547e-03, 3.0833e-02, 9.6548e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 9.71411964201252e-08 tensor([9.9075e-01, 6.3627e-05, 2.3423e-12, 9.1875e-03, 9.7141e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0039240699261426926 tensor([7.4167e-01, 8.0796e-02, 9.0327e-06, 1.7360e-01, 3.9241e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.008408424444496632 tensor([5.1021e-04, 1.4204e-02, 8.4084e-03, 6.6803e-02, 9.1007e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 3.832572474493645e-06 tensor([3.8326e-06, 2.0005e-06, 7.6378e-06, 5.3238e-01, 4.6761e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008036643266677856 tensor([4.2670e-01, 2.8999e-02, 1.0449e-05, 5.3625e-01, 8.0366e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.672890435766021e-07 tensor([9.2126e-01, 7.8615e-02, 2.6242e-08, 1.2755e-04, 6.6729e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0013484725495800376 tensor([8.3326e-06, 2.6911e-02, 4.5114e-01, 1.3485e-03, 5.2059e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021309254691004753 tensor([0.0748, 0.8683, 0.0048, 0.0213, 0.0307], grad_fn=<SoftmaxBackward0>)\n",
      "1 2.1627178625749366e-07 tensor([5.4074e-04, 2.1627e-07, 1.6590e-10, 9.9920e-01, 2.5492e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004111290443688631 tensor([3.1629e-04, 2.4452e-01, 3.0188e-01, 4.1113e-03, 4.4917e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 1.6456096929573505e-08 tensor([9.9370e-01, 2.6947e-05, 2.5545e-13, 6.2729e-03, 1.6456e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.030549056828022003 tensor([0.0039, 0.1794, 0.0305, 0.0643, 0.7219], grad_fn=<SoftmaxBackward0>)\n",
      "3 9.481321467319503e-05 tensor([1.4365e-07, 3.0595e-03, 7.8093e-01, 9.4813e-05, 2.1591e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008410483598709106 tensor([3.5609e-05, 1.0159e-02, 4.3079e-02, 8.4105e-03, 9.3832e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 8.12691567375623e-08 tensor([3.8977e-12, 1.9528e-05, 9.8143e-01, 8.1269e-08, 1.8548e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.003756355494260788 tensor([2.8160e-01, 5.5492e-03, 1.0181e-06, 7.0909e-01, 3.7564e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.569176103861537e-06 tensor([4.7216e-04, 9.9925e-01, 2.7237e-04, 1.4371e-07, 2.5692e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01061305496841669 tensor([6.6772e-04, 1.5953e-01, 9.7373e-02, 1.0613e-02, 7.3182e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.808701421803562e-06 tensor([2.2617e-05, 3.8087e-06, 2.2883e-06, 7.2551e-01, 2.7446e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.172106739337323e-06 tensor([1.8488e-08, 5.1721e-06, 2.1010e-03, 2.9268e-03, 9.9497e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 3.080610255778993e-08 tensor([9.9866e-01, 2.8573e-04, 5.8769e-12, 1.0503e-03, 3.0806e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 2.2531123988756008e-07 tensor([3.5497e-03, 9.9644e-01, 7.2739e-06, 8.8048e-08, 2.2531e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 9.187065006699413e-05 tensor([6.4647e-07, 1.0195e-02, 8.5427e-01, 9.1871e-05, 1.3545e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 5.973449333396275e-06 tensor([8.7983e-07, 5.9734e-06, 1.0506e-04, 8.6204e-02, 9.1368e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00017381874204147607 tensor([1.7388e-07, 2.2049e-03, 5.9099e-01, 1.7382e-04, 4.0663e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.017997290939092636 tensor([0.0180, 0.1595, 0.0074, 0.2170, 0.5981], grad_fn=<SoftmaxBackward0>)\n",
      "4 7.689530320931226e-06 tensor([6.1488e-01, 3.8488e-01, 1.0318e-06, 2.3165e-04, 7.6895e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017397814663127065 tensor([1.1667e-05, 2.3792e-02, 3.6401e-01, 1.7398e-03, 6.1044e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.3408242516277369e-08 tensor([1.0985e-04, 1.3408e-08, 7.9821e-12, 9.9982e-01, 7.3735e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012494264170527458 tensor([0.0013, 0.3093, 0.1139, 0.0125, 0.5630], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4], [2, 0], [0, 3], [0, 2], [3, 0], [3, 4], [2, 0], [0, 3], [0, 1], [0, 1], [0, 3], [0, 3], [0, 3], [0, 1], [0, 1], [2, 4], [2, 4], [0, 3], [2, 1], [0, 1], [2, 1], [2, 1], [0, 3], [2, 1], [0, 2], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [2, 4], [0, 1], [0, 3], [2, 4], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [0, 3], [2, 4], [3, 0], [0, 3], [0, 3], [0, 3], [2, 4], [3, 0], [0, 3], [2, 4], [0, 3], [2, 4], [3, 4], [0, 3], [2, 1], [0, 2], [2, 4], [3, 4], [0, 3], [3, 2], [0, 1], [2, 0], [3, 4], [0, 1], [2, 1], [0, 1], [2, 4], [2, 4], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [2, 1], [0, 1], [2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [0, 3], [3, 0], [0, 3], [2, 1], [0, 3], [2, 4], [3, 0], [0, 1], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 4], [0, 3], [2, 4], [2, 0], [2, 4], [3, 0], [0, 3], [2, 0], [0, 3], [2, 1], [2, 4], [0, 2], [2, 1], [0, 3], [2, 4], [2, 4], [0, 3], [2, 1], [0, 3], [2, 1], [3, 0], [0, 1], [2, 1], [2, 4], [2, 4], [0, 3], [0, 3], [0, 1], [0, 3], [2, 4], [2, 0], [0, 3], [2, 1], [0, 2], [2, 4], [2, 0], [0, 3], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [0, 3], [2, 1], [3, 0], [0, 3], [0, 2], [0, 1], [2, 4], [2, 4], [0, 1], [0, 1], [0, 2], [2, 4], [2, 4], [0, 3], [1, 0], [0, 1], [2, 4], [3, 4], [0, 3], [2, 1], [0, 2], [2, 1], [2, 4], [3, 2], [2, 1], [0, 3], [2, 3], [3, 4], [0, 3], [0, 1], [0, 3], [2, 1], [0, 3], [0, 2], [0, 1], [0, 1], [2, 4], [0, 1], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [3, 0], [3, 4], [3, 0], [2, 4], [3, 4], [0, 2], [2, 4], [0, 1], [2, 4], [0, 3], [0, 1], [0, 3], [2, 4], [2, 4], [0, 3], [3, 2], [0, 1], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [3, 2], [2, 4], [0, 1], [3, 0], [0, 1], [0, 3], [2, 4], [0, 1], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [2, 1], [0, 1], [2, 4], [2, 4], [0, 2], [1, 0], [2, 4], [2, 4], [0, 3], [2, 3], [2, 1], [0, 3], [2, 4], [0, 2], [0, 3], [0, 3], [0, 3], [2, 4], [3, 4], [0, 3], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [0, 1], [0, 3], [2, 0], [2, 4], [0, 3], [2, 1], [0, 3]]\n",
      "[[0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 2], [1, 3, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 3, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 4], [2, 3, 4], [1, 3, 4], [0, 1, 2], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [1, 3, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [1, 3, 4], [1, 2, 4], [0, 3, 4], [0, 1, 3], [1, 3, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [2, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [2, 3, 4], [2, 3, 4], [1, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [1, 3, 4], [0, 3, 4], [0, 1, 3], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 1, 4], [0, 1, 2], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [1, 3, 4], [2, 3, 4], [2, 3, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 2], [1, 2, 4], [0, 1, 3], [0, 1, 2], [1, 3, 4], [0, 1, 3], [2, 3, 4], [0, 1, 3], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 1, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [2, 3, 4], [1, 2, 4], [0, 1, 3], [2, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [1, 2, 4], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 3], [1, 2, 4], [0, 1, 4], [0, 3, 4], [1, 2, 4], [0, 1, 3], [1, 3, 4], [1, 2, 4], [1, 2, 4], [1, 2, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [0, 3, 4], [2, 3, 4], [0, 1, 3], [0, 1, 2], [1, 2, 4], [2, 3, 4], [1, 2, 4], [1, 3, 4], [0, 1, 3], [1, 2, 4], [0, 3, 4], [1, 2, 4]]\n",
      "NL_pred of 1th iteration [[2, 4], [2, 0], [0, 3], [0, 2], [3, 0], [3, 4], [2, 0], [0, 3], [0, 1], [0, 1], [0, 3], [0, 3], [0, 3], [0, 1], [0, 1], [2, 4], [2, 4], [0, 3], [2, 1], [0, 1], [2, 1], [2, 1], [0, 3], [2, 1], [0, 2], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [2, 4], [0, 1], [0, 3], [2, 4], [2, 1], [2, 4], [0, 3], [0, 3], [2, 0], [0, 3], [2, 4], [3, 0], [0, 3], [0, 3], [0, 3], [2, 4], [3, 0], [0, 3], [2, 4], [0, 3], [2, 4], [3, 4], [0, 3], [2, 1], [0, 2], [2, 4], [3, 4], [0, 3], [3, 2], [0, 1], [2, 0], [3, 4], [0, 1], [2, 1], [0, 1], [2, 4], [2, 4], [0, 1], [0, 3], [0, 3], [2, 4], [2, 4], [0, 1], [2, 1], [0, 1], [2, 4], [0, 3], [0, 3], [2, 1], [0, 3], [0, 3], [3, 0], [0, 3], [2, 1], [0, 3], [2, 4], [3, 0], [0, 1], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [0, 3], [2, 4], [3, 4], [0, 3], [2, 4], [2, 0], [2, 4], [3, 0], [0, 3], [2, 0], [0, 3], [2, 1], [2, 4], [0, 2], [2, 1], [0, 3], [2, 4], [2, 4], [0, 3], [2, 1], [0, 3], [2, 1], [3, 0], [0, 1], [2, 1], [2, 4], [2, 4], [0, 3], [0, 3], [0, 1], [0, 3], [2, 4], [2, 0], [0, 3], [2, 1], [0, 2], [2, 4], [2, 0], [0, 3], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [2, 4], [0, 3], [2, 1], [3, 0], [0, 3], [0, 2], [0, 1], [2, 4], [2, 4], [0, 1], [0, 1], [0, 2], [2, 4], [2, 4], [0, 3], [1, 0], [0, 1], [2, 4], [3, 4], [0, 3], [2, 1], [0, 2], [2, 1], [2, 4], [3, 2], [2, 1], [0, 3], [2, 3], [3, 4], [0, 3], [0, 1], [0, 3], [2, 1], [0, 3], [0, 2], [0, 1], [0, 1], [2, 4], [0, 1], [0, 3], [2, 1], [0, 3], [2, 4], [0, 3], [3, 0], [3, 4], [3, 0], [2, 4], [3, 4], [0, 2], [2, 4], [0, 1], [2, 4], [0, 3], [0, 1], [0, 3], [2, 4], [2, 4], [0, 3], [3, 2], [0, 1], [0, 3], [2, 4], [2, 4], [0, 3], [0, 3], [3, 2], [2, 4], [0, 1], [3, 0], [0, 1], [0, 3], [2, 4], [0, 1], [0, 3], [0, 3], [0, 3], [2, 4], [0, 3], [0, 3], [2, 1], [0, 1], [2, 4], [2, 4], [0, 2], [1, 0], [2, 4], [2, 4], [0, 3], [2, 3], [2, 1], [0, 3], [2, 4], [0, 2], [0, 3], [0, 3], [0, 3], [2, 4], [3, 4], [0, 3], [2, 1], [0, 1], [2, 4], [3, 4], [0, 3], [0, 1], [0, 3], [2, 0], [2, 4], [0, 3], [2, 1], [0, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004491033554077148  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.0044910283088684085  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004491016864776611  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004491002082824707  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.00449098253250122  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004490958213806152  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.0044909329414367675  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004490903377532959  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004490871906280517  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.0044908390045166014  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004490802764892578  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0044907665252685545  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004490728855133057  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004490690231323242  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004490649700164795  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004490610122680664  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.0044905695915222164  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004490530014038086  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.00449048900604248  Accuracy on Support set:0.0\n",
      "torch.Size([250, 2048]) torch.Size([250])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004490447044372558  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "1 0.002316270489245653 tensor([9.9088e-01, 2.3163e-03, 1.7430e-09, 6.7986e-03, 2.5793e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.027577221393585205 tensor([0.0078, 0.0276, 0.0011, 0.3802, 0.5834], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01685350015759468 tensor([1.6211e-06, 5.9746e-02, 9.2339e-01, 1.1228e-05, 1.6854e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00012360289110802114 tensor([5.9272e-05, 1.2360e-04, 1.1795e-04, 3.2662e-01, 6.7308e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05438254028558731 tensor([0.0036, 0.8654, 0.0544, 0.0027, 0.0739], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0056519596837460995 tensor([5.6520e-03, 9.8246e-01, 7.3679e-03, 3.9788e-04, 4.1251e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07853738963603973 tensor([0.0608, 0.0785, 0.0008, 0.6237, 0.2362], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.18151892721652985 tensor([5.2348e-04, 5.5006e-01, 2.6649e-01, 1.4022e-03, 1.8152e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011862723622471094 tensor([1.9056e-07, 2.8123e-04, 4.4356e-02, 1.1863e-03, 9.5418e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002930479240603745 tensor([2.3480e-05, 1.1446e-04, 2.9305e-04, 1.7286e-01, 8.2671e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06584358215332031 tensor([1.1758e-05, 1.5692e-01, 7.7715e-01, 8.2162e-05, 6.5844e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.13474369049072266 tensor([0.0018, 0.4518, 0.1347, 0.0100, 0.4015], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09115496277809143 tensor([0.0045, 0.7345, 0.0912, 0.0100, 0.1598], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011707788798958063 tensor([3.5707e-08, 5.1078e-05, 2.1837e-02, 1.1708e-03, 9.7694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0037178127095103264 tensor([1.1622e-05, 5.7091e-04, 3.7178e-03, 3.2454e-02, 9.6325e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004208169411867857 tensor([9.4481e-01, 5.4763e-02, 3.6339e-08, 4.2082e-04, 2.0226e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04981121048331261 tensor([8.5737e-01, 4.9811e-02, 1.9197e-06, 9.2091e-02, 7.2857e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00013320283323992044 tensor([4.6791e-12, 1.3320e-04, 9.9953e-01, 2.0398e-09, 3.3513e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 7.549256224592682e-06 tensor([7.5493e-06, 9.4078e-08, 1.7572e-08, 9.8429e-01, 1.5699e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006293004844337702 tensor([5.4052e-05, 3.7032e-04, 6.2930e-04, 1.9527e-01, 8.0368e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002000828506425023 tensor([2.0008e-03, 3.6881e-04, 1.3122e-05, 9.2089e-01, 7.6730e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.009130937978625298 tensor([9.1309e-03, 5.9034e-04, 3.5616e-06, 9.4421e-01, 4.6069e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2228231132030487 tensor([0.0013, 0.5195, 0.2228, 0.0059, 0.2504], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.824918141821399e-05 tensor([4.8249e-05, 7.7755e-07, 4.7362e-08, 9.8215e-01, 1.7796e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010903635993599892 tensor([5.7636e-04, 1.0904e-02, 6.6036e-03, 8.1278e-02, 9.0064e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00034715543733909726 tensor([3.4716e-04, 3.5317e-07, 6.2090e-10, 9.9901e-01, 6.4251e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0003266312414780259 tensor([2.9600e-01, 3.2663e-04, 6.0977e-09, 7.0345e-01, 2.1967e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00039187504444271326 tensor([6.6767e-11, 3.9188e-04, 9.9885e-01, 1.3408e-08, 7.5779e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009321192279458046 tensor([1.0252e-05, 9.3212e-03, 9.5259e-02, 3.4945e-03, 8.9192e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10064038634300232 tensor([0.0494, 0.1006, 0.0015, 0.4894, 0.3591], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007962273666635156 tensor([9.0113e-01, 9.8060e-02, 1.9331e-07, 7.9623e-04, 8.8611e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003454666817560792 tensor([1.1782e-04, 3.1848e-03, 3.4547e-03, 7.6513e-02, 9.1673e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05703026428818703 tensor([1.3922e-05, 5.7030e-02, 6.3545e-01, 5.8838e-04, 3.0692e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.024008311331272125 tensor([4.5755e-01, 2.4008e-02, 5.3351e-06, 5.1188e-01, 6.5551e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0008192107197828591 tensor([8.1921e-04, 2.4595e-04, 2.1088e-05, 6.9652e-01, 3.0239e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002238683635368943 tensor([9.9310e-01, 4.6555e-03, 2.1567e-09, 2.2387e-03, 1.2701e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0811319425702095 tensor([0.0011, 0.7776, 0.1386, 0.0015, 0.0811], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06836239993572235 tensor([0.0041, 0.7377, 0.0684, 0.0071, 0.1826], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0923532173037529 tensor([0.0664, 0.0924, 0.0008, 0.5496, 0.2908], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.022778345271945 tensor([0.0138, 0.8024, 0.0228, 0.0159, 0.1452], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0105399489402771 tensor([4.9224e-01, 1.0540e-02, 1.1405e-06, 4.9277e-01, 4.4447e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0025471732951700687 tensor([1.1397e-06, 1.7586e-01, 8.2159e-01, 9.9074e-07, 2.5472e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004089732654392719 tensor([5.8318e-08, 4.0897e-03, 9.5904e-01, 1.0099e-05, 3.6861e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02716405875980854 tensor([1.0839e-04, 2.7164e-02, 6.9747e-02, 1.4818e-02, 8.8816e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.015110159292817116 tensor([5.8119e-05, 1.5110e-02, 5.8649e-02, 1.0193e-02, 9.1599e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0017838521162047982 tensor([9.9507e-01, 3.1434e-03, 1.0166e-09, 1.7839e-03, 1.2225e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0001597481605131179 tensor([1.4217e-04, 9.8339e-01, 1.6303e-02, 1.5781e-06, 1.5975e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0009719220688566566 tensor([9.6230e-08, 3.7381e-02, 9.6165e-01, 1.9754e-07, 9.7192e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 4.180115865892731e-06 tensor([3.2899e-01, 4.1801e-06, 1.8161e-12, 6.7100e-01, 1.4785e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0026060387026518583 tensor([7.3999e-08, 2.6060e-03, 8.7811e-01, 3.1935e-05, 1.1925e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0055629112757742405 tensor([3.5179e-01, 6.4159e-01, 8.0286e-05, 5.5629e-03, 9.7742e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005653222324326634 tensor([5.6532e-04, 9.9560e-01, 3.7040e-03, 3.9229e-06, 1.2602e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07291575521230698 tensor([0.0024, 0.2867, 0.0729, 0.0241, 0.6139], grad_fn=<SoftmaxBackward0>)\n",
      "0 4.89115564050735e-06 tensor([4.8912e-06, 4.3879e-08, 8.4071e-09, 9.8461e-01, 1.5381e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0019471179693937302 tensor([5.0030e-04, 1.9471e-03, 8.7841e-04, 2.7697e-01, 7.1970e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003625198733061552 tensor([9.9185e-01, 7.7889e-03, 1.1281e-09, 3.6252e-04, 3.2721e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00012512794637586921 tensor([1.2513e-04, 9.8910e-01, 1.0668e-02, 8.7227e-07, 1.0108e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0012373060453683138 tensor([6.8611e-09, 1.2373e-03, 9.8902e-01, 1.3983e-06, 9.7410e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.008235672488808632 tensor([0.0263, 0.9583, 0.0041, 0.0030, 0.0082], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003937374800443649 tensor([1.1831e-06, 1.5594e-04, 3.9374e-03, 7.6648e-03, 9.8824e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.26046764850616455 tensor([0.0625, 0.3436, 0.0039, 0.3296, 0.2605], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0001588186132721603 tensor([1.0851e-02, 9.8893e-01, 1.5882e-04, 1.7851e-05, 4.3103e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0010484230006113648 tensor([1.6901e-07, 4.4158e-04, 1.6459e-01, 1.0484e-03, 8.3392e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 6.019389911671169e-05 tensor([6.0194e-05, 5.9935e-07, 2.4175e-08, 9.9128e-01, 8.6596e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002122689038515091 tensor([1.6754e-07, 1.2876e-04, 2.4822e-02, 2.1227e-03, 9.7293e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003453344979789108 tensor([9.3208e-01, 6.7575e-02, 5.4059e-08, 3.4533e-04, 2.8004e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.012917010113596916 tensor([3.1756e-01, 6.6675e-01, 1.3306e-04, 1.2917e-02, 2.6438e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0020785462111234665 tensor([1.4224e-06, 1.6636e-03, 8.2441e-02, 2.0785e-03, 9.1382e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.000367127766367048 tensor([9.0321e-10, 3.6713e-04, 9.8064e-01, 9.0783e-07, 1.8991e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00020695464627351612 tensor([1.1257e-08, 2.0695e-04, 2.9472e-01, 1.3092e-04, 7.0494e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.003848978551104665 tensor([9.1732e-01, 7.8804e-02, 3.7708e-07, 3.8490e-03, 2.9094e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001253406226169318 tensor([9.9058e-01, 9.2936e-03, 4.6313e-10, 1.2534e-04, 9.2707e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.002050120150670409 tensor([1.8307e-08, 6.5920e-06, 2.3869e-03, 2.0501e-03, 9.9556e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0010083322413265705 tensor([2.8583e-02, 6.0688e-05, 1.3590e-08, 9.7035e-01, 1.0083e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0007534768665209413 tensor([2.1697e-07, 6.0904e-04, 9.5414e-02, 7.5348e-04, 9.0322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0004899909836240113 tensor([1.5666e-01, 8.4261e-01, 6.5456e-05, 4.8999e-04, 1.7029e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07167822867631912 tensor([0.0014, 0.2017, 0.0717, 0.0211, 0.7042], grad_fn=<SoftmaxBackward0>)\n",
      "1 1.007596802082844e-05 tensor([1.0793e-12, 1.0076e-05, 9.8635e-01, 3.7397e-08, 1.3642e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.3867939944466343e-06 tensor([2.3868e-06, 1.4049e-09, 2.4820e-11, 9.9915e-01, 8.4791e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00018042784358840436 tensor([5.8086e-09, 1.8043e-04, 4.4125e-01, 7.5465e-05, 5.5850e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1087929904460907 tensor([3.1924e-05, 1.0879e-01, 6.1417e-01, 9.2518e-04, 2.7608e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.013168197125196457 tensor([0.0085, 0.9544, 0.0132, 0.0023, 0.0217], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007375387474894524 tensor([5.1200e-08, 1.0340e-02, 9.8228e-01, 1.0845e-06, 7.3754e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 8.923854693421163e-06 tensor([8.9239e-06, 1.5731e-09, 5.2447e-12, 9.9988e-01, 1.1520e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0006277073989622295 tensor([6.9640e-09, 6.2771e-04, 9.0494e-01, 1.0512e-05, 9.4417e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 2.35953411902301e-05 tensor([9.9881e-01, 1.1634e-03, 2.0084e-12, 2.3595e-05, 1.2938e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008735712035559118 tensor([1.0448e-05, 7.2805e-01, 2.7107e-01, 9.2783e-07, 8.7357e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0030349816661328077 tensor([1.1217e-04, 1.9093e-03, 3.0350e-03, 1.0630e-01, 8.8865e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 1.7726561054587364e-05 tensor([1.7727e-05, 5.0504e-07, 1.1196e-07, 9.5420e-01, 4.5785e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00010483170626685023 tensor([4.0705e-11, 4.5651e-07, 1.3410e-02, 1.0483e-04, 9.8648e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0024284704122692347 tensor([9.8524e-01, 1.2327e-02, 1.2701e-08, 2.4285e-03, 4.4495e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00019289887859486043 tensor([9.8287e-03, 9.8992e-01, 1.9290e-04, 1.7188e-05, 4.3475e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016437520971521735 tensor([3.7787e-09, 1.6438e-03, 9.9406e-01, 3.6580e-07, 4.2969e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0018114730482921004 tensor([9.8046e-01, 1.8115e-03, 2.7928e-09, 1.7723e-02, 3.9914e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005843498511239886 tensor([1.2940e-08, 5.8435e-04, 8.0876e-01, 3.9104e-05, 1.9062e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0016245614970102906 tensor([3.6452e-01, 1.6246e-03, 7.9174e-08, 6.3308e-01, 7.7599e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006849811761640012 tensor([2.1757e-03, 9.9708e-01, 6.8498e-04, 5.3658e-06, 5.4150e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022413406521081924 tensor([2.4621e-05, 2.9701e-01, 6.8051e-01, 4.3031e-05, 2.2413e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0349721796810627 tensor([9.0583e-01, 3.4972e-02, 9.9489e-07, 5.8666e-02, 5.3521e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09259382635354996 tensor([0.0549, 0.0926, 0.0016, 0.5562, 0.2947], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031393696554005146 tensor([8.1830e-01, 1.7848e-01, 1.7391e-06, 3.1394e-03, 7.0999e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002282497240230441 tensor([6.7938e-04, 9.6892e-01, 2.8076e-02, 4.3300e-05, 2.2825e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0029725907370448112 tensor([3.2179e-07, 3.7223e-02, 9.5980e-01, 1.2612e-06, 2.9726e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014382489025592804 tensor([1.3258e-03, 1.4382e-03, 1.8413e-04, 5.6911e-01, 4.2794e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0014124612789601088 tensor([9.4332e-08, 1.4125e-03, 6.5110e-01, 1.6319e-04, 3.4733e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.006321959663182497 tensor([2.7496e-02, 3.7437e-04, 4.3002e-07, 9.6581e-01, 6.3220e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003379938134457916 tensor([3.1128e-01, 6.8833e-01, 7.8103e-06, 3.3799e-04, 3.7298e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06961871683597565 tensor([0.0034, 0.0696, 0.0161, 0.1406, 0.7703], grad_fn=<SoftmaxBackward0>)\n",
      "0 1.150759453594219e-05 tensor([1.1508e-05, 1.2366e-06, 9.9395e-07, 8.2819e-01, 1.7179e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07550612092018127 tensor([0.0060, 0.5970, 0.0755, 0.0225, 0.2990], grad_fn=<SoftmaxBackward0>)\n",
      "1 7.4178756221954245e-06 tensor([9.4206e-01, 7.4179e-06, 1.3594e-13, 5.7937e-02, 4.3504e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007166684605181217 tensor([3.1632e-01, 6.7474e-01, 1.0552e-04, 7.1667e-03, 1.6682e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05039370059967041 tensor([4.6753e-06, 5.0394e-02, 8.7710e-01, 9.2822e-05, 7.2409e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00039421924157068133 tensor([3.9422e-04, 4.0962e-06, 5.1731e-08, 9.8764e-01, 1.1957e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1136971265077591 tensor([0.0014, 0.2530, 0.1137, 0.0149, 0.6169], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00034417654387652874 tensor([1.9086e-01, 2.4621e-04, 8.1121e-09, 8.0855e-01, 3.4418e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010175338014960289 tensor([9.1266e-04, 9.2882e-01, 5.9924e-02, 1.7242e-04, 1.0175e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016867732629179955 tensor([3.7542e-05, 6.4718e-03, 3.5860e-02, 1.6868e-02, 9.4076e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.009800335392355919 tensor([3.0721e-02, 7.6544e-04, 9.0547e-07, 9.5871e-01, 9.8003e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 3.123419446637854e-05 tensor([3.9732e-01, 3.1234e-05, 5.9674e-11, 6.0264e-01, 9.5054e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00011296314187347889 tensor([9.9877e-01, 1.1141e-03, 9.1525e-12, 1.1296e-04, 8.7133e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15419794619083405 tensor([0.0007, 0.6511, 0.1922, 0.0018, 0.1542], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00012558717571664602 tensor([3.6141e-11, 1.2559e-04, 9.9667e-01, 4.6196e-08, 3.2008e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00012018278357572854 tensor([2.6742e-07, 3.1919e-06, 1.2018e-04, 4.1759e-02, 9.5812e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0301217008382082 tensor([1.4325e-05, 1.8522e-01, 7.8460e-01, 4.3325e-05, 3.0122e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.141596814908553e-05 tensor([9.1482e-01, 2.1416e-05, 1.6895e-12, 8.5162e-02, 2.9049e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2258652001619339 tensor([0.0971, 0.2259, 0.0017, 0.4283, 0.2470], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0030061283614486456 tensor([1.0022e-08, 3.0061e-03, 9.9182e-01, 5.6114e-07, 5.1755e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 4.427059320732951e-05 tensor([4.4271e-05, 5.3048e-09, 4.7675e-12, 9.9988e-01, 7.5444e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0004947483539581299 tensor([2.1410e-04, 4.9475e-04, 3.2487e-04, 3.4612e-01, 6.5284e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 4.58754766441416e-05 tensor([6.7297e-01, 3.2699e-01, 1.9367e-07, 4.5875e-05, 1.1414e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24507957696914673 tensor([0.0720, 0.4132, 0.0046, 0.2451, 0.2650], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08032660186290741 tensor([3.0654e-05, 1.8343e-01, 7.3604e-01, 1.7554e-04, 8.0327e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0016072764992713928 tensor([1.6073e-03, 1.7749e-04, 5.4671e-06, 9.1923e-01, 7.8975e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.004850704688578844 tensor([2.0875e-06, 1.0189e-03, 3.7066e-02, 4.8507e-03, 9.5706e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005999307031743228 tensor([9.5789e-01, 4.1510e-02, 3.0178e-08, 5.9993e-04, 2.1862e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.003262682119384408 tensor([3.2627e-03, 9.9186e-01, 4.0483e-03, 7.7715e-05, 7.4647e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002737395465373993 tensor([1.7309e-07, 2.7374e-03, 6.6253e-01, 1.4325e-04, 3.3459e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00014743837527930737 tensor([7.3665e-01, 1.4744e-04, 2.0747e-10, 2.6319e-01, 9.4120e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.6645937396096997e-05 tensor([1.2961e-11, 2.6646e-05, 9.7183e-01, 2.5791e-07, 2.8142e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0025464838836342096 tensor([1.1599e-02, 5.3554e-05, 3.1978e-08, 9.8580e-01, 2.5465e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0006680992664769292 tensor([1.6065e-04, 9.6290e-01, 3.6262e-02, 5.5456e-06, 6.6810e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.005822273436933756 tensor([4.4429e-06, 1.7451e-01, 8.1965e-01, 5.3241e-06, 5.8223e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04500334709882736 tensor([0.0018, 0.1148, 0.0285, 0.0450, 0.8098], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00031771123758517206 tensor([4.2213e-09, 2.2206e-05, 4.4411e-02, 3.1771e-04, 9.5525e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005314272711984813 tensor([9.8980e-01, 9.6650e-03, 2.3413e-09, 5.3143e-04, 4.6285e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 3.0858242098474875e-05 tensor([4.7806e-01, 5.2191e-01, 2.8007e-07, 3.0858e-05, 9.5442e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.001355651649646461 tensor([4.8318e-07, 8.3552e-04, 1.3015e-01, 1.3557e-03, 8.6766e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.8234138881089166e-05 tensor([1.3335e-08, 1.4855e-07, 2.8234e-05, 3.6916e-02, 9.6306e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010083295404911041 tensor([4.4500e-04, 1.0083e-02, 6.6624e-03, 9.7577e-02, 8.8523e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002983279409818351 tensor([9.9886e-01, 8.4157e-04, 1.5215e-11, 2.9833e-04, 2.3711e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0005635385750792921 tensor([9.9701e-01, 5.6354e-04, 5.6342e-11, 2.4246e-03, 2.6239e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008646977948956192 tensor([4.0530e-10, 8.6470e-04, 9.9767e-01, 4.5833e-08, 1.4694e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 6.982474133110372e-06 tensor([8.5848e-07, 7.2309e-07, 6.9825e-06, 3.8335e-01, 6.1664e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002540038258302957 tensor([3.2844e-09, 2.7451e-05, 7.6391e-02, 2.5400e-04, 9.2333e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005214054137468338 tensor([8.4161e-01, 5.2141e-03, 6.7155e-08, 1.5300e-01, 1.7617e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00021191885753069073 tensor([1.2614e-02, 9.8707e-01, 2.1192e-04, 2.6737e-05, 7.3917e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0032132703345268965 tensor([1.0192e-07, 3.2133e-03, 8.8382e-01, 4.1728e-05, 1.1293e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0003621542127802968 tensor([3.6215e-04, 1.4474e-04, 2.5495e-05, 7.0053e-01, 2.9893e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004746097605675459 tensor([3.1755e-04, 4.7461e-03, 3.4716e-03, 1.2601e-01, 8.6546e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.051975905895233154 tensor([5.1976e-02, 1.1043e-02, 4.2034e-05, 8.4847e-01, 8.8471e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00227066851221025 tensor([8.9207e-01, 2.2707e-03, 1.1869e-08, 1.0562e-01, 4.4031e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.019516298547387123 tensor([0.0195, 0.9463, 0.0084, 0.0038, 0.0220], grad_fn=<SoftmaxBackward0>)\n",
      "0 6.824798674642807e-06 tensor([6.8248e-06, 1.0874e-07, 2.5575e-08, 9.8307e-01, 1.6928e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00092940719332546 tensor([1.6574e-07, 9.2941e-04, 2.7249e-01, 4.6758e-04, 7.2611e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013414270244538784 tensor([0.0377, 0.9408, 0.0031, 0.0050, 0.0134], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0006862212321721017 tensor([8.4981e-04, 9.9845e-01, 6.8622e-04, 8.5142e-07, 1.6230e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01791873574256897 tensor([2.5143e-06, 8.0452e-02, 9.0161e-01, 1.1857e-05, 1.7919e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0005197320715524256 tensor([1.2388e-08, 3.5973e-05, 3.2506e-02, 5.1973e-04, 9.6694e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08462459594011307 tensor([0.0021, 0.8095, 0.1014, 0.0025, 0.0846], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0007684633601456881 tensor([1.4299e-03, 1.5762e-06, 1.1273e-09, 9.9780e-01, 7.6846e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.14608094096183777 tensor([1.9882e-04, 1.4608e-01, 2.7310e-01, 7.2412e-03, 5.7337e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03943949565291405 tensor([5.8537e-04, 3.9439e-02, 4.1281e-02, 5.3412e-02, 8.6528e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.3767466700519435e-05 tensor([1.7124e-06, 2.5953e-06, 2.3767e-05, 2.7384e-01, 7.2613e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0001234853989444673 tensor([9.9426e-10, 1.4720e-05, 8.5803e-02, 1.2349e-04, 9.1406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 1.4007368008606136e-05 tensor([4.1727e-01, 5.8272e-01, 3.1935e-07, 1.4007e-05, 7.3514e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 2.461880285409279e-05 tensor([5.9539e-06, 9.0775e-06, 2.4619e-05, 2.7128e-01, 7.2868e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00031046278309077024 tensor([4.0173e-10, 3.1046e-04, 9.9328e-01, 2.8437e-07, 6.4135e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07104968279600143 tensor([1.8690e-01, 3.3767e-02, 4.6675e-05, 7.0824e-01, 7.1050e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.11694759875535965 tensor([0.0020, 0.6060, 0.1169, 0.0061, 0.2690], grad_fn=<SoftmaxBackward0>)\n",
      "3 5.15662832185626e-05 tensor([1.4379e-01, 8.5614e-01, 8.7957e-06, 5.1566e-05, 1.0261e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07265292853116989 tensor([4.1199e-05, 7.2653e-02, 3.4125e-01, 2.1157e-03, 5.8394e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0020373377483338118 tensor([7.7499e-04, 9.7061e-01, 2.6530e-02, 4.6044e-05, 2.0373e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0007545928820036352 tensor([5.1686e-03, 9.9396e-01, 7.5459e-04, 1.9237e-05, 9.8682e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.016523994505405426 tensor([0.0079, 0.9452, 0.0165, 0.0025, 0.0279], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0018823507707566023 tensor([9.8342e-01, 1.8824e-03, 2.0471e-09, 1.4690e-02, 5.8269e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 4.285938121029176e-05 tensor([1.9637e-02, 9.8029e-01, 4.2859e-05, 1.1293e-05, 1.4445e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12689831852912903 tensor([0.0170, 0.3891, 0.0251, 0.1269, 0.4419], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00101624452508986 tensor([4.4835e-01, 1.0162e-03, 3.0892e-08, 5.5021e-01, 4.1937e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031687505543231964 tensor([3.9979e-07, 1.7229e-04, 1.2855e-02, 3.1688e-03, 9.8380e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003292494220659137 tensor([4.6853e-01, 5.3111e-01, 4.1821e-06, 3.2925e-04, 2.6615e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009422463364899158 tensor([8.8368e-08, 9.4225e-03, 9.7697e-01, 3.0913e-06, 1.3608e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.001307573402300477 tensor([1.0884e-04, 9.3041e-04, 1.3076e-03, 1.3883e-01, 8.5883e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10864517092704773 tensor([1.7344e-05, 1.0865e-01, 7.4605e-01, 2.7605e-04, 1.4501e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0019995744805783033 tensor([4.8704e-01, 5.1073e-01, 1.6722e-05, 1.9996e-03, 2.1957e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00043725420255213976 tensor([9.9824e-01, 4.3725e-04, 1.7885e-11, 1.3216e-03, 6.8300e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09341910481452942 tensor([1.2426e-05, 1.2615e-01, 7.8028e-01, 1.3946e-04, 9.3419e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.011023723520338535 tensor([0.0117, 0.9648, 0.0107, 0.0017, 0.0110], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002523715083952993 tensor([2.8099e-10, 1.1214e-06, 7.3511e-03, 2.5237e-04, 9.9240e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03572646155953407 tensor([0.0102, 0.7099, 0.0357, 0.0261, 0.2182], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022500144317746162 tensor([3.1922e-01, 2.2500e-02, 7.3151e-06, 6.4285e-01, 1.5423e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.007284829393029213 tensor([8.6501e-01, 1.2756e-01, 1.4735e-06, 7.2848e-03, 1.4260e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0032535656355321407 tensor([1.7614e-07, 3.2536e-03, 7.8431e-01, 1.1264e-04, 2.1232e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.04311737045645714 tensor([0.0032, 0.2295, 0.0431, 0.0390, 0.6852], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018769947811961174 tensor([0.0188, 0.9279, 0.0131, 0.0079, 0.0324], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07249584794044495 tensor([2.7382e-01, 7.2496e-02, 8.2195e-05, 5.9841e-01, 5.5197e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0009713187464512885 tensor([3.6463e-06, 1.0017e-04, 9.7132e-04, 5.0424e-02, 9.4850e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007770986296236515 tensor([2.8517e-05, 4.6203e-01, 5.3015e-01, 1.6271e-05, 7.7710e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.007484325207769871 tensor([1.1837e-04, 5.6917e-03, 7.4843e-03, 4.7176e-02, 9.3953e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03662218526005745 tensor([8.9572e-05, 3.6622e-02, 1.0499e-01, 8.6510e-03, 8.4964e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.005643832962960005 tensor([6.1401e-01, 3.7991e-01, 1.6370e-05, 5.6438e-03, 4.1397e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006958387792110443 tensor([3.4614e-06, 1.1189e-03, 2.4115e-02, 6.9584e-03, 9.6780e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.04707271605730057 tensor([3.2926e-06, 4.7073e-02, 8.9783e-01, 5.5898e-05, 5.5042e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002659472171217203 tensor([7.9406e-08, 2.6595e-03, 8.8474e-01, 3.2011e-05, 1.1257e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0008677055593580008 tensor([4.4672e-08, 8.6771e-04, 6.6846e-01, 1.1554e-04, 3.3055e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0003298824594821781 tensor([4.6538e-01, 5.3427e-01, 4.1504e-06, 3.2988e-04, 2.0927e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.047196902334690094 tensor([0.0041, 0.5969, 0.0472, 0.0149, 0.3369], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009870637441053987 tensor([6.2718e-10, 9.8706e-04, 9.9701e-01, 7.5507e-08, 2.0053e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.9080589229124598e-05 tensor([2.9081e-05, 4.9756e-07, 5.1331e-08, 9.7766e-01, 2.2312e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003204427193850279 tensor([9.8799e-06, 4.8919e-04, 3.2044e-03, 2.8972e-02, 9.6732e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 6.534193380502984e-05 tensor([9.9042e-01, 6.5342e-05, 2.5105e-12, 9.5099e-03, 1.0382e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08459849655628204 tensor([7.2770e-01, 8.4598e-02, 1.0305e-05, 1.8326e-01, 4.4253e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.013402581214904785 tensor([4.5328e-04, 1.3403e-02, 8.5602e-03, 6.3087e-02, 9.1450e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 8.039053682296071e-06 tensor([3.5474e-06, 1.9699e-06, 8.0391e-06, 5.1697e-01, 4.8301e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02968500554561615 tensor([4.0970e-01, 2.9685e-02, 1.1616e-05, 5.5178e-01, 8.8216e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.00013115770707372576 tensor([9.1752e-01, 8.2351e-02, 2.9055e-08, 1.3116e-04, 7.1881e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.025279395282268524 tensor([7.4267e-06, 2.5279e-02, 4.5721e-01, 1.2741e-03, 5.1623e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03293408453464508 tensor([0.0704, 0.8700, 0.0053, 0.0214, 0.0329], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00026739886379800737 tensor([5.1472e-04, 2.1500e-07, 1.7484e-10, 9.9922e-01, 2.6740e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23114345967769623 tensor([2.8185e-04, 2.3114e-01, 3.1059e-01, 3.9252e-03, 4.5406e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 2.7636791855911724e-05 tensor([9.9350e-01, 2.7637e-05, 2.7292e-13, 6.4772e-03, 1.7494e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0612989142537117 tensor([0.0035, 0.1701, 0.0314, 0.0613, 0.7336], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002877582795917988 tensor([1.2918e-07, 2.8776e-03, 7.8382e-01, 8.9947e-05, 2.1322e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009679035283625126 tensor([3.2231e-05, 9.6790e-03, 4.4065e-02, 8.0314e-03, 9.3819e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 1.9161803720635362e-05 tensor([3.7350e-12, 1.9162e-05, 9.8140e-01, 7.9913e-08, 1.8581e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.005613730289041996 tensor([2.6797e-01, 5.6137e-03, 1.1152e-06, 7.2234e-01, 4.0708e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0002904440916609019 tensor([4.4961e-04, 9.9926e-01, 2.9044e-04, 1.4383e-07, 2.6875e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10016512125730515 tensor([5.9662e-04, 1.5124e-01, 1.0017e-01, 1.0118e-02, 7.3788e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 2.116598807333503e-05 tensor([2.1166e-05, 3.7810e-06, 2.4301e-06, 7.1295e-01, 2.8703e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0021380800753831863 tensor([1.7409e-08, 5.0661e-06, 2.1381e-03, 2.8358e-03, 9.9502e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0002944789011962712 tensor([9.9862e-01, 2.9448e-04, 6.3164e-12, 1.0816e-03, 3.2739e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 7.700769856455736e-06 tensor([3.3929e-03, 9.9660e-01, 7.7008e-06, 8.7562e-08, 2.3303e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.009521701373159885 tensor([5.7527e-07, 9.5217e-03, 8.5694e-01, 8.6654e-05, 1.3345e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.00010763827594928443 tensor([8.1856e-07, 5.8395e-06, 1.0764e-04, 8.2631e-02, 9.1725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002110465196892619 tensor([1.5973e-07, 2.1105e-03, 5.9553e-01, 1.6617e-04, 4.0220e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.152937650680542 tensor([0.0162, 0.1529, 0.0077, 0.2087, 0.6145], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0002359418576816097 tensor([6.0164e-01, 3.9812e-01, 1.1342e-06, 2.3594e-04, 8.2491e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.022404465824365616 tensor([1.0416e-05, 2.2404e-02, 3.6980e-01, 1.6448e-03, 6.0614e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 7.760132575640455e-05 tensor([1.0423e-04, 1.3319e-08, 8.4328e-12, 9.9982e-01, 7.7601e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1181444302201271 tensor([0.0012, 0.2952, 0.1181, 0.0120, 0.5735], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1], [2, 0, 1], [0, 3, 4], [0, 2, 1], [3, 0, 2], [3, 4, 0], [2, 0, 1], [0, 3, 4], [0, 1, 3], [0, 1, 2], [0, 3, 4], [0, 3, 2], [0, 3, 2], [0, 1, 3], [0, 1, 2], [2, 4, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0], [0, 1, 2], [2, 1, 0], [2, 1, 0], [0, 3], [2, 1, 0], [0, 2, 1], [2, 1, 0], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 1, 2], [0, 3, 1], [2, 4, 1], [2, 1, 0], [2, 4, 3], [0, 3, 4], [0, 3, 2], [2, 0, 1], [0, 3, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3], [3, 4, 0], [0, 3, 2], [2, 1, 0], [0, 2, 1], [2, 4, 3], [3, 4, 0], [0, 3, 1], [3, 2, 4], [0, 1, 2], [2, 0], [3, 4, 2], [0, 1, 3], [2, 1, 0], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 1, 4], [0, 1, 3], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [3, 0, 2], [0, 3, 4], [2, 1, 0], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 1, 2], [2, 1, 0], [0, 1, 3], [2, 4, 3], [3, 4, 2], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [3, 4, 2], [0, 3, 4], [2, 4, 1], [2, 0, 1], [2, 4, 3], [3, 0, 4], [0, 3, 4], [2, 0, 1], [0, 3, 1], [2, 1, 4], [2, 4, 3], [0, 2, 1], [2, 1, 0], [0, 3, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 2], [2, 1, 4], [3, 0, 4], [0, 1, 3], [2, 1, 4], [2, 4, 1], [2, 4, 3], [0, 3, 4], [0, 3, 1], [0, 1, 2], [0, 3, 4], [2, 4, 1], [2, 0], [0, 3, 1], [2, 1, 0], [0, 2, 1], [2, 4, 3], [2, 0], [0, 3, 4], [2, 1, 0], [0, 1, 3], [2, 4, 3], [3, 4, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 1, 4], [3, 0, 4], [0, 3, 4], [0, 2, 3], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 2, 1], [2, 4, 3], [2, 4, 1], [0, 3, 1], [1, 0, 2], [0, 1, 3], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 1, 0], [0, 2, 1], [2, 1, 0], [2, 4, 1], [3, 2, 0], [2, 1, 0], [0, 3, 1], [2, 3, 4], [3, 4, 2], [0, 3, 4], [0, 1, 3], [0, 3, 4], [2, 1, 4], [0, 3, 1], [0, 2, 1], [0, 1, 2], [0, 1, 3], [2, 4, 3], [0, 1, 2], [0, 3, 1], [2, 1, 4], [0, 3, 2], [2, 4, 3], [0, 3, 1], [3, 0, 4], [3, 4, 2], [3, 0, 2], [2, 4, 1], [3, 4, 2], [0, 2, 3], [2, 4, 1], [0, 1, 3], [2, 4, 3], [0, 3, 1], [0, 1, 2], [0, 3, 1], [2, 4, 3], [2, 4, 1], [0, 3, 4], [3, 2, 4], [0, 1, 3], [0, 3, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 3, 2], [3, 2, 0], [2, 4, 1], [0, 1, 2], [3, 0, 4], [0, 1, 2], [0, 3, 1], [2, 4, 3], [0, 1, 3], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 0], [0, 1, 2], [2, 4, 1], [2, 4, 1], [0, 2, 1], [1, 0, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 3, 4], [2, 1, 4], [0, 3], [2, 4, 1], [0, 2, 3], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [3, 4, 2], [0, 3, 2], [2, 1, 0], [0, 1, 2], [2, 4, 1], [3, 4, 2], [0, 3, 1], [0, 1, 2], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4], [0, 3, 2]]\n",
      "[[0, 3], [3, 4], [1, 2], [3, 4], [1, 4], [1, 2], [3, 4], [1, 2], [2, 4], [3, 4], [1, 2], [1, 4], [1, 4], [2, 4], [3, 4], [0, 1], [0, 3], [2, 4], [3, 4], [3, 4], [3, 4], [3, 4], [1, 2, 4], [3, 4], [3, 4], [3, 4], [0, 3], [2, 4], [2, 4], [3, 4], [0, 1], [3, 4], [2, 4], [0, 3], [3, 4], [0, 1], [1, 2], [1, 4], [3, 4], [1, 4], [0, 3], [1, 2], [2, 4], [2, 4], [2, 4], [0, 1], [1, 2], [1, 2], [0, 3], [2, 4], [0, 1], [1, 2], [1, 4], [3, 4], [3, 4], [0, 1], [1, 2], [2, 4], [0, 1], [3, 4], [1, 3, 4], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 1], [2, 4], [2, 4], [2, 4], [0, 1], [0, 1], [2, 4], [0, 3], [2, 4], [0, 1], [1, 4], [2, 4], [3, 4], [2, 4], [2, 4], [1, 4], [1, 2], [3, 4], [2, 4], [0, 1], [1, 2], [3, 4], [3, 4], [2, 4], [0, 1], [0, 1], [2, 4], [0, 3], [2, 4], [0, 3], [0, 1], [1, 2], [0, 3], [3, 4], [0, 1], [1, 2], [1, 2], [3, 4], [2, 4], [0, 3], [0, 1], [3, 4], [3, 4], [1, 4], [0, 3], [0, 1], [2, 4], [3, 4], [1, 4], [0, 3], [1, 2], [2, 4], [0, 3], [0, 3], [0, 1], [1, 2], [2, 4], [3, 4], [1, 2], [0, 3], [1, 3, 4], [2, 4], [3, 4], [3, 4], [0, 1], [1, 3, 4], [1, 2], [3, 4], [2, 4], [0, 1], [1, 2], [2, 4], [0, 3], [2, 4], [0, 3], [1, 2], [1, 2], [1, 4], [2, 4], [0, 1], [0, 1], [2, 4], [3, 4], [3, 4], [0, 1], [0, 3], [2, 4], [3, 4], [2, 4], [0, 3], [0, 1], [2, 4], [3, 4], [3, 4], [3, 4], [0, 3], [1, 4], [3, 4], [2, 4], [0, 1], [0, 1], [1, 2], [2, 4], [1, 2], [0, 3], [2, 4], [3, 4], [3, 4], [2, 4], [0, 1], [3, 4], [2, 4], [0, 3], [1, 4], [0, 1], [2, 4], [1, 2], [0, 1], [1, 4], [0, 3], [0, 1], [1, 4], [0, 3], [2, 4], [0, 1], [2, 4], [3, 4], [2, 4], [0, 1], [0, 3], [1, 2], [0, 1], [2, 4], [1, 4], [0, 3], [0, 1], [2, 4], [1, 4], [1, 4], [0, 3], [3, 4], [1, 2], [3, 4], [2, 4], [0, 1], [2, 4], [2, 4], [2, 4], [2, 4], [0, 1], [1, 4], [2, 4], [3, 4], [3, 4], [0, 3], [0, 3], [3, 4], [3, 4], [0, 3], [0, 1], [2, 4], [0, 1], [0, 3], [1, 2, 4], [0, 3], [1, 4], [2, 4], [2, 4], [2, 4], [0, 3], [0, 1], [1, 4], [3, 4], [3, 4], [0, 3], [0, 1], [2, 4], [3, 4], [2, 4], [3, 4], [0, 1], [2, 4], [0, 3], [1, 4]]\n",
      "NL_pred of 2th iteration [[2, 4, 1], [2, 0, 1], [0, 3, 4], [0, 2, 1], [3, 0, 2], [3, 4, 0], [2, 0, 1], [0, 3, 4], [0, 1, 3], [0, 1, 2], [0, 3, 4], [0, 3, 2], [0, 3, 2], [0, 1, 3], [0, 1, 2], [2, 4, 3], [2, 4, 1], [0, 3, 1], [2, 1, 0], [0, 1, 2], [2, 1, 0], [2, 1, 0], [2, 1, 0], [0, 2, 1], [2, 1, 0], [2, 4, 1], [0, 3, 1], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 1, 2], [0, 3, 1], [2, 4, 1], [2, 1, 0], [2, 4, 3], [0, 3, 4], [0, 3, 2], [2, 0, 1], [0, 3, 2], [2, 4, 1], [3, 0, 4], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 4, 3], [3, 4, 0], [0, 3, 2], [2, 1, 0], [0, 2, 1], [2, 4, 3], [3, 4, 0], [0, 3, 1], [3, 2, 4], [0, 1, 2], [3, 4, 2], [0, 1, 3], [2, 1, 0], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 3, 1], [0, 3, 1], [2, 4, 3], [2, 4, 3], [0, 1, 3], [2, 1, 4], [0, 1, 3], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 0], [0, 3, 1], [0, 3, 1], [3, 0, 2], [0, 3, 4], [2, 1, 0], [0, 3, 1], [2, 4, 3], [3, 0, 4], [0, 1, 2], [2, 1, 0], [0, 1, 3], [2, 4, 3], [3, 4, 2], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 4, 1], [3, 4, 2], [0, 3, 4], [2, 4, 1], [2, 0, 1], [2, 4, 3], [3, 0, 4], [0, 3, 4], [2, 0, 1], [0, 3, 1], [2, 1, 4], [2, 4, 3], [0, 2, 1], [2, 1, 0], [0, 3, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 1, 0], [0, 3, 2], [2, 1, 4], [3, 0, 4], [0, 1, 3], [2, 1, 4], [2, 4, 1], [2, 4, 3], [0, 3, 4], [0, 3, 1], [0, 1, 2], [0, 3, 4], [2, 4, 1], [0, 3, 1], [2, 1, 0], [0, 2, 1], [2, 4, 3], [0, 3, 4], [2, 1, 0], [0, 1, 3], [2, 4, 3], [3, 4, 0], [0, 3, 1], [2, 4, 1], [0, 3, 1], [2, 1, 4], [3, 0, 4], [0, 3, 4], [0, 2, 3], [0, 1, 3], [2, 4, 3], [2, 4, 3], [0, 1, 3], [0, 1, 2], [0, 2, 1], [2, 4, 3], [2, 4, 1], [0, 3, 1], [1, 0, 2], [0, 1, 3], [2, 4, 1], [3, 4, 2], [0, 3, 1], [2, 1, 0], [0, 2, 1], [2, 1, 0], [2, 4, 1], [3, 2, 0], [2, 1, 0], [0, 3, 1], [2, 3, 4], [3, 4, 2], [0, 3, 4], [0, 1, 3], [0, 3, 4], [2, 1, 4], [0, 3, 1], [0, 2, 1], [0, 1, 2], [0, 1, 3], [2, 4, 3], [0, 1, 2], [0, 3, 1], [2, 1, 4], [0, 3, 2], [2, 4, 3], [0, 3, 1], [3, 0, 4], [3, 4, 2], [3, 0, 2], [2, 4, 1], [3, 4, 2], [0, 2, 3], [2, 4, 1], [0, 1, 3], [2, 4, 3], [0, 3, 1], [0, 1, 2], [0, 3, 1], [2, 4, 3], [2, 4, 1], [0, 3, 4], [3, 2, 4], [0, 1, 3], [0, 3, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [0, 3, 2], [3, 2, 0], [2, 4, 1], [0, 1, 2], [3, 0, 4], [0, 1, 2], [0, 3, 1], [2, 4, 3], [0, 1, 3], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1], [2, 1, 0], [0, 1, 2], [2, 4, 1], [2, 4, 1], [0, 2, 1], [1, 0, 2], [2, 4, 1], [2, 4, 3], [0, 3, 1], [2, 3, 4], [2, 1, 4], [2, 4, 1], [0, 2, 3], [0, 3, 1], [0, 3, 1], [0, 3, 1], [2, 4, 1], [3, 4, 2], [0, 3, 2], [2, 1, 0], [0, 1, 2], [2, 4, 1], [3, 4, 2], [0, 3, 1], [0, 1, 2], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4], [0, 3, 2]]\n",
      "Start of Epoch\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.004646908507055166  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.004646872014415508  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.004646804381390007  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.004646708527389838  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.004646587371826172  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.004646446753521355  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.004646288132180973  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.004646114913784727  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.004645930017743792  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.00464573587690081  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.004645535410666952  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.004645330078747807  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.004645121827417491  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.004644911629813058  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.004644702405345683  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.004644493667446837  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.004644288822096222  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.004644085923019721  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.00464388691649145  Accuracy on Support set:0.0\n",
      "torch.Size([245, 2048]) torch.Size([245])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.004643692775648468  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.007290887646377087 tensor([9.9071e-01, 1.9918e-03, 1.4133e-09, 7.2909e-03, 2.4237e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.4179278612136841 tensor([0.0079, 0.0233, 0.0009, 0.4179, 0.5500], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06401289999485016 tensor([1.9803e-06, 6.4013e-02, 9.1672e-01, 1.3996e-05, 1.9253e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3623637855052948 tensor([6.1468e-05, 1.0576e-04, 9.2907e-05, 3.6236e-01, 6.3738e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.08269783854484558 tensor([0.0043, 0.8603, 0.0493, 0.0034, 0.0827], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006757838651537895 tensor([6.6819e-03, 9.8155e-01, 6.7578e-03, 4.8902e-04, 4.5171e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2138477861881256 tensor([5.9771e-02, 6.3691e-02, 6.0959e-04, 6.6208e-01, 2.1385e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24338197708129883 tensor([0.0006, 0.5538, 0.2434, 0.0017, 0.2005], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03641212359070778 tensor([2.0042e-07, 2.5028e-04, 3.6412e-02, 1.3259e-03, 9.6201e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19666068255901337 tensor([2.4965e-05, 1.0045e-04, 2.3672e-04, 1.9666e-01, 8.0298e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.16467690467834473 tensor([1.4419e-05, 1.6468e-01, 7.5759e-01, 1.0683e-04, 7.7608e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.43319857120513916 tensor([0.0021, 0.4332, 0.1179, 0.0123, 0.4344], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17618615925312042 tensor([0.0054, 0.7238, 0.0820, 0.0126, 0.1762], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.017933877184987068 tensor([3.7481e-08, 4.5449e-05, 1.7934e-02, 1.3081e-03, 9.8071e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.037752922624349594 tensor([1.2640e-05, 5.1332e-04, 3.0789e-03, 3.7753e-02, 9.5864e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.047075070440769196 tensor([9.5248e-01, 4.7075e-02, 2.9182e-08, 4.4067e-04, 1.8594e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.10019496828317642 tensor([8.5730e-01, 4.1816e-02, 1.5232e-06, 1.0019e-01, 6.8787e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0003545704821590334 tensor([4.9718e-12, 1.3304e-04, 9.9951e-01, 2.2198e-09, 3.5457e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.01404483150690794 tensor([7.2335e-06, 8.0402e-08, 1.4133e-08, 9.8595e-01, 1.4045e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2218777984380722 tensor([5.7130e-05, 3.2134e-04, 5.0252e-04, 2.2188e-01, 7.7724e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06782679259777069 tensor([1.8773e-03, 2.9274e-04, 9.7915e-06, 9.2999e-01, 6.7827e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.039790015667676926 tensor([8.4327e-03, 4.5178e-04, 2.5604e-06, 9.5132e-01, 3.9790e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.20225173234939575 tensor([0.0016, 0.5121, 0.2023, 0.0073, 0.2767], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0156744085252285 tensor([4.5310e-05, 6.4014e-07, 3.6728e-08, 9.8428e-01, 1.5674e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09472772479057312 tensor([6.2247e-04, 9.6501e-03, 5.3879e-03, 9.4728e-02, 8.8961e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0005773724988102913 tensor([3.2479e-04, 2.9624e-07, 4.9800e-10, 9.9910e-01, 5.7737e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2787993252277374 tensor([2.7880e-01, 2.7110e-04, 4.8617e-09, 7.2073e-01, 1.9993e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0008042260888032615 tensor([7.1062e-11, 3.9124e-04, 9.9880e-01, 1.4635e-08, 8.0423e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07942232489585876 tensor([1.0785e-05, 8.3457e-03, 7.9422e-02, 3.9481e-03, 9.0827e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3331146240234375 tensor([0.0496, 0.0834, 0.0012, 0.5327, 0.3331], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08447133004665375 tensor([9.1468e-01, 8.4471e-02, 1.5482e-07, 8.3686e-04, 8.1291e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.08829469233751297 tensor([1.2678e-04, 2.8336e-03, 2.8376e-03, 8.8295e-02, 9.0591e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34943675994873047 tensor([1.6542e-05, 5.7431e-02, 5.9237e-01, 7.4596e-04, 3.4944e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.43878304958343506 tensor([4.3878e-01, 1.9063e-02, 3.9804e-06, 5.3626e-01, 5.8892e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.27017414569854736 tensor([8.0053e-04, 1.9866e-04, 1.5707e-05, 7.2881e-01, 2.7017e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.004047710448503494 tensor([9.9356e-01, 4.0477e-03, 1.7798e-09, 2.3928e-03, 1.2001e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.12772595882415771 tensor([0.0013, 0.7795, 0.1277, 0.0019, 0.0896], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19963587820529938 tensor([0.0048, 0.7252, 0.0616, 0.0088, 0.1996], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2659912109375 tensor([6.5747e-02, 7.4927e-02, 5.8384e-04, 5.9275e-01, 2.6599e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15919220447540283 tensor([0.0163, 0.7838, 0.0205, 0.0202, 0.1592], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.472858190536499 tensor([4.7286e-01, 8.6217e-03, 8.8494e-07, 5.1447e-01, 4.0510e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18330170214176178 tensor([1.3112e-06, 1.8330e-01, 8.1392e-01, 1.1476e-06, 2.7720e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0441136471927166 tensor([7.2110e-08, 4.3431e-03, 9.5153e-01, 1.3297e-05, 4.4114e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05833764746785164 tensor([1.1649e-04, 2.4609e-02, 5.8338e-02, 1.6983e-02, 8.9995e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.048442404717206955 tensor([6.2711e-05, 1.3645e-02, 4.8442e-02, 1.1643e-02, 9.2621e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.002738436684012413 tensor([9.9536e-01, 2.7384e-03, 8.3889e-10, 1.9010e-03, 1.1545e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.015144385397434235 tensor([1.6024e-04, 9.8453e-01, 1.5144e-02, 1.7694e-06, 1.6428e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038168810307979584 tensor([1.0602e-07, 3.8169e-02, 9.6080e-01, 2.1981e-07, 1.0354e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31083235144615173 tensor([3.1083e-01, 3.5828e-06, 1.5198e-12, 6.8916e-01, 1.3832e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.14168499410152435 tensor([9.0102e-08, 2.7078e-03, 8.5557e-01, 4.1749e-05, 1.4168e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.39483702182769775 tensor([3.9484e-01, 5.9752e-01, 6.8998e-05, 6.5819e-03, 9.9545e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003416741732507944 tensor([6.4405e-04, 9.9580e-01, 3.4167e-03, 4.4648e-06, 1.3054e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.26734164357185364 tensor([0.0026, 0.2673, 0.0621, 0.0285, 0.6394], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.013607996515929699 tensor([4.6486e-06, 3.6788e-08, 6.5977e-09, 9.8639e-01, 1.3608e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3103582561016083 tensor([5.2139e-04, 1.6677e-03, 6.9233e-04, 3.1036e-01, 6.8676e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0067974720150232315 tensor([9.9282e-01, 6.7975e-03, 9.2830e-10, 3.8149e-04, 3.0466e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.009916997514665127 tensor([1.4077e-04, 9.8984e-01, 9.9170e-03, 9.7564e-07, 1.0389e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010839637368917465 tensor([7.6434e-09, 1.2491e-03, 9.8791e-01, 1.6370e-06, 1.0840e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.031427185982465744 tensor([0.0314, 0.9520, 0.0038, 0.0038, 0.0090], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.008691701106727123 tensor([1.2423e-06, 1.3762e-04, 3.2136e-03, 8.6917e-03, 9.8796e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25590792298316956 tensor([0.0655, 0.3021, 0.0032, 0.3734, 0.2559], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.01253702212125063 tensor([1.2537e-02, 9.8725e-01, 1.4528e-04, 2.0690e-05, 4.4789e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.1388692557811737 tensor([1.8083e-07, 4.0209e-04, 1.3887e-01, 1.1937e-03, 8.5953e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.007662152871489525 tensor([5.6849e-05, 4.9920e-07, 1.8964e-08, 9.9228e-01, 7.6622e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.020484158769249916 tensor([1.7363e-07, 1.1410e-04, 2.0484e-02, 2.3536e-03, 9.7705e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05783060938119888 tensor([9.4181e-01, 5.7831e-02, 4.2948e-08, 3.6104e-04, 2.5613e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.35510188341140747 tensor([3.5510e-01, 6.2655e-01, 1.1720e-04, 1.5476e-02, 2.7536e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06797043979167938 tensor([1.5092e-06, 1.4913e-03, 6.7970e-02, 2.3472e-03, 9.2819e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.022113868966698647 tensor([1.0447e-09, 3.7317e-04, 9.7751e-01, 1.1290e-06, 2.2114e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25366997718811035 tensor([1.2382e-08, 1.9273e-04, 2.5367e-01, 1.5338e-04, 7.4598e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.06722476333379745 tensor([9.2865e-01, 6.7225e-02, 2.9943e-07, 4.1013e-03, 2.6911e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008108172565698624 tensor([9.9176e-01, 8.1082e-03, 3.8191e-10, 1.3214e-04, 8.6596e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0019613592885434628 tensor([1.9337e-08, 5.8898e-06, 1.9614e-03, 2.2979e-03, 9.9573e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.026544831693172455 tensor([2.6545e-02, 5.0113e-05, 1.0741e-08, 9.7251e-01, 8.9753e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07916107773780823 tensor([2.3143e-07, 5.4986e-04, 7.9161e-02, 8.5093e-04, 9.1944e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.18151576817035675 tensor([1.8152e-01, 8.1767e-01, 5.8374e-05, 5.8434e-04, 1.7627e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1845947802066803 tensor([0.0016, 0.1846, 0.0601, 0.0247, 0.7290], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014903761446475983 tensor([1.1743e-12, 1.0113e-05, 9.8509e-01, 4.2321e-08, 1.4904e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.000771018851082772 tensor([2.2976e-06, 1.2298e-09, 2.0533e-11, 9.9923e-01, 7.7102e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.39503729343414307 tensor([6.4608e-09, 1.7260e-04, 3.9504e-01, 8.9326e-05, 6.0470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3137388527393341 tensor([3.7101e-05, 1.0849e-01, 5.7658e-01, 1.1591e-03, 3.1374e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024170750752091408 tensor([0.0100, 0.9508, 0.0121, 0.0029, 0.0242], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010854862630367279 tensor([6.0256e-08, 1.0855e-02, 9.8082e-01, 1.3076e-06, 8.3277e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00010521514195716009 tensor([8.5685e-06, 1.3769e-09, 4.3535e-12, 9.9989e-01, 1.0522e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11241480708122253 tensor([8.3632e-09, 6.4626e-04, 8.8693e-01, 1.3665e-05, 1.1241e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.001031014253385365 tensor([9.9894e-01, 1.0310e-03, 1.7034e-12, 2.4833e-05, 1.2272e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.26048314571380615 tensor([1.1707e-05, 7.3859e-01, 2.6048e-01, 1.0364e-06, 9.1232e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.12214142829179764 tensor([1.2094e-04, 1.7040e-03, 2.4906e-03, 1.2214e-01, 8.7354e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0406087189912796 tensor([1.6796e-05, 4.2122e-07, 8.7865e-08, 9.5937e-01, 4.0609e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.011173048056662083 tensor([4.2083e-11, 4.0460e-07, 1.1173e-02, 1.1585e-04, 9.8871e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.010380992665886879 tensor([9.8705e-01, 1.0381e-02, 9.9351e-09, 2.5616e-03, 4.0637e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.011421429924666882 tensor([1.1421e-02, 9.8834e-01, 1.7581e-04, 1.9985e-05, 4.5026e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.004668330308049917 tensor([4.1452e-09, 1.6502e-03, 9.9368e-01, 4.1475e-07, 4.6683e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.019054587930440903 tensor([9.7937e-01, 1.5671e-03, 2.2927e-09, 1.9055e-02, 3.7693e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.22320488095283508 tensor([1.5655e-08, 6.0287e-04, 7.7614e-01, 5.0377e-05, 2.2320e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3446044921875 tensor([3.4460e-01, 1.3173e-03, 6.1352e-08, 6.5337e-01, 7.0315e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.002527231816202402 tensor([2.5272e-03, 9.9679e-01, 6.2320e-04, 6.2439e-06, 5.6557e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3132961392402649 tensor([3.0187e-05, 3.1330e-01, 6.6120e-01, 5.4006e-05, 2.5417e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06340064108371735 tensor([9.0686e-01, 2.9243e-02, 7.7943e-07, 6.3401e-02, 4.9853e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2699176073074341 tensor([0.0543, 0.0753, 0.0012, 0.5993, 0.2699], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15453626215457916 tensor([8.4202e-01, 1.5454e-01, 1.3963e-06, 3.3757e-03, 6.6293e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.025809353217482567 tensor([7.8795e-04, 9.7093e-01, 2.5809e-02, 5.0947e-05, 2.4228e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.038858503103256226 tensor([3.6916e-07, 3.8859e-02, 9.5791e-01, 1.4583e-06, 3.2333e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3907783329486847 tensor([1.3173e-03, 1.1828e-03, 1.3990e-04, 6.0658e-01, 3.9078e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3940250873565674 tensor([1.1080e-07, 1.4123e-03, 6.0436e-01, 2.0412e-04, 3.9403e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.025317803025245667 tensor([2.5318e-02, 2.9168e-04, 3.1655e-07, 9.6890e-01, 5.4912e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3491577208042145 tensor([3.4916e-01, 6.5041e-01, 6.8058e-06, 3.9051e-04, 3.7565e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16263487935066223 tensor([0.0036, 0.0610, 0.0131, 0.1626, 0.7596], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15273849666118622 tensor([1.1096e-05, 1.0267e-06, 7.6841e-07, 8.4725e-01, 1.5274e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3233550786972046 tensor([0.0070, 0.5739, 0.0671, 0.0286, 0.3234], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06297621130943298 tensor([9.3702e-01, 6.7030e-06, 1.2034e-13, 6.2976e-02, 4.3119e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3578191101551056 tensor([3.5782e-01, 6.3177e-01, 9.1195e-05, 8.5991e-03, 1.7180e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0865066796541214 tensor([5.7906e-06, 5.3140e-02, 8.6022e-01, 1.2301e-04, 8.6507e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.010506738908588886 tensor([3.6989e-04, 3.3654e-06, 4.0013e-08, 9.8912e-01, 1.0507e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.23558074235916138 tensor([0.0015, 0.2356, 0.0978, 0.0176, 0.6475], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.17896755039691925 tensor([1.7897e-01, 2.0191e-04, 6.3220e-09, 8.2052e-01, 3.0714e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.05499996617436409 tensor([1.0759e-03, 9.3268e-01, 5.5000e-02, 2.0817e-04, 1.1036e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.02972155064344406 tensor([4.0814e-05, 5.8548e-03, 2.9722e-02, 1.9555e-02, 9.4483e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.028632398694753647 tensor([2.8632e-02, 6.0483e-04, 6.7655e-07, 9.6216e-01, 8.6029e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3783338963985443 tensor([3.7833e-01, 2.6821e-05, 4.9659e-11, 6.2163e-01, 8.8630e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0009777563391253352 tensor([9.9890e-01, 9.7776e-04, 7.6390e-12, 1.1917e-04, 8.1991e-09],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.17708998918533325 tensor([0.0008, 0.6508, 0.1771, 0.0022, 0.1691], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0034788036718964577 tensor([3.9091e-11, 1.2512e-04, 9.9640e-01, 5.2185e-08, 3.4788e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.047668565064668655 tensor([2.8458e-07, 2.8473e-06, 9.9588e-05, 4.7669e-02, 9.5223e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19721901416778564 tensor([1.7660e-05, 1.9722e-01, 7.6810e-01, 5.4774e-05, 3.4611e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09258672595024109 tensor([9.0739e-01, 1.9307e-05, 1.4952e-12, 9.2587e-02, 2.8803e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.1922503113746643 tensor([0.0990, 0.1923, 0.0014, 0.4725, 0.2349], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.00566752627491951 tensor([1.1222e-08, 3.0661e-03, 9.9127e-01, 6.4374e-07, 5.6675e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 6.917213613633066e-05 tensor([4.2034e-05, 4.6042e-09, 3.9719e-12, 9.9989e-01, 6.9172e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.38365334272384644 tensor([2.2141e-04, 4.2003e-04, 2.5318e-04, 3.8365e-01, 6.1545e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2919262945652008 tensor([7.0802e-01, 2.9193e-01, 1.5993e-07, 4.9653e-05, 1.0872e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2656536102294922 tensor([0.0769, 0.3708, 0.0039, 0.2827, 0.2657], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19264942407608032 tensor([3.7614e-05, 1.9265e-01, 7.1387e-01, 2.2573e-04, 9.3216e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06850177049636841 tensor([1.5179e-03, 1.3924e-04, 3.9698e-06, 9.2984e-01, 6.8502e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0303505752235651 tensor([2.1918e-06, 9.0476e-04, 3.0351e-02, 5.4537e-03, 9.6329e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03543904423713684 tensor([9.6393e-01, 3.5439e-02, 2.4024e-08, 6.2807e-04, 1.9997e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.003722844645380974 tensor([3.7970e-03, 9.9159e-01, 3.7228e-03, 9.2621e-05, 8.0028e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38193202018737793 tensor([2.0565e-07, 2.7497e-03, 6.1514e-01, 1.8154e-04, 3.8193e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2797916829586029 tensor([7.2007e-01, 1.2901e-04, 1.7554e-10, 2.7979e-01, 8.9785e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03157963976264 tensor([1.4413e-11, 2.6760e-05, 9.6839e-01, 3.0320e-07, 3.1580e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010754679329693317 tensor([1.0755e-02, 4.3553e-05, 2.4802e-08, 9.8696e-01, 2.2441e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.0336870402097702 tensor([1.8366e-04, 9.6542e-01, 3.3687e-02, 6.3867e-06, 6.9983e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.18561425805091858 tensor([5.3565e-06, 1.8561e-01, 8.0789e-01, 6.4516e-06, 6.4849e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.103905089199543 tensor([0.0019, 0.1039, 0.0242, 0.0525, 0.8175], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.03650226816534996 tensor([4.4257e-09, 1.9741e-05, 3.6502e-02, 3.5462e-04, 9.6312e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.008352519944310188 tensor([9.9109e-01, 8.3525e-03, 1.9135e-09, 5.5867e-04, 4.3159e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4815521240234375 tensor([5.1841e-01, 4.8155e-01, 2.3955e-07, 3.4397e-05, 9.3810e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.10882788896560669 tensor([5.1547e-07, 7.5615e-04, 1.0883e-01, 1.5370e-03, 8.8888e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04222972318530083 tensor([1.4342e-08, 1.3397e-07, 2.3504e-05, 4.2230e-02, 9.5775e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11319214105606079 tensor([4.7693e-04, 8.8689e-03, 5.4178e-03, 1.1319e-01, 8.7204e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.0007399751921184361 tensor([9.9894e-01, 7.3998e-04, 1.2791e-11, 3.1817e-04, 2.2621e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0025873216800391674 tensor([9.9692e-01, 4.9585e-04, 4.7061e-11, 2.5873e-03, 2.4835e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0015607315581291914 tensor([4.3247e-10, 8.6433e-04, 9.9757e-01, 5.0130e-08, 1.5607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.42008787393569946 tensor([8.8478e-07, 6.2043e-07, 5.5177e-06, 4.2009e-01, 5.7991e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.06318042427301407 tensor([3.4602e-09, 2.4533e-05, 6.3180e-02, 2.8478e-04, 9.3651e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.16398048400878906 tensor([8.3147e-01, 4.3820e-03, 5.3455e-08, 1.6398e-01, 1.6462e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014792930334806442 tensor([1.4793e-02, 9.8491e-01, 1.9227e-04, 3.1484e-05, 7.7317e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13375923037528992 tensor([1.2499e-07, 3.3676e-03, 8.6282e-01, 5.4562e-05, 1.3376e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2687831223011017 tensor([3.5305e-04, 1.1771e-04, 1.9226e-05, 7.3073e-01, 2.6878e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.14523351192474365 tensor([3.3978e-04, 4.1631e-03, 2.8008e-03, 1.4523e-01, 8.4746e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07751080393791199 tensor([4.8410e-02, 8.5249e-03, 3.0570e-05, 8.6552e-01, 7.7511e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.11411558836698532 tensor([8.8391e-01, 1.9291e-03, 9.6025e-09, 1.1412e-01, 4.1808e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.024160083383321762 tensor([0.0232, 0.9402, 0.0077, 0.0047, 0.0242], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.014955512247979641 tensor([6.4412e-06, 9.0220e-08, 1.9957e-08, 9.8504e-01, 1.4956e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.23446212708950043 tensor([1.7837e-07, 8.5389e-04, 2.3446e-01, 5.4007e-04, 7.6414e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04509512707591057 tensor([0.0451, 0.9312, 0.0028, 0.0063, 0.0146], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0009550159447826445 tensor([9.5502e-04, 9.9839e-01, 6.3651e-04, 9.4665e-07, 1.6588e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08650778979063034 tensor([3.1143e-06, 8.6508e-02, 8.9283e-01, 1.4968e-05, 2.0640e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.026800768449902534 tensor([1.2986e-08, 3.2064e-05, 2.6801e-02, 5.7927e-04, 9.7259e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.09254137426614761 tensor([0.0024, 0.8076, 0.0925, 0.0031, 0.0943], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0013328788336366415 tensor([1.3329e-03, 1.3140e-06, 8.9881e-10, 9.9798e-01, 6.8883e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.24220333993434906 tensor([2.2020e-04, 1.3816e-01, 2.4220e-01, 8.5940e-03, 6.1082e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06299637258052826 tensor([6.3990e-04, 3.5286e-02, 3.4027e-02, 6.2996e-02, 8.6705e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30494871735572815 tensor([1.7900e-06, 2.2573e-06, 1.9071e-05, 3.0495e-01, 6.9503e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.07134648412466049 tensor([1.0443e-09, 1.3138e-05, 7.1346e-02, 1.3850e-04, 9.2850e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.45455577969551086 tensor([4.5456e-01, 5.4543e-01, 2.7756e-07, 1.5570e-05, 7.2348e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.30376189947128296 tensor([6.2683e-06, 7.8917e-06, 1.9642e-05, 3.0376e-01, 6.9620e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0070097134448587894 tensor([4.3778e-10, 3.1045e-04, 9.9268e-01, 3.2415e-07, 7.0097e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.1790148913860321 tensor([1.7901e-01, 2.6705e-02, 3.4316e-05, 7.3162e-01, 6.2628e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29323285818099976 tensor([0.0023, 0.5926, 0.1043, 0.0076, 0.2932], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.16414675116539001 tensor([1.6415e-01, 8.3577e-01, 7.9797e-06, 6.0374e-05, 1.0627e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.29920676350593567 tensor([4.5743e-05, 6.8289e-02, 2.9921e-01, 2.5372e-03, 6.2992e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.024213731288909912 tensor([9.0738e-04, 9.7266e-01, 2.4214e-02, 5.4573e-05, 2.1595e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.005946568213403225 tensor([5.9466e-03, 9.9323e-01, 6.9405e-04, 2.2318e-05, 1.0348e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03080037236213684 tensor([0.0094, 0.9417, 0.0151, 0.0031, 0.0308], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.01589267887175083 tensor([9.8249e-01, 1.6154e-03, 1.6626e-09, 1.5893e-02, 5.5218e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0227605439722538 tensor([2.2761e-02, 9.7717e-01, 3.9304e-05, 1.3163e-05, 1.4960e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.35818004608154297 tensor([0.0189, 0.3582, 0.0214, 0.1508, 0.4507], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4280513823032379 tensor([4.2805e-01, 8.5333e-04, 2.4839e-08, 5.7071e-01, 3.8532e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01047954335808754 tensor([4.1850e-07, 1.5248e-04, 1.0480e-02, 3.5394e-03, 9.8583e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4887235164642334 tensor([5.1088e-01, 4.8872e-01, 3.5483e-06, 3.6930e-04, 2.6085e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.015742739662528038 tensor([1.0655e-07, 9.9826e-03, 9.7427e-01, 3.8821e-06, 1.5743e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.15907037258148193 tensor([1.1627e-04, 8.1835e-04, 1.0585e-03, 1.5907e-01, 8.3894e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17041164636611938 tensor([2.0876e-05, 1.1150e-01, 7.1771e-01, 3.5879e-04, 1.7041e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.46361595392227173 tensor([5.3390e-01, 4.6362e-01, 1.3859e-05, 2.2590e-03, 2.1350e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0014228143263608217 tensor([9.9820e-01, 3.8115e-04, 1.4875e-11, 1.4228e-03, 6.5123e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13098935782909393 tensor([1.5011e-05, 1.3099e-01, 7.5920e-01, 1.8021e-04, 1.0961e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.014067706651985645 tensor([0.0141, 0.9621, 0.0098, 0.0021, 0.0119], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.006110648158937693 tensor([2.9411e-10, 1.0050e-06, 6.1106e-03, 2.7979e-04, 9.9361e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.23678593337535858 tensor([0.0120, 0.6861, 0.0320, 0.0332, 0.2368], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3044571876525879 tensor([3.0446e-01, 1.7917e-02, 5.4692e-06, 6.6388e-01, 1.3741e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.10926131904125214 tensor([8.8271e-01, 1.0926e-01, 1.1754e-06, 7.8900e-03, 1.3377e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2464667111635208 tensor([2.1307e-07, 3.3558e-03, 7.5003e-01, 1.4455e-04, 2.4647e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21023888885974884 tensor([0.0036, 0.2102, 0.0366, 0.0466, 0.7029], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.03548317402601242 tensor([0.0225, 0.9199, 0.0120, 0.0101, 0.0355], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2643090784549713 tensor([2.6431e-01, 5.7339e-02, 6.0504e-05, 6.2897e-01, 4.9321e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05823631212115288 tensor([3.9296e-06, 8.9591e-05, 8.0396e-04, 5.8236e-02, 9.4087e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4827556908130646 tensor([3.4209e-05, 4.8276e-01, 5.0873e-01, 1.9569e-05, 8.4607e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.054458409547805786 tensor([1.2677e-04, 5.0801e-03, 6.2305e-03, 5.4458e-02, 9.3410e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.08899027109146118 tensor([9.5195e-05, 3.3190e-02, 8.8990e-02, 9.8443e-03, 8.6788e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.33921411633491516 tensor([6.5407e-01, 3.3921e-01, 1.3530e-05, 6.3026e-03, 4.0037e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.01961534097790718 tensor([3.6380e-06, 9.8796e-04, 1.9615e-02, 7.8765e-03, 9.7152e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06602007150650024 tensor([4.1271e-06, 5.0094e-02, 8.8381e-01, 7.4483e-05, 6.6020e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.13430409133434296 tensor([9.7512e-08, 2.7775e-03, 8.6288e-01, 4.2143e-05, 1.3430e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3761173188686371 tensor([5.2058e-08, 8.6404e-04, 6.2287e-01, 1.4425e-04, 3.7612e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4898397922515869 tensor([5.0976e-01, 4.8984e-01, 3.5023e-06, 3.7292e-04, 2.0526e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3625248074531555 tensor([0.0047, 0.5730, 0.0417, 0.0181, 0.3625], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.002157734241336584 tensor([6.7657e-10, 9.8603e-04, 9.9686e-01, 8.4088e-08, 2.1577e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.019703291356563568 tensor([2.7436e-05, 4.1257e-07, 4.0050e-08, 9.8027e-01, 1.9703e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03365423530340195 tensor([1.0690e-05, 4.3846e-04, 2.6548e-03, 3.3654e-02, 9.6324e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.010282926261425018 tensor([9.8966e-01, 5.9001e-05, 2.1991e-12, 1.0283e-02, 1.0148e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19825789332389832 tensor([7.2743e-01, 7.0200e-02, 7.9897e-06, 1.9826e-01, 4.1076e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.07336646318435669 tensor([4.8960e-04, 1.1952e-02, 7.0639e-03, 7.3366e-02, 9.0713e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4455086290836334 tensor([3.5848e-06, 1.6617e-06, 6.2498e-06, 5.5448e-01, 4.4551e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3945428729057312 tensor([3.9454e-01, 2.3536e-02, 8.5473e-06, 5.7408e-01, 7.8303e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07059935480356216 tensor([9.2926e-01, 7.0599e-02, 2.3096e-08, 1.3790e-04, 6.6117e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4088722765445709 tensor([8.4234e-06, 2.4341e-02, 4.0887e-01, 1.5488e-03, 5.6523e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0833035260438919 tensor([0.0833, 0.8495, 0.0047, 0.0269, 0.0355], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.00048089883057400584 tensor([4.8090e-04, 1.8129e-07, 1.4183e-10, 9.9928e-01, 2.4219e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.22435538470745087 tensor([3.2782e-04, 2.2436e-01, 2.7577e-01, 4.8423e-03, 4.9470e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.006961761973798275 tensor([9.9301e-01, 2.5069e-05, 2.4038e-13, 6.9618e-03, 1.7071e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.15469242632389069 tensor([0.0038, 0.1547, 0.0268, 0.0715, 0.7433], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.24921537935733795 tensor([1.5601e-07, 2.9512e-03, 7.4772e-01, 1.1641e-04, 2.4922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.036186397075653076 tensor([3.3676e-05, 8.5646e-03, 3.6186e-02, 9.0195e-03, 9.4620e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.020544148981571198 tensor([4.0798e-12, 1.9193e-05, 9.7944e-01, 9.1771e-08, 2.0544e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2535184919834137 tensor([2.5352e-01, 4.4851e-03, 8.4194e-07, 7.3837e-01, 3.6290e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0005013541085645556 tensor([5.0135e-04, 9.9923e-01, 2.7077e-04, 1.5809e-07, 2.7332e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13987493515014648 tensor([6.5979e-04, 1.3987e-01, 8.4694e-02, 1.1819e-02, 7.6295e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.259121298789978 tensor([2.0778e-05, 3.1975e-06, 1.9112e-06, 7.4085e-01, 2.5912e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0031757806427776814 tensor([1.8172e-08, 4.4940e-06, 1.7653e-03, 3.1758e-03, 9.9505e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0011597415432333946 tensor([9.9858e-01, 2.6017e-04, 5.3455e-12, 1.1597e-03, 3.1325e-08],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0037974566221237183 tensor([3.7975e-03, 9.9620e-01, 7.1774e-06, 9.7444e-08, 2.3933e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.15832971036434174 tensor([7.1140e-07, 9.9884e-03, 8.3157e-01, 1.1405e-04, 1.5833e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.09447731077671051 tensor([8.7262e-07, 5.1854e-06, 8.8263e-05, 9.4477e-02, 9.0543e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4498348832130432 tensor([1.8401e-07, 2.0787e-03, 5.4788e-01, 2.0465e-04, 4.4983e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24059157073497772 tensor([0.0170, 0.1317, 0.0063, 0.2406, 0.6045], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3565753698348999 tensor([6.4315e-01, 3.5658e-01, 9.3781e-07, 2.6123e-04, 7.9285e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.32532307505607605 tensor([1.1617e-05, 2.1242e-02, 3.2532e-01, 1.9608e-03, 6.5146e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 9.924123150995001e-05 tensor([9.9241e-05, 1.1561e-08, 6.9926e-12, 9.9983e-01, 7.0883e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.2779938280582428 tensor([0.0013, 0.2780, 0.1016, 0.0144, 0.6047], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [2, 0, 1], [0, 3, 4, 1], [0, 2, 1], [3, 0, 2, 4], [3, 4, 0, 2], [2, 0, 1], [0, 3, 4], [0, 1, 3, 2], [0, 1, 2, 3], [0, 3, 4, 1], [0, 3, 2], [0, 3, 2, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 3, 1], [2, 4, 1], [2, 1, 0], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 2, 4], [2, 0, 1], [0, 3, 2, 4], [2, 4, 1], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 4, 3], [3, 4, 0, 2], [0, 3, 2], [2, 1, 0, 4], [0, 2, 1], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [3, 2, 4, 0], [0, 1, 2, 3], [2, 0], [3, 4, 2, 0], [0, 1, 3, 2], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1], [0, 3, 1], [3, 0, 2, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4], [0, 1, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1], [2, 4, 1], [3, 4, 2, 0], [0, 3, 4], [2, 4, 1, 3], [2, 0, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 0, 1], [0, 3, 1], [2, 1, 4, 0], [2, 4, 3], [0, 2, 1, 3], [2, 1, 0, 4], [0, 3, 2], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 2], [2, 1, 4, 0], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 2, 1], [2, 4, 3], [2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 4, 1], [0, 2, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3], [0, 1, 3, 2], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [1, 0, 2], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 0], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 2, 0, 4], [2, 1, 0, 4], [0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [0, 3, 4, 2], [2, 1, 4, 0], [0, 3, 1], [0, 2, 1, 3], [0, 1, 2], [0, 1, 3, 2], [2, 4, 3], [0, 1, 2], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2], [2, 4, 3, 0], [0, 3, 1], [3, 0, 4, 2], [3, 4, 2, 0], [3, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 2, 3], [2, 4, 1], [0, 1, 3, 2], [2, 4, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1, 3], [0, 3, 4, 1], [3, 2, 4, 0], [0, 1, 3, 2], [0, 3, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1], [0, 3, 2], [3, 2, 0, 4], [2, 4, 1], [0, 1, 2, 3], [3, 0, 4], [0, 1, 2, 3], [0, 3, 1, 2], [2, 4, 3], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 1, 3], [1, 0, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1], [3, 4, 2, 0], [0, 3, 2, 1], [2, 1, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1], [2, 0, 1], [2, 4, 3], [0, 3, 1], [2, 1, 4, 0], [0, 3, 2]]\n",
      "[[0], [3, 4], [2], [3, 4], [1], [1], [3, 4], [1, 2], [4], [4], [2], [1, 4], [1], [4], [4], [0], [0], [2], [3], [3, 4], [3], [3], [1, 2, 4], [3], [4], [3], [0, 3], [2], [4], [3, 4], [0], [4], [2, 4], [0, 3], [3, 4], [0], [1], [1], [3, 4], [1], [0, 3], [2], [2], [4], [4], [0], [1], [2], [0, 3], [2], [0, 1], [1], [1, 4], [3], [3, 4], [0], [1], [2], [1], [4], [1, 3, 4], [1], [4], [3], [4], [0], [0, 1], [4], [2], [2, 4], [0], [0], [4], [3], [4], [1], [4], [2], [3], [2, 4], [2, 4], [1], [2], [3], [2], [0], [1, 2], [4], [3], [4], [0], [1], [2], [0], [2, 4], [0, 3], [1], [1, 2], [0], [3, 4], [0], [1], [2], [3, 4], [2, 4], [3], [0, 1], [4], [3], [1, 4], [0], [0, 1], [2], [3], [1, 4], [3], [1], [4], [3], [0, 3], [0], [1], [2], [4], [2], [0], [3, 4], [2], [3], [3, 4], [0, 1], [1, 3, 4], [2], [3], [4], [0], [1], [2, 4], [0, 3], [2], [3], [1], [2], [4], [4], [0], [0, 1], [4], [4], [4], [0], [0], [2], [3, 4], [4], [0], [1], [2], [3, 4], [4], [3], [0], [1], [3], [2, 4], [1], [1], [2], [4], [1], [3], [2, 4], [4], [3, 4], [4], [0, 1], [3, 4], [2], [3], [1, 4], [1], [2, 4], [1], [1], [1], [0], [1], [1, 4], [0, 3], [4], [0, 1], [2], [4], [2], [0, 1], [0], [2], [1], [4], [1, 4], [0, 3], [0], [2, 4], [1, 4], [1], [0, 3], [4], [1, 2], [4], [4], [0, 1], [4], [2], [2], [2, 4], [0, 1], [1, 4], [2], [3], [4], [0], [0], [4], [3, 4], [0, 3], [0], [2, 4], [1], [3], [1, 2, 4], [0], [4], [2, 4], [4], [2], [0, 3], [1], [4], [3, 4], [4], [0], [1], [2], [4], [2, 4], [3, 4], [0, 1], [2, 4], [3], [1, 4]]\n",
      "NL_pred of 3th iteration [[2, 4, 1, 3], [0, 3, 4, 1], [3, 0, 2, 4], [3, 4, 0, 2], [0, 1, 3, 2], [0, 1, 2, 3], [0, 3, 4, 1], [0, 3, 2, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 0, 4], [2, 1, 0, 4], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [0, 3, 1, 4], [0, 3, 1, 2], [2, 4, 3, 1], [0, 1, 2, 3], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 2, 4], [0, 3, 2, 4], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [0, 3, 1, 4], [3, 4, 0, 2], [2, 1, 0, 4], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [3, 2, 4, 0], [0, 1, 2, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [0, 1, 3, 2], [0, 3, 1, 4], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [3, 0, 2, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [3, 4, 2, 0], [2, 4, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 1, 4, 0], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 4, 1], [0, 2, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [0, 1, 3, 2], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 2, 0, 4], [2, 1, 0, 4], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [0, 3, 4, 2], [2, 1, 4, 0], [0, 2, 1, 3], [0, 1, 3, 2], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 0], [3, 0, 4, 2], [3, 4, 2, 0], [3, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 4, 1], [3, 2, 4, 0], [0, 1, 3, 2], [2, 4, 3, 1], [3, 2, 0, 4], [0, 1, 2, 3], [0, 1, 2, 3], [0, 3, 1, 2], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 1, 3], [2, 4, 3, 1], [2, 3, 4, 0], [2, 1, 4, 0], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 2], [0, 3, 1, 4], [3, 4, 2, 0], [0, 3, 2, 1], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [2, 1, 4, 0]]\n",
      "Start of Epoch\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.00685217188692641  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.00685197114944458  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.00685159639380444  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.006851071598886073  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.006850428964899874  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.006849695896280223  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.006848896371907201  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.006848053000439173  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.006847184964980202  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.006846311448634356  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.006845446838729683  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.0068446014119290756  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.006843785444895427  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.006842998937628735  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.006842241205018142  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.006841516357728805  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.006840820970206425  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.006840152987118425  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.006839509668021366  Accuracy on Support set:0.0\n",
      "torch.Size([174, 2048]) torch.Size([174])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.006838886217139233  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.3537960350513458 tensor([0.0060, 0.0224, 0.0011, 0.3538, 0.6167], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2955024540424347 tensor([4.5520e-05, 1.0214e-04, 1.1760e-04, 2.9550e-01, 7.0423e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2641217112541199 tensor([0.0504, 0.0692, 0.0009, 0.6154, 0.2641], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2985723912715912 tensor([4.3157e-04, 4.9507e-01, 2.9857e-01, 1.3803e-03, 2.0454e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.39589154720306396 tensor([0.0015, 0.3959, 0.1461, 0.0099, 0.4466], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.1718701720237732 tensor([4.0294e-05, 3.0069e-04, 6.2256e-04, 1.7187e-01, 8.2717e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.25126487016677856 tensor([0.0011, 0.4605, 0.2513, 0.0057, 0.2815], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.26670628786087036 tensor([2.6671e-01, 2.9483e-04, 6.2612e-09, 7.3276e-01, 2.4348e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.39621463418006897 tensor([0.0398, 0.0858, 0.0016, 0.4767, 0.3962], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.30879467725753784 tensor([1.0304e-05, 4.5977e-02, 6.4470e-01, 5.2208e-04, 3.0879e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.41956591606140137 tensor([4.1957e-01, 2.2508e-02, 5.8099e-06, 5.5026e-01, 7.6598e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3345206081867218 tensor([6.6293e-04, 2.1426e-04, 2.2218e-05, 6.6458e-01, 3.3452e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32217922806739807 tensor([0.0550, 0.0806, 0.0008, 0.5414, 0.3222], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.45133113861083984 tensor([4.5133e-01, 9.7159e-03, 1.2148e-06, 5.3379e-01, 5.1625e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3029017150402069 tensor([3.0290e-01, 3.7974e-06, 1.8331e-12, 6.9709e-01, 1.6046e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.336107462644577 tensor([3.3611e-01, 6.5644e-01, 9.7896e-05, 6.1396e-03, 1.2121e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.24350401759147644 tensor([0.0019, 0.2435, 0.0760, 0.0228, 0.6557], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24779300391674042 tensor([3.7830e-04, 1.5974e-03, 8.7699e-04, 2.4779e-01, 7.4935e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.29402726888656616 tensor([0.0544, 0.3108, 0.0041, 0.3366, 0.2940], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.31118446588516235 tensor([3.1118e-01, 6.7076e-01, 1.5485e-04, 1.4646e-02, 3.2498e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.28805848956108093 tensor([9.2874e-09, 1.7795e-04, 2.8806e-01, 1.2150e-04, 7.1164e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4355538487434387 tensor([4.7657e-09, 1.5580e-04, 4.3555e-01, 6.9626e-05, 5.6422e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.28441327810287476 tensor([2.5341e-05, 9.1189e-02, 6.2351e-01, 8.6442e-04, 2.8441e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.31057727336883545 tensor([9.1048e-06, 6.8843e-01, 3.1058e-01, 9.2859e-07, 9.7804e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19101060926914215 tensor([1.0150e-08, 4.9285e-04, 8.0846e-01, 3.5234e-05, 1.9101e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.33010604977607727 tensor([3.3011e-01, 1.4845e-03, 8.3109e-08, 6.6753e-01, 8.7821e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25184696912765503 tensor([1.8873e-05, 2.5185e-01, 7.2470e-01, 3.9042e-05, 2.3391e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3307456970214844 tensor([0.0445, 0.0810, 0.0017, 0.5420, 0.3307], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.45931392908096313 tensor([1.0618e-03, 1.2139e-03, 1.8521e-04, 5.3823e-01, 4.5931e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3470836579799652 tensor([7.3686e-08, 1.1909e-03, 6.5158e-01, 1.4642e-04, 3.4708e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.30448776483535767 tensor([3.0449e-01, 6.9510e-01, 8.9724e-06, 3.6466e-04, 4.3404e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.34591057896614075 tensor([0.0051, 0.5423, 0.0837, 0.0230, 0.3459], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.3017878532409668 tensor([3.0179e-01, 6.8795e-01, 1.2909e-04, 8.0285e-03, 2.1086e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.21290534734725952 tensor([0.0011, 0.2129, 0.1174, 0.0141, 0.6545], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.36594557762145996 tensor([3.6595e-01, 2.9136e-05, 6.3387e-11, 6.3401e-01, 1.0757e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2787225842475891 tensor([0.0827, 0.1980, 0.0018, 0.4389, 0.2787], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.3133472800254822 tensor([1.6743e-04, 4.2017e-04, 3.3065e-04, 3.1335e-01, 6.8573e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3272678852081299 tensor([6.7268e-01, 3.2727e-01, 2.0917e-07, 4.7856e-05, 1.2441e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2569451332092285 tensor([0.0646, 0.3744, 0.0049, 0.2569, 0.2993], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3371700346469879 tensor([1.3100e-07, 2.2269e-03, 6.6047e-01, 1.2927e-04, 3.3717e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.29040461778640747 tensor([7.0944e-01, 1.4282e-04, 2.2791e-10, 2.9040e-01, 1.1075e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4774628281593323 tensor([4.7746e-01, 5.2250e-01, 3.0458e-07, 3.2479e-05, 1.0510e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.35744819045066833 tensor([7.1707e-07, 6.2902e-07, 6.8655e-06, 3.5745e-01, 6.4254e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.32570016384124756 tensor([2.9640e-04, 1.2456e-04, 2.6042e-05, 6.7385e-01, 3.2570e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.2695687711238861 tensor([1.3247e-07, 7.8712e-04, 2.6957e-01, 4.2714e-04, 7.2922e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.27911752462387085 tensor([1.5266e-04, 1.2108e-01, 2.7912e-01, 6.6155e-03, 5.9303e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.2523338794708252 tensor([1.3983e-06, 2.2065e-06, 2.2972e-05, 2.5233e-01, 7.4764e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4133857190608978 tensor([4.1339e-01, 5.8660e-01, 3.4896e-07, 1.4409e-05, 7.9793e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.24323934316635132 tensor([4.5500e-06, 7.4521e-06, 2.4344e-05, 2.4324e-01, 7.5672e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3104954659938812 tensor([0.0017, 0.5503, 0.1313, 0.0063, 0.3105], grad_fn=<SoftmaxBackward0>)\n",
      "2 0.3444007337093353 tensor([3.0836e-05, 5.8547e-02, 3.4440e-01, 1.9104e-03, 5.9511e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3459472954273224 tensor([0.0137, 0.3459, 0.0277, 0.1205, 0.4922], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.40863272547721863 tensor([4.0863e-01, 9.4520e-04, 3.3509e-08, 5.8993e-01, 4.8810e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.46265289187431335 tensor([4.6265e-01, 5.3696e-01, 4.7130e-06, 3.5161e-04, 3.0375e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4778209328651428 tensor([4.7782e-01, 5.1973e-01, 1.9402e-05, 2.1689e-03, 2.5852e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2585357427597046 tensor([0.0088, 0.6646, 0.0410, 0.0269, 0.2585], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.28447380661964417 tensor([2.8447e-01, 2.0241e-02, 7.6258e-06, 6.7774e-01, 1.7541e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.20725062489509583 tensor([1.2916e-07, 2.6345e-03, 7.9002e-01, 9.6836e-05, 2.0725e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19100303947925568 tensor([0.0025, 0.1910, 0.0445, 0.0362, 0.7259], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.23982970416545868 tensor([2.3983e-01, 6.6527e-02, 9.0526e-05, 6.2870e-01, 6.4848e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.4073634147644043 tensor([2.2558e-05, 4.0736e-01, 5.8434e-01, 1.5052e-05, 8.2634e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.3900943100452423 tensor([6.0315e-01, 3.9009e-01, 1.9335e-05, 6.2331e-03, 5.0004e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3331938087940216 tensor([3.5869e-08, 7.4090e-04, 6.6596e-01, 1.0581e-04, 3.3319e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.4545096755027771 tensor([4.5451e-01, 5.4511e-01, 4.7603e-06, 3.4699e-04, 2.3877e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.38706618547439575 tensor([0.0035, 0.5417, 0.0524, 0.0153, 0.3871], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.48471927642822266 tensor([2.9207e-06, 1.7161e-06, 8.1417e-06, 4.8472e-01, 5.1527e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.37029829621315 tensor([3.7030e-01, 2.8426e-02, 1.3335e-05, 5.9064e-01, 1.0619e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.4545537531375885 tensor([5.6323e-06, 2.0412e-02, 4.5455e-01, 1.1553e-03, 5.2387e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.19685935974121094 tensor([2.2154e-04, 1.9686e-01, 3.2390e-01, 3.6293e-03, 4.7539e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.21314264833927155 tensor([9.6208e-08, 2.3245e-03, 7.8445e-01, 7.9821e-05, 2.1314e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.2393968254327774 tensor([2.3940e-01, 5.0417e-03, 1.1457e-06, 7.5103e-01, 4.5281e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.31401270627975464 tensor([1.7293e-05, 3.2622e-06, 2.4934e-06, 6.8596e-01, 3.1401e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.4070594608783722 tensor([1.2826e-07, 1.7846e-03, 5.9100e-01, 1.5392e-04, 4.0706e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.19706466794013977 tensor([0.0126, 0.1259, 0.0078, 0.1971, 0.6567], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.40602177381515503 tensor([5.9372e-01, 4.0602e-01, 1.2812e-06, 2.4769e-04, 9.2943e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2 0.36623167991638184 tensor([7.7476e-06, 1.7828e-02, 3.6623e-01, 1.4788e-03, 6.1445e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.25359007716178894 tensor([0.0009, 0.2536, 0.1236, 0.0114, 0.6105], grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [2, 0, 1], [0, 3, 4, 1], [0, 2, 1], [3, 0, 2, 4], [3, 4, 0, 2], [2, 0, 1], [0, 3, 4], [0, 1, 3, 2], [0, 1, 2, 3], [0, 3, 4, 1], [0, 3, 2], [0, 3, 2, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [2, 0, 1], [2, 4, 3, 1], [0, 1, 2, 3], [0, 3, 1], [2, 4, 1], [2, 1, 0], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 2, 4], [2, 0, 1], [0, 3, 2, 4], [2, 4, 1], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 1], [0, 3, 1, 4], [2, 4, 3], [3, 4, 0, 2], [0, 3, 2], [2, 1, 0, 4], [0, 2, 1], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [3, 2, 4, 0], [0, 1, 2, 3], [2, 0], [3, 4, 2, 0], [0, 1, 3, 2], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1], [0, 3, 1], [3, 0, 2, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4], [0, 1, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1], [3, 4, 2, 0], [0, 3, 4], [2, 4, 1, 3], [2, 0, 1], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 0, 1], [0, 3, 1], [2, 1, 4, 0], [2, 4, 3], [0, 2, 1, 3], [2, 1, 0, 4], [0, 3, 2], [2, 4, 1, 3], [2, 4, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 2], [2, 1, 4, 0], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 2, 1], [2, 4, 3], [2, 0], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 4, 1], [0, 2, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3], [0, 1, 3, 2], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [1, 0, 2], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 0], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 2, 0, 4], [2, 1, 0, 4], [0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [0, 3, 4, 2], [2, 1, 4, 0], [0, 3, 1], [0, 2, 1, 3], [0, 1, 2], [0, 1, 3, 2], [2, 4, 3], [0, 1, 2], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2], [2, 4, 3, 0], [0, 3, 1], [3, 0, 4, 2], [3, 4, 2, 0], [3, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 2, 3], [2, 4, 1], [0, 1, 3, 2], [2, 4, 3], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 3], [2, 4, 1, 3], [0, 3, 4, 1], [3, 2, 4, 0], [0, 1, 3, 2], [0, 3, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1], [0, 3, 2, 1], [3, 2, 0, 4], [2, 4, 1], [0, 1, 2, 3], [3, 0, 4], [0, 1, 2, 3], [0, 3, 1, 2], [2, 4, 3], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3], [0, 3, 2], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 1, 3], [1, 0, 2], [2, 4, 1], [2, 4, 3, 1], [0, 3, 1], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1], [3, 4, 2, 0], [0, 3, 2, 1], [2, 1, 0], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1], [2, 0, 1, 3], [2, 4, 3], [0, 3, 1], [2, 1, 4, 0], [0, 3, 2]]\n",
      "[[0], [3, 4], [2], [3, 4], [1], [1], [3, 4], [1, 2], [4], [4], [2], [1, 4], [1], [4], [4], [0], [0], [2], [3], [4], [3], [3], [1, 2, 4], [3], [4], [3], [0, 3], [2], [4], [3, 4], [0], [4], [2, 4], [0, 3], [3, 4], [0], [1], [1], [3, 4], [1], [0, 3], [2], [2], [4], [4], [0], [1], [2], [0, 3], [2], [0, 1], [1], [1, 4], [3], [3, 4], [0], [1], [2], [1], [4], [1, 3, 4], [1], [4], [3], [4], [0], [0, 1], [4], [2], [2, 4], [0], [0], [4], [3], [4], [1], [4], [2], [3], [2, 4], [2, 4], [1], [2], [3], [2], [0], [1, 2], [4], [3], [4], [0], [1], [2], [0], [2], [0, 3], [1], [1, 2], [0], [3, 4], [0], [1], [2], [3, 4], [2, 4], [3], [0, 1], [4], [3], [1, 4], [0], [0, 1], [2], [3], [1, 4], [3], [1], [4], [3], [0, 3], [0], [1], [2], [4], [2], [0], [3, 4], [2], [3], [3, 4], [0, 1], [1, 3, 4], [2], [3], [4], [0], [1], [2, 4], [0, 3], [2], [3], [1], [2], [4], [4], [0], [0, 1], [4], [4], [4], [0], [0], [2], [3, 4], [4], [0], [1], [2], [3, 4], [4], [3], [0], [1], [3], [2, 4], [1], [1], [2], [4], [1], [3], [2, 4], [4], [3, 4], [4], [0, 1], [3, 4], [2], [3], [1, 4], [1], [2, 4], [1], [1], [1], [0], [1], [1, 4], [0, 3], [4], [0, 1], [2], [4], [2], [0, 1], [0], [2], [1], [4], [1, 4], [0, 3], [0], [2, 4], [4], [1], [0, 3], [4], [1, 2], [4], [4], [0, 1], [4], [2], [2], [2, 4], [0, 1], [1, 4], [2], [3], [4], [0], [0], [4], [3, 4], [0, 3], [0], [2, 4], [1], [3], [2, 4], [0], [4], [2, 4], [4], [2], [0, 3], [1], [4], [3, 4], [4], [0], [1], [2], [4], [2, 4], [4], [0, 1], [2, 4], [3], [1, 4]]\n",
      "NL_pred of 4th iteration [[0, 1, 2, 3], [0, 3, 1, 4], [0, 3, 2, 1], [0, 3, 1], [2, 0, 1, 3]]\n",
      "Start of Epoch\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.2624232530593872  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.26077382564544677  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.25784828662872317  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.2541233777999878  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.2501142740249634  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.24624218940734863  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.24280214309692383  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.23986527919769288  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.23741321563720702  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.2354283809661865  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.23381905555725097  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.2325150489807129  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.23145692348480223  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.23061158657073974  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.22992141246795655  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.22934455871582032  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.22885594367980958  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.22843852043151855  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.2280855655670166  Accuracy on Support set:0.0\n",
      "torch.Size([5, 2048]) torch.Size([5])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.22778525352478027  Accuracy on Support set:0.0\n",
      "\n",
      "********************************************  Training with complimentatry labels\n",
      "3 0.019454611465334892 tensor([4.8342e-05, 4.0003e-03, 7.8190e-03, 1.9455e-02, 9.6868e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.015160703100264072 tensor([5.3385e-07, 2.6546e-05, 9.1725e-04, 1.5161e-02, 9.8390e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0763583779335022 tensor([8.1664e-04, 2.3662e-02, 1.2350e-02, 7.6358e-02, 8.8681e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.028136394917964935 tensor([1.5927e-06, 2.8136e-02, 8.7463e-01, 4.9024e-05, 9.7184e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.034924186766147614 tensor([7.2573e-06, 3.4924e-02, 6.7293e-01, 4.1313e-04, 2.9172e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03555506467819214 tensor([5.3600e-06, 3.5555e-02, 8.1442e-01, 2.2137e-04, 1.4980e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.05500190332531929 tensor([5.5002e-02, 5.5414e-04, 2.4779e-07, 9.4013e-01, 4.3116e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04042365774512291 tensor([4.5273e-04, 2.0790e-02, 1.5462e-02, 4.0424e-02, 9.2287e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.061757903546094894 tensor([3.3088e-08, 1.9799e-03, 9.3625e-01, 1.0585e-05, 6.1758e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.058803919702768326 tensor([0.0588, 0.0740, 0.0008, 0.6184, 0.2480], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06813929975032806 tensor([1.3468e-05, 9.2048e-05, 2.8601e-04, 6.8139e-02, 9.3147e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05235477164387703 tensor([7.7139e-04, 2.6063e-02, 1.0811e-02, 5.2355e-02, 9.1000e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09220695495605469 tensor([9.2207e-02, 2.7841e-02, 9.2972e-05, 7.5259e-01, 1.2727e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06944020837545395 tensor([6.9440e-02, 4.9466e-06, 3.9650e-11, 9.3053e-01, 2.0806e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.020349789410829544 tensor([0.0203, 0.9541, 0.0063, 0.0031, 0.0161], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.03187590464949608 tensor([1.5712e-05, 3.1876e-02, 4.0990e-01, 1.4410e-03, 5.5677e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.011727911420166492 tensor([3.8345e-06, 3.5715e-04, 5.9329e-03, 1.1728e-02, 9.8198e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04013312608003616 tensor([0.0009, 0.0972, 0.0462, 0.0401, 0.8156], grad_fn=<SoftmaxBackward0>)\n",
      "0 0.0274980328977108 tensor([0.0275, 0.9232, 0.0060, 0.0086, 0.0347], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3243565261363983 tensor([3.9530e-10, 5.9533e-05, 6.7557e-01, 1.0336e-05, 3.2436e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.2448127716779709 tensor([2.8421e-10, 5.6639e-05, 7.5512e-01, 7.0962e-06, 2.4481e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.06912263482809067 tensor([1.0156e-07, 4.2267e-03, 9.2663e-01, 2.2014e-05, 6.9123e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.08164717257022858 tensor([1.5800e-07, 8.1647e-02, 9.1736e-01, 1.4487e-07, 9.8793e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06577114760875702 tensor([6.5771e-02, 3.9675e-03, 5.7725e-06, 9.0969e-01, 2.0562e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.00790884904563427 tensor([5.2475e-08, 7.9088e-03, 9.8541e-01, 1.1155e-06, 6.6785e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05264831334352493 tensor([5.5023e-04, 2.1415e-02, 1.9298e-02, 5.2648e-02, 9.0609e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04125631973147392 tensor([1.8478e-05, 4.6018e-04, 1.9675e-03, 4.1256e-02, 9.5630e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09225106984376907 tensor([1.2965e-09, 1.7941e-04, 9.0756e-01, 6.5404e-06, 9.2251e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.029347727075219154 tensor([2.9348e-02, 9.6993e-01, 2.7514e-04, 1.6630e-04, 2.8177e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.07207006961107254 tensor([3.8416e-05, 7.2070e-02, 5.4633e-01, 1.5474e-03, 3.8002e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.018275877460837364 tensor([0.0183, 0.9424, 0.0076, 0.0041, 0.0276], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.02587805688381195 tensor([1.1396e-05, 2.5878e-02, 4.5459e-01, 1.0441e-03, 5.1848e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06516116112470627 tensor([6.5161e-02, 3.8734e-05, 2.1388e-09, 9.3458e-01, 2.1829e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.05450466275215149 tensor([0.0015, 0.0705, 0.0228, 0.0545, 0.8507], grad_fn=<SoftmaxBackward0>)\n",
      "3 0.017251189798116684 tensor([2.0353e-06, 1.0270e-04, 2.2266e-03, 1.7251e-02, 9.8042e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.13209807872772217 tensor([1.3210e-01, 8.6786e-01, 8.4963e-06, 2.7175e-05, 8.5744e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03976081684231758 tensor([0.0016, 0.1274, 0.0439, 0.0398, 0.7873], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.07917056977748871 tensor([1.1662e-09, 2.0701e-04, 9.2062e-01, 4.2386e-06, 7.9171e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.27449831366539 tensor([2.7450e-01, 3.7368e-04, 1.1473e-08, 7.2481e-01, 3.1906e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08481254428625107 tensor([8.4813e-02, 9.1516e-01, 6.7729e-06, 1.7334e-05, 5.4418e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.03457324579358101 tensor([3.3172e-08, 4.1793e-07, 5.7905e-05, 3.4573e-02, 9.6537e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.06760904937982559 tensor([5.6864e-06, 5.1792e-05, 3.4037e-04, 6.7609e-02, 9.3199e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.3032011389732361 tensor([3.4168e-09, 1.9099e-04, 6.9658e-01, 2.7620e-05, 3.0320e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.25408899784088135 tensor([8.2185e-07, 8.6165e-03, 7.3705e-01, 2.4354e-04, 2.5409e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.021635090932250023 tensor([5.8945e-08, 1.3581e-06, 1.7747e-04, 2.1635e-02, 9.7819e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06096509099006653 tensor([6.0965e-02, 9.3902e-01, 7.3870e-06, 6.1127e-06, 3.3355e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.016268720850348473 tensor([9.6260e-08, 2.6402e-06, 1.6895e-04, 1.6269e-02, 9.8356e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.05541018769145012 tensor([1.0132e-05, 5.5410e-02, 6.8170e-01, 3.5181e-04, 2.6253e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.191604882478714 tensor([1.3379e-07, 3.5458e-03, 8.0479e-01, 5.6210e-05, 1.9160e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.055810607969760895 tensor([9.9057e-05, 5.5811e-02, 2.2288e-01, 7.1093e-03, 7.1410e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.09878868609666824 tensor([9.8789e-02, 3.1087e-03, 2.8367e-06, 8.8456e-01, 1.3541e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.045558616518974304 tensor([4.5559e-02, 9.5380e-01, 2.1689e-04, 1.7175e-04, 2.5005e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04340711236000061 tensor([0.0434, 0.9518, 0.0010, 0.0011, 0.0026], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.13358861207962036 tensor([8.6817e-05, 1.3359e-01, 4.3062e-01, 2.3281e-03, 4.3338e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.030732160434126854 tensor([3.0732e-02, 4.0593e-02, 5.5593e-04, 5.7284e-01, 3.5528e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.0465414822101593 tensor([8.9283e-10, 1.9104e-04, 9.5326e-01, 2.9131e-06, 4.6541e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.010863960720598698 tensor([0.0109, 0.0713, 0.0043, 0.2216, 0.6920], grad_fn=<SoftmaxBackward0>)\n",
      "1 0.020001200959086418 tensor([1.1729e-07, 2.0001e-02, 9.7648e-01, 7.5168e-07, 3.5226e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.06732762604951859 tensor([0.0673, 0.9162, 0.0016, 0.0052, 0.0096], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.09222325682640076 tensor([8.4577e-10, 1.3919e-04, 9.0763e-01, 5.3771e-06, 9.2223e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04647911712527275 tensor([4.6479e-02, 9.5302e-01, 1.8395e-04, 1.5179e-04, 1.6631e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.09284592419862747 tensor([3.7749e-05, 9.2846e-02, 3.9698e-01, 1.3795e-03, 5.0875e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.04798382893204689 tensor([1.0777e-07, 1.0076e-06, 7.5386e-05, 4.7984e-02, 9.5194e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.04000965505838394 tensor([0.0400, 0.0769, 0.0017, 0.5624, 0.3190], grad_fn=<SoftmaxBackward0>)\n",
      "4 0.1344342976808548 tensor([3.4374e-08, 1.5078e-03, 8.6403e-01, 3.2588e-05, 1.3443e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.17977488040924072 tensor([4.4423e-07, 7.0125e-03, 8.1312e-01, 9.0562e-05, 1.7977e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.04534546658396721 tensor([8.0065e-10, 1.9734e-04, 9.5445e-01, 2.3913e-06, 4.5345e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.040771741420030594 tensor([4.0772e-02, 1.1901e-02, 6.9068e-05, 8.5905e-01, 8.8204e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3 0.0962233692407608 tensor([7.4664e-07, 2.2975e-06, 3.2612e-05, 9.6223e-02, 9.0374e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.11315768957138062 tensor([2.5246e-09, 2.8330e-04, 8.8655e-01, 7.7251e-06, 1.1316e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "0 0.08300837129354477 tensor([8.3008e-02, 9.1673e-01, 5.7194e-05, 1.3213e-04, 7.6652e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "4 0.19120745360851288 tensor([5.1901e-08, 1.4554e-03, 8.0729e-01, 5.0745e-05, 1.9121e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "1 0.026544028893113136 tensor([6.4976e-06, 2.6544e-02, 5.4980e-01, 5.9971e-04, 4.2305e-01],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[[2, 4, 1, 3], [2, 0, 1, 3], [0, 3, 4, 1], [0, 2, 1, 3], [3, 0, 2, 4], [3, 4, 0, 2], [2, 0, 1, 3], [0, 3, 4, 1], [0, 1, 3, 2], [0, 1, 2, 3], [0, 3, 4, 1], [0, 3, 2, 1], [0, 3, 2, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 0], [0, 3, 1, 4], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 3, 1], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 2, 4], [2, 0, 1, 3], [0, 3, 2, 4], [2, 4, 1, 0], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 1, 0], [0, 3, 1, 4], [2, 4, 3, 0], [3, 4, 0, 2], [0, 3, 2, 1], [2, 1, 0, 4], [0, 2, 1, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [3, 2, 4, 0], [0, 1, 2, 3], [2, 0, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1], [0, 3, 1, 4], [3, 0, 2, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 0, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 0], [0, 2, 1, 3], [2, 1, 0, 4], [0, 3, 2, 1], [2, 4, 1, 3], [2, 4, 3, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 2, 1], [2, 1, 4, 0], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1, 0], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 2, 1, 3], [2, 4, 3, 0], [2, 0, 3], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 4, 1], [0, 2, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3, 0], [0, 1, 3, 2], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [1, 0, 2, 3], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 0, 3], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 2, 0, 4], [2, 1, 0, 4], [0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [0, 3, 4, 2], [2, 1, 4, 0], [0, 3, 1], [0, 2, 1, 3], [0, 1, 2, 3], [0, 1, 3, 2], [2, 4, 3, 0], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2, 1], [2, 4, 3, 0], [0, 3, 1, 4], [3, 0, 4, 2], [3, 4, 2, 0], [3, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 2, 3, 1], [2, 4, 1, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 3, 0], [2, 4, 1, 3], [0, 3, 4, 1], [3, 2, 4, 0], [0, 1, 3, 2], [0, 3, 2, 1], [2, 4, 1, 0], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [3, 2, 0, 4], [2, 4, 1, 0], [0, 1, 2, 3], [3, 0, 4, 1], [0, 1, 2, 3], [0, 3, 1, 2], [2, 4, 3, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 1, 3], [1, 0, 2, 3], [2, 4, 1, 0], [2, 4, 3, 1], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1, 0], [3, 4, 2, 0], [0, 3, 2, 1], [2, 1, 0, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 3, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2, 1]]\n",
      "[[0], [4], [2], [4], [1], [1], [4], [2], [4], [4], [2], [4], [1], [4], [4], [0], [0], [2], [3], [4], [3], [3], [2, 4], [3], [4], [3], [3], [2], [4], [4], [0], [4], [2], [3], [4], [0], [1], [1], [4], [1], [3], [2], [2], [4], [4], [0], [1], [2], [3], [2], [1], [1], [4], [3], [4], [0], [1], [2], [1], [4], [1, 4], [1], [4], [3], [4], [0], [1], [4], [2], [2, 4], [0], [0], [4], [3], [4], [1], [4], [2], [3], [2, 4], [2], [1], [2], [3], [2], [0], [2], [4], [3], [4], [0], [1], [2], [0], [2], [3], [1], [2], [0], [4], [0], [1], [2], [4], [2], [3], [1], [4], [3], [4], [0], [1], [2], [3], [4], [3], [1], [4], [3], [3], [0], [1], [2], [4], [2], [0], [4], [2], [3], [4], [1], [1, 4], [2], [3], [4], [0], [1], [2], [0, 3], [2], [3], [1], [2], [4], [4], [0], [1], [4], [4], [4], [0], [0], [2], [4], [4], [0], [1], [2], [4], [4], [3], [0], [1], [3], [2, 4], [1], [1], [2], [4], [1], [3], [2, 4], [4], [4], [4], [1], [4], [2], [3], [4], [1], [2], [1], [1], [1], [0], [1], [4], [3], [4], [1], [2], [4], [2], [1], [0], [2], [1], [4], [4], [3], [0], [2], [4], [1], [3], [4], [2], [4], [4], [1], [4], [2], [2], [2], [1], [4], [2], [3], [4], [0], [0], [4], [4], [3], [0], [2], [1], [3], [2], [0], [4], [2], [4], [2], [3], [1], [4], [4], [4], [0], [1], [2], [4], [2], [4], [1], [2], [3], [4]]\n",
      "NL_pred of 5th iteration [[2, 0, 1, 3], [0, 2, 1, 3], [2, 0, 1, 3], [0, 3, 4, 1], [0, 3, 2, 1], [0, 3, 1], [2, 4, 1, 0], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [2, 0, 1, 3], [2, 4, 1, 0], [2, 4, 1, 0], [2, 4, 3, 0], [0, 3, 2, 1], [0, 2, 1, 3], [2, 0, 3], [2, 4, 3, 0], [0, 3, 1, 4], [3, 0, 4, 1], [2, 4, 1, 0], [0, 3, 4, 1], [2, 0, 1, 3], [2, 0, 1, 3], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 2, 1], [2, 4, 3, 0], [0, 3, 2, 1], [2, 4, 1, 0], [2, 0, 1, 3], [0, 2, 1, 3], [2, 4, 3, 0], [2, 0, 3], [0, 3, 1, 4], [2, 4, 3, 0], [1, 0, 2, 3], [2, 1, 0, 3], [0, 1, 2, 3], [2, 4, 3, 0], [0, 1, 2, 3], [0, 3, 2, 1], [0, 3, 1, 4], [0, 2, 3, 1], [2, 4, 1, 0], [2, 4, 3, 0], [2, 4, 3, 0], [0, 3, 2, 1], [2, 4, 1, 0], [0, 3, 1, 4], [2, 4, 1, 0], [3, 0, 4, 1], [2, 4, 3, 0], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 2, 1], [1, 0, 2, 3], [2, 4, 1, 0], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 1, 4], [0, 3, 2, 1]]\n",
      "Start of Epoch\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 0  Train_Loss: 0.01743248806280248  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 1  Train_Loss: 0.017420512788435993  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 2  Train_Loss: 0.01739834336673512  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 3  Train_Loss: 0.017367999343311086  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 4  Train_Loss: 0.017331584411508897  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 5  Train_Loss: 0.017291121623095346  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 6  Train_Loss: 0.017248449956669527  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 7  Train_Loss: 0.01720510861452888  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 8  Train_Loss: 0.017162475515814388  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 9  Train_Loss: 0.01712134655784158  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 10  Train_Loss: 0.017082435243269977  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 11  Train_Loss: 0.017046106212279376  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 12  Train_Loss: 0.017012552303426406  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 13  Train_Loss: 0.016981882207533893  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 14  Train_Loss: 0.01695395041914547  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 15  Train_Loss: 0.01692860442049363  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 16  Train_Loss: 0.01690580915002262  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 17  Train_Loss: 0.016885282362208646  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 18  Train_Loss: 0.016866778626161462  Accuracy on Support set:0.0\n",
      "torch.Size([68, 2048]) torch.Size([68])\n",
      "Train_Epoch_NL: 19  Train_Loss: 0.01685005601714639  Accuracy on Support set:0.0\n",
      "Start of training with pseudo labels\n",
      "\n",
      "Global NL pred list : [[2, 4, 1, 3], [2, 0, 1, 3], [0, 3, 4, 1], [0, 2, 1, 3], [3, 0, 2, 4], [3, 4, 0, 2], [2, 0, 1, 3], [0, 3, 4, 1], [0, 1, 3, 2], [0, 1, 2, 3], [0, 3, 4, 1], [0, 3, 2, 1], [0, 3, 2, 4], [0, 1, 3, 2], [0, 1, 2, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 1, 0, 4], [2, 1, 0, 4], [0, 3, 1], [2, 1, 0, 4], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 0], [0, 3, 1, 4], [0, 3, 1, 2], [2, 0, 1, 3], [2, 4, 3, 1], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 1, 0], [2, 1, 0, 3], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 2, 4], [2, 0, 1, 3], [0, 3, 2, 4], [2, 4, 1, 0], [3, 0, 4, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 2], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 4, 1, 0], [0, 3, 1, 4], [2, 4, 3, 0], [3, 4, 0, 2], [0, 3, 2, 1], [2, 1, 0, 4], [0, 2, 1, 3], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [3, 2, 4, 0], [0, 1, 2, 3], [2, 0, 3], [3, 4, 2, 0], [0, 1, 3, 2], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1], [2, 4, 3, 1], [2, 4, 3, 1], [0, 1, 3, 2], [2, 1, 4, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 1], [0, 3, 1, 4], [3, 0, 2, 4], [0, 3, 4, 1], [2, 1, 0, 4], [0, 3, 1, 4], [2, 4, 3, 1], [3, 0, 4, 1], [0, 1, 2, 3], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 2, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 3, 1, 4], [2, 4, 1, 0], [3, 4, 2, 0], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1, 3], [2, 4, 3, 1], [3, 0, 4, 2], [0, 3, 4, 1], [2, 0, 1, 3], [0, 3, 1, 4], [2, 1, 4, 0], [2, 4, 3, 0], [0, 2, 1, 3], [2, 1, 0, 4], [0, 3, 2, 1], [2, 4, 1, 3], [2, 4, 3, 0], [0, 3, 1, 4], [2, 1, 0, 4], [0, 3, 2, 1], [2, 1, 4, 0], [3, 0, 4, 2], [0, 1, 3, 2], [2, 1, 4, 0], [2, 4, 1, 0], [2, 4, 3, 1], [0, 3, 4, 2], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 4, 1], [2, 4, 1, 3], [2, 0, 1, 3], [0, 3, 1, 4], [2, 1, 0, 4], [0, 2, 1, 3], [2, 4, 3, 0], [2, 0, 3], [0, 3, 4, 1], [2, 1, 0, 4], [0, 1, 3, 2], [2, 4, 3, 1], [3, 4, 0, 2], [0, 3, 1, 4], [2, 4, 1], [0, 3, 1, 4], [2, 1, 4, 0], [3, 0, 4, 2], [0, 3, 4, 1], [0, 2, 3, 1], [0, 1, 3, 2], [2, 4, 3, 1], [2, 4, 3, 0], [0, 1, 3, 2], [0, 1, 2, 3], [0, 2, 1, 3], [2, 4, 3, 1], [2, 4, 1, 3], [0, 3, 1, 4], [1, 0, 2, 3], [0, 1, 3, 2], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [2, 1, 0, 3], [0, 2, 1, 3], [2, 1, 0, 4], [2, 4, 1, 3], [3, 2, 0, 4], [2, 1, 0, 4], [0, 3, 1], [2, 3, 4, 0], [3, 4, 2, 0], [0, 3, 4, 1], [0, 1, 3, 2], [0, 3, 4, 2], [2, 1, 4, 0], [0, 3, 1], [0, 2, 1, 3], [0, 1, 2, 3], [0, 1, 3, 2], [2, 4, 3, 0], [0, 1, 2, 3], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2, 1], [2, 4, 3, 0], [0, 3, 1, 4], [3, 0, 4, 2], [3, 4, 2, 0], [3, 0, 2, 4], [2, 4, 1, 3], [3, 4, 2, 0], [0, 2, 3, 1], [2, 4, 1, 0], [0, 1, 3, 2], [2, 4, 3, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 4, 3, 0], [2, 4, 1, 3], [0, 3, 4, 1], [3, 2, 4, 0], [0, 1, 3, 2], [0, 3, 2, 1], [2, 4, 1, 0], [2, 4, 3, 1], [0, 3, 1, 4], [0, 3, 2, 1], [3, 2, 0, 4], [2, 4, 1, 0], [0, 1, 2, 3], [3, 0, 4, 1], [0, 1, 2, 3], [0, 3, 1, 2], [2, 4, 3, 0], [0, 1, 3, 2], [0, 3, 1, 4], [0, 3, 1, 4], [0, 3, 1, 4], [2, 4, 3, 0], [0, 3, 2, 1], [0, 3, 1, 4], [2, 1, 0, 4], [0, 1, 2, 3], [2, 4, 1, 3], [2, 4, 1, 3], [0, 2, 1, 3], [1, 0, 2, 3], [2, 4, 1, 0], [2, 4, 3, 1], [0, 3, 1, 4], [2, 3, 4, 0], [2, 1, 4, 0], [0, 3, 1, 4], [2, 4, 1, 3], [0, 2, 3, 1], [0, 3, 1, 4], [0, 3, 1, 2], [0, 3, 1, 4], [2, 4, 1, 0], [3, 4, 2, 0], [0, 3, 2, 1], [2, 1, 0, 3], [0, 1, 2, 3], [2, 4, 1, 3], [3, 4, 2, 0], [0, 3, 1, 4], [0, 1, 2, 3], [0, 3, 1, 4], [2, 0, 1, 3], [2, 4, 3, 0], [0, 3, 1, 4], [2, 1, 4, 0], [0, 3, 2, 1]]\n",
      "POSITION :  [[0], [4], [2], [4], [1], [1], [4], [2], [4], [4], [2], [4], [1], [4], [4], [0], [0], [2], [3], [4], [3], [3], [2, 4], [3], [4], [3], [3], [2], [4], [4], [0], [4], [2], [3], [4], [0], [1], [1], [4], [1], [3], [2], [2], [4], [4], [0], [1], [2], [3], [2], [1], [1], [4], [3], [4], [0], [1], [2], [1], [4], [1, 4], [1], [4], [3], [4], [0], [1], [4], [2], [2, 4], [0], [0], [4], [3], [4], [1], [4], [2], [3], [2, 4], [2], [1], [2], [3], [2], [0], [2], [4], [3], [4], [0], [1], [2], [0], [2], [3], [1], [2], [0], [4], [0], [1], [2], [4], [2], [3], [1], [4], [3], [4], [0], [1], [2], [3], [4], [3], [1], [4], [3], [3], [0], [1], [2], [4], [2], [0], [4], [2], [3], [4], [1], [1, 4], [2], [3], [4], [0], [1], [2], [0, 3], [2], [3], [1], [2], [4], [4], [0], [1], [4], [4], [4], [0], [0], [2], [4], [4], [0], [1], [2], [4], [4], [3], [0], [1], [3], [2, 4], [1], [1], [2], [4], [1], [3], [2, 4], [4], [4], [4], [1], [4], [2], [3], [4], [1], [2], [1], [1], [1], [0], [1], [4], [3], [4], [1], [2], [4], [2], [1], [0], [2], [1], [4], [4], [3], [0], [2], [4], [1], [3], [4], [2], [4], [4], [1], [4], [2], [2], [2], [1], [4], [2], [3], [4], [0], [0], [4], [4], [3], [0], [2], [1], [3], [2], [0], [4], [2], [4], [2], [3], [1], [4], [4], [4], [0], [1], [2], [4], [2], [4], [1], [2], [3], [4]]\n",
      "[0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3\n",
      " 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0\n",
      " 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2\n",
      " 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4\n",
      " 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1\n",
      " 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4] (250,)\n",
      "Accuracy of Pseudo labels : 0.532\n",
      "tensor([0, 4, 2, 4, 1, 1, 4, 2, 4, 4, 2, 4, 1, 4, 4, 0, 0, 2, 3, 4, 3, 3, 3, 4,\n",
      "        3, 3, 2, 4, 4, 0, 4, 2, 3, 4, 0, 1, 1, 4, 1, 3, 2, 2, 4, 4, 0, 1, 2, 3,\n",
      "        2, 1, 1, 4, 3, 4, 0, 1, 2, 1, 4, 1, 4, 3, 4, 0, 1, 4, 2, 0, 0, 4, 3, 4,\n",
      "        1, 4, 2, 3, 2, 1, 2, 3, 2, 0, 2, 4, 3, 4, 0, 1, 2, 0, 2, 3, 1, 2, 0, 4,\n",
      "        0, 1, 2, 4, 2, 3, 1, 4, 3, 4, 0, 1, 2, 3, 4, 3, 1, 4, 3, 3, 0, 1, 2, 4,\n",
      "        2, 0, 4, 2, 3, 4, 1, 2, 3, 4, 0, 1, 2, 2, 3, 1, 2, 4, 4, 0, 1, 4, 4, 4,\n",
      "        0, 0, 2, 4, 4, 0, 1, 2, 4, 4, 3, 0, 1, 3, 1, 1, 2, 4, 1, 3, 4, 4, 4, 1,\n",
      "        4, 2, 3, 4, 1, 2, 1, 1, 1, 0, 1, 4, 3, 4, 1, 2, 4, 2, 1, 0, 2, 1, 4, 4,\n",
      "        3, 0, 2, 4, 1, 3, 4, 2, 4, 4, 1, 4, 2, 2, 2, 1, 4, 2, 3, 4, 0, 0, 4, 4,\n",
      "        3, 0, 2, 1, 3, 2, 0, 4, 2, 4, 2, 3, 1, 4, 4, 4, 0, 1, 2, 4, 2, 4, 1, 2,\n",
      "        3, 4])\n",
      "Start of final training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n",
      "C:\\Users\\kanad\\AppData\\Local\\Temp\\ipykernel_17364\\1539893006.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(targets[i]).long() # Changed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 89.25619834710744\n",
      "Epoch: 1  Loss: 88.84297520661157\n",
      "Epoch: 2  Loss: 93.80165289256198\n",
      "Epoch: 3  Loss: 97.93388429752066\n",
      "Epoch: 4  Loss: 98.7603305785124\n",
      "Epoch: 5  Loss: 99.17355371900827\n",
      "Epoch: 6  Loss: 99.58677685950413\n",
      "Epoch: 7  Loss: 99.17355371900827\n",
      "Epoch: 8  Loss: 99.17355371900827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [12:37<00:00, 53.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Loss: 99.58677685950413\n",
      "Start of testing\n",
      "Accuracy of testing on Query Set:  55.333333333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [12:38<00:00, 50.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Start the training process \n",
    "\n",
    "# in the below code I am not using query set which should be concated with unlabeled data.\n",
    "\n",
    "for data in tqdm(trainloader):\n",
    "\n",
    "        # create different sets of data from the train loader\n",
    "        data = data.cpu()\n",
    "        targets = torch.arange(args.num_ways).repeat(args.k_shot+args.query+args.unlabel).long()\n",
    "\n",
    "        #print(data,targets)\n",
    "        \n",
    "        support_data = data[:num_support]\n",
    "        query_data = data[num_support:num_support+num_query]\n",
    "        unlabel_data = data[num_support+num_query:]\n",
    "                    \n",
    "        support_inputs=normalize(get_features(model, support_data))  # get feature embeddings for \n",
    "        #support_inputs=np.array(support_inputs)\n",
    "        support_targets = targets[:num_support].cpu().numpy()\n",
    "\n",
    "        #print(support_inputs.shape,support_targets)\n",
    "       \n",
    "        query_inputs=normalize(get_features(model, query_data))\n",
    "        query_targets = targets[num_support:num_support+num_query].cpu().numpy()\n",
    "\n",
    "        #print(query_inputs.shape,query_targets.shape)\n",
    "      \n",
    "        unlabel_inputs=normalize(get_features(model, unlabel_data))\n",
    "        unlabel_targets = targets[num_support+num_query:].cpu().numpy()\n",
    "\n",
    "        # The classifier has already been decided as linear classifier with a single dense layer and the output dimension=5\n",
    "        \n",
    "        ori_index = [x for x in range(args.unlabel*args.num_ways)]  # Store the index position of 250 images\n",
    "        _POSITION = [[] for _ in range(args.unlabel*args.num_ways)] # Create a 2D list to store the list of 5 classes in passed along with the image batch.\n",
    "        POSITION = [[0, 1, 2, 3, 4] for _ in range(args.unlabel*args.num_ways)] # [0,1,2,3,4] was chosen for encoding the 5 classes\n",
    "        global_nl_pred=[[] for _ in range(args.unlabel*args.num_ways)] # Store the negative labels of each image after every iteration\n",
    "        temp_nl_pred=[[] for _ in range(args.unlabel*args.num_ways)]\n",
    "        \n",
    "        # Define the loss criterion and the SGD optimizer used here for initial training of model.\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "      \n",
    "        optimizer = torch.optim.SGD(Cls.parameters(), lr = 1e-3, momentum=0.9, weight_decay=5e-4)  # weight decay is for L2 regularization.\n",
    "\n",
    "        # Begin initial training\n",
    "\n",
    "        print('\\n****************  Initial training the model on Support set')\n",
    "        for epoch in range(50):\n",
    "              loss,_,acc=train_loop2(Cls, None, criterion, optimizer, support_inputs, support_targets)\n",
    "              print(f\"Train_Epoch: {epoch}  Train_Loss: {loss}  Accuracy on Support set:{acc}\")\n",
    "        \n",
    "        print(\"Testing after training on support set\")\n",
    "        print(\"Accuracy of testing on Query Set: \",test_loop(Cls, query_inputs,query_targets))\n",
    "        \n",
    "        # Start of code using complimentary labels\n",
    "        '''\n",
    "        Important : Stop loss propagation in the next step only. Not immediately when label is found\n",
    "        '''\n",
    "        i=0\n",
    "        while(True and i<6):\n",
    "            print('\\n********************************************  Training with complimentatry labels')\n",
    "            select_idx=[]\n",
    "            nl_pred=[]\n",
    "            unselect_idx=[]\n",
    "            unlabel_out = Cls(torch.tensor(unlabel_inputs).to(device))\n",
    "            #print(\"unlabel_out shape\", unlabel_out)\n",
    "            nl_pred, unselect_idx,_POSITION,POSITION = get_preds_position_(unlabel_out, POSITION, _POSITION, args.threshold)  # Changed\n",
    "            #print(unselect_idx)\n",
    "            print(_POSITION)\n",
    "            print(POSITION)\n",
    "            select_idx = [x for x in ori_index if x not in unselect_idx]\n",
    "            _unlabel_embeddings = unlabel_inputs[select_idx]\n",
    "            #print(_unlabel_embeddings)\n",
    "            negative_pred=[global_nl_pred[x] for x in ori_index if x in select_idx]# list containing all the negative labels predicted for that class\n",
    "            #nl_pred = [nl_pred[x] for x in ori_index if x in sel\n",
    "            # ect_idx]  # May not be required because nl_pred comes without unselect indexes\n",
    "            print(f\"NL_pred of {i}th iteration\",negative_pred)\n",
    "            if(len(nl_pred)==0  or len(select_idx)==0):   # \n",
    "                  break\n",
    "            optimizer_NL = torch.optim.SGD(Cls.parameters(), lr = 1e-3, momentum=0.9, weight_decay=5e-4) #, weight_decay=5e-4\n",
    "            print(\"Start of Epoch\")\n",
    "            for epoch in range(20):\n",
    "                loss,_,acc = train_loop(Cls, None, NL_loss, optimizer_NL, _unlabel_embeddings, nl_pred,negative_pred)\n",
    "                print(f\"Train_Epoch_NL: {epoch}  Train_Loss: {loss}  Accuracy on Support set:{acc}\")\n",
    "                #print(\"Classifier Parameters :\",Classifier.parameters())\n",
    "                \n",
    "\n",
    "            # Break condition no negative label found below threshold condition\n",
    "            i=i+1\n",
    "\n",
    "\n",
    "        print(\"Start of training with pseudo labels\\n\")    \n",
    "        print(\"Global NL pred list :\",global_nl_pred) # Printing NULL . Check\n",
    "\n",
    "        print(\"POSITION : \",POSITION)\n",
    "        acc=0\n",
    "        c=0\n",
    "        print(unlabel_targets,unlabel_targets.shape)\n",
    "        for i in unlabel_targets:\n",
    "              if i in POSITION[c]:\n",
    "                    acc+=1\n",
    "              c+=1\n",
    "        print(\"Accuracy of Pseudo labels :\",acc/c)\n",
    "\n",
    "        # to be corrected\n",
    "\n",
    "        \n",
    "        class_num = [0 for _ in range(5)]\n",
    "        pseudo_label = []\n",
    "        index_pl = []\n",
    "        for idx in range(len(POSITION)):\n",
    "            item = POSITION[idx]\n",
    "            if len(item) == 1:\n",
    "                lab = item[0]\n",
    "                pseudo_label.append(item[-1])\n",
    "                class_num[lab] += 1\n",
    "                index_pl.append(idx)\n",
    "        pseudo_label = np.asarray(pseudo_label)\n",
    "        t1_ = unlabel_inputs[index_pl]\n",
    "        t2_ = torch.tensor(pseudo_label, dtype=torch.int64)\n",
    "        print(t2_)\n",
    "        if(len(t2_)!=0):\n",
    "          print(\"Start of final training\")\n",
    "          for epoch in range(10):\n",
    "            loss,_,acc=train_loop2(Cls, None, criterion, optimizer, t1_, t2_)\n",
    "            print(f\"Epoch: {epoch}  Loss: {acc}\")    \n",
    "        \n",
    "        print(\"Start of testing\")\n",
    "        print(\"Accuracy of testing on Query Set: \",test_loop(Cls, query_inputs,query_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "793_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
